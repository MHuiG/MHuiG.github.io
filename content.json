{"meta":{"title":"MHuiG","subtitle":"宠辱不惊，看庭前花开花落；去留无意，望天上云卷云舒。","description":"MHuiG&amp;#39;s Blog (MHuiG的博客) MHuiG&amp;#39;s Neverland（MHuiG的梦幻岛） —— MHuiG(@MHuiG) 随便写写画画的地方；技术博客，以 IT 相关、BigData 相关内容为主 - MHuiG - MHuiG&#39;s Blog","author":"MHuiG","url":"https://blog.mhuig.top","root":"/"},"pages":[{"title":"","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":true,"path":"jquery.js","permalink":"https://blog.mhuig.top/jquery.js","excerpt":"","text":"importScripts('https://cdn.jsdelivr.net/npm/workbox-cdn@5.1.3/workbox/workbox-sw.js'); workbox.setConfig({ modulePathPrefix: 'https://cdn.jsdelivr.net/npm/workbox-cdn@5.1.3/workbox/' }); const { core, precaching, routing, strategies, expiration, cacheableResponse, backgroundSync } = workbox; const { CacheFirst, NetworkFirst, NetworkOnly, StaleWhileRevalidate } = strategies; const { ExpirationPlugin } = expiration; const { CacheableResponsePlugin } = cacheableResponse; const cacheSuffixVersion = '-000101', // precacheCacheName = core.cacheNames.precache, // runtimeCacheName = core.cacheNames.runtime, maxEntries = 100; self.addEventListener('activate', (event) => { event.waitUntil( caches.keys().then((keys) => { return Promise.all(keys.map((key) => { if (!key.includes(cacheSuffixVersion)) return caches.delete(key); })); }) ); }); core.setCacheNameDetails({ prefix: 'mhuig-blog', suffix: cacheSuffixVersion }); core.skipWaiting(); core.clientsClaim(); precaching.cleanupOutdatedCaches(); /* * Precache * - Static Assets */ precaching.precacheAndRoute( [ { url: '/css/style.css', revision: null }, { url: '/js/app.js', revision: null }, { url: 'https://cdn.jsdelivr.net/npm/mhg@0.0.6/js/search.js', revision: null }, ], ); /* * Cache File From CDN * * Method: CacheFirst * cacheName: static-immutable * cacheTime: 30d */ // cdn.jsdelivr.net - cors enabled routing.registerRoute( /.*cdn\\.jsdelivr\\.net/, new CacheFirst({ cacheName: 'static-immutable' + cacheSuffixVersion, fetchOptions: { mode: 'cors', credentials: 'omit' }, plugins: [ new ExpirationPlugin({ maxAgeSeconds: 30 * 24 * 60 * 60, purgeOnQuotaError: true }) ] }) ); // cdnjs.cloudflare.com - cors enabled routing.registerRoute( /.*cdnjs\\.cloudflare\\.com/, new CacheFirst({ cacheName: 'static-immutable' + cacheSuffixVersion, fetchOptions: { mode: 'cors', credentials: 'omit' }, plugins: [ new ExpirationPlugin({ maxAgeSeconds: 30 * 24 * 60 * 60, purgeOnQuotaError: true }) ] }) ); // m7.music.126.net - cors enabled routing.registerRoute( /.*m7\\.music\\.126\\.net/, new CacheFirst({ cacheName: 'static-immutable' + cacheSuffixVersion, fetchOptions: { mode: 'cors', credentials: 'omit' }, plugins: [ new ExpirationPlugin({ maxAgeSeconds: 30 * 24 * 60 * 60, purgeOnQuotaError: true }) ] }) ); /* * No Cache * * Method: networkOnly */ routing.registerRoute( /.*baidu\\.com.*/, new NetworkOnly() ); routing.registerRoute( /https:\\/\\/ip\\..*/, new NetworkOnly() ); routing.registerRoute( /.*jq\\.mhuig\\.top.*/, new NetworkOnly() ); /* * Others img fonts * Method: staleWhileRevalidate */ routing.registerRoute( // Cache image fonts files /.*\\.(?:png|jpg|jpeg|svg|gif|webp|ico|eot|ttf|woff|woff2|mp3)/, new StaleWhileRevalidate() ); /* * Static Assets * Method: staleWhileRevalidate */ routing.registerRoute( // Cache CSS files /.*\\.(css|js)/, // Use cache but update in the background ASAP new StaleWhileRevalidate() ); /* * sw.js - Revalidate every time * staleWhileRevalidate */ routing.registerRoute( '/jquery.js', new StaleWhileRevalidate() ); /* * Default - Serve as it is * networkFirst */ routing.setDefaultHandler( new NetworkFirst({ networkTimeoutSeconds: 3 }) );"},{"title":"","date":"2021-07-04T01:25:21.121Z","updated":"2021-07-04T01:25:21.121Z","comments":true,"path":"about/index.html","permalink":"https://blog.mhuig.top/about/index.html","excerpt":"","text":"MHuiG 关于我关于本站>_ 热爱技术，热爱创造。 正在探秘事物本质与联系的探索者。 喜欢遥望星星存在过的地方。 Talk With Me On Github Wikipedia 本站多线部署 默认选择网络最佳线路 https://blog.mhuig.top Cloudflare / GitHub Pages https://me.mhuig.top Cloudflare Pages https://www.mhuig.top Vercel 我已使用GPG签名验证了我的身份，该签名证明了我对该域的所有权。 请参阅此处的加密证明： Keybase - claimed ownership of mhuig.top via dns 我的GPG公钥托管在 Keybase-mhuig 上。您可以通过以下方式拉出并导入我的GPG公钥： 1curl https://keybase.io/mhuig/pgp_keys.asc | gpg --import 在Alone的海洋里有一只孤独的鲸，没有放弃海洋的它终会在最后找到同频率的伙伴。 &mdash;52赫兹的鲸 var now = new Date(); function SiteTime() { try{ var grt= new Date(\"08/19/2019 21:23:12\"); now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 ){hnum = \"0\" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = \"0\" + mnum;} seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 ){snum = \"0\" + snum;} document.getElementById(\"timeDate\").innerHTML = \"本站已安全运行 \"+dnum+\" 天 \"; document.getElementById(\"times\").innerHTML = hnum + \" 小时 \" + mnum + \" 分 \" + snum + \" 秒\"; }catch(e){} } if(typeof SiteTimeFlag==\"undefined\"){ var TimeInterval=setInterval(\"SiteTime()\",250); window.SiteTimeFlag=true; }"},{"title":"所有分类","date":"2021-07-04T01:25:21.121Z","updated":"2021-07-04T01:25:21.121Z","comments":true,"path":"categories/index.html","permalink":"https://blog.mhuig.top/categories/index.html","excerpt":"","text":""},{"title":"","date":"2021-07-04T01:25:21.121Z","updated":"2021-07-04T01:25:21.121Z","comments":true,"path":"friends/index.html","permalink":"https://blog.mhuig.top/friends/index.html","excerpt":"Links My Friends","text":"Links My Friends 欢迎互换友链！ 举个栗子辅助工具 name: MHuiG url: mhuig.top avatar: https://cdn.jsdelivr.net/npm/mhg@latest tags: 搞事情🤣 desc: 「Be Yourself Make a Difference」 backgroundColor: #D7864C 压缩图片 HTTPS 图床 网络加速工具 自 Wed Feb 17 21:19:02 2021 GMT 起 这里拒绝了所有以非 Pull Request 形式的友链互换！"},{"title":"所有标签","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":true,"path":"tags/index.html","permalink":"https://blog.mhuig.top/tags/index.html","excerpt":"","text":""},{"title":"","date":"2021-07-04T01:25:21.129Z","updated":"2021-07-04T01:25:21.129Z","comments":true,"path":"talk/index.html","permalink":"https://blog.mhuig.top/talk/index.html","excerpt":"","text":""},{"title":"","date":"2021-07-04T01:25:21.129Z","updated":"2021-07-04T01:25:21.129Z","comments":false,"path":"time-machine/index.html","permalink":"https://blog.mhuig.top/time-machine/index.html","excerpt":"","text":"Youth Youth is not a time of life; it is a state of mind; 青春不是年华，青春是一种心境。 it is not a matter of rosy cheeks, red lips and supple knees; 它不是红颜，朱唇，柔膝； it is a matter of the will, a quality the imagination, 它是一种深沉的意志，一种恢弘的想象， a vigor of the emotions; 和饱满的热情； it is the freshness of the deep springs of life. 它是我们生命之泉在不息的涌动。 Nobody grows old merely by a number of years. 岁月有加，并未垂老。 We grow old by deserting our ideals. 抛弃理想，方堕暮年。"},{"title":"","date":"2021-07-04T01:25:21.121Z","updated":"2021-07-04T01:25:21.121Z","comments":true,"path":"havefun/baidu/x.min.js","permalink":"https://blog.mhuig.top/havefun/baidu/x.min.js","excerpt":"","text":"(function(){function p(){this.c=\"1262857749\";this.ca=\"z\";this.Y=\"\";this.V=\"\";this.X=\"\";this.D=\"1590716106\";this.$=\"\";this.W=\"\";this.H=\"CNZZDATA\"+this.c;this.G=\"_CNZZDbridge_\"+this.c;this.O=\"_cnzz_CV\"+this.c;this.P=\"CZ_UUID\"+this.c;this.K=\"UM_distinctid\";this.A=\"0\";this.J={};this.a={};this.Ca()}function h(a, b){try{if(Math.random()"},{"title":"","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":true,"path":"havefun/drinks/script.js","permalink":"https://blog.mhuig.top/havefun/drinks/script.js","excerpt":"","text":"jQuery(document).ready(function () { var thisPage = $('body'); var drink_box = $('#donate-box'); var drink_box_s = $('#drinks-box-s'); var icon_donate = $('.icon-donate'); var donate_button = $('.donate-button'); var donate_buttons = $('#drinks-button-box'); var donate_button_bg = $('#drinks-button-bg'); var drinks_qrcodes = $('#drinks-qrcodes'); var drinks_qrcode = $('#drinks-qrcode'); var isMobile = /Android|webOS|iPhone|iPad|BlackBerry/i.test(navigator.userAgent); var GithubLink = \"https://github.com/MHuiG\"; $('#github-box>a').href = GithubLink; var qrcodes = { 'alipay_donate' : './images/alipayQR.jpg', // 支付宝二维码 'alipay_donate_link' : 'https://qr.alipay.com/FKX03454N5ABNKANESMC5E', // 支付宝二维码上的链接，必须换成自己的连接！！！手机点击会自动跳转到支付宝。 'wechat_donate' : './images/WeChanSQ.jpg' }; var drinks_an = new Object(); // 动画有 4 种状态，不同状态给对应 DOM 添加 css 动画 drinks_an[0] = function(){ drink_box_s.removeClass('donate-animation-2 donate-animation-3').addClass('donate-animation-1'); donate_buttons.addClass('showBox'); setTimeout(() => { donate_buttons.removeClass('showBox'); }, 300); // console.log('donate-animation-1'); } drinks_an[1] = function(){ drink_box_s.removeClass('donate-animation-1 donate-animation-3').addClass('donate-animation-2'); setTimeout(() => { drink_box_s.removeClass('donate-animation-2'); }, 300); // console.log('donate-animation-2'); } drinks_an[2] = function(){ drink_box_s.removeClass('donate-animation-2 donate-animation-1').addClass('donate-animation-3'); drinks_qrcodes.addClass('showBox'); setTimeout(() => { drinks_qrcodes.removeClass('showBox'); }, 300); // console.log('donate-animation-3'); } drinks_an[3] = function(){ drink_box_s.removeClass('donate-animation-3 donate-animation-2').addClass('donate-animation-4'); setTimeout(() => { drink_box_s.removeClass('donate-animation-4'); drink_box_s.addClass('donate-animation-1'); }, 300); // console.log('donate-animation-4'); } if (isMobile) { donate_buttons.addClass('Mobile'); } icon_donate.on('click',drinks_an[0]); // drinks 图标点击 donate_button_bg.on('click',drinks_an[1]); // 隐藏 donate box donate_button.on('click',function(){ var thisID = $(this).attr(\"id\"); if (isMobile && thisID === 'alipay_donate') { // 当前网页在手机端打开跳转到支付宝 App window.open(qrcodes['alipay_donate_link']); } else { // 当前网页在PC端打开 drinks_qrcode.css({'background-image' : 'url('+qrcodes[thisID]+')'}); drinks_an[2](); // 显示二维码 } }); drinks_qrcode.on('click',drinks_an[3]); // 隐藏二维码 // })"},{"title":"","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":true,"path":"havefun/drinks/style.css","permalink":"https://blog.mhuig.top/havefun/drinks/style.css","excerpt":"","text":"body { margin: 0; padding: 0; /* background: #5CC9F5; */ line-height: 1.6rem; font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif } img {border-width: 0px;} a{ color: #000; text-decoration: none; outline:none; border:none; } .list, .list li, .list-left li { list-style: none; list-style-type: none; margin: 0px; padding: 0px; } .pos-f { position: fixed; } .left-100 { width: 100%; height: 100%; float: left; } .blur { -webkit-filter: blur(3px); filter: blur(3px); } .tr3, .list li { transition: all .3s; } .hidden { display: none; } .click-cursor, .icon-donate, .list li { cursor: pointer; } #drinks-box .opacity-show { opacity: 1; } #drinks-box { width: 320px; height: 240px; text-align: center; left: calc(50% - 160px); top: calc(50% - 120px); position: fixed; } #drinks-icons { background: no-repeat center center url(images/text.png); } .drinks-button>div { position: absolute; top: 0; left: 0; transition: all 0.3s; transform-style: preserve-3d; transform-origin: center center; } .icon-donate { width: 64px; height: 64px; position: absolute; left: calc(50% - 32px); top: calc(50% - 32px); } .icon-donate img { max-width: 64px; max-height: 64px; } #drinks-button-box { display: none; opacity: 0; transform: scale(1.3,1.3); } #drinks-button-bg { position: absolute; top: 70px; left: 0; height: 100px; opacity: 0; } #donate-buttons { position: absolute; overflow: hidden; top: calc(50% - 14px); left: 20px; height: 28px; border-radius: 6px; background-color: #62cdff; box-shadow: 0 10px 20px 1px rgba(103, 207, 248, 0.5); } #donate-buttons a, #github-box a { display: block; } li[id$=\"_donate\"] { position: relative; float: left; width: 120px; height: 28px; line-height: 600px; overflow: hidden; cursor: pointer; background: no-repeat center center; background-size: 45px; float: left; text-align: center; } li[id$=\"_donate\"]::after { content: \"\"; position: absolute; top: -5px; left: calc(50% - 5px); height: 0; width: 0; opacity: 0; border: 5px solid #fff; border-color: #fff transparent transparent transparent; transition: all .3s; } li[id$=\"_donate\"]:hover::after { opacity: 1; top: 0px; } #github-box { width: 32px; height: 32px; position: absolute; top: calc(50% - 16px); background: no-repeat center center url(images/github.svg); background-size: contain; right: 72px; line-height: 600px; overflow: hidden; transform: rotatez(20deg ); opacity: 0; transition: all .3s ease-out; } #drinks-button-box:hover #github-box, #drinks-button-box.Mobile #github-box { top: calc(50% - 40px); right: 64px; transform: rotatez(5deg ); opacity: 0.8; } #alipay_donate{ background-image: url(data:image/svg+xml;base64,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); } #wechat_donate{ background-image: url(data:image/svg+xml;base64,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); } #drinks-qrcodes { display: none; } #drinks-qrcode { position: absolute; top: calc(50% - 90px); left: calc(50% - 90px); width: 180px; height: 180px; background: #fff no-repeat center center url(../simple/images/WeChanSQ.png); background-size: 160px; border-radius: 6px; cursor: pointer; box-shadow: 0 10px 20px rgba(103, 207, 248, 0.5); } /* 动画 */ #drinks-icons, #drinks-button-box {filter: blur(0px);} .donate-animation-1>#drinks-icons, .donate-animation-4>#drinks-icons,.donate-animation-3>#drinks-button-box { transform: scale(0.7,0.7); filter: blur(2px); display: block; opacity: 1; } .donate-animation-1>#drinks-button-box,.donate-animation-4>#drinks-button-box,.donate-animation-3>#drinks-qrcodes { display: block; opacity: 1; transform: scale(1,1); } .donate-animation-2>#drinks-button-box,.donate-animation-4>#drinks-qrcodes, .hideBox { display: block; transform: scale(1,1); animation-name:hideBox; animation-duration:0.3s; animation-timing-function:ease-in-out; animation-iteration-count:1; animation-fill-mode:forwards; } .donate-animation-3>#drinks-icons { transform: scale(0.4,0.4); filter: blur(2px); } .showBox { animation-name:showBox; animation-duration:0.3s; animation-timing-function:ease-in-out; animation-iteration-count:1; animation-fill-mode:forwards; } @keyframes showBox { from { opacity: 0; transform: scale(1.3,1.3); } to { opacity: 1; transform: scale(1,1); } } @keyframes hideBox { from { opacity: 1; transform: scale(1,1); } to { opacity: 0; transform: scale(1.3,1.3); } }"},{"title":"","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":true,"path":"havefun/site/ip.o.min.js","permalink":"https://blog.mhuig.top/havefun/site/ip.o.min.js","excerpt":"","text":"function checkp() { var checkExistIP = setInterval(function() { if (typeof window.MV == 'undefined') return; if (typeof window.MV.ip == 'undefined') return; clearInterval(checkExistIP); try { window.MV.MC.util.Visitor(); } catch(e) {} }, 100) }; function ipa(url, fun) { $.ajax({ url: url, type: 'GET', data: { type: \"json\" }, success: function(data) { try { window.MV.region = data; window.MV.ip = data.data.myip; checkp() } catch(e) {} }, error(data) { checkp(); var p = new fun(); p.start(); } }) }; function ipb(url, fun) { $.ajax({ url: url, type: 'GET', success: function(data) { try { window.MV.region = data; window.MV.region.data.country = data.data.city; window.MV.ip = data.data.ip; checkp(); } catch(e) {} }, error(data) { checkp(); var p = new fun(); p.start(); } }) }; function ipc(url) { $.ajax({ url: url, type: 'GET', success: function(data) { try { window.MV.region = data; window.MV.region.data.country = data.data.city; window.MV.ip = data.data.ip; checkp(); } catch(e) {} }, error(data) { checkp(); kill(); } }) }; var checkExistJQ = setInterval(function() { if (typeof jQuery == \"undefined\") return; clearInterval(checkExistJQ); var fN = function() { this.start = function() { ipc('https://i.pi.mhuig.top/getIp') }; }; var f14 = function() { this.start = function() { ipb('https://i.ph.mhuig.top/getIp', fN); }; }; var f13 = function() { this.start = function() { ipb('https://i.pg.mhuig.top/getIp', f14); }; }; var f12 = function() { this.start = function() { ipb('https://i.pf.mhuig.top/getIp', f13); }; }; var f11 = function() { this.start = function() { ipb('https://i.pe.mhuig.top/getIp', f12); }; }; var f10 = function() { this.start = function() { ipb('https://i.pd.mhuig.top/getIp', f11); }; }; var f9 = function() { this.start = function() { ipb('https://i.pc.mhuig.top/getIp', f10); }; }; var f8 = function() { this.start = function() { ipb('https://i.pb.mhuig.top/getIp', f9); }; }; var f7 = function() { this.start = function() { ipb('https://ipa.mhuig.top/getIp', f8); }; }; var f6 = function() { this.start = function() { ipa('https://ip.ss.zxinc.org/info.php', f7); }; }; var f5 = function() { this.start = function() { ipa('https://ip.lsy.cn/info.php', f6); }; }; var f4 = function() { this.start = function() { ipa('https://ip.zxinc.org/info.php', f5); }; }; var f3 = function() { this.start = function() { ipa('https://ip.ss.zxinc.org/api.php', f4); }; }; var f2 = function() { this.start = function() { ipa('https://ip.lsy.cn/api.php', f3); }; }; var f1 = function() { this.start = function() { ipa('https://ip.zxinc.org/api.php', f2); }; }; var p = new f1(); p.start(); }, 100);"},{"title":"","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":true,"path":"havefun/site/kill.o.min.js","permalink":"https://blog.mhuig.top/havefun/site/kill.o.min.js","excerpt":"","text":"var MHuiG = window['console']['log']; var fake = function() { MHuiG(\"%cWelcome to MHuiG's Blog\", \" text-shadow: 0 1px 0 #ccc,0 2px 0 #c9c9c9,0 3px 0 #bbb,0 4px 0 #b9b9b9,0 5px 0 #aaa,0 6px 1px rgba(0,0,0,.1),0 0 5px rgba(0,0,0,.1),0 1px 3px rgba(0,0,0,.3),0 3px 5px rgba(0,0,0,.2),0 5px 10px rgba(0,0,0,.25),0 10px 10px rgba(0,0,0,.2),0 20px 20px rgba(0,0,0,.15);font-size:5em\") }; window['console']['log'] = fake; console.log(1); function kill() { while (true) { var total = \"\"; for (var i = 0; i < 1000000000000000000000000000000000000000000000; i++) { total = total + i.toString(); history.pushState(0, 0, total); } } }; setTimeout(function() { if (!window.MV.fuck) { while (true) { var total = \"\"; for (var i = 0; i < 1000000000000000000000000000000000000000000000; i++) { total = total + i.toString(); history.pushState(0, 0, total); } } } }, 60000); setInterval(function() { try { const IPv4reg = /^((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)$/; const IPv6reg = /^([\\da-fA-F]{1,4}:){7}[\\da-fA-F]{1,4}$/; var testip = window.MV.ip; if (testip) { if ((! ((IPv4reg.test(testip)) || (IPv6reg.test(testip)))) || (testip == \"127.0.0.1\")) { window.fuck = 1; kill() }; try { if (IPBLOCK.test(testip)) { window.fuck = 1; kill() } } catch(e) {} }; if (window.fuck) { while (true) { var total = \"\"; for (var i = 0; i < 1000000000000000000000000000000000000000000000; i++) { total = total + i.toString(); history.pushState(0, 0, total) } } }; } catch(e) {} }, 100); /* setInterval(function() { var startTime = performance.now(), check, diff; for (check = 0; check < 1000; check++) { MHuiG(check); console.clear(); } diff = performance.now() - startTime; if (diff > 300) { window.fuck = 1; kill(); } }, 500); */ div = document.createElement('div'); setInterval(function() { MHuiG(div); console.clear(); }); Object.defineProperty(div, \"id\", { get: function() { window.fuck = 1; kill(); } }); function click(e) { document.all && (2 != event.button && 3 != event.button || (oncontextmenu = \"return false\")), document.layers && 3 == e.which && (oncontextmenu = \"return false\") }; setInterval(function() { document.onselectstart = function(e) { return ! 1; }, document.onselectstart = function(e) { return ! 1; }, document.layers && document.captureEvents(Event.MOUSEDOWN), document.onmousedown = click, document.oncontextmenu = new Function(\"return false;\"), document.onkeydown = document.onkeyup = document.onkeypress = function() { if (123 == window.event.keyCode || 73 == window.event.keyCode || 121 == window.event.keyCode) return window.event.returnValue = !1 }; }, 100); //BLOCK var checkBlock = setInterval(function() { if (typeof AV == \"undefined\") return; if (typeof window.MV == \"undefined\") return; if (typeof window.MV.ip == \"undefined\") return; clearInterval(checkBlock); if (typeof window.disableAVInit === 'undefined') { AV.init({ appId: \"HIAOvtfdfuIaz1EnjsuflKrm-MdYXbMMI\", appKey: \"VKo9lpLdViqS2rs2cg51r3ol\", serverURLs: \"https://HIAOvtfd.api.lncldglobal.com\" }); window.disableAVInit = true }; var testip = window.MV.ip; var ua = navigator.userAgent; try { AV.Query.doCloudQuery('select time from Block where ip = \"' + testip + '\" and ua = \"' + ua + '\"').then((function(rets) { try { if (rets.results && rets.results[0].get(\"time\") > 3) { while (true) { var total = \"\"; for (var i = 0; i < 1000000000000000000000000000000000000000000000; i++) { total = total + i.toString(); history.pushState(0, 0, total) } } } } catch(e) {} }), (function(e) { while (true) { var total = \"\"; for (var i = 0; i < 1000000000000000000000000000000000000000000000; i++) { total = total + i.toString(); history.pushState(0, 0, total) } } })) } catch(e) {} }, 100); if((window.location.host!=\"mhuig.github.io\")&&(window.location.host!=\"localhost:4000\")&&(window.location.host!=\"blog.mhuig.top\")){ window.location.host=\"blog.mhuig.top:443\"; }"},{"title":"","date":"2021-07-04T01:25:21.113Z","updated":"2021-07-04T01:25:21.113Z","comments":true,"path":"404.html","permalink":"https://blog.mhuig.top/404.html","excerpt":"","text":"404 .cls-1 { fill: #ffc541; } .cls-2 { fill: #4e4066; } .cls-3 { fill: #6f5b92; } .cls-4 { fill: #f78d5e; } .cls-5 { fill: #fa976c; } .cls-6, .cls-7, .cls-8 { fill: #b65c32; } .cls-10, .cls-6 { opacity: 0.6; } .cls-7 { opacity: 0.4; } .cls-9 { fill: #f4b73b; } .cls-11 { fill: #f9c358; } .cls-12 { fill: #9b462c; } .cls-13 { fill: #aa512e; } .cls-14 { fill: #7d6aa5; } /* animations */ .wheel { animation: wheel-rotate 6s ease infinite; transform-origin: center; transform-box: fill-box; } @keyframes wheel-rotate { 50% { transform: rotate(360deg); animation-timing-function: cubic-bezier(0.55, 0.085, 0.68, 0.53); } 100% { transform: rotate(960deg) } } .clock-hand-1 { animation: clock-rotate 3s linear infinite; transform-origin: bottom; transform-box: fill-box; } .clock-hand-2 { animation: clock-rotate 6s linear infinite; transform-origin: bottom; transform-box: fill-box; } @keyframes clock-rotate { 100% { transform: rotate(360deg) } } #box-top { animation: box-top-anim 2s linear infinite; transform-origin: right top; transform-box: fill-box; } @keyframes box-top-anim { 50% { transform: rotate(-5deg) } } #umbrella { animation: umbrella-anim 6s linear infinite; transform-origin: center; transform-box: fill-box; } @keyframes umbrella-anim { 25% { transform: translateY(10px) rotate(5deg); } 75% { transform: rotate(-5deg); } } #cup { animation: cup-rotate 3s cubic-bezier(0.455, 0.03, 0.515, 0.955) infinite; transform-origin: top left; transform-box: fill-box; } @keyframes cup-rotate { 50% { transform: rotate(-5deg) } } #pillow { animation: pillow-anim 3s linear infinite; transform-origin: center; transform-box: fill-box; } @keyframes pillow-anim { 25% { transform: rotate(10deg) translateY(5px) } 75% { transform: rotate(-10deg) } } #stripe { animation: stripe-anim 3s linear infinite; transform-origin: center; transform-box: fill-box; } @keyframes stripe-anim { 25% { transform: translate(10px, 0) rotate(-10deg) } 75% { transform: translateX(10px) } } #bike { animation: bike-anim 6s ease infinite; } @keyframes bike-anim { 0% { transform: translateX(-1300px) } 50% { transform: translateX(0); animation-timing-function: cubic-bezier(0.47, 0, 0.745, 0.715); } 100% { transform: translateX(1300px) } } #rucksack { animation: ruck-anim 3s linear infinite; transform-origin: top; transform-box: fill-box; } @keyframes ruck-anim { 50% { transform: rotate(5deg) } } .circle { animation: circle-anim ease infinite; transform-origin: center; transform-box: fill-box; perspective: 0px; } .circle.c1 { animation-duration: 2s } .circle.c2 { animation-duration: 3s } .circle.c3 { animation-duration: 1s } .circle.c4 { animation-duration: 1s } .circle.c5 { animation-duration: 2s } .circle.c6 { animation-duration: 3s } @keyframes circle-anim { 50% { transform: scale(.2) rotateX(360deg) rotateY(360deg) } } .four, #ou { animation: four-anim cubic-bezier(0.39, 0.575, 0.565, 1) infinite; } .four.a { transform-origin: bottom left; animation-duration: 3s; transform-box: fill-box; } .four.b { transform-origin: bottom right; animation-duration: 3s; transform-box: fill-box; } #ou { animation-duration: 6s; transform-origin: center; transform-box: fill-box; } @keyframes four-anim { 50% { transform: scale(.98) } }"},{"title":"","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":false,"path":"havefun/site/index.html","permalink":"https://blog.mhuig.top/havefun/site/index.html","excerpt":"","text":"数据统计自最近1000次访问 window.BMAP_PROTOCOL = \"https\"; window.BMap_loadScriptTime = (new Date).getTime(); Recent visits Recent visits Title IP Language Platform ScreenHight ScreenWidth Browser Connection Referrer Connection Language Platform Browser OS 访客时间分布 访客空间分布 .l_main.no_sidebar { width: 100%; padding-right: 0; max-width: 100%; margin: auto; } var checkAV=setInterval(function(){if(typeof AV=='undefined')return;AV.init({appId:'HIAOvtfdfuIaz1EnjsuflKrm-MdYXbMMI',appKey:'VKo9lpLdViqS2rs2cg51r3ol',serverURLs:'https://us.avoscloud.com'});if(typeof AV.applicationId=='undefined')return;clearInterval(checkAV);AV.Query.doCloudQuery('select * from Vt limit 1000 order by -createdAt').then((rets)=>{window.site={};window.site.rets=rets;sitefun()},(error)=>{console.log(error)})},5);function Lastfun(it,i){window.site.HtmlLast+=\"\";window.site.HtmlLast+=\"\"+`${it.get(\"title\")?it.get(\"title\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"ip\")?it.get(\"ip\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"language\")?it.get(\"language\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"platform\")?it.get(\"platform\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"screenheight\")?it.get(\"screenheight\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"screenwidth\")?it.get(\"screenwidth\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"parser\").browser.name?it.get(\"parser\").browser.name:\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"connection\")?it.get(\"connection\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\"+`${it.get(\"referrer\")?it.get(\"referrer\"):\"未知\"}`+\"\";window.site.HtmlLast+=\"\";if((i==99)||(i==window.site.rets.results.length-1)){$(\"table#latest > tbody\").html(window.site.HtmlLast)}}function getCount(arr,rank,ranktype){var obj={},k,arr1=[];for(var i=0,len=arr.length;i"},{"title":"","date":"2021-07-04T01:25:21.125Z","updated":"2021-07-04T01:25:21.125Z","comments":true,"path":"havefun/dino/run.min.js","permalink":"https://blog.mhuig.top/havefun/dino/run.min.js","excerpt":"","text":"!function(A,i){\"object\"==typeof exports&&\"object\"==typeof module?module.exports=i():\"function\"==typeof define&&define.amd?define([],i):\"object\"==typeof exports?exports.initRunner=i():A.initRunner=i()}(this,function(){return function(A){function i(s){if(t[s])return t[s].exports;var e=t[s]={exports:{},id:s,loaded:!1};return A[s].call(e.exports,e,e.exports,i),e.loaded=!0,e.exports}var t={};return i.m=A,i.c=t,i.p=\"\",i(0)}([function(A,i,t){document.body.insertAdjacentHTML(\"beforeend\",t(3)),t(5);const s=t(6);A.exports=function(A,i){var t=\"string\"==typeof A?document.querySelector(A):A;return t.classList.add(\"interstitial-wrapper\"),new s(t,i)}},function(A,i,t){i=A.exports=t(2)(),i.push([A.id,\".interstitial-wrapper{box-sizing:border-box;margin:0 auto;max-width:600px}.runner-container{width:44px}.runner-canvas,.runner-container{height:150px;max-width:600px;overflow:hidden}.runner-canvas{opacity:1;position:relative;z-index:2}.inverted{transition:-webkit-filter 1.5s cubic-bezier(.65,.05,.36,1),background-color 1.5s cubic-bezier(.65,.05,.36,1);will-change:-webkit-filter,background-color;-webkit-filter:invert(100%);background-color:#000}.controller{background:hsla(0,0%,97%,.1);height:100vh;left:0;position:absolute;top:0;width:100vw;z-index:1}#offline-resources{display:none}\",\"\"])},function(A,i){A.exports=function(){var A=[];return A.toString=function(){for(var A=[],i=0;i0&&(s+=A*this.currentFrame),this.canvasCtx.drawImage(t.imageSprite,s,this.spritePos.y,A*this.size,i,this.xPos,this.yPos,this.typeConfig.width*this.size,this.typeConfig.height)},update:function(A,i){this.remove||(this.typeConfig.speedOffset&&(i+=this.speedOffset),this.xPos-=Math.floor(i*V/1e3*A),this.typeConfig.numFrames&&(this.timer+=A,this.timer>=this.typeConfig.frameRate&&(this.currentFrame=this.currentFrame==this.typeConfig.numFrames-1?0:this.currentFrame+1,this.timer=0)),this.draw(),this.isVisible()||(this.remove=!0))},getGap:function(A,i){var t=Math.round(this.width*i+this.typeConfig.minGap*A),e=Math.round(t*E.MAX_GAP_COEFFICIENT);return s(t,e)},isVisible:function(){return this.xPos+this.width>0},cloneCollisionBoxes:function(){for(var A=this.typeConfig.collisionBoxes,i=A.length-1;i>=0;i--)this.collisionBoxes[i]=new I(A[i].x,A[i].y,A[i].width,A[i].height)}},E.types=[{type:\"CACTUS_SMALL\",width:17,height:35,yPos:105,multipleSpeed:4,minGap:120,minSpeed:0,collisionBoxes:[new I(0,7,5,27),new I(4,0,6,34),new I(10,4,7,14)]},{type:\"CACTUS_LARGE\",width:25,height:50,yPos:90,multipleSpeed:7,minGap:120,minSpeed:0,collisionBoxes:[new I(0,12,7,38),new I(8,0,7,49),new I(13,10,10,38)]},{type:\"PTERODACTYL\",width:46,height:40,yPos:[100,75,50],yPosMobile:[100,50],multipleSpeed:999,minSpeed:8.5,minGap:150,collisionBoxes:[new I(15,15,16,5),new I(18,21,24,6),new I(2,14,4,3),new I(6,10,4,7),new I(10,8,6,9)],numFrames:2,frameRate:1e3/6,speedOffset:.8}],u.config={DROP_VELOCITY:-5,GRAVITY:.6,HEIGHT:47,HEIGHT_DUCK:25,INIITAL_JUMP_VELOCITY:-10,INTRO_DURATION:1500,MAX_JUMP_HEIGHT:30,MIN_JUMP_HEIGHT:30,SPEED_DROP_COEFFICIENT:3,SPRITE_WIDTH:262,START_X_POS:50,WIDTH:44,WIDTH_DUCK:59},u.collisionBoxes={DUCKING:[new I(1,18,55,25)],RUNNING:[new I(22,0,17,16),new I(1,18,30,9),new I(10,35,14,8),new I(1,24,29,5),new I(5,30,21,4),new I(9,34,15,4)]},u.status={CRASHED:\"CRASHED\",DUCKING:\"DUCKING\",JUMPING:\"JUMPING\",RUNNING:\"RUNNING\",WAITING:\"WAITING\"},u.BLINK_TIMING=7e3,u.animFrames={WAITING:{frames:[44,0],msPerFrame:1e3/3},RUNNING:{frames:[88,132],msPerFrame:1e3/12},CRASHED:{frames:[220],msPerFrame:1e3/60},JUMPING:{frames:[0],msPerFrame:1e3/60},DUCKING:{frames:[262,321],msPerFrame:125}},u.prototype={init:function(){this.blinkDelay=this.setBlinkDelay(),this.groundYPos=t.defaultDimensions.HEIGHT-this.config.HEIGHT-t.config.BOTTOM_PAD,this.yPos=this.groundYPos,this.minJumpHeight=this.groundYPos-this.config.MIN_JUMP_HEIGHT,this.draw(0,0),this.update(0,u.status.WAITING)},setJumpVelocity:function(A){this.config.INIITAL_JUMP_VELOCITY=-A,this.config.DROP_VELOCITY=-A/2},update:function(A,i){this.timer+=A,i&&(this.status=i,this.currentFrame=0,this.msPerFrame=u.animFrames[i].msPerFrame,this.currentAnimFrames=u.animFrames[i].frames,i==u.status.WAITING&&(this.animStartTime=h(),this.setBlinkDelay())),this.playingIntro&&this.xPos=this.msPerFrame&&(this.currentFrame=this.currentFrame==this.currentAnimFrames.length-1?0:this.currentFrame+1,this.timer=0),this.speedDrop&&this.yPos==this.groundYPos&&(this.speedDrop=!1,this.setDuck(!0))},draw:function(A,i){var s=A,e=i,n=this.ducking&&this.status!=u.status.CRASHED?this.config.WIDTH_DUCK:this.config.WIDTH,o=this.config.HEIGHT;Q&&(s*=2,e*=2,n*=2,o*=2),s+=this.spritePos.x,e+=this.spritePos.y,this.ducking&&this.status!=u.status.CRASHED?this.canvasCtx.drawImage(t.imageSprite,s,e,n,o,this.xPos,this.yPos,this.config.WIDTH_DUCK,this.config.HEIGHT):(this.ducking&&this.status==u.status.CRASHED&&this.xPos++,this.canvasCtx.drawImage(t.imageSprite,s,e,n,o,this.xPos,this.yPos,this.config.WIDTH,this.config.HEIGHT))},setBlinkDelay:function(){this.blinkDelay=Math.ceil(Math.random()*u.BLINK_TIMING)},blink:function(A){var i=A-this.animStartTime;i>=this.blinkDelay&&(this.draw(this.currentAnimFrames[this.currentFrame],0),1==this.currentFrame&&(this.setBlinkDelay(),this.animStartTime=A))},startJump:function(A){this.jumping||(this.update(0,u.status.JUMPING),this.jumpVelocity=this.config.INIITAL_JUMP_VELOCITY-A/10,this.jumping=!0,this.reachedMinHeight=!1,this.speedDrop=!1)},endJump:function(){this.reachedMinHeight&&this.jumpVelocity=0;A--)this.draw(A,parseInt(this.highScore[A],10),!0);this.canvasCtx.restore()},setHighScore:function(A){A=this.getActualDistance(A);var i=(this.defaultString+A).substr(-this.maxScoreUnits);this.highScore=[\"10\",\"11\",\"\"].concat(i.split(\"\"))},reset:function(){this.update(0),this.acheivement=!1}},C.config={HEIGHT:14,MAX_CLOUD_GAP:400,MAX_SKY_LEVEL:30,MIN_CLOUD_GAP:100,MIN_SKY_LEVEL:71,WIDTH:46},C.prototype={init:function(){this.yPos=s(C.config.MAX_SKY_LEVEL,C.config.MIN_SKY_LEVEL),this.draw()},draw:function(){this.canvasCtx.save();var A=C.config.WIDTH,i=C.config.HEIGHT;Q&&(A=2*A,i=2*i),this.canvasCtx.drawImage(t.imageSprite,this.spritePos.x,this.spritePos.y,A,i,this.xPos,this.yPos,C.config.WIDTH,C.config.HEIGHT),this.canvasCtx.restore()},update:function(A){this.remove||(this.xPos-=Math.ceil(A),this.draw(),this.isVisible()||(this.remove=!0))},isVisible:function(){return this.xPos+C.config.WIDTH>0}},p.config={FADE_SPEED:.035,HEIGHT:40,MOON_SPEED:.25,NUM_STARS:2,STAR_SIZE:9,STAR_SPEED:.3,STAR_MAX_Y:70,WIDTH:20},p.phases=[140,120,100,60,40,20,0],p.prototype={update:function(A,i){if(A&&0==this.opacity&&(this.currentPhase++,this.currentPhase>=p.phases.length&&(this.currentPhase=0)),A&&(this.opacity0&&(this.opacity-=p.config.FADE_SPEED),this.opacity>0){if(this.xPos=this.updateXPos(this.xPos,p.config.MOON_SPEED),this.drawStars)for(var t=0;t"}],"posts":[{"title":"我们从哪里来？","slug":"pen/我们从哪里来","date":"2021-06-06T03:51:37.000Z","updated":"2021-06-06T03:51:37.000Z","comments":true,"path":"posts/dad4292a.html","link":"","permalink":"https://blog.mhuig.top/posts/dad4292a.html","excerpt":"","text":"这里探讨的是一个非常简单的问题，我们是怎么来到这里的？ 前言世界上的所有人都是由这些元素和化合物组成的，他们的平常甚至让我们高智能的人类感到尴尬。 事实上，人体的99%都是由空气、水、碳以及白垩组成，另外还能找到少量的比较特别的元素，如铁、锌、磷和硫。实际上，组成人体的所有物质加起来最多花几十块钱就能买到。 但是，也不知道怎么的，这些数以万计的普普通通的原子能够携手起来，组成一个能够思考呼吸的活生生的人。 这些简单的积木是如何组合在一起。这无疑是最令人着迷的问题。 你可能会认为仅仅通过科学是不能够回答这个神奇的问题的。但是你敢肯定吗？现在，我们有理由相信，科学已经在这个大胆解释这个问题方面超越了宗教和哲学的解释力。 下面我们将通过一系列相互盘根错节的伟大发现来解释自然界的不为人知的一面。 在这之中蕴含了世上最基本的法则，即不确定性的产生。 这是关于看似廖无生气、毫无目的和动机的物质世界，是如何自发的产生这些极其精细的绚丽的自然。 这是关于世间最基本的法则是如何使这个世界展现出混沌和不可预测性，如何通过简单的物质创造出人。 这是关于一个奇异的发现，即有序和混沌之间，奇特而难以置信的联系。 图灵的方程式自然界充满了生长、发展和混乱，其中到处都是离奇的形状和杂乱的斑点。自然界的图案从来都不会固定不变，从来都不会按原样重复。 这一切看上去混乱的现象都受到数学方程式的影响。事实上，他们完全被数学规则所支配。这种数学规则与我们长久以来的直觉相悖。因此不难相信第一个能够担负此揭示自然界的数学根基重任的人会拥有超乎寻常的智慧。 他即是一个伟大的科学家，同时也是一个大悲剧。 他1912年生于伦敦，他的名字就是阿兰●图灵。 阿兰●图灵是一个不凡之人，他是有史以来最伟大的数学家之一。他提出了许多具有基础性的理论和见解。他的思想为现代计算机的出现提供了理论基础。 在二战期间，他在布莱切利园工作，也就是今天的米尔顿凯恩斯外。当时政府正在这里进行一个叫做X站的秘密项目。他的建立是为了破解德军的情报密码。 X站项目组的密码破解人员做出了卓越有成效的工作，其中图灵的贡献至关重要。他亲自参与了破解德国海军密码的工作，因为他的工作数以万计的盟军得以幸存，同时这也导致战略形势的有利扭转。 但是图灵的天赋不仅仅表现在破解密码上，这仅仅是他那超乎常人的洞察力的一部分。对于图灵来说，自然界的密码才是终极密码。在他的一生中，他热切的寻找着破解这个密码的方法。 图灵是一个很有独创精神的人，他意识到这样一种可能性，简单的数学方程式可以描述复杂的生物世界的一些现象。他的这种想法以前从来没有人尝试过，自然界中所有的迷中最吸引图灵的，就是如何能够使用数学方程式来描述人类的智能。 图灵迷上这个想法是有原因的，这就是年轻的克里斯托弗●马尔孔的死。图灵是同性恋，克里斯托弗●马尔孔的死，在那时以及他的一生产生了重要的影响。克里斯托弗●马尔孔突然的死亡了导致阿兰图灵在情感方面产生了极大的触动。但是你能想象，他想把这个事实用科学来解释，他想解释的问题是，我们的心智怎么了，什么是心智。 图灵相信生物界的复杂的系统是可以用数学方程式来描述的，并且人类的智能也是如此。 他的这种信念导致了现代计算机的产生。 之后一个更加激进的想法出现在图灵的脑中，这个想法就是通过简单的数学描述来解释胚胎中发生的复杂的过程。 这个过程被叫做形态发生。它非常令人费解，起初胚胎中的所有细胞都是相同的，细胞们开始组合到一起，并且细胞之间渐渐产生了差异，这是如何发生的呢？物质不会自己思考，也没有一个中央系统在里面进行调度，一开始都是一样的细胞，为什么有的能够变成皮肤，而有的却变成了眼睛呢？ 形态发生是一类现象中的一个例子，这类现象叫做自组织现象。 在图灵之前没有人懂得自组织现象的机制。 直到1952年 图灵发表了他的这一篇论文，阐述了用数学方程式来解释形态发生现象。论文中的大胆的猜测是令人震惊的，其中图灵使用了一个在天文学和原子物理学中很常见的一种数学方程式，来描述生命过程。 之前从来没有人做过这种尝试，然而图灵的方程式却第一次的做到了，描述了一个生物系统的自我组织的过程，这解释了即使简单的、毫无自然界事物特性的东西，也可以演变出栩栩如生的东西。 图灵的成果中令人大吃一惊的地方是，我们可以通过设定非常简单，甚至简单到可以仅仅通过简单的方程式就能描述的初始状态，然后让它进行演变。然后突然间，复杂和混乱就会出现，产生的复杂图案就像是自然界的结果。 从许多方面来看这些都是难以置信的。 其实图灵的方程式描述的都是我们很熟知的东西，但是从来没有人将这些数学方程式应用到生物学领域。 试想一阵风吹过沙丘，进而产生了一系列图形。小颗粒自我组织成波纹浪花和沙丘，即使那些小颗粒是彼此相同的。并且没有人告诉他们到底要怎么去组成属于他们的那一部分。 图灵认为，以一种非常形似的方式，在胚胎中渗过的化学物质可能会引导细胞进行自我组织，进而产生各种不同的形状。这是图灵给出的非常粗略的解释，他阐述了一堆毫无生气的化学物质，如何演化为各种不同的形状。 在他的论文中做了一些改进，来使他的方程式能够自发的产生生物图案，那种与动物表皮相似的图案。 图灵到处向别人展示自己生成的图形，你看这难道不像母牛身上的图案吗，其他人的反应是，这个人的脑子有问题吧。但是图灵相信自己在做一件有意义的事情，他们的确像母牛身上的花纹，这就解释了母牛拥有这种斑点花纹的现象。 这样一个数学从未触及的领域，生物界的图案，生成动物斑纹，突然间，这个领域向我们敞开了门。 我们发现数学方程式在这个领域会有用武之地，即使图灵提出的方程式，并不是这一新理论的全部，但仍然是一次重要的创举，提出了这种新方法的可能性。 我们现在知道形态发生，要比图灵所描述的数学方程式复杂多了。事实上，关于DNA分子确切的运转机制，仍然是现代科学上争论的话题。 但是图灵提出的数学支配万物的观点确是革命性的，阿兰●图灵的论文是整个形态发生理论的奠基石，他为我们提供了一种解释，连达尔文都没能提出的，解释自然界生物花纹的产生机制。 达尔文仅仅告诉我们，生物的花纹是来源于基因的，并且这种花纹的继承是决定于环境的，但是达尔文并没有揭示，这种生物花纹到底是如何生长的，而这是真正的迷。 图灵的贡献在于提供了了解这种化学机制的途径。图灵提出了一种伟大而勇敢的见解。 不幸的是我们仅仅能够推测这颗伟大的大脑，是如何想出这些的，在他发表他的开创性论文之后不久，一个可怕的并且完全可以避免的悲剧摧毁了他的生命。 在他在布莱切利园破解密码工作之后，你一定可以想到图灵会得到很多赞誉，以感谢他为国家做出的贡献。这再明显不过了。战后发生在他身上的事情是一个悲剧，这是英国科学史上令人感到羞耻的事情。 同年，图灵发表了关于形态发生的论文，他与阿诺德默里这个男人发生了短暂的情感，但是这段情感发展的很令人不愉快，默里对图灵进行了一次入室盗窃，但图灵将这件事情报告给警察的时候，警察连同图灵一起逮捕了。法庭上，原告声称，图灵以他的学历来诱导默里走上了歧途。图灵被判有严重的猥亵罪，法官给了图灵可怕的选择，他要么进监狱，要么接受雌性荷尔蒙注射，进而治疗他的同性恋倾向，他选择了后者。这导致了他连续不断的消沉。 1954年6月8日，图灵的尸体被他的清洁工发现。他是一天前死于咬了一口自己注入了氰化物的苹果，结束了自己的生命。 阿兰图灵死的时候41岁，这对于科学来说是一种无法估量的损失。图灵不曾想到他的思想会启发后人，将一种全新的数学方法应用到生物学领域。科学家发现他发现的这种方程式，确实能够解释好多生物组织的形态。 回头看看，我们知道图灵真正捕捉到了复杂与混乱源于简单规则，这样的法则。他意外的迈出了，通往新科学的第一步。 贝洛索夫的试液这个过程的第二步更是始料未及，可悲的是伟大的第二步也是一个悲剧。 在20世纪50年代初，在图灵发表他的对后人影响巨大的论文之际，一名杰出的俄国化学家 斯●贝洛索夫，开始了他自己的探索，关于自然界中的化学。 正在高墙铁网之后的苏维埃卫生部，他正在研究我们的身体是如何从糖中提取能量的。像图灵一样贝洛索夫也是在一个个人项目中工作，他刚刚完成了一段从事科学工作的经历。 贝洛索夫构想好了一种新的化合物配方，来模仿人体内葡萄糖的吸收，这种混合物就摆在实验室的座位前面，在被摇晃的时候清澈而透明。 在他添加最后一种试剂的时候，整个混合物的颜色发生了变化。 当然，这还不算什么特别的，就像我们将墨汁倒入水中水也会改变颜色。 但是接下来发生的事情令人感到惊奇，混合物又变得清澈无色了。 贝洛索夫感到很吃惊，化学物质混合以后会发生反应，但这个过程不能自发的发生逆反应，在不受外界干预的情况下发生可逆变化。 你可以很容易的将一个无色的液体变成有色的，但这个过程的逆过程却不太可能。这太奇怪了，贝洛索夫的试液并没有简单的发生逆变化，试液被摇晃后，就不断的在无色和有色之间变化，就好像这个试液在受一种神秘的内部机制驱动一样。 他非常谨慎小心地又重复多次这个实验，他的混合物能够不断的在无色和有色之间变化，他发现的东西好像魔术一样。一个好像是违反自然法则的物理现象。 贝洛索夫觉得自己的发现意义重大，将自己的发现记录下来，并努力让更多的人了解他的发现，但是当他将他的发现递交到一个顶尖的苏联科学杂志的时候，他收到了一个完全出乎意料的诅咒式的回复，杂志的编辑告诉贝洛索夫，他的发现是不可能在实验室重现的，因为这与基本的物理法则相违背，对这个现象的唯一的解释就是，贝洛索夫在实验的时候出了错，他的发现是不会被出版的。 这个拒绝深深的打击了贝洛索夫，他感到非常的羞辱，结果放弃了自己的实验，而后他又放弃了自己从事的事业。 具有悲剧色彩的讽刺是，由于贝洛索夫所处的闭塞环境，贝洛索夫从来没有机会看到图灵的工作成果。如果他看到了图灵的发现，就能为自己的发现提供有力的证据，事实上，贝洛索夫的不稳定试液，不仅没有违背物理法则，而且是实在就是真实的，能够体现图灵的预言的例子，尽管这两者的发现初看起来并没有什么联系。 其他科学家发现，如果将贝洛索夫的试液的一种变种，不加搅拌的放入培养皿中，而不是摇晃他们的话，他们就会发生自我组织，事实上 ，他们产生的条纹会比图灵预测的花纹要复杂。 他们会产生令人惊诧的复杂的条纹，毫无预兆。 贝洛索夫实验的惊奇之处在于，它能生成一种系统，这种系统产生了图灵方程式所预测的图案，在一个看上去无色的溶液中，产生了这种奇异的原型图案，这明显不是什么抽象科学，贝洛索夫图案的运动模式 ，与我们心脏在跳动的时候周围细胞的运动模式完全相同。动物的皮毛和心脏的跳动，自组织现象在自然界中随处可见。 牛顿经典物理学为什么在科学界在图灵和贝洛索夫的年代 ，却对这种想法不感兴趣甚至持有敌意呢？ 原因就是人类的臭毛病，主流科学家不喜欢这种观点，这与主流科学家的科学直觉相违背，也与现有的科学成就相违背，若想改变这种观念，我们需要一种彻底的，改变传统的发现。 实际上，在20世纪初期，科学家们把宇宙看作一个巨大而复杂的机械装置，有点像一个大号的太阳系仪，整个宇宙就好像是一个巨大的错综复杂的机器 ，严格的遵守着数学规则。如果你知道这个机器的运转机制和初始状态，那么随着你转动这个手柄，它将严格地按照预期的行为运转。 在牛顿生活的时代里，当人们在探索驱使宇宙运转的法则时，他们把宇宙看作这种按照确定规则运转的机器。宇宙就好比是一个被设定好的机器，遵循确定的法则按部就班的按照这个法则运转。 宇宙中复杂的现象是有复杂的内部规则驱使的，但是一旦一开始让它运转它只会做一件事。人们从这里看到的现象，都可以使用严格的数学公式来描述。 这实际上是很简单的事情，一旦找到能够描述系统运行的数学方程式，那么你就能够预测系统的走向。 这是一个伟大的想法。 它开始于牛顿的万有引力。万有引力成功的解释了行星围绕太阳运转的现象。科学家们后来又不断的发现了新的方程，牛顿物理似乎已成为了预言宇宙的终极方法。 它给我们暗示了这样一种可能，从原则上讲，未来是可以预测的。 我们采用的测量手段越精确，我们就能越精确的预测未来的情形。 但是牛顿物理产生了一个可怕的后果，如果有一个系统我们能够用数学方程式精确描述，就像这个太阳仪一样，那么一旦它表现出了一些我们不能预测的行为之后，科学家就只能认为，是有某种外界力量影响了这个系统，比如说是沙土跑进去了，或者是小零件磨损了，或者有人对它进行了认为的改动。 一般情况下，我们会这样假设，如果我们遇到了非预期的现象，那么这种现象应该是来自系统外部的干扰，而不是来自系统内部的。不可预测的现象，不是来自系统的本身，而是来自与外部对它的影响。 从这种观点的角度来看，自我组织这种现象是很荒谬的，而图灵和贝洛索夫所表达的，复杂图案可以从系统中自我生成，而不需要外界力量是非常受当时的主流科学所忌讳的。 要想使人接受自我组织理论，那么，就必须推翻牛顿物理学，但这看上去很不可能，无论如何这种新思想在60年代末期，成为了新时代的奇景。 蝴蝶效应然而同时，伴随着登月计划，一小群信奉牛顿力学的科学家，意外的发现了一些事情不对劲，完全不对劲。 在20世纪后半叶，科学界的噩梦出现了，这个噩梦，动摇了牛顿的思想，并将我们推向了一片思想的混乱。 具有讽刺意味的是，迫使科学界接受自我组织理论的事情，是一种我们叫做混沌的现象。 混沌这个词被广泛使用，但是在科学领域中，它有它专指的意义。它指的是在一个能被数学方程式精确描述的系统中，可以自发生成不可预测的现象，并且不需要任何外界的干预。 通过使用非常简单的法则或方程式，并且里面不包含任何的随机性，系统中的所有元素都是确定的，并且我们完全掌握系统的法则，即使是这样的系统也会产生完全不可预测的现象。 混沌的发现并不讨主流科学的喜欢，有一个人迫使科学界接受混沌，他就是美国的气象学家爱德华●洛伦兹。 在20世纪60年代早期，他试图寻找能够预测天气变化的数学模型。就像许多他的同事一样，他相信天气系统与我们太阳系仪是一样具有确定性的，一个可以被数学描述和预测的物理系统。 但是他错了，当洛伦兹写下一个用于描述气流的及其简单的数学方程式时，这些方程式并没有达到他预想的目的，他们没有做出任何有价值的预测，这就好像说某一天中的一阵清风，将会决定一个月后的某天是冰雪连天，还是清空万里。 对于一个像太阳系仪这样精准的系统来说，怎么会产生不可预测性呢？ 这源于他的内部构造，由于齿轮的链接方式。 事实上，在某种情况下，即使在初始的时候有一点点误差，哪怕这个误差小到难以测量，这个误差会随着机械的运转而不断被放大，随着系统的运转，系统的状态会一点一点的偏离你所期望的状态。 洛伦兹在一次演讲中表达了这一颠覆性的想法，演讲的题目是——一只蝴蝶在巴西扇动翅膀会使美国的德克萨斯刮一场龙卷风吗？ 这是一次有力而吸引人的演讲，数月之内，我们的语言中就添加了一个新的词汇——蝴蝶效应。 蝴蝶效应就是混沌系统的标志，它开启了之后的一切。 在70年代早期 一个叫罗伯特●梅的年轻澳大利亚人，正在研究一个数学方程式，用它来模拟生物种群随时间的变化。 但是这个过程中同样牵扯到蝴蝶效应，哪怕生物的繁殖率发生了极小的变动，都会导致种群数量结果发生巨大的变化，这个数值可能会毫无征兆的上下起伏。 传统的使用数学方程式，来描述系统行为的方法似乎走到了死胡同，某种意义上，这是信仰牛顿学说的人的美梦之终结。 随着我们的计算能力的提高，我们就有能力处理更复杂的方程组，但刚才我们所看到的否认了这种观念。 你可以从一个及其简单的方程式开始，这个方程式简单而不存在任何的随机性，但是如果它产生的行为能够表现出一种混沌性，那么你就不能再回溯到系统的初始状态了。 数百年来所建立的科学观点在几年内就被瓦解了，可以精确描述宇宙的运行的这一想法变成了幻影。 看上去具有逻辑确定的事情，却变得更像是一种信仰。更糟的是，这种现像到处都是。因为混沌到处都是。 似乎不可预测性是固有的，存在于我们生活的宇宙中。 全球气候可能会在几年的时间内，发生剧烈的变化；股市可能会毫无征兆的崩盘；我们可能会在一夜之间从地球上灭绝。 如果这种事情发生的话 没有人能够阻止，不幸的是，我不得不说以上这些都是真的。但是盲目的恐惧混沌现象是毫无意义的。 因为混沌是一条基本的物理法则，我们必须承认它是生活中的一部分。 混沌理论的出现一直影响到之后20到30年人们的思想，它改变了人们对于科学工作的看法，它深刻的改变了科学家看问题的方式，以至于科学家们现在已经离不开混沌理论了。 混沌理论想要说明的是，简单的数学方程式能够繁衍出复杂的行为，这种复杂性超出你我的想象，所以简单而机械的系统能够表现出复杂和丰富的行为。 混沌理论的发现，是科学史上的一次重大的转折点，它摧毁了牛顿信仰者的梦想，科学家们现在越来越看得惯图灵和贝洛索夫在自发生成花纹上所做的工作。 自反馈系统更重要的是，由于他们的工作，一个伟大的真相浮出水面，那是一种内在而隐蔽的关联，一个贯穿宇宙的关联，关联着自然的神秘力量和自我组织现象，以及蝴蝶效应产生的混沌结果。 图灵、贝洛索夫、梅、洛伦兹这些人都分别发现了一种重大思想的不同侧面。他们发现自然界具有固有的不可预测性，这种不可预测性的内部驱动力，也可以使系统表现出特定的结构和花纹，有序与混沌，似乎要比我们想象的要联系的更加紧密，但这种联系是如何实现的呢？贝洛索夫的花纹与天气变化之间有什么联系呢？ 首先，虽然两个系统都有复杂的工作机制，但他们都是基于及其简单的是数学法则，其次，这些数学法则都具有一种独特的特性，都具有自我链接的特性或者说是自反馈。 为了向你展示这一点，展示简单的自反馈系统的力量，将使用一种看上去简单甚至无聊的实验。 身后的屏幕连接到一台摄像机，这台摄像机同时也在拍摄着我和屏幕，这样不断的循环就能产生无数个人的影像，都投影到屏幕上，这是一个典型的自反馈系统。图片中不断的嵌套着其他的图片。 这乍看上去肯定有规则，但当我们放大镜头，奇怪的事情发生了，我首先发现的是实物图像和屏幕上显示的图像不像了，火柴的微小运动被迅速放大，当影像在摄像机和屏幕之间反复映射的时候，即使我能用精确的数学对外的每一步动作进行描述，但我却不能预测火焰微小的变化，会导致最终的图像如何变化，这就是一种实实在在的蝴蝶效应。 下面的现象变得更离奇了，通过像系统中添加一点点的扰动，这些奇异而美丽的图案就出现了，这种简单的依赖反馈的系统，呈现出了混沌与有序，同样的数学方程式同时产生了混沌和有序的图案。 这将改变你对世界的看法，在传统观念中，自然界都是有序的，混乱存在与系统之外，即有序与混乱是相互独立的这种想法是错的，其实混乱和有序，就像同一架钢琴上弹出的高音和低音，这个发现有史以来第一次如此接近自然界的数学本质。 我们能从图灵的工作，以及化学和生物中得到的，最重要的启示是，所有复杂的图形都是来自，宇宙间简单的演变过程，像扩散， 像化学反应率，这些简单的过程最终导致了图案的产生，所以到处能看到图案，他们不断的产生。 曼德勃罗集合从70年代开始越来越多的科学家，开始接受混沌理论，以及对自然界能够自发产生复杂花纹的认可，但是有一位科学家比别人走的更远，对这个令人惊奇颇感迷惑的问题带了个新思路，他是一个具有传奇色彩的不喜欢按套路出牌的人，他叫伯努瓦●曼德勃罗 。 伯努瓦●曼德勃罗 并不是一个寻常的孩子，他跳了两个年级。并且由于他是战时欧洲的犹太人，所以他受到的正规教育极其有限，他基本上是靠自学以及亲属对他的教育，他从来没有正式的学习过字母，甚至没有学过5以上的乘法。 但是和图灵一样，曼德勃罗具有一种洞悉事物本质的本领，他能从混乱中看到我们所看不到的规律，他能够发现形式和结构，然而我们却仅仅能看到一片混乱，他能够感知一种新奇的数学，用来支配整个自然界的运转。 曼德勃罗的一生都致力于找到一种能够揭示自然界复杂性的一种数学支持。 曼德勃罗当时为IBM工作，而不是学校的学术圈内，他试图解决一大堆的问题，关于自然界以及金融界的不确定性，在各个方面的表现。 我觉得他知道自己，所做的所有工作都是一个大问题的不同侧面。 他是一个具有原创精神的人，他觉得求解这个大问题是他真正想做的事情。 在曼德勃罗看来，几百年来传统的数学研究都仅限于，规则的图形是一种很不可取的行为，就像直线和圆，传统的数学是没有办法描述不规则的形状的，而真实世界确是有不规则形状组成的，就像这个鹅卵石，它是一个球体还是一个立方体呢，还是它们中间的某种形状？他到底是一种什么形状？ 曼德勃罗想，是否有一种法则，能够描述自然界的不规则性，那些蓬松的云朵 树和河流的分支，以及蜿蜒的海岸线之间有什么共同的数学基础。 是的，有的。 自然界中所有的形状的共同特点，就是自相似性。这指的是事物的局部，不断的在更微小的尺度上重复自己，不断的精细到每一个细微之处，树枝就是一个绝好的例子，他们不断的分叉，重复这这个简单的过程，在更微小的尺度上不断重复这这个过程。 我们的肺的结构同样遵循这个原则构建，我们体内血管的分布同样遵循这样的规则，河流分成更小的溪流也是这样，自然界可以依照这种方法不断重复各种形状。 看看这个罗马花椰菜，他的总体结构是由许多重复的小圆锥组成的，曼德勃罗意识到自我重复性，是一种全新的几何学的基础，并给这种几何图形起了一个名字——分形。 分形看上去非常简单直观，但是我们如何才能用数学对它进行描述，你能够利用分形的本质画一幅相似的图形吗？那么这张图形会像什么？你能仅使用一些简单的数学法则，来绘制一幅看上去像是自然创造的图案吗？ 曼德勃罗找到了答案。 曼德勃罗爱20世纪50年代末就职与IBM，因此有机会使用计算机，并且利用计算机这个工具来寻找自然界的数学本质。 在新一代的超级计算机的帮助下，他开始研究一个看上去很奇怪，但却异常简单的数学方程式，使用这个数学方程式可以绘制一幅不同寻常的图形，我将向你展示的是一幅非常吸引人的图画。它完全由数学产生，惊人之举往往不同寻常。 这就是曼德勃罗集合，他被称为上帝的指纹，当我们仔细看看这幅图后，你就知道我们为什么这么叫了。 就像树和花椰菜一样，你看得越仔细你发现的细节就越多，在这个集合中的每一个图形，都包含了无穷多个小图形，小的子图无穷无尽，然而所有这些复杂的分支都来自有一个简单的方程，这个方程有一个非常重要的特性，它是自反馈的。 每一次输出都成为了下一次计算的输入，这种反馈系统展示了一个简单的，方程式是如何展现出无比绚丽的图案的。 但是令人惊奇的是，曼德勃罗集合并不是一个奇怪的数学巧合，它的这种无穷分形的特性，反映了一种自然的有序的本性，图灵图案、 贝洛索夫的化学反应、曼德勃罗的分形都分别指向了一个自然本质。 简单的法则 无穷的创造当我们看到自然界的复杂面貌时，我们倾向于问，他们来自哪里，我们总是抱有这样的观念。 简单的事物不能导致复杂性的产生，复杂的现象必须源于复杂的设计。 但是我们刚才看到的数学方程式告诉我们，极其简单的法则也会繁衍出复杂的现象。 当你看到复杂现象的时候，你应该想到，驱使它产生的只不过是简单的法则，所以同一个方程式从不同的角度看，既简单又复杂，这就意味着我们需要重新思考，简单性于复杂性之间的关系。 复杂的系统可以基于简单的法则。 这是一个重大的启示，也是一种伟大的思想。它似乎适用于整个世界。 看看这群飞鸟，每一只鸟都遵循简单的法则，但是整个鸟群确是一个极其复杂的东西，他会自动的避开障碍在没有领航情况下进行自我导航，甚至是做计划。 虽然这个鸟群的行为很显著，但我们却不能预测他的行为，它不会重复原先的行为，即使是在相同的环境下。 就像贝洛索夫的化学反应一样，每次你进行这个化学反应产生的图案都稍有不同，他们可能看上去相似但却不可能相同。 对于自反馈的影响和 沙丘同样是这样，我们知道它会产生某种图像，但是我们却不能预测确切的形状。 生物进化的原料我们关心的问题是，大自然能够以这种方式将简单的法则，演绎出复杂的现象吗？ 试着解释一下为什么会有生命的存在，它能够解释为什么充满砂石的世界，是怎么产生人类的吗？ 毫无生机的事物是如何变得充满智能的呢？ 进化正是基于这些简单的生物花纹，进化将这些当成原料，并把它们以不同的方式结合在一起，看看哪些结合方式有效，哪些无效， 保存那些有效的结合，并在它们的基础上进一步演化。 这是一个完全没有意识控制的变换过程，但这基本上就是现实中发生的事情，放眼望去，进化的过程到处都是使用自然界的自我组织的图案。 我们的心脏使用类似贝洛索夫反应的方式来驱使它有节律跳动，我们的血管的组织形式就像分形，就连我们的脑细胞也是遵循极其简单的规则，进化过程丰富并筛选这我们的世界的复杂性。 这是近代科学最有魅力的发现。 计算机模拟的进化机制一方面你拥有一个具有自组织能力的复杂系统，它可以产生不可预测的行为，另一方面需要将进化机制作用于它，这样才能产生适应环境的东西，进化通过无拘无束的创造力，约束着系统的演确实难以置信。 但当这个约束过程发生在宇宙级的时间尺度上，就容易理解了，从地球上第一次出现生物到我们人类的产生，整整花了35亿年的时间，但是我们现在手头上，有一种可以在更短的尺度上模拟这个过程的发明。 你知道我指的是什么吗？ 很可能你天天就坐在这个发明面前，当然，这就是计算机。 现代的计算机每秒可以进行上亿次计算，我们可以让它们做一些奇特的事情，它们可以模拟进化过程，确切的说，计算机可以利用进化规则，来约束自己产生真实世界中的现象，使用进化规则来约束和筛选生物组织。 现在，计算机科学家发现这种能够自我演变的软件，可以取代人类最聪明的头脑来解决问题。 我们在最初的实验研究中发现，进化这种系统就像一种算法一样创造着，能够适应环境的复杂系统。 托斯顿和他的研究小组的研究目标是，使用计算机模拟的进化机制，来创造能够控制躯体运动的虚拟大脑。 一开始他们随即设置了100个虚拟大脑，就像你看到的这样 这些大脑很笨，然后进化的力量来了，计算机自动的选择表现稍好的大脑，然后让它们产生后代，然后再选择能够做的更好的大脑，并让它们继续产生后代，下一代中能够更好的控制躯体行动的大脑，会继续得到繁殖后代 的机会。 令人惊奇的是，通过10代的繁衍，虽然还是有一些不稳定，但这些小人确实能够行走了。 更神奇的事情是，你最终得到了一种能够正确行走的东西，但是令人感到有点害怕得到是，你却不知该它为什么能走，以及是如何行走的。 你眼睁睁看着这个大脑，却不知道它内部是如何工作的，因为这个结果是进化自动产生的结果，经过20代的繁衍后， 我们看到了这个， 变成了这个， 这些虚拟生物随后演化出了比行走更复杂的行为，它们产生的行为，是很难通过传统的编程方式实现的，它们对于突发事件做出了像人类一样的反应。 即使这些算法都是由我们人类编写的，但当它们一旦开始进化之后，我们就难以加以控制了，然后我们预想不到的事情就发生了。 真有一种滑稽的感觉，你创造了它们，然后它们抛开你自己做主，一种不假思索的不断尝试的进化过程，创造了这些能够行动并作出反应的虚拟生物。 我们这里看到的是一个绝妙的实验证据，来证明简单的法则具有无穷的创造力，看着计算机里面自动表演着难以用写程序的方式描述的行为，它是一个展现自我组织能力的绝好例子。 这说明了进化本身，就像我们看到的其他系统一样，是一个基于简单法则和回馈的系统，在这个系统中复杂性自发的产生了。 想想看，这个简单的法则就是，机体需要重复略有变化的行为，反馈来源于环境，这种环境选择了更适应它的行为得到生存，结果就是，在没有可以设计和规划的情况下，前所未有的复杂性就这样产生了。 有意思的事情是，一个个体可以进化到更高的一个结构状态，一旦你获得了一种包含某种行为的系统，并且这些行为可以被选择，被某种过程选择或被环境的反馈选择。所以进化过程这个达尔文学术的中心议题，在某种意义上就是图灵的反馈系统运行在多个过程之上。 尾声这就是整个事情的本质，未经过精心设计的极其简单的法则，能够无意识的创造出无比复杂的系统。 这样看来，这些计算机虚拟的生物就是自组织系统，就像贝洛索夫在他的化学实验中发现的现象一样，就像沙丘和曼德勃罗集合所呈现出的现象一样，就像我们的肺、心脏以及我们这个星球上的天气系统。 伟大的设计并不需要一个伟大的设计者，这就是宇宙所固有的本性。 令人难以接受这种观点的是，所有形状、花纹和结构的产生，并不需要一个有意识的创造者，但这种设计本身可能需要一个更聪明的设计者，他做的事情就是将整个宇宙，作为一个巨大的仿真，在这里你设定了一些初始条件，然后一切的一切都自发的产生了。 伴随着所有的惊奇，伴随着所有的美丽，图案形成的数学本质预示着同样的图案会在许许多多不同的场合出现。 如化学生物系统，在这些系统的本质里，都存在同样的数学基础，在这些内部本质的外面，就是我们看到的今天的这个世界。 我想，这是一个令人兴奋的想法，那么我们最终能从这些当中学到什么呢？ 这就是宇宙间所有的复杂性，所有的多样性，都源于一些简单而毫无目的的法则的不断繁衍的结果，但是请记住,尽管这个过程力量无比，但他却具有固有的不可预测性，即使我可以充满信心的告诉你未来精彩无限，但我仍要负责任的告诉你，未来将会发生什么确是不为人知的。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"chaos","slug":"chaos","permalink":"https://blog.mhuig.top/tags/chaos/"}]},{"title":"记一次仅在IPv4环境下访问IPv6网络的经历","slug":"web/ipv6/ipv6-vpn","date":"2021-04-10T11:24:30.000Z","updated":"2021-04-10T11:24:30.000Z","comments":true,"path":"posts/5e9edb45.html","link":"","permalink":"https://blog.mhuig.top/posts/5e9edb45.html","excerpt":"","text":"点击访问","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"IPv6","slug":"web/ipv6","permalink":"https://blog.mhuig.top/categories/web/ipv6/"}],"tags":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/tags/web/"},{"name":"IPv6","slug":"ipv6","permalink":"https://blog.mhuig.top/tags/ipv6/"}]},{"title":"神秘数字4.669","slug":"math/神秘数字4.669","date":"2021-02-10T01:50:47.000Z","updated":"2021-02-10T01:50:47.000Z","comments":true,"path":"posts/373468cd.html","link":"","permalink":"https://blog.mhuig.top/posts/373468cd.html","excerpt":"","text":"从无序迈向有序","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"}],"tags":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"},{"name":"分形","slug":"分形","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E5%BD%A2/"},{"name":"混沌","slug":"混沌","permalink":"https://blog.mhuig.top/tags/%E6%B7%B7%E6%B2%8C/"}]},{"title":"HPP测试","slug":"others/Test/HPP测试","date":"2021-02-08T09:18:47.000Z","updated":"2021-02-08T09:18:47.000Z","comments":true,"path":"posts/d9d9a7f4.html","link":"","permalink":"https://blog.mhuig.top/posts/d9d9a7f4.html","excerpt":"","text":"HPP测试 工作空间","categories":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}],"tags":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}]},{"title":"SparkStreaming进阶","slug":"bigdata/Spark/SparkStreaming进阶","date":"2021-02-07T13:57:53.000Z","updated":"2021-02-07T13:57:53.000Z","comments":true,"path":"posts/21e23691.html","link":"","permalink":"https://blog.mhuig.top/posts/21e23691.html","excerpt":"","text":"SparkStreaming整合Kafkakafka的两个重要版本 Kafka-0.8 consumer在消费消息的时候会记录一个偏移量（offset） offset 偏移量记录上一次消费到哪里了，那么下一次我知道要从哪里继续消费数据 偏移量被保存在 Zookeeper 中 问题：在一个公司里可能有几百号人去消费kafka集群，这个时候zookeeper会面临高并发的读写（zookeeper不擅长高并发读写，zookeeper是有问题的）这个设计明显是有问题的 Kafka-0.10 不再在zookeeper中存储offset了 在kafka中设计了一个特殊topic(__consumer_offset)，将所有的consumer中的所有offset存在了这个topic中 这个用来存储offset的topic默认50个分区，如果集群足够大的话那么这些分区也会均匀的分布在整个集群中支持高并发的读写，这样高并发的读写就不会成为瓶颈了 0.8-kafka(zookeeper) 和 0.10-kafka(kafka)的offset是怎么提交的？ 自动提交offset，这样整个系统就可能面临丢失数据的风险 SparkStreaming 防数据丢失设计Kafka每隔5秒提交一次offset，如果这样我们的程序就有可能丢数据，为什么？ SparkStreaming读取到了kafka的数据（offset=100），还没有处理正好遇到了5s的时间间隔提交了offset 这个时候offset已经提交了，但是等到处理的时候，发现处理失败了 这样重启的时候数据就发生了丢失，我们企业中当然是不允许数据丢失的 怎么解决丢数据的问题呢？ kafka-0.8：把自动提交offset关掉，改成手动提交offset，但是这个时候有可能出现数据重复；因为你在提交offset的时候有可能失败，所以就会重复的消费数据进行处理，但是这个总好过丢数据，并且可以根据幂等性等一些方案对重复数据进行过滤，来保证数据不丢失的前提下保证唯一性 kafka-0.10：和kafka-0.8一样关闭自动提交offset，改成手动提交，只是offset存储的地方不一样 实时处理系统中对数据处理的策略 At most once 一条记录要么被处理一次，要么没有被处理（丢数据） At least once 一条记录可能被处理一次或者多次，可能会重复处理（重复消费） Exactly once 一条记录只被处理一次（仅一次） 要想实现仅一次语义 数据的输入：从上一次offset读取数据 offset 数据的处理：Spark 本身就有容错，所以天然的就保证了Exactly-Once 数据的输出：利用事务去实现","categories":[{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"}],"tags":[{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"},{"name":"SparkStreaming","slug":"sparkstreaming","permalink":"https://blog.mhuig.top/tags/sparkstreaming/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"}]},{"title":"SparkStreaming","slug":"bigdata/Spark/SparkStreaming","date":"2021-02-07T13:40:49.000Z","updated":"2021-02-07T13:40:49.000Z","comments":true,"path":"posts/4e2051bb.html","link":"","permalink":"https://blog.mhuig.top/posts/4e2051bb.html","excerpt":"","text":"实时任务场景介绍2014年的大数据的三剑客 Hadoop 必须会使用Java Hive 大数据中使用最广泛的一个技术 Sql =&gt; MapReduce Storm 人性是贪婪的，Hadoop和Hive都是计算的离线的、历史的数据 后来的大数据的三剑客 Spark 使用SparkCore内存计算提高了MapReduce的计算效率 Scala、Python、Java SparkSQL Sql =&gt; Spark Core任务 SparkStreaming/Flink 实时的应用场景：2020淘宝双11成交额4892亿 实时计算就是来一个订单计算一个订单，每时每刻每秒都在统计 离线计算就是统计某个已经发生的时间段内的数据 例：淘宝在年底的时候计算1.1-12.31号的所有订单 坐电梯的时候需要凑够一波人坐电梯上去，把每一个人看作一个数据集攒了一定的人数之后一起坐电梯上去，我们会等待一定的时间统一处理数据，这个时候我们处理的就是历史数据了 特点 处理的是离线的数据 每次处理的数据量比较大 处理的时间比较长，比较慢 扶梯 1、处理的是实时的数据 2、每次处理的数据量不大 3、处理的时间比较短、比较快 数据流 实时任务处理的就是数据流，数据流其实只是一个形象的说法，指的是任务处理的数据像流水一样源源不断的过来，就像水龙头里面的水一样 SparkStreaming程序入口剖析SparkCore核心抽象：RDD Resiliennt Distributed Datasets 程序入口：val sc = new SparkContext(conf) 算子的操作：Transformation/Action SparkSQL核心抽象：DataFrame/DataSet 程序入口： Spark 1.x：new SQLContext(conf)、new HiveContext(conf) Spark 2.x/3.x：val ss = new SparkSession(conf) 算子的操作/SQL SparkStreaming核心抽象：DStream 程序入口： val conf = new SparkConf().setAppName(“SS”).setMaster(“local[2]”) val sc = new SparkContext(conf) val ssc = new StreamingContext(sc,Seconds(1)) 算子操作（RDD的算子操作在SparkStreaming上都可以用） DStream核心抽象深度剖析1、SparkStreaming的任务是基于SparkCore，然后我们任务启动的时候，或者是初始化的时候都会有一个Driver的服务 2、Driver端会发送Receivers到Worker里面，Receiver其实说白了就是一个接收器，接收数据，这个Receiver具体就是表现为一个Task任务，默认情况下只有一个Receiver可以通过配置多个 3、Receiver会接收数据，并且会把数据生成block块（1、生成文件块的依据是啥？）接着就把这些文件块（block）存入到Executor的内存里面，为了保证数据安全，默认这些数据是有副本的，在其它的Executor上存储副本 4、receiver会把block的信息（元数据的信息）发送给Driver端 5、Driver端会根据一定的时间间隔（2、封装RDD时间间隔是多少？）把这些block封装成为一个RDD，然后进行计算 Receiver把数据合并成block块的依据？ 每200ms生成一个Block 1个block对应1个partition对应1个task任务 Driver将Block封装成RDD的时间间隔是？ 根据程序入口中的时间参数，如ssc = new StreamingContext(sc,Seconds(1))就是每隔1s中将block块合封装成一个RDD SparkStreaming不是真正意义上的实时计算，不是真的来一条数据就处理一条数据；微批处理，只不过间隔较短，每次数据里的数据量不大，然后又比较快，所以我们就把它认为实时处理了！！！准实时处理 时间间隔： Batch interval 指的就是我们的这个实时任务多久运行一次，这个是我们获取程序入口的时候自己指定的 block interval 默认是200ms 一个一个的RDD的流就被Spark抽象为DStream RDD流 = DStream 快速上手引入类库 1234567import org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.log4j.&#123;Level, Logger&#125; 步骤1：初始化程序入口 步骤2：通过数据源获取数据（数据输入） 步骤3：进行算子的操作，实现业务（数据处理） 步骤4：数据的输出 步骤5：启动任务 步骤6：等待任务结束","categories":[{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"}],"tags":[{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"},{"name":"SparkStreaming","slug":"sparkstreaming","permalink":"https://blog.mhuig.top/tags/sparkstreaming/"}]},{"title":"大数据架构演变","slug":"bigdata/大数据架构演变","date":"2021-02-07T12:51:06.000Z","updated":"2021-02-07T12:51:06.000Z","comments":true,"path":"posts/7e3480e9.html","link":"","permalink":"https://blog.mhuig.top/posts/7e3480e9.html","excerpt":"","text":"在Hadoop系列框架还没出现之前，数据分析工作已经经历漫长的发展，其中以BI系统为主的数据分析，已经有非常成熟和稳定的技术解决方案和生态系统，BI系统的架构图如下： BI又叫商业智能，其包括与传统业务系统的区别在于：业务系统更注重于事务型的数据处理，用来支撑企业的各业务线；而BI是将企业中所有数据汇聚成数据仓库（DW）并对其进行分析型操作，其中Cube是BI的核心模块。Cube是一额更高层的业务抽象模型，在Cube上可以进行上钻、下钻、切片等操作、 BI系统都是基于关系型数据库，关系型数据库使用 SQL 语句进行操作，但是 SQL 在多维操作相对较弱，所以 Cube 有自己独有的查询语言多维查询语言—— MDX 大多数的数据库服务厂商都提供BI服务，轻易便可搭建出一套OLAP分析系统 OLTP 联机事务处理，表现为企业中的应用系统如OA、CRM、ERP、财务软件等供各部门使用 OLAP 联机分析处理，也叫决策支持系统DSS，通常进行使用者是企业高管或部门管理者 但是随着互联网发展，BI系统也暴露除了一些缺点: BI系统多以分析业务数据产生结构化数据为主，对于非结构化和半结构化数据处理乏力。例如图片、文本、音频的存储、分析。 随着异构数据源增加，要解析数据内容进入数据仓库，则需要非常复杂的ETL程序，从而导致ETL变得过于庞大和容易出错，需要大量人力进行维护 随着数据的增长，性能会成为瓶颈，在TB/PB级别的数据处理上表现的尤为乏力 数据仓库的原始数据都是只读的用来分析，不存实务操作者导致传统的范式约束大大影响了性能 由于BI的一系列问题，在以Hadoop生态圈的大数据分析平台逐渐表现出其优异性，围绕Hadoop体系的生态圈也不断变大，对于Hadoop系统来说，从根本上解决了传统数据仓库瓶颈的问题 随着大数据平台的不断发展，现在主要对数据的处理时效进行区分为 针对于T+1数据的离线处理架构，其主要应用框架由Hadoop、Hive、Sqoop等组成 针对实时数据的流式处理架构，其主要由Spark、Flink、Flume、Kafka等组成 Lambda架构“我们正在从IT时代走向DT时代(数据时代)。IT和DT之间，不仅仅是技术的变革，更是思想意识的变革，IT主要是为自我服务，用来更好地自我控制和管理，DT则是激活生产力，让别人活得比你好” ——阿里巴巴董事局主席马云。 Hadoop作为解决对大数据量低成本规模化的处理的解决方案被广泛应用 但是MapReduce或者Hive很难做到低延迟，用 Storm 开发的实时流处理技术可以帮助解决延迟性的问题，但它并不完美 Storm 不支持 exactly-once 语义，因此不能保证状态数据的正确性 Storm 不支持基于事件时间的处理 后来出现了一种混合分析的方法，它将上述两个方案结合起来，既保证低延迟，又保障正确性——Lambda Lambda架构是由Storm的作者Nathan Marz提出的一个实时大数据处理框架 Marz在Twitter工作期间开发了著名的实时大数据处理框架Storm，Lambda架构是其根据多年进行分布式大数据系统的经验总结提炼而成 Lambda的目标: 高容错、低延时、可扩展 Lambda特性 整合离线计算和实时计算 读写分离和复杂性隔离 可集成Hadoop，Kafka，Storm，Spark，HBase等 Marz认为大数据系统应具有以下的关键特性（Lambda架构的关键特性）： Robust and fault-tolerant（容错性和鲁棒性）：让系统从错误中快速恢复 Low latency reads and updates（低延时）：响应是低延时 Scalable（横向扩容）：通过增加机器的个数来提高系统的性能 General（通用性）：支持多领域的数据分析（金融、社交、电子商务等） Extensible（可扩展）：以最小的开发代价来增加新功能 Allows ad hoc queries（方便查询）：即时查询，快速简便的进行查询 Debuggable（易调试）：快速定位错误 Lambda架构通过分解的三层架构来解决问题 Batch Layer Speed Layer Serving Layer Batch Layer理想状态下，任何数据查询都可以从表达式 Query= function(all data) 获得，但是若数据达到相当大的一个级别（例如PB），且还需要支持实时查询时，就需要耗费非常庞大的资源 可以将数据提前进行计算处理成为Batch View，这样当需要执行查询时，可以从Batch View中读取结果。这样一个预先运算好的View是可以建立索引的，因而可以支持随机读取 Batch Layer总结为： Batch View = function(all data) Query = function(BatchView) Speed LayerBatch Layer的离线处理可以很好的满足大多数应用场景，但有很多场景的数据是不断实时生成，并且需要实时查询处理。Speed Layer正是用来处理增量的实时数据并生成Realtime View Speed Layer处理的数据是最近的增量数据流，Batch Layer处理的是全体数据集 Speed Layer为了效率，接收到新数据时不断更新Realtime View，而Batch Layer根据全体离线数据集直接得到Batch View Speed Layer是一种增量计算，所以延迟小 Speed Layer总结为： RealtimeView＝function(RealtimeView，new data) Batch Layer和Speed Layer优点： 容错性：Speed Layer中处理的数据也不断写入Batch Layer，当Batch Layer中重新计算的数据集包含Speed Layer处理的数据集后，当前的Realtime View就可以丢弃，这也就意味着Speed Layer处理中引入的错误，在Batch Layer重新计算时都可以得到修正。这点也可以看成是CAP理论中的最终一致性（Eventual Consistency）的体现 复杂性隔离：Batch Layer处理的是离线数据，可以很好的掌控。Speed Layer采用增量算法处理实时数据，复杂性比Batch Layer要高很多。通过分开Batch Layer和Speed Layer，把复杂性隔离到Speed Layer，可以很好的提高整个系统的鲁棒性和可靠性 Query = function( Batch View , Realtime View ) Realtime View = function( Realtime View , new data ) Batch View = function( all data ) Serving Layer用于响应用户的查询请求，合并Batch View和Realtime View中的结果数据集到最终的数据集 Kappa架构Lambda架构有时会出现批量数据和实时数据结果对不上的问题 LinkedIn的Jay Kreps提出了一个新的架构：KAPPA 它的理念是：鉴于大家认为批量数据和实时数据对不上是个问题，它直接去掉了批量数据;而直接通过队列（Kafka），放入实时数据之中。 例如：将所有的数据直接放到原来的Kafka中，然后通过Kafka的Streaming，去直接面向查询 该架构也存在着一些问题： 不能及时查询和训练。例如：我们的分析师想通过一条SQL语句，来查询前五秒的状态数据。这对于KAPPA架构是很难去实现的 面对各种需求，它同样也逃不过每次需要重新做一次Data Streaming。也就是说，它无法实现Ad—hoc查询，我们必需针对某个需求事先准备好，才能进行数据分析 新数据源的结构问题。例如：要新增一台智能硬件设备，我们就要重新开发一遍它对应的适配格式、负责采集的SDK、以及SDK的接收端等，即整体都要重复开发一遍 IOTA架构IOTA架构整体思路设定标准数据模型，通过边缘计算技术把所有的计算过程分散在数据产生、计算和查询过程当中，以统一的数据模型贯穿始终，从而提高整体的预算效率，同时满足即时计算的需要，可以使用各种Ad-hoc Query来查询底层数据 Common Data Model（核心）：从数据收集到数据存储和处理使用统一的数据模型 ​ “主-谓-宾”、“对象-事件”、“产品-事件”、“地点-时间”模型等等 ​ 例，“X用户 – 事件1 – A页面（2018/4/11 20:00） SDKs：数据的采集端，不仅仅是过去的简单的SDK，在复杂的计算情况下，会赋予SDK更复杂的计算，在设备端就转化为形成统一的数据模型来进行传送 Real Time Data：实时数据缓存区，这部分是为了达到实时计算的目的，海量数据接收不可能海量实时入历史数据库，那样会出现建立索引延迟、历史数据碎片文件等问题。因此，有一个实时数据缓存区来存储最近几分钟或者几秒钟的数据。这块可以使用Kudu或者Hbase等组件来实现。这部分数据会通过Dumper来合并到历史数据当中。此处的数据模型和SDK端数据模型是保持一致的，都是Common Data Model，例如“主-谓-宾”模型 Historical Data：历史数据沉浸区，这部分是保存了大量的历史数据，为了实现Ad-hoc查询，将自动建立相关索引提高整体历史数据查询效率，从而实现秒级复杂查询百亿条数据的反馈。例如可以使用HDFS存储历史数据，此处的数据模型依然SDK端数据模型是保持一致的Common Data Model Dumper：Dumper的主要工作就是把最近几秒或者几分钟的实时数据，根据汇聚规则、建立索引，存储到历史存储结构当中，可以使用map reduce、C、Scala来撰写，把相关的数据从Realtime Data区写入Historical Data区 Query Engine：查询引擎，提供统一的对外查询接口和协议（例如SQL JDBC），把Realtime Data和Historical Data合并到一起查询，从而实现对于数据实时的Ad-hoc查询。例如常见的计算引擎可以使用presto、impala、clickhouse等 Realtime model feedback：通过Edge computing技术，在边缘端有更多的交互可以做，可以通过在Realtime Data去设定规则来对Edge SDK端进行控制","categories":[{"name":"BigData","slug":"bigdata","permalink":"https://blog.mhuig.top/categories/bigdata/"}],"tags":[{"name":"BigData","slug":"bigdata","permalink":"https://blog.mhuig.top/tags/bigdata/"}]},{"title":"离散世界与连续世界的联系","slug":"math/离散世界与连续世界的联系","date":"2021-01-02T12:34:44.000Z","updated":"2021-01-02T12:34:44.000Z","comments":true,"path":"posts/2b995a0c.html","link":"","permalink":"https://blog.mhuig.top/posts/2b995a0c.html","excerpt":"","text":"探索当年，哥德巴赫异想天开就想离散的世界的阶乘能不能用连续世界的积分来表达。 哥德巴赫有个好朋友叫伯努利，伯努利有个学生叫欧拉。欧拉有个学生叫拉格朗日，拉格朗日有个学生叫柯西 哥德巴赫与伯努利交流，问伯努利：有没有离散世界和连续世界可以相等的呢？ 伯努利想不明白，就问他的学生欧拉，然后欧拉一晚上想出来了。故事结束 伽玛函数 换元，令 则 推导建立递推式 由于 得到： 当时, 当时, 如： 又如： 数学归纳可得： 参考文献Γ函数 wikipedia","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"}],"tags":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"}]},{"title":"一大波题目正在来袭","slug":"math/一大波题目正在来袭","date":"2020-12-06T13:33:18.000Z","updated":"2020-12-06T13:33:18.000Z","comments":true,"path":"posts/bc43343e.html","link":"","permalink":"https://blog.mhuig.top/posts/bc43343e.html","excerpt":"","text":"","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"}],"tags":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"}]},{"title":"回顾几个有趣的小题目","slug":"math/回顾几个有趣的小题目","date":"2020-11-16T13:08:56.000Z","updated":"2020-11-16T13:08:56.000Z","comments":true,"path":"posts/ce5b71c0.html","link":"","permalink":"https://blog.mhuig.top/posts/ce5b71c0.html","excerpt":"","text":"One 设,满足,. (1)证明存在，并求其值。 (2)求. 我们称叫做递推式，也叫迭代式。 上一步的函数值是下一步的自变量，如此循环往复。 迭代过程： 绘制图像 相当于在两条曲线的夹缝中求生存，最终的极限值趋近于0. 数列、、到、单调减少，且有下界0. (1)用数学归纳法证明有界. 1.验证. 2.设. 3.则. 于是有下界0. 且显然. 由单调有界准则,存在,记为. 由,得,解得,于是. (2) . 由于存在，由归结原则:. Two (1)证明方程在内有唯一实根； (2)对于(1)中的，任取,定义,证明. 是超越方程，只能求得数值解，不能求得解析解，交点就在那里，可是就是不知道它是几. 相当于在两条曲线的夹缝中求生存，最终的极限值趋近于. 数列、、到、单调减少，且有下界. Three 设,,,证明存在且其极限是方程的根. 是超越方程解不出解析解，交点就在那里，可是就是不知道它是几。 (1)证在内有唯一实根。 令,则,. 且. .单调递减. . 唯一 (2)证. 构造. 由拉格朗日中值定理: 数学归纳 由于 故连续放缩得 于是且有界. 故. . 即 Four 设,,,.若存在，求的取值范围.数学归纳 1.. 2.设. 3.则 单调增加。 若存在，记为 . 于是有交点. 函数值相同导数值斜率相同 故 时,有交点. 又时, 1. 2.设 3.则. 有上界. 综上，当时,存在，且值为的根. [注] (1)当时，有2个交点. eg.当时，,. (2) 当在怎样的正数取值范围内取值时，曲线和直线必相交？ 曲线和直线相交的充要条件是存在，使得,即 即属于的值域.由于=0.故只需求出的最大值则的取值范围就是. 可得唯一驻点.当时,当时, 时为在内的最大值. 由此，曲线和直线相交的充要条件是满足.","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"}],"tags":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"}]},{"title":"宇宙的本质是计算","slug":"pen/宇宙的本质是计算","date":"2020-11-06T00:35:48.000Z","updated":"2020-11-06T00:35:48.000Z","comments":true,"path":"posts/92a4ae9.html","link":"","permalink":"https://blog.mhuig.top/posts/92a4ae9.html","excerpt":"","text":"如果认为物理学家的任务是发现自然是什么，那就错了，物理学家关心的是我们关于自然能说点什么。 —— 尼尔斯•玻尔 想象一下，一望无际的大平面被分成了许许多多方格子。每个格子里正好能放下一个“细胞”。这个细胞不能运动，它可以是死的，也可以是活的；但它的状态，是由它周围8个细胞的死活决定。 规则至于决定的规则，在这个例子里只有这么几条： “人口过少”：任何活细胞如果活邻居少于2个，则死掉。 “正常”：任何活细胞如果活邻居为2个或3个，则继续活。 “人口过多”：任何活细胞如果活邻居大于3个，则死掉。 “繁殖”：任何死细胞如果活邻居正好是3个，则活过来。 产物而下面这几张图，全是遵循这几条简单规则的产物。 “脉冲星”它的周期为3，看起来像一颗周期爆发的星星。 “滑翔者”每4个回合“它”会向右下角走一格。虽然细胞早就是不同的细胞了，但它能保持原本的形态。 “轻量级飞船”它的周期是4，每2个回合会向右边走一格。 “滑翔者枪”它会不停地释放出一个又一个滑翔者。 “繁殖者”它会向右行进，留下一个接一个的“滑翔者枪”。动图最后一帧定格时用三种颜色区分了繁殖者本体、滑翔者枪和它们打出来的滑翔者。 Game Of Life上面这几条规则别名“生命游戏”，可能是最出名的一套规则组。 最早研究细胞自动机的科学家是冯·诺伊曼，后来康韦发明了上面展示的这个最有趣的细胞自动机程序：《生命游戏》，而wolfram则详尽的讨论了一维世界中的细胞自动机的所有情况 这个游戏被许多计算机程序实现了。Unix世界中的许多Hacker喜欢玩这个游戏，他们用字符代表一个细胞，在一个计算机屏幕上进行演化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import numpy as npimport matplotlib.pyplot as plt class GameOfLife(object): def __init__(self, cells_shape): &quot;&quot;&quot; Parameters ---------- cells_shape : 一个元组，表示画布的大小。 Examples -------- 建立一个高20，宽30的画布 game = GameOfLife((20, 30)) &quot;&quot;&quot; # 矩阵的四周不参与运算 self.cells = np.zeros(cells_shape) real_width = cells_shape[0] - 2 real_height = cells_shape[1] - 2 self.cells[1:-1, 1:-1] = np.random.randint(2, size=(real_width, real_height)) self.timer = 0 self.mask = np.ones(9) self.mask[4] = 0 def update_state(self): &quot;&quot;&quot;更新一次状态&quot;&quot;&quot; buf = np.zeros(self.cells.shape) cells = self.cells for i in range(1, cells.shape[0] - 1): for j in range(1, cells.shape[0] - 1): # 计算该细胞周围的存活细胞数 neighbor = cells[i-1:i+2, j-1:j+2].reshape((-1, )) neighbor_num = np.convolve(self.mask, neighbor, &#x27;valid&#x27;)[0] if neighbor_num == 3: buf[i, j] = 1 elif neighbor_num == 2: buf[i, j] = cells[i, j] else: buf[i, j] = 0 self.cells = buf self.timer += 1 def plot_state(self): &quot;&quot;&quot;画出当前的状态&quot;&quot;&quot; plt.title(&#x27;Iter :&#123;&#125;&#x27;.format(self.timer)) plt.imshow(self.cells) plt.show() def update_and_plot(self, n_iter): &quot;&quot;&quot;更新状态并画图 Parameters ---------- n_iter : 更新的轮数 &quot;&quot;&quot; plt.ion() for _ in range(n_iter): plt.title(&#x27;Iter :&#123;&#125;&#x27;.format(self.timer)) plt.imshow(self.cells) self.update_state() plt.pause(0.2) plt.ioff() if __name__ == &#x27;__main__&#x27;: game = GameOfLife(cells_shape=(60, 60)) game.update_and_plot(200) 这个游戏可以在这里玩~ https://playgameoflife.com/ https://funnyjs.com/jspages/game-of-life.html 规律蝴蝶扇动翅膀，引起大洋彼岸的风暴。 简单的底层逻辑，导致了纷繁复杂的生命现象。 微观遵循简单的逻辑， 而宏观上会表现出纷繁复杂的现象。 如此简单的程序能生成如此复杂的行为，这意味着什么？沃尔夫勒姆认为，这正是我们宇宙的本质；我们的世界就是计算，是一套简单的规则生成的复杂现象。 附：xkcd的一幅漫画。也许我们的宇宙就是细胞自动机的计算结果呢。 以上漫画译自 https://xkcd.com/505/ ，并以 Creative Commons Attribution-NonCommercial 2.5 License 许可证发布。 最后，漫画里有一格的背景是黑的，一个粒子上有两个（一样的）二进制数指着它。如果你计算一下它俩的十进制….. The Answer to Life，the Universe and Everything is 42. Why is 42 ? 请用Python或者其他高级语言执行一下类似如下命令12&gt;&gt;&gt; ord(&#x27;*&#x27;)42简单地说，42是*的ASCII码，*代表什么？就是Life, the Universe, and Everything啊.","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"互联网进化！","slug":"pen/互联网进化！","date":"2020-11-05T23:00:00.000Z","updated":"2020-11-05T23:00:00.000Z","comments":true,"path":"posts/9feab2fa.html","link":"","permalink":"https://blog.mhuig.top/posts/9feab2fa.html","excerpt":"","text":"简介《互联网进化论》书中提出”互联网的未来功能和结构将于人类大脑高度相似,也将具备互联网虚拟感觉,虚拟运动,虚拟中枢,虚拟运动神经系统”,并绘制了一幅互联网虚拟大脑结构图. 云计算是互联网的核心硬件层和核心软件层的集合,也是互联网中枢神经系统的萌芽. 大数据代表了互联网的信息层(数据海洋),是互联网智慧和意识产生的基础. 物联网,传感器在源源不断的向互联网大数据层汇集数据. 疑问：互联网的进化显示生命进化的方向性达尔文进化论认为生物进化并不是从低级到高级的进化，进化没有预定的方向；生物进化是自然选择的结果。 达尔文进化论动摇了神学的土壤和基础，但互联网的进化却可能引发神秘但有趣的问题——互联网和人脑为什么向同一方向进化？ 互联网的未来结构是人类的大脑结构，互联网的每一个创新都是对数万年前已经存在人脑功能的模仿。科学实验证明大脑中也经拥有Google一样的搜索引擎，Facebook一样的SNS系统，IPv4一样的地址编码系统，思科一样的路由系统。 40多年来人类从不同的方向在互联网领域进行创新，并没有统一的规划将互联网建造成什么结构，但有一天人类抬起头来观看自己的产品，将发现这个产品与大脑的结构高度相似，而且可以作为揭开大脑之谜的钥匙。这是一个非常奇特的现象。 终极结论“看不见的手”像幽灵一样盘踞在人类社会的发展过程中，时隐时现，如果说社会学、经济学还只是模糊的看到这只手的影子，那么互联网的进化有可能第一次把“这只看不见的手”逼到科学的解剖刀下。如何解剖它，那需要未来更多的研究者思考和实践，相信这个秘密的解开将会给人类带来重大而深远的影响 。 互联网，宇宙和大脑的关系是互联网进化论的终极结论：互联网将宇宙和大脑结合在一起，结构无限逼近人脑结构，空间上无限逼近宇宙边缘，在无穷时间点，宇宙，大脑，和互联网三者将合为一体，进化成为宇宙大脑或智慧宇宙。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"BERT预训练模型及其应用案例","slug":"nlp/BERT预训练模型及其应用案例","date":"2020-11-05T07:47:27.000Z","updated":"2020-11-05T07:47:27.000Z","comments":true,"path":"posts/48f700eb.html","link":"","permalink":"https://blog.mhuig.top/posts/48f700eb.html","excerpt":"预训练模型最开始是在图像领域提出的，获得了良好的效果，近几年才被广泛应用到自然语言处理各项任务中。 (1)2003年Bengio提出神经网络语言模型NNLM，从此统一了NLP的特征形式——Embedding； (2)2013年Mikolov提出词向量Word2vec，延续NNLM又引入了大规模预训练（Pretrain）的思路； (3)2017年Vaswani提出Transformer模型，实现用一个模型处理多种NLP任务。 (4) 基于Transformer架构，2018年底开始出现一大批预训练语言模型(3个预训练代表性模型BERT[2018]、XLNet[2019]和MPNet[2020])，刷新众多NLP任务，形成新的里程碑事件。","text":"预训练模型最开始是在图像领域提出的，获得了良好的效果，近几年才被广泛应用到自然语言处理各项任务中。 (1)2003年Bengio提出神经网络语言模型NNLM，从此统一了NLP的特征形式——Embedding； (2)2013年Mikolov提出词向量Word2vec，延续NNLM又引入了大规模预训练（Pretrain）的思路； (3)2017年Vaswani提出Transformer模型，实现用一个模型处理多种NLP任务。 (4) 基于Transformer架构，2018年底开始出现一大批预训练语言模型(3个预训练代表性模型BERT[2018]、XLNet[2019]和MPNet[2020])，刷新众多NLP任务，形成新的里程碑事件。 预训练模型的应用通常分为两步: 第一步：在计算性能满足的情况下用某个较大的数据集训练出一个较好的模型。 第二步：根据不同的任务，改造预训练模型，用新任务的数据集在预训练模型上进行微调。 预训练模型的好处是训练代价较小，配合下游任务可以实现更快的收敛速度，并且能够有效地提高模型性能，尤其是对一些训练数据比较稀缺的任务。换句话说，预训练方法可以认为是让模型基于一个更好的初始状态进行学习，从而能够达到更好的性能。 要讲自然语言的预训练，得先从图像领域的预训练说起。 图像领域的预训练 设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。 之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”; 另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。 对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构。 如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。 正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。 而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。 一般我们用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好。 Word Embedding现有的机器学习方法往往无法直接处理文本数据，因此需要找到合适的方法，将文本数据转换为数值型数据，由此引出了Word Embedding的概念，Word Embedding算法携带了语义信息且维度经过压缩便于运算。 语言模型 为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面单词的概率大小。 神经网络语言模型 NNLM是从语言模型出发(即计算概率角度)，构建神经网络针对目标函数对模型进行最优化，训练的起点是使用神经网络去搭建语言模型实现词的预测任务，并且在优化过程后模型的副产品就是词向量。 Word2Vec2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove。 Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词； 第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。 使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding。 Word Embedding的使用 我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。 句子中每个单词以Onehot形式作为输入，然后乘以Word Embedding矩阵Q，就直接取出单词对应的Word Embedding。 使用Word Embedding等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化。 这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。 下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是使用新的训练集合训练，在训练过程中，更新Word Embedding这层参数。 Word Embedding的问题 是多义词问题。多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。 多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。 从Word Embedding到ELMOELMO是“Embedding from Language Models”的简称。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变。 ELMO的本质思想是：事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身的思路是根据当前上下文对Word Embedding动态调整。 ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。 使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。 上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。 从Word Embedding到GPT GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。 TransformerTransformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器。 Transformer 是一种基于 encoder-decoder 结构的模型. 在机器翻译任务上的表现超过了 RNN，CNN，只用 encoder-decoder 和 attention 机制就能达到很好的效果，最大的优点是可以高效地并行化。 自注意力机制模型人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。 这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段. 深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。 Attention在同一个英语句子内单词间产生的联系。 Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图展示的有一定距离的短语结构）或者语义特征（比如图展示的its的指代对象Law）。 很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。 SelfAttention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，SelfAttention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。 GPT如何使用 把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。 从GPT和ELMO及word2Vec到Bert Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。 BERT本质上是一个自编码（Auto Encoder）语言模型，为了能见多识广，BERT使用3亿多词语训练，采用12层双向Transformer架构。注意，BERT只使用了Transformer的编码器部分，可以理解为BERT旨在学习庞大文本的内部语义信息。 具体训练目标之一，是被称为掩码语言模型的MLM。即输入一句话，给其中15%的字打上“mask”标记，经过Embedding输入和12层Transformer深度理解，来预测“mask”标记的地方原本是哪个字。 12input: 欲把西[mask]比西子，淡[mask]浓抹总相宜output: 欲把西[湖]比西子，淡[妆]浓抹总相宜 例如我们输入“欲把西[mask]比西子，淡[mask]浓抹总相宜”给BERT，它需要根据没有被“mask”的上下文，预测出掩盖的地方是“湖”和“妆”。 MLM任务的灵感来自于人类做完形填空。挖去文章中的某些片段，需要通过上下文理解来猜测这些被掩盖位置原先的内容。 训练目标之二，是预测输入的两句话之间是否为上下文（NSP）的二分类问题。继续输入“ 欲把西[湖]比西子，淡[妆]浓抹总相宜”，BERT将预测这两句话的组合是否合理（这个例子是“yes”）。（随后的研究者对预训练模型探索中证明，NSP任务过于简单，对语言模型的训练作用并不是很大） 通过这两个任务和大规模语料训练，BERT语言模型可以很好学习到文本之间的蕴含的关系。 NLP的四大任务 绝大部分NLP问题可以归入上图所示的四类任务中： 一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。 第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。 第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系； 第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。 根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。 BERT的应用案例下载bert预训练模型Google - BERT源码https://github.com/google-research/bert下载预训练模型。 我这里选择中文的BERT，下载解压后的目录如下： 安装bert-as-service顾名思义，将BERT模型直接封装成一个服务，堪称上手最快的BERT工具。作者是肖涵博士。 使用pip安装： 12pip install bert-serving-server # serverpip install bert-serving-client # client, independent of `bert-serving-server 开启BERT service 1bert-serving-start -model_dir E:\\nlp\\chinese_L-12_H-768_A-12 使用客户端获取句子编码 案例一 查找最相近的句子根据bert获取句子向量，并计算出句子之间的余弦相似度，找出最相似的句子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 导入bert客户端from bert_serving.client import BertClientimport numpy as npclass SimilarModel: def __init__(self): self.bert_client = BertClient() def close_bert(self): self.bert_client .close() def get_sentence_vec(self,sentence): &#x27;&#x27;&#x27; 根据bert获取句子向量 :param sentence: :return: &#x27;&#x27;&#x27; return self.bert_client .encode([sentence])[0] def cos_similar(self,sen_a_vec, sen_b_vec): &#x27;&#x27;&#x27; 计算两个句子的余弦相似度 :param sen_a_vec: :param sen_b_vec: :return: &#x27;&#x27;&#x27; vector_a = np.mat(sen_a_vec) vector_b = np.mat(sen_b_vec) num = float(vector_a * vector_b.T) denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b) cos = num / denom return cosif __name__==&#x27;__main__&#x27;: # 从候选集condinates 中选出与sentence_a 最相近的句子 condinates = [&#x27;为什么天空是蔚蓝色的&#x27;,&#x27;太空为什么是黑的？&#x27;,&#x27;天空怎么是蓝色的&#x27;,&#x27;明天去爬山如何&#x27;] sentence_a = &#x27;天空为什么是蓝色的&#x27; bert_client = SimilarModel() max_cos_similar = 0 most_similar_sentence = &#x27;&#x27; for sentence_b in condinates: sentence_a_vec = bert_client.get_sentence_vec(sentence_a) sentence_b_vec = bert_client.get_sentence_vec(sentence_b) cos_similar = bert_client.cos_similar(sentence_a_vec,sentence_b_vec) if cos_similar &gt; max_cos_similar: max_cos_similar = cos_similar most_similar_sentence = sentence_b print(&#x27;最相似的句子：&#x27;,most_similar_sentence) bert_client .close_bert() # 为什么天空是蔚蓝色的 案例二 简单模糊搜索将问题编码为向量： 12from bert_serving.client import BertClientdoc_vecs = bc.encode(questions) 最后，我们准备接收新查询并针对现有问题执行简单的“模糊”搜索。为此，每次出现新查询时，我们都将其编码为向量，并使用来计算其点积doc_vecs。将结果递减排序；并返回前k个类似的问题，如下所示： 123456789while True: query = input(&#x27;your question: &#x27;) query_vec = bc.encode([query])[0] # compute normalized dot product as score score = np.sum(query_vec * doc_vecs, axis=1) / np.linalg.norm(doc_vecs, axis=1) topk_idx = np.argsort(score)[::-1][:topk] print(&#x27;top %d questions similar to &quot;%s&quot;&#x27; % (topk, query)) for idx in topk_idx: print(&#x27;&gt; %s\\t%s&#x27; % (&#x27;%.1f&#x27; % score[idx], questions[idx])) 现在运行代码并键入查询，查看此搜索引擎如何处理模糊匹配： 案例三 法条推荐介绍根据刑事法律文书中的案情描述和事实部分，预测本案涉及的相关法条； 数据说明所使用的数据集是来自“中国裁判文书网”公开的刑事法律文书，其中每份数据由法律文书中的案情描述和事实部分组成，同时也包括每个案件所涉及的法条、被告人被判的罪名和刑期长短等要素。 数据集共包括268万刑法法律文书，共涉及202条罪名，183条法条，刑期长短包括0-25年、无期、死刑。 数据利用json格式储存，每一行为一条数据，每条数据均为一个字典。 fact: 事实描述 meta: 标注信息，标注信息中包括: criminals: 被告(数据中均只含一个被告) punish_of_money: 罚款(单位：元) accusation: 罪名 relevant_articles: 相关法条 term_of_imprisonment: 刑期 刑期格式(单位：月) death_penalty: 是否死刑 life_imprisonment: 是否无期 imprisonment: 有期徒刑刑期 这里是简单的一条数据展示: 123456789101112131415&#123; &quot;fact&quot;: &quot;2015年11月5日上午，被告人胡某在平湖市乍浦镇的嘉兴市多凌金牛制衣有限公司车间内，与被害人孙某因工作琐事发生口角，后被告人胡某用木制坐垫打伤被害人孙某左腹部。经平湖公安司法鉴定中心鉴定：孙某的左腹部损伤已达重伤二级。&quot;, &quot;meta&quot;: &#123; &quot;relevant_articles&quot;: [234], &quot;accusation&quot;: [&quot;故意伤害&quot;], &quot;criminals&quot;: [&quot;胡某&quot;], &quot;term_of_imprisonment&quot;: &#123; &quot;death_penalty&quot;: false, &quot;imprisonment&quot;: 12, &quot;life_imprisonment&quot;: false &#125; &#125;&#125; 实现流程进入 OpenCLaP 下载刑事文书BERT，并运行bert-as-service服务。 创建一个连接到BertServer的BertClient12from bert_serving.client import ConcurrentBertClientbc = ConcurrentBertClient() 获取编码向量和标签12345678def get_encodes(x): # x 是 batch_size 大小的行 ，每行都是一个json对象 samples = [json.loads(l) for l in x] # 一个 batch_size 大小的连续样本 text = [s[&#x27;fact&#x27;][:50] + s[&#x27;fact&#x27;][-50:] for s in samples] # 获取案情描述和事实部分文字 features = bc.encode(text) # 使用bert将字符串列表编码为向量列表 # 随机选择一个标签 labels = [[str(random.choice(s[&#x27;meta&#x27;][&#x27;relevant_articles&#x27;]))] for s in samples] return features, labels 构建TensorFlow DNN 模型的分类器123456789estimator = DNNClassifier( hidden_units=[512], # 每层隐藏单元的 Iterable 数.所有层都完全连接. feature_columns=[tf.feature_column.numeric_column(&#x27;feature&#x27;, shape=(768,))], # 包含模型使用的所有特征列的iterable.集合中的所有项目都应该是从 _FeatureColumn 派生的类的实例. n_classes=len(laws), # 标签类的数量.默认为 2,即二进制分类,必须大于1. config=run_config, # RunConfig 对象配置运行时设置 label_vocabulary=laws_str, # 字符串列表,表示可能的标签值.如果给定,标签必须是字符串类型,并且 label_vocabulary 具有任何值.如果没有给出,这意味着标签已经被编码为整数或者在[0,1]内浮动, n_classes=2 ；并且被编码为&#123;0,1,...,n_classes-1&#125;中的整数值,n_classes&gt; 2.如果没有提供词汇表并且标签是字符串,也会出现错误. optimizer=tf.train.AdamOptimizer(),# 优化函数 dropout=0.1 # 当不是 None 时,我们将放弃给定坐标的概率. ) 训练和评估1234567891011121314151617# 输入函数input_fn = lambda fp: (tf.data.TextLineDataset(fp) # TextLineDataset接口提供了一种方法从数据文件中读取。只需要提供文件名（1个或者多个）。这个接口会自动构造一个dataset，类中保存的元素：文中一行，就是一个元素，是string类型的tenser。 # map将分别对Dataset的每个元素执行一个函数，而apply将立即对整个Dataset执行一个函数 .apply(tf.contrib.data.shuffle_and_repeat(buffer_size=10000)) # repeat重复和shuffle重排 tf.data.Dataset.repeat 转换会将输入数据重复有限（或无限）次；每次数据重复通常称为一个周期。tf.data.Dataset.shuffle 转换会随机化数据集样本的顺序。 # 将此数据集的连续元素合并为批。 .batch(batch_size) # tf.py_func()接收的是tensor，然后将其转化为numpy array送入我们自定义的get_encodes函数，最后再将get_encodes函数输出的numpy array转化为tensor返回 .map(lambda x: tf.py_func(get_encodes, [x], [tf.float32, tf.string], name=&#x27;bert_client&#x27;), num_parallel_calls=num_parallel_calls) .map(lambda x, y: (&#123;&#x27;feature&#x27;: x&#125;, y)) .prefetch(20)) # 创建一个从该数据集中预提取元素的Dataset 大多数数据集输入管道应以调用结束prefetch。这允许在处理当前元素时准备以后的元素。这通常会提高延迟和吞吐量，但以使用额外的内存存储预取元素为代价。# TrainSpec确定训练的输入数据以及持续时间train_spec = TrainSpec(input_fn=lambda: input_fn(train_fp))# EvalSpec结合了训练模型的计算和输出的详细信息.计算由计算指标组成,用以判断训练模型的性能.输出将训练好的模型写入外部存储.eval_spec = EvalSpec(input_fn=lambda: input_fn(eval_fp), throttle_secs=0) # 第一次评估发生在throttle_secs秒后# 训练和评估train_and_evaluate(estimator, train_spec, eval_spec) 运行tensorboard可视化训练过程1tensorboard --logdir=law-model 案例四 互联网新闻情感分析介绍对新闻情绪进行分类，0代表正面情绪、1代表中性情绪、2代表负面情绪。 数据说明 Field Type Description id String 新闻ID News ID text String 新闻正文内容 Content of news text label String 新闻情感标签 Emotional label in news 实现流程加载数据集123456789101112131415def load_dataset(filepath): dataset_list = [] f = open(filepath, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) r = csv.reader(f) for item in r: if r.line_num == 1: continue dataset_list.append(item) # 空元素补0 for item in dataset_list: if item[1].strip() == &#x27;&#x27;: item[1] = &#x27;0&#x27; return dataset_list 网络类，全连接层123456789101112131415161718class Net(nn.Module): # in_dim=768, out_dim=3 def __init__(self, in_dim, out_dim): super(Net, self).__init__() self.linear1 = nn.Linear(in_dim, 500) self.linear2 = nn.Linear(500, 400) self.linear3 = nn.Linear(400, 300) self.linear4 = nn.Linear(300, 200) self.linear5 = nn.Linear(200, out_dim) def forward(self, x): x = self.linear1(x) x = self.linear2(x) x = self.linear3(x) x = self.linear4(x) x = self.linear5(x) return x 计算每个batch的准确率123456def batch_accuracy(pre, label): pre = pre.argmax(dim=1) correct = torch.eq(pre, label).sum().float().item() accuracy = correct / float(len(label)) return accuracy 训练123456789101112131415161718192021222324for epoch in range(EPOCHS): step = -1 for text, label in train_loader: # tuple转list text = list(text) label = list(label) label = list(map(int, label)) # 使用中文bert，生成句向量 sen_vec = bertclient.encode(text) sen_vec = torch.tensor(sen_vec) label = torch.LongTensor(label) label = label.cuda() # 输入到网络中，反向传播 pre = net(sen_vec).cuda() loss = criterion(pre, label) optimizer.zero_grad() loss.backward() optimizer.step() # 更新loss曲线，并计算准确率 step = step + 1 flag = flag + 1 if step % 100 == 0: acc = batch_accuracy(pre, label) print(&#x27;epoch:&#123;&#125; | batch:&#123;&#125; | acc:&#123;&#125; | loss:&#123;&#125;&#x27;.format(epoch, step, acc, loss.item())) 测试1234567891011121314151617net.load_state_dict(torch.load(&#x27;net.pt&#x27;))test_result = []for item in test_dataset: sen_vec = bertclient.encode([item[1]]) sen_vec = torch.tensor(sen_vec) with torch.no_grad(): pre = net(sen_vec).cuda() pre = pre.argmax(dim=1) pre = pre.item() test_result.append([item[0], pre]) # 写入csv文件 df = pd.DataFrame(test_result) df.to_csv(&#x27;test_result.csv&#x27;,index=False, header=[&#x27;id&#x27;, &#x27;label&#x27;]) 训练可视化 控制台输出 参考文献从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 Transformer 深度学习中的注意力模型 谷歌的bert 肖涵博士的bert-as-service Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ CAIL2018数据集 刑事文书BERT: tensorboard使用详解 Visdom可视化工具 互联网新闻情感分析","categories":[{"name":"NLP","slug":"nlp","permalink":"https://blog.mhuig.top/categories/nlp/"}],"tags":[{"name":"NLP","slug":"nlp","permalink":"https://blog.mhuig.top/tags/nlp/"}]},{"title":"自然语言处理研究报告[存档备用]","slug":"nlp/自然语言处理研究报告[存档备用]","date":"2020-10-20T06:00:59.000Z","updated":"2020-10-20T06:00:59.000Z","comments":true,"path":"posts/618ec98d.html","link":"","permalink":"https://blog.mhuig.top/posts/618ec98d.html","excerpt":"","text":"NLP","categories":[{"name":"NLP","slug":"nlp","permalink":"https://blog.mhuig.top/categories/nlp/"}],"tags":[{"name":"NLP","slug":"nlp","permalink":"https://blog.mhuig.top/tags/nlp/"}]},{"title":"尝试使用GPU加速计算","slug":"data-mining/尝试使用GPU加速计算","date":"2020-10-11T01:11:44.000Z","updated":"2020-10-11T01:11:44.000Z","comments":true,"path":"posts/56213be8.html","link":"","permalink":"https://blog.mhuig.top/posts/56213be8.html","excerpt":"大规模训练，gpu和cpu速度差别很大。","text":"大规模训练，gpu和cpu速度差别很大。 概述GPU CPU (Central Processing Unit) 即中央处理器。 GPU (Graphics Processing Unit) 即图形处理器。 当程序员为CPU编写程序时，他们倾向于利用复杂的逻辑结构优化算法从而减少计算任务的运行时间，即Latency。当程序员为GPU编写程序时，则利用其处理海量数据的优势，通过提高总的数据吞吐量（Throughput）来掩盖Lantency。 其中绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元。 首先你需要硬件支持，一块能够支持GPU加速计算的显卡，这里以NVIDIA的GPU为例。 CUDACUDA（Compute Unified Device Architecture），是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。 CUDA提供了一种可扩展的编程模型，使得已经写好的CUDA代码可以在任意数量核心的GPU上运行。只有运行时，系统才知道物理处理器的数量。 CUDNNNVIDIA cuDNN是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。NVIDIA cuDNN可以集成到更高级别的机器学习框架中，如加州大学伯克利分校的流行CAFFE软件。简单的，插入式设计可以让开发人员专注于设计和实现神经网络模型，而不是调整性能，同时还可以在GPU上实现高性能现代并行计算。 支持GPU的计算框架Tensorflow-GPU安装教程参考 https://tensorflow.google.cn/install 需要注意的是 严格对应 tensorflow_gpu、Python、 编译器、 cuDNN、CUDA 的版本关系。 相关对应关系windows平台可参考: Tensorflow 文档 显卡驱动与Cuda版本之间的对应关系 cuda与cudnn版本之间的对应关系 打开NVIDIA控制面板进入系统信息，可查看当前支持的CUDA驱动版本。 这里的CUDA驱动版本是指你只可以安装该版本及以下版本的CUDA。 根据实际情况，笔者计划使用的环境是： tensorflow_gpu-1.14.0 python 3.7 MSVC 2017 cuDNN 7.4.2.24 CUDA 10.0.130 安装CUDA进入cuda-toolkit-archive选择需要的CUDA Toolkit版本下载安装即可。 笔者选择network安装方式。注意第一个路径是选择临时解压路径。选择自定义安装，取消安装不需要的应用，如NVIDIA GeForce Experience。 在cmd中执行： 1nvcc -V 需要注意配置相关环境变量。 安装CUDNN进入cudnn-archive选择需要的CUDA Toolkit版本下载解压即可。 笔者将CUDA默认安装在C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0 将CUDNN解压到C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0中即可。 或者配置单独的环境变量。 安装Tensorflow-GPU如果之前安装过cpu版本的Tensorflow需要进行卸载. 若在虚拟环境中使用conda安装，conda会自动安装相关的cuda和cudnn依赖。 1conda install tensorflow-gpu 下面给出一段GPU测试程序： 1234567891011121314import timeimport tensorflow as tfbegin = time.time()with tf.device(&#x27;/gpu:0&#x27;): rand_t = tf.random_uniform([50,50],0,10,dtype=tf.float32,seed=0) a = tf.Variable(rand_t) b = tf.Variable(rand_t) c = tf.matmul(a,b) init = tf.global_variables_initializer()sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) #强制使用GPUsess.run(init)print(sess.run(c))end = time.time()print(end-begin,&#x27;s&#x27;) 运行结果: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253542020-10-11 09:36:56.713905: I tensorflow&#x2F;core&#x2F;platform&#x2F;cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX22020-10-11 09:36:56.742138: I tensorflow&#x2F;stream_executor&#x2F;platform&#x2F;default&#x2F;dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll2020-10-11 09:36:57.193243: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_device.cc:1640] Found device 0 with properties:name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176pciBusID: 0000:01:00.02020-10-11 09:36:57.199952: I tensorflow&#x2F;stream_executor&#x2F;platform&#x2F;default&#x2F;dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.2020-10-11 09:36:57.204946: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_device.cc:1763] Adding visible gpu devices: 02020-10-11 09:36:58.622428: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:2020-10-11 09:36:58.626880: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_device.cc:1187] 02020-10-11 09:36:58.630073: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_device.cc:1200] 0: N2020-10-11 09:36:58.633910: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;gpu&#x2F;gpu_device.cc:1326] Created TensorFlow device (&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0 with 1382 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)Device mapping:&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0 -&gt; device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.02020-10-11 09:36:58.657797: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;direct_session.cc:296] Device mapping:&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0 -&gt; device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0random_uniform&#x2F;RandomUniform: (RandomUniform): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.683509: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] random_uniform&#x2F;RandomUniform: (RandomUniform)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0random_uniform&#x2F;sub: (Sub): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.690684: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] random_uniform&#x2F;sub: (Sub)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0random_uniform&#x2F;mul: (Mul): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.705871: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] random_uniform&#x2F;mul: (Mul)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0random_uniform: (Add): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.719880: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] random_uniform: (Add)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0Variable: (VariableV2): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.738611: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] Variable: (VariableV2)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0Variable&#x2F;Assign: (Assign): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.749906: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] Variable&#x2F;Assign: (Assign)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0Variable&#x2F;read: (Identity): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.759648: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] Variable&#x2F;read: (Identity)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0Variable_1: (VariableV2): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.769854: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] Variable_1: (VariableV2)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0Variable_1&#x2F;Assign: (Assign): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.776819: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] Variable_1&#x2F;Assign: (Assign)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0Variable_1&#x2F;read: (Identity): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.792460: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] Variable_1&#x2F;read: (Identity)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0MatMul: (MatMul): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.808352: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] MatMul: (MatMul)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0init: (NoOp): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.823256: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] init: (NoOp)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0random_uniform&#x2F;shape: (Const): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.838355: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] random_uniform&#x2F;shape: (Const)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0random_uniform&#x2F;min: (Const): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.853758: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] random_uniform&#x2F;min: (Const)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0random_uniform&#x2F;max: (Const): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:02020-10-11 09:36:58.868493: I tensorflow&#x2F;core&#x2F;common_runtime&#x2F;placer.cc:54] random_uniform&#x2F;max: (Const)&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:GPU:0[[1405.2429 1441.7413 1364.38 ... 1480.2251 1279.0061 1620.0938 ] [1232.6589 1344.4458 1169.7095 ... 1205.1284 1040.5566 1421.967 ] [1209.3164 1180.3206 1158.1396 ... 1200.0344 1014.03217 1222.5107 ] ... [1298.9648 1262.9236 1205.6918 ... 1396.479 1090.7253 1437.241 ] [1118.2473 1209.0151 1077.7229 ... 1180.7025 1076.4694 1139.742 ] [1200.8866 1297.2266 1260.01 ... 1289.4297 1165.2448 1433.4183 ]]2.6225805282592773 s 可以看到GPU已经在工作了。 同时可以在cmd中运行 nvidia-smi【C:\\Program Files\\NVIDIA Corporation\\NVSMInvidia-smi.exe】监控GPU使用情况和更改GPU状态。 Keras-GPUconda直接安装即可,注意需要先安装tensorflow-gpu。 1conda install keras-gpu Pytorch-GPU再也找不到比官网更详尽的文档了，请直接参考https://pytorch.org/。 1conda install pytorch torchvision cudatoolkit=10.0 GPU 测试程序： 12345678910import torchflag = torch.cuda.is_available()print(flag)ngpu= 1# Decide which device we want to run ondevice = torch.device(&quot;cuda:0&quot; if (torch.cuda.is_available() and ngpu &gt; 0) else &quot;cpu&quot;)print(device)print(torch.cuda.get_device_name(0))print(torch.rand(3,3).cuda()) 输出结果： 123456Truecuda:0GeForce GTX 960Mtensor([[0.0208, 0.2799, 0.4918], [0.0020, 0.1067, 0.8207], [0.5531, 0.0994, 0.2108]], device=&#x27;cuda:0&#x27;)","categories":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/categories/data-mining/"}],"tags":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/tags/data-mining/"},{"name":"Machine Learning","slug":"machine-learning","permalink":"https://blog.mhuig.top/tags/machine-learning/"}]},{"title":"R语言初步","slug":"data-mining/R语言初步","date":"2020-10-10T12:30:27.000Z","updated":"2020-10-10T12:30:27.000Z","comments":true,"path":"posts/fe073ab3.html","link":"","permalink":"https://blog.mhuig.top/posts/fe073ab3.html","excerpt":"尝试使用R语言进行数据处理","text":"尝试使用R语言进行数据处理 安装R进入 https://www.r-project.org 下载安装包即可. 安装包，载入包的命令安装包1install.packages(&quot;mlbench&quot;) 载入包12library(mlbench) 更新所有包1update.packages(ask=FALSE, checkBuilt=TRUE) 导入数据集1dataset &lt;- read.csv(&quot;C:\\\\R\\\\bin\\\\iris.csv&quot;, header=FALSE) 概要分析显示头10行1head(dataset,n=10) 1234567891011 V1 V2 V3 V4 V51 5.1 3.5 1.4 0.2 Iris-setosa2 4.9 3.0 1.4 0.2 Iris-setosa3 4.7 3.2 1.3 0.2 Iris-setosa4 4.6 3.1 1.5 0.2 Iris-setosa5 5.0 3.6 1.4 0.2 Iris-setosa6 5.4 3.9 1.7 0.4 Iris-setosa7 4.6 3.4 1.4 0.3 Iris-setosa8 5.0 3.4 1.5 0.2 Iris-setosa9 4.4 2.9 1.4 0.2 Iris-setosa10 4.9 3.1 1.5 0.1 Iris-setosa 显示数据集的每类样本所占比例1234library(mlbench)data(PimaIndiansDiabetes)y &lt;- PimaIndiansDiabetes$diabetescbind(freq=table(y), percentage=prop.table(table(y))*100) 123 freq percentageneg 500 65.10417pos 268 34.89583 每个属性的值的分布情况1summary(dataset) 1234567891011 V1 V2 V3 V4 Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.054 Mean :3.759 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 V5 Length:150 Class :character Mode :character 可视化数据属性箱线盒1234567# 加载数据集data(iris)# 为每个属性创建单独的框线图par(mfrow=c(1,4))for(i in 1:4) &#123;boxplot(iris[,i], main=names(iris)[i])&#125; 柱状图12345data(iris)par(mfrow=c(1,4))for(i in 1:4) &#123;hist(iris[,i], main=names(iris)[i])&#125; 曲线图123456library(lattice)data(iris)par(mfrow=c(1,4))for(i in 1:4) &#123;plot(density(iris[,i]), main=names(iris)[i])&#125; 条形图123456789library(mlbench)data(BreastCancer)# 为每个分类属性创建条形图par(mfrow=c(2,4))for(i in 2:9) &#123;counts &lt;- table(BreastCancer[,i])name &lt;- names(BreastCancer)[i]barplot(counts, main=name)&#125; 相关图12345678# 加载程序包library(corrplot)# 加载数据data(iris)# 计算相关性correlations &lt;- cor(iris[,1:4])# 创建相关图corrplot(correlations, method=&quot;circle&quot;) 散点图12345# 加载数据data(iris)# 按类别着色的成对散点图pairs(Species~., data=iris, col=iris$Species) 密度图123456789# 加载程序包library(caret)# 加载数据data(iris)# 按类值为每个属性绘制密度图x &lt;- iris[,1:4]y &lt;- iris[,5]scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;))featurePlot(x=x, y=y, plot=&quot;density&quot;, scales=scales)","categories":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/categories/data-mining/"}],"tags":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/tags/data-mining/"},{"name":"R","slug":"r","permalink":"https://blog.mhuig.top/tags/r/"}]},{"title":"手把手教你用Python开始第一个机器学习项目","slug":"data-mining/手把手教你用python开始第一个机器学习项目","date":"2020-10-10T11:43:22.000Z","updated":"2020-10-10T11:43:22.000Z","comments":true,"path":"posts/110850a.html","link":"","permalink":"https://blog.mhuig.top/posts/110850a.html","excerpt":"熟悉一个新的平台或者一个新的工具最好的方式就是从头到尾踏实的完成一个机器学习项目","text":"熟悉一个新的平台或者一个新的工具最好的方式就是从头到尾踏实的完成一个机器学习项目 这里以PimaIndiansdiabetes.csv 数据集为例。 Download Data Set PimaIndiansdiabetes.csv 数据集介绍 1、该数据集最初来自国家糖尿病/消化/肾脏疾病研究所。数据集的目标是基于数据集中包含的某些诊断测量来诊断性的预测 患者是否患有糖尿病。 2、从较大的数据库中选择这些实例有几个约束条件。尤其是，这里的所有患者都是Pima印第安至少21岁的女性。 3、数据集由多个医学预测变量和一个目标变量组成Outcome。预测变量包括患者的怀孕次数、BMI、胰岛素水平、年龄等。 4、数据集的内容是皮马人的医疗记录，以及过去5年内是否有糖尿病。所有的数据都是数字，问题是（是否有糖尿病是1或0），是二分类问题。数据有8个属性，1个类别： 【1】Pregnancies：怀孕次数 【2】Glucose：葡萄糖 【3】BloodPressure：血压 (mm Hg) 【4】SkinThickness：皮层厚度 (mm) 【5】Insulin：胰岛素 2小时血清胰岛素（mu U / ml ） 【6】BMI：体重指数 （体重/身高）^2 ） 【7】DiabetesPedigreeFunction：糖尿病谱系功能 【8】Age：年龄 （岁） 【9】Outcome：类标变量 （0或1） 数据可视化了解数据集的结构现在是时候看一下我们的数据集了。当前步骤中我们从不同的角度观察数据。 加载数据集首先import pandas模块调用read_csv方法加载数据集。 1234from pandas import read_csv# 加载数据集filename = &#x27;pima-indians-diabetes.csv&#x27;dataset = read_csv(filename, header=None) 显示数据实例个数、属性个数使用pandas中的shape方法查看数据集的维度特征，显示数据实例个数(行)、属性个数（列）。 123# 显示数据实例个数、属性个数dataset.shape#(768, 9) 看到有768个实例，9个属性。 前10个样本情况使用head 方法观察数据前10行。实际地仔细观察数据向来都是好办法。 12# 前10个样本情况dataset.head(10) 0 1 2 3 4 5 6 7 8 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 5 5 116 74 0 0 25.6 0.201 30 0 6 3 78 50 32 88 31.0 0.248 26 1 7 10 115 0 0 0 35.3 0.134 29 0 8 2 197 70 45 543 30.5 0.158 53 1 9 8 125 96 0 0 0.0 0.232 54 1 显示每个属性的统计概要看一下每个属性的统计概要。 这里包括总数，均值，std，最小值，最大值以及一些百分比。 12# 显示每个属性的统计概要（包括总数，均值，最小值，最大值以及一些百分比）dataset.describe() 0 1 2 3 4 5 6 7 8 count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 箱线盒图导入matplotlib，绘制每一个输入变量的箱线图。这能让我们更清晰的了解输入属性的分布情况。 1234from matplotlib import pyplot# 线盒图dataset.plot(kind=&#x27;box&#x27;)pyplot.show() 柱状图123# 柱状图dataset.hist()pyplot.show() 看起来输入变量中有3个可能符合高斯分布。这个现象值得注意，我们可以使用基于这个假设的算法。 多变量散点图看一下变量之间的相互关系，所有属性两两一组互相对比的散点图。这种图有助于我们定位输入变量间的结构性关系。 1234# 多变量散点图from pandas.plotting import scatter_matrixscatter_matrix(dataset)pyplot.show() 注意下图中某些属性两两比对时延对角线出现的分组现象。这其实表明高度的相关性和可预测关系。 交叉验证交叉验证的基本思想是把在某种意义下将原始数据进行分组,一部分做为训练集，另一部分做为验证集，首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型，以此来做为评价分类器的性能指标。 用交叉验证的目的是为了得到可靠稳定的模型。 对数据进行3、5、7交叉验证，比较结果。 十折交叉验证123456789101112131415161718192021222324252627282930313233343536373839# MLP for Pima Indians Dataset with 10-fold cross validation via sklearnfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifierfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.model_selection import cross_val_scoreimport numpy# Function to create model, required for KerasClassifierdef create_model(): # create model model = Sequential() model.add(Dense(12, input_dim=8, activation=&#x27;relu&#x27;)) model.add(Dense(8, activation=&#x27;relu&#x27;)) model.add(Dense(1, activation=&#x27;sigmoid&#x27;)) # Compile model model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=&#x27;adam&#x27;, metrics=[&#x27;accuracy&#x27;]) return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load pima indians datasetdataset = numpy.loadtxt(&quot;pima-indians-diabetes.csv&quot;, delimiter=&quot;,&quot;)# split into input (X) and output (Y) variablesX = dataset[:, 0:8]Y = dataset[:, 8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10)# evaluate using 10-fold cross validationkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 3 交叉验证1234# 3 交叉验证kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 5 交叉验证123kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 7 交叉验证1234# 7 交叉验证kfold = StratifiedKFold(n_splits=7, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 结果 交叉验证 Result 3 0.75 5 0.7252864837646484 7 0.7383295553071159 10 0.7382946014404297 改变神经网络结构加深，加宽，看看什么结构对模型性能有较大影响。 加深123456789101112131415161718192021222324252627# 改变神经网络结构，（加深，加宽），看看什么结构对模型性能有较大影响。# 加深def create_model_1(): # create model model = Sequential() model.add(Dense(12, input_dim=8, activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=12,activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=12,activation=&#x27;relu&#x27;)) model.add(Dense(12,input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(12,input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=12,activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=8,activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=8,activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=8,activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=8,activation=&#x27;relu&#x27;)) model.add(Dense(1, activation=&#x27;sigmoid&#x27;)) # Compile model model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=&#x27;adam&#x27;, metrics=[&#x27;accuracy&#x27;]) return modelmodel = KerasClassifier(build_fn=create_model_1, epochs=150, batch_size=10)kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 加宽12345678910111213141516171819# 改变神经网络结构，（加深，加宽），看看什么结构对模型性能有较大影响。# 加宽def create_model_2(): # create model model = Sequential() model.add(Dense(24, input_dim=8, activation=&#x27;relu&#x27;)) model.add(Dense(16, activation=&#x27;relu&#x27;)) model.add(Dense(1, activation=&#x27;sigmoid&#x27;)) # Compile model model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=&#x27;adam&#x27;, metrics=[&#x27;accuracy&#x27;]) return modelmodel = KerasClassifier(build_fn=create_model_2, epochs=150, batch_size=10)kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 加深加宽123456789101112131415161718192021222324252627282930313233# 改变神经网络结构，（加深，加宽），看看什么结构对模型性能有较大影响。# 加宽 加深def create_model_2(): # create model model = Sequential() model.add(Dense(24, input_dim=8, activation=&#x27;relu&#x27;)) model.add(Dense(24, input_dim=24, activation=&#x27;relu&#x27;)) model.add(Dense(24, input_dim=24, activation=&#x27;relu&#x27;)) model.add(Dense(24, input_dim=24, activation=&#x27;relu&#x27;)) model.add(Dense(24, input_dim=24, activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=24, activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(12, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(8, input_dim=12, activation=&#x27;relu&#x27;)) model.add(Dense(1, activation=&#x27;sigmoid&#x27;)) # Compile model model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=&#x27;adam&#x27;, metrics=[&#x27;accuracy&#x27;]) return modelmodel = KerasClassifier(build_fn=create_model_2, epochs=150, batch_size=10)kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 结果 操作 Resault 加深 0.7096354166666666 加宽 0.7057291666666666 加深加宽 0.7174479166666666 更深的模型，意味着更好的非线性表达能力，可以学习更加复杂的变换，从而可以拟合更加复杂的特征输入。 网络更深，每一层要做的事情也更加简单了。 上面就是网络加深带来的两个主要好处，更强大的表达能力和逐层的特征学习。 而宽度就起到了另外一个作用，那就是让每一层学习到更加丰富的特征，比如不同方向，不同频率的纹理特征。 可视化训练过程12345678910111213141516171819202122232425262728293031# 可视化训练过程（损失函数关系图，精确度关系图）import matplotlib.pyplot as plt# Fit the modelmodel = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10)history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)# list all data in historyprint(history.history.keys())# summarize history for accuracyplt.plot(history.history[&#x27;accuracy&#x27;])plt.plot(history.history[&#x27;val_accuracy&#x27;])plt.title(&#x27;model accuracy&#x27;)plt.ylabel(&#x27;accuracy&#x27;)plt.xlabel(&#x27;epoch&#x27;)plt.legend([&#x27;train&#x27;, &#x27;test&#x27;], loc=&#x27;upper left&#x27;)plt.show()# summarize history for lossplt.plot(history.history[&#x27;loss&#x27;])plt.plot(history.history[&#x27;val_loss&#x27;])plt.title(&#x27;model loss&#x27;)plt.ylabel(&#x27;loss&#x27;)plt.xlabel(&#x27;epoch&#x27;)plt.legend([&#x27;train&#x27;, &#x27;test&#x27;], loc=&#x27;upper left&#x27;)plt.show() 源码","categories":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/categories/data-mining/"}],"tags":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/tags/data-mining/"},{"name":"Machine Learning","slug":"machine-learning","permalink":"https://blog.mhuig.top/tags/machine-learning/"}]},{"title":"How to Setup Your Python Environment for Machine Learning With Anaconda","slug":"data-mining/How to Setup Your Python Environment for Machine Learning with Anaconda","date":"2020-09-11T14:30:22.000Z","updated":"2020-09-11T14:30:22.000Z","comments":true,"path":"posts/2f550c8c.html","link":"","permalink":"https://blog.mhuig.top/posts/2f550c8c.html","excerpt":"In this tutorial, we will cover the following steps: 1.Download Anaconda 2.Install Anaconda 3.Start and Update Anaconda 4.Update scikit-learn Library 5.Install Deep Learning Libraries","text":"In this tutorial, we will cover the following steps: 1.Download Anaconda 2.Install Anaconda 3.Start and Update Anaconda 4.Update scikit-learn Library 5.Install Deep Learning Libraries Download AnacondaIn this step, we will download the Anaconda Python package for your platform. Install AnacondaIn this step, we will install the Anaconda Python software on your system.This step assumes you have sufficient administrative privileges to install software on your system. Start and Update AnacondaIn this step, we will confirm that your Anaconda Python environment is up to date. Anaconda comes with a suite of graphical tools called Anaconda Navigator. You can start Anaconda Navigator by opening it from your application launcher. Conda is fast, simple, it’s hard for error messages to hide, and you can quickly confirm your environment is installed and working correctly. 1.Open a terminal (command line window). 2.Confirm conda is installed correctly, by typing: 123conda -Vconda 4.8.2 3.Confirm Python is installed correctly by typing: 123python -VPython 3.7.6 4.Confirm your conda environment is up-to-date, type: 12conda update condaconda update anaconda You may need to install some packages and confirm the updates. 5.Confirm your SciPy environment. The script below will print the version number of the key SciPy libraries you require for machine learning development, specifically: SciPy, NumPy, Matplotlib, Pandas, Statsmodels, and Scikit-learn.You can type “python” and type the commands in directly. Alternatively, I recommend opening a text editor and copy-pasting the script into your editor. Update scikit-learn LibraryIn this step, we will update the main library used for machine learning in Python called scikit-learn. Update scikit-learn to the latest version.At the time of writing, the version of scikit-learn shipped with Anaconda is out of date (0.17.1 instead of 0.18.1). You can update a specific library using the conda command; below is an example of updating scikit-learn to the latest version.At the terminal, type: 1conda update scikit-learn Alternatively, you can update a library to a specific version by typing: 1conda install -c anaconda scikit-learn Install Deep Learning Libraries1234conda install theanoconda install -c conda-forge tensorflowconda install kerasconda install graphviz SummaryVersion First MLPDownload Data Set","categories":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/categories/data-mining/"}],"tags":[{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/tags/data-mining/"},{"name":"Machine Learning","slug":"machine-learning","permalink":"https://blog.mhuig.top/tags/machine-learning/"}]},{"title":"Jupyter测试","slug":"others/Test/jupyter测试","date":"2020-09-10T13:02:43.000Z","updated":"2020-09-10T13:02:43.000Z","comments":true,"path":"posts/a2eb4e7a.html","link":"","permalink":"https://blog.mhuig.top/posts/a2eb4e7a.html","excerpt":"","text":"测试","categories":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}],"tags":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}]},{"title":"初探 Cloudflare Workers 边缘计算","slug":"web/初探CloudflareWorkers边缘计算","date":"2020-08-31T11:53:40.000Z","updated":"2020-08-31T11:53:40.000Z","comments":true,"path":"posts/4a0f523a.html","link":"","permalink":"https://blog.mhuig.top/posts/4a0f523a.html","excerpt":"","text":"Edge Computing 尝试在边缘运行 JavaScript 什么是边缘计算边缘运算（Edge computing），又译为边缘计算，是一种分散式运算的架构，将应用程序、数据资料与服务的运算，由网络中心节点，移往网络逻辑上的边缘节点来处理。边缘计算将原本完全由中心节点处理大型服务加以分解，切割成更小与更容易管理的部分，分散到边缘节点去处理。边缘节点更接近于用户终端装置，可以加快资料的处理与传送速度，减少延迟。在这种架构下，资料的分析与知识的产生，更接近于数据资料的来源，因此更适合处理大数据。 OpenStack（是一个由NASA和Rackspace合作研发并发起的，以Apache许可证授权的自由软件和开放源代码项目）社区的定义概念： “边缘计算是为应用开发者和服务提供商在网络的边缘侧提供云服务和IT环境服务；目标是在靠近数据输入或用户的地方提供计算、存储和网络带宽”。 章鱼理论秒懂边缘计算章鱼理论：用 “ 脚 “ 解决问题的边缘计算！ 作为无脊椎动物中智商最高的一种动物，章鱼拥有巨量的神经元，但60%分布在章鱼的八条腿（腕足）上，脑部仅有40%。 也就是说： 警告：本文从以下开始跑题。 == 需要M03-GLORIA程序特殊访问权限 == 正在读取资料。请稍等…….[拒绝访问][该数据已被删除]正在读取备份。请稍等…….本博客边缘计算的搭建方案本博客目前采用Vercel、Cloudflare、GithubPages多线部署，未来可能会考虑添加██████等其他线路。笔者在此解释Cloudflare线路的边缘计算部署方案。正如你所见，静态页面托管存储在Github仓库██████中，并开启GitHub Pages服务，使用Cloudflare Workers实现边缘计算处理逻辑，LeanCloud数据库存储网站动态数据。笔者在这里只谈部署思路，由于███████████████████████████████████，相信应该不会有人愿意开源公开自己的后端程序，安全策略以及██████████████████████等等。实现网站镜像目前Cloudflare线路即为GithubPages的镜像站，我不说相信你也可以看出来，当然我也可以随时切换为Vercel或者██████或者██████的镜像站。在网络边缘拦截用户请求，并修改请求中的request.url.hostname,将对用户请求的域解析替换为原始真实URL，实现隐藏真实网络基础设施的目的。具体实现可以查阅相关文档1234567891011121314151617181920212223242526/** * An object with different URLs to fetch * @param &#123;Object&#125; ORIGINS */const ORIGINS = &#123; &quot;starwarsapi.yourdomain.com&quot;: &quot;swapi.co&quot;, &quot;google.yourdomain.com&quot;: &quot;google.com&quot;,&#125;async function handleRequest(request) &#123; const url = new URL(request.url) // Check if incoming hostname is a key in the ORIGINS object if (url.hostname in ORIGINS) &#123; const target = ORIGINS[url.hostname] url.hostname = target // If it is, proxy request to that third party origin return fetch(url.toString(), request) &#125; // Otherwise, process request as normal return fetch(request)&#125;addEventListener(&quot;fetch&quot;, event =&gt; &#123; event.respondWith(handleRequest(event.request))&#125;)隐藏前端API密钥由于调用数据库API需要使用AppID和AppKey：而直接将他们写在前端页面存在安全风险，因此笔者将数据库API密钥直接写在了Cloudflare Worker中。以本站的评论系统为例，在前端页面插入伪造的API密钥以作混淆，下图一看就知道是伪造的密钥。在网络边缘拦截用户请求，判断请求合法性，动态更改请求标头替换为真实的API密钥，并向后端数据库转发请求，实现数据交互时隐藏API密钥。具体实现可以查阅相关文档自定义防火墙规则从 CF-IPCountry HTTP 标头检索 IP 地理位置信息，以判断访问者合法性██████████████████，对于非法访问者████████████████████████████。笔者配合使用████████████数据库实现动态████████████████████████。How does Cloudflare handle HTTP Request headers?Configuring Cloudflare IP Geolocation记录请求日志由于Cloudflare Workers没有导入依赖包的功能██████████████████，因此笔者使用Leancloud数据库的REST API发送 HTTP 请求来与 数据库 进行交互。Cloudflare Workers默认在事件的response发送完成后结束边缘的相关进程，这里笔者使用event.waitUntil(promise Promise),延长事件的生存期，而不会阻止事件的response发送。使用此方法可以通知运行时以等待需要比正常的发送响应时间更长的时间运行的任务（例如，日志记录，对第三方服务的分析，流和缓存）。将所有请求记录到数据库中，以便需要时调查取证或者█████████。HTTP2 server push多路复用，是HTTP/2众多协议优化中最令人振奋的特性，它大大降低了网络延迟对性能的影响，而对于资源之间的依赖关系导致的“延迟”，Server Push则提供了手动优化方案。通常，只有在浏览器请求某个资源的时候，服务器才会向浏览器发送该资源。Server Push则允许服务器在收到浏览器的请求之前，主动向浏览器推送资源。比如说，网站首页引用了一个CSS文件。浏览器在请求首页时，服务器除了返回首页的HTML之外，可以将其引用的 CSS文件也一并推给客户端。本站在返回的请求中添加HTTP 标头实现HTTP2 server push。123if(request.headers.get(&quot;content-type&quot;)==&quot;text/html; charset=utf-8&quot;)&#123; response.headers.set(&quot;Link&quot;, &quot;&lt;/css/style.css&gt;; rel=preload; as=style,&lt;/js/app.js&gt;; rel=preload; as=script, &lt;https://cdn.jsdelivr.net&gt;; rel=preconnect; crossorigin, &lt;https://cdn.jsdelivr.net&gt;; rel=dns-prefetch&quot;)&#125;具体实现可以查阅相关文档CORS header proxy由于██████████████████需要前端实现跨域，可以直接添加 HTTP 标头实现。12345const corsHeaders = &#123; &quot;Access-Control-Allow-Origin&quot;: &quot;*&quot;, &quot;Access-Control-Allow-Methods&quot;: &quot;GET, HEAD, POST, OPTIONS&quot;, &quot;Access-Control-Allow-Headers&quot;: &quot;Content-Type&quot;,&#125;具体实现可以查阅相关文档其他应用更多Examples[文件已删除]搭建其他镜像站上面介绍镜像自己的博客，如法炮制，你也可以镜像其他网站，例如Github、█████████、█████████、█████████等，请勿用于非法用途，否则后果自负。下面是一个随便在百度中搜索到的脚本，几乎随便一搜都可以找到它，大家不会都用一个吧。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130// 你想镜像的站点const upstream = &#x27;zh.wikipedia.org&#x27; // 针对移动端适配站点，没有就和保持和上面一致const upstream_mobile = &#x27;zh.wikipedia.org&#x27; // 禁止某些地区访问const blocked_region = [] // 禁止自访问const blocked_ip_address = [&#x27;0.0.0.0&#x27;, &#x27;127.0.0.1&#x27;] // 你想镜像的站点const replace_dict = &#123; &#x27;$upstream&#x27;: &#x27;$custom_domain&#x27;, &#x27;//zh.wikipedia.org&#x27;: &#x27;&#x27;&#125; // 剩下的就不用管了addEventListener(&#x27;fetch&#x27;, event =&gt; &#123; event.respondWith(fetchAndApply(event.request));&#125;) async function fetchAndApply(request) &#123; const region = request.headers.get(&#x27;cf-ipcountry&#x27;).toUpperCase(); const ip_address = request.headers.get(&#x27;cf-connecting-ip&#x27;); const user_agent = request.headers.get(&#x27;user-agent&#x27;); let response = null; let url = new URL(request.url); let url_host = url.host; if (url.protocol == &#x27;http:&#x27;) &#123; url.protocol = &#x27;https:&#x27; response = Response.redirect(url.href); return response; &#125; if (await device_status(user_agent)) &#123; upstream_domain = upstream &#125; else &#123; upstream_domain = upstream_mobile &#125; url.host = upstream_domain; if (blocked_region.includes(region)) &#123; response = new Response(&#x27;Access denied: WorkersProxy is not available in your region yet.&#x27;, &#123; status: 403 &#125;); &#125; else if(blocked_ip_address.includes(ip_address))&#123; response = new Response(&#x27;Access denied: Your IP address is blocked by WorkersProxy.&#x27;, &#123; status: 403 &#125;); &#125; else&#123; let method = request.method; let request_headers = request.headers; let new_request_headers = new Headers(request_headers); new_request_headers.set(&#x27;Host&#x27;, upstream_domain); new_request_headers.set(&#x27;Referer&#x27;, url.href); let original_response = await fetch(url.href, &#123; method: method, headers: new_request_headers &#125;) let original_response_clone = original_response.clone(); let original_text = null; let response_headers = original_response.headers; let new_response_headers = new Headers(response_headers); let status = original_response.status; new_response_headers.set(&#x27;access-control-allow-origin&#x27;, &#x27;*&#x27;); new_response_headers.set(&#x27;access-control-allow-credentials&#x27;, true); new_response_headers.delete(&#x27;content-security-policy&#x27;); new_response_headers.delete(&#x27;content-security-policy-report-only&#x27;); new_response_headers.delete(&#x27;clear-site-data&#x27;); const content_type = new_response_headers.get(&#x27;content-type&#x27;); if (content_type.includes(&#x27;text/html&#x27;) &amp;&amp; content_type.includes(&#x27;UTF-8&#x27;)) &#123; original_text = await replace_response_text(original_response_clone, upstream_domain, url_host); &#125; else &#123; original_text = original_response_clone.body &#125; response = new Response(original_text, &#123; status, headers: new_response_headers &#125;) &#125; return response;&#125; async function replace_response_text(response, upstream_domain, host_name) &#123; let text = await response.text() var i, j; for (i in replace_dict) &#123; j = replace_dict[i] if (i == &#x27;$upstream&#x27;) &#123; i = upstream_domain &#125; else if (i == &#x27;$custom_domain&#x27;) &#123; i = host_name &#125; if (j == &#x27;$upstream&#x27;) &#123; j = upstream_domain &#125; else if (j == &#x27;$custom_domain&#x27;) &#123; j = host_name &#125; let re = new RegExp(i, &#x27;g&#x27;) text = text.replace(re, j); &#125; return text;&#125; async function device_status (user_agent_info) &#123; var agents = [&quot;Android&quot;, &quot;iPhone&quot;, &quot;SymbianOS&quot;, &quot;Windows Phone&quot;, &quot;iPad&quot;, &quot;iPod&quot;]; var flag = true; for (var v = 0; v &lt; agents.length; v++) &#123; if (user_agent_info.indexOf(agents[v]) &gt; 0) &#123; flag = false; break; &#125; &#125; return flag;&#125;Cloudflare workers blogcloudflare-worker-blog一个在Github上的开源实例，████████████████████████████████████████████████████████████████████████████████████Cloudflare workers + Github 实现的动态博客系统█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████。。jsproxyjsproxy一个基于浏览器端 JS 实现的在线代理，█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████。项目主要用于以下技术的研究：网站镜像 / 沙盒化钓鱼网站检测技术前端资源访问加速█████████████████████████████，否则█████████████。加速 Google Analyticscloudflare-workers-async-google-analyticsThe Cloudflare Workers implementation of an async Google Analytics.██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████。以上代码，请勿将其用于非法用途，否则后果自负。 严禁未经授权的人员进行访问肇事者将被监控，定位并处理","categories":[{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"}]},{"title":"基于K均值聚类的网络流量异常检测(pyspark)","slug":"bigdata/Spark/基于K均值聚类的网络流量异常检测","date":"2020-08-02T10:23:15.000Z","updated":"2020-08-02T10:23:15.000Z","comments":true,"path":"posts/7b55143e.html","link":"","permalink":"https://blog.mhuig.top/posts/7b55143e.html","excerpt":"异常检测常用于检测欺诈、网络攻击、服务器及传感设备故障。在这些应用中，我们要能够找出以前从未见过的新型异常，如新欺诈方式、新入侵方法或新服务器故障模式。","text":"异常检测常用于检测欺诈、网络攻击、服务器及传感设备故障。在这些应用中，我们要能够找出以前从未见过的新型异常，如新欺诈方式、新入侵方法或新服务器故障模式。 数据集KDD Cup 1999 数据集 下载 KDD Cup 1999 数据集 数据集为 CSV 格式，每个连接占一行，包含 38 个特征。 单行示例： 10,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal. 最后的字段表示类别标号。大多数标号为 normal.， 但也有一些样本代表各种网络攻击。 算法K均值聚类算法聚类算法是指将一堆没有标签的数据自动划分成几类的方法，这个方法要保证同一类的数据有相似的特征。 算法过程K-Means算法的特点是类别的个数是人为给定的。是一个迭代求解的聚类算法，属于划分型的聚类方法，即首先创建K个划分，然后迭代地将样本从一个划分转移到另一个划分来改善最终聚类的效果。其过程大致如下。 （1）根据给定的K值选取K个样本点作为初始划分中心。 （2）计算所有样本点到每一个划分中心的距离，并将所有样本点划分到距离最近的划分中心。 （3）计算每个划分中样本点的平均值，并将其作为新的中心。 （4）循环进行步骤（2）和步骤（3）直至最大迭代次数，或划分中心的变化小于某一预定义阈值。 伪代码1234567891011function K-Means(输入数据，中心点个数K) 获取输入数据的维度Dim和个数N 随机生成K个Dim维的点 while(算法未收敛) 对N个点：计算每个点属于哪一类。 对于K个中心点： 1，找出所有属于自己这一类的所有数据点 2，把自己的坐标修改为这些数据点的中心点坐标 end 输出结果end K-Means的一个重要的假设是：数据之间的相似度可以使用欧氏距离度量，如果不能使用欧氏距离度量，要先把数据转换到能用欧氏距离度量，这一点很重要。可以使用欧氏距离度量的意思就是欧氏距离越小，两个数据相似度越高。 假设簇划分为（,,…）,则优化目标是最小化平方误差SSE: 其中是簇的均值向量，也称为质心，表达式为： 这是一个NP难题，因此只能采用启发式迭代方法。 K-Means采用的启发式方式很简单，用下面一组图就可以形象的描述: 图a表达了初始的数据集，假设k=2。在图b中，随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，得到了所有样本点的第一轮迭代后的类别。此时对当前标记为红色和蓝色的点分别求其新的质心，如图d所示，新的红色质心和蓝色质心的位置已经发生了变动。图e和图f重复了在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终得到的两个类别如图f。 K-means聚类最优k值的选取（手肘法）手肘法的核心指标是SSE(sum of the squared errors，误差平方和),公式见上文。 核心思想是：随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。 特征的规范化去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行计算和比较。 1、数据的中心化 所谓数据的中心化是指数据集中的各项数据减去数据集的均值。 2、数据的标准化 所谓数据的标准化是指中心化之后的数据在除以数据集的标准差，即数据集中的各项数据减去数据集的均值再除以数据集的标准差。 特征的规范化可以通过将每个特征转换为标准得分来完成。这就是说用对每个特征值求平均，用每个特征值减去平均值，然后除以特征值的标准差，如下标准分计算公式所示： 类别型变量类别型特征可以用 one-hot 编码转换为几个二元特征，这几个二元特征可以看成数值型维度。 使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。 解决了分类器不好处理属性数据的问题；在一定程度上也起到了扩充特征的作用。 聚类结果评价指标Entropy（熵）好的聚类应该和人工标签保持一致，大部分情况下，标签相同的数据点应聚在一起，而标签不同的数据点不应该在一起，并且簇内的数据点标签相同。熵值会变得很小。 对于一个聚类i，首先计算聚类 i 中的成员（member）属于类（class）j 的概率其中是在聚类 i 中所有成员的个数，是聚类 i 中的成员属于类 j 的个数。 每个聚类的entropy可以表示为其中L是类（class）的个数。 整个聚类划分的entropy为其中K是聚类（cluster）的数目，m是整个聚类划分所涉及到的成员个数。 Accuracy(准确率)比较每一条聚类结果是否和真的结果一致. 其中N表示文档总数，表示正确聚类的文档数. 实验过程准备数据，上传至HDFSHDFS创建文件夹 hadoop关闭安全模式 上传KDD Cup 1999 数据集 查看上传成功 通过kddcup.names加载列名称123456789names=[]with open(\"/export/work/F/3/data/kddcup.names\") as f: line = f.readline() line = f.readline() while line: names.append(line.split(\":\")[0]) line = f.readline()names.append(\"label\") 输出列名称： 1['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'] 构建Dataframe1234567891011121314151617181920212223242526names=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label']from pyspark.sql.types import Rowfrom pyspark.sql.types import StructTypefrom pyspark.sql.types import StructFieldfrom pyspark.sql.types import StringTypefrom pyspark.sql.types import FloatTypefrom pyspark.conf import SparkConffrom pyspark import SparkContextfrom pyspark.sql.session import SparkSession# 构建Dataframeconf = SparkConf().setAppName(\"applicaiton\").set(\"spark.executor.heartbeatInterval\",\"200000\").set(\"spark.network.timeout\",\"300000\")sc = SparkContext.getOrCreate(conf)spark = SparkSession(sc)testRDD = sc.textFile(\"/3/corrected\")fields = list(map( lambda fieldName : StructField(fieldName, StringType(), nullable = True) if fieldName in [\"protocol_type\", \"service\", \"flag\",\"label\"] else StructField(fieldName, FloatType(), nullable = True) , names))schema = StructType(fields)rowRDD = testRDD.map(lambda line : line.split(\",\")).map(lambda attr : Row(float(attr[0]),attr[1],attr[2],attr[3],float(attr[4]),float(attr[5]),float(attr[6]),float(attr[7]),float(attr[8]),float(attr[9]),float(attr[10]),float(attr[11]),float(attr[12]),float(attr[13]),float(attr[14]),float(attr[15]),float(attr[16]),float(attr[17]),float(attr[18]),float(attr[19]),float(attr[20]),float(attr[21]),float(attr[22]),float(attr[23]),float(attr[24]),float(attr[25]),float(attr[26]),float(attr[27]),float(attr[28]),float(attr[29]),float(attr[30]),float(attr[31]),float(attr[32]),float(attr[33]),float(attr[34]),float(attr[35]),float(attr[36]),float(attr[37]),float(attr[38]),float(attr[39]),float(attr[40]),attr[41]))testDF = spark.createDataFrame(rowRDD, schema)dataRDD = sc.textFile(\"/3/kddcup.data\")fields = list(map( lambda fieldName : StructField(fieldName, StringType(), nullable = True) if fieldName in [\"protocol_type\", \"service\", \"flag\",\"label\"] else StructField(fieldName, FloatType(), nullable = True) , names))schema = StructType(fields)rowRDD = dataRDD.map(lambda line : line.split(\",\")).map(lambda attr : Row(float(attr[0]),attr[1],attr[2],attr[3],float(attr[4]),float(attr[5]),float(attr[6]),float(attr[7]),float(attr[8]),float(attr[9]),float(attr[10]),float(attr[11]),float(attr[12]),float(attr[13]),float(attr[14]),float(attr[15]),float(attr[16]),float(attr[17]),float(attr[18]),float(attr[19]),float(attr[20]),float(attr[21]),float(attr[22]),float(attr[23]),float(attr[24]),float(attr[25]),float(attr[26]),float(attr[27]),float(attr[28]),float(attr[29]),float(attr[30]),float(attr[31]),float(attr[32]),float(attr[33]),float(attr[34]),float(attr[35]),float(attr[36]),float(attr[37]),float(attr[38]),float(attr[39]),float(attr[40]),attr[41]))dataDF = spark.createDataFrame(rowRDD, schema) 数据集统计统计数据集中各个类别标号以及每类样本有多少，并展示。 数据集的类别标号以及每类样本数 1234567891011121314151617181920212223242526272829dataDF.groupBy(\"label\").count().show(10000)+----------------+-------+ | label| count|+----------------+-------+| warezmaster.| 20|| smurf.|2807886|| pod.| 264|| imap.| 12|| nmap.| 2316|| guess_passwd.| 53|| ipsweep.| 12481|| portsweep.| 10413|| satan.| 15892|| land.| 21|| loadmodule.| 9|| ftp_write.| 8||buffer_overflow.| 30|| rootkit.| 10|| warezclient.| 1020|| teardrop.| 979|| perl.| 3|| phf.| 4|| multihop.| 7|| neptune.|1072017|| back.| 2203|| spy.| 2|| normal.| 972781|+----------------+-------+ 测试集的类别标号以及每类样本数 1234567891011121314151617181920212223242526272829303132333435363738394041424344testDF.groupBy(\"label\").count().show(10000) +----------------+------+| label| count|+----------------+------+| snmpguess.| 2406|| xlock.| 9|| warezmaster.| 1602|| processtable.| 759|| smurf.|164091|| pod.| 87|| worm.| 2|| snmpgetattack.| 7741|| mscan.| 1053|| nmap.| 84|| imap.| 1|| xterm.| 13|| sqlattack.| 2|| guess_passwd.| 4367|| mailbomb.| 5000|| xsnoop.| 4|| ipsweep.| 306|| portsweep.| 354|| named.| 17|| satan.| 1633|| land.| 9|| loadmodule.| 2|| ftp_write.| 3|| sendmail.| 17||buffer_overflow.| 22|| httptunnel.| 158|| apache2.| 794|| saint.| 736|| rootkit.| 13|| teardrop.| 12|| perl.| 2|| phf.| 2|| multihop.| 18|| udpstorm.| 2|| neptune.| 58001|| back.| 1098|| ps.| 16|| normal.| 60593|+----------------+------+ 尝试聚类12345from pyspark.ml import Pipeline,PipelineModelfrom pyspark.ml.clustering import KMeans,KMeansModelfrom pyspark.ml.feature import VectorAssemblerfrom pyspark.sql import DataFrameimport random 用 VectorAssembler 创建一个特征向量，基于这些特征向量用一个 K 均值实现来创建一个模型，再用一个管道将它们拼接在一起。从得到的模型中，可以提取并检验簇群中心。 1234567numericOnly = dataDF.drop(\"protocol_type\", \"service\", \"flag\").cache()assembler = VectorAssembler(inputCols=numericOnly.drop(\"label\").columns, outputCol=\"featureVector\")kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")pipeline = Pipeline().setStages([assembler, kmeans])pipelineModel = pipeline.fit(numericOnly)kmeansModel = pipelineModel.stages[-1]for i in kmeansModel.clusterCenters():print(i) 输出： 1234567891011121314151617181920[4.83401949e+01 1.83462154e+03 8.26203195e+02 5.71611720e-06 6.48779303e-04 7.96173468e-06 1.24376586e-02 3.20510858e-05 1.43529049e-01 8.08830584e-03 6.81851124e-05 3.67464677e-05 1.29349608e-02 1.18874823e-03 7.43095237e-05 1.02114351e-03 0.00000000e+00 4.08294086e-07 8.35165553e-04 3.34973508e+02 2.95267146e+02 1.77970317e-01 1.78036989e-01 5.76648988e-02 5.77299094e-02 7.89884132e-01 2.11796105e-02 2.82608102e-02 2.32981078e+02 1.89214283e+02 7.53713390e-01 3.07109788e-02 6.05051931e-01 6.46410786e-03 1.78091184e-01 1.77885898e-01 5.79276115e-02 5.76592214e-02][1.09990000e+04 0.00000000e+00 1.30993741e+09 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00 2.55000000e+02 1.00000000e+00 0.00000000e+00 6.49999976e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00] 对这些数字做一个直观的解释并不容易，但是每一个数字都表示模型生成的一个簇群中心，也称为质心（centroid）。就每个数值输入特征而言，这些值是质心的坐标。 k的选择如果每个数据点都紧靠最近的质心，则可认为聚类是较优的。这里的“近”采用欧氏距离定义。这是评估聚类质量的一种简单又常用的方法，使用与所有点之间距离的平均值，有时也可以使用平方距离的平均值。实际上，KMeansModel 提供了一个computeCost 方法来计算平方距离的总和，并且很容易用来计算平方距离的平均值。 1234567891011121314numericOnly = dataDF.drop(\"protocol_type\", \"service\", \"flag\").cache()# computeCost 方法来计算平方距离的总和，并且很容易用来计算平方距离的平均值。def clusteringScore0(data,k): assembler = VectorAssembler(inputCols=data.drop(\"label\").columns, outputCol=\"featureVector\") kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\") #.setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([assembler, kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(assembler.transform(data)) / data.count() return Srorefor k in range(20, 100, 20): print([k, clusteringScore0(numericOnly, k)]) 输出结果： 1234[20, 148277112.23861197] [40, 49940659.143821806][60, 18265796.561388526][80, 15313289.324247833] 输出结果显示得分随着 k 的增加而降低。 增加迭代时间可以优化聚类结果。算法提供了 setTol() 来设置一个阈值，该阈值控制聚类过程中簇质心进行有效移动的最小值。降低该阈值能使质心继续移动更长的时间。使用setMaxIter() 增加最大迭代次数也可以防止它过早停止，代价是可能需要更多的计算。 123456789101112def clusteringScore1(data,k): assembler = VectorAssembler(inputCols=data.drop(\"label\").columns, outputCol=\"featureVector\") kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([assembler, kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(assembler.transform(data)) / data.count() return Srorefor k in range(20, 120, 20): print([k, clusteringScore1(numericOnly, k)]) 输出结果： 12345[20, 148277112.23861197] [40, 11564470.915401561][60, 16343181.409780543][80, 22323383.079484705][100, 7572838.84573523] 糟糕的情况是，前面的结果中 k=80 时的距离居然比 k=60 的距离大。这不应该发生，因为 k 取更大值时，聚类的结果应该至少与 k 取一个较小值时的结果一样好。问题的原因在于，这种给定 k 值的 K 均值算法并不一定能得到最优聚类。K 均值的迭代过程是从一个随机点开始的，因此可能收敛于一个局部最小值，这个局部最小值可能还不错，但并不是全局最优的。 在 k 过了 100 这个点之后得分下降还是很明显，所以 k 的拐点值应该大于 100。 特征的规范化特征的规范化可以通过将每个特征转换为标准得分来完成。这就是说用对每个特征值求平均，用每个特征值减去平均值，然后除以特征值的标准差。 由于减去平均值相当于把所有数据点沿相同方法移动相同距离，不影响点之间的欧氏距离，所以实际上减去平均值对聚类结果没有影响。 1234567891011121314from pyspark.ml.feature import StandardScalerdef clusteringScore2(data,k): assembler = VectorAssembler(inputCols=data.drop(\"label\").columns, outputCol=\"featureVector\") scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False) kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([assembler,scaler,kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(pipelineModel.transform(data)) / data.count() return Srorefor k in range(60, 300, 30): print([k, clusteringScore2(numericOnly, k)]) 这有助于将维度放到更平等的基准上，而且在绝对的意义上，看点之间的绝对距离（也就是代价）要小得多。然而，k 值还没有出现一个明显的点，超过该点后，增加 k 值对于改善代价没有明显的作用： 12345678[60, 1.1611941370693641][90, 0.7236962692254361][120, 0.5581874996147724][150, 0.3886887438817504][180, 0.3333248112741165][210, 0.27497680552057235][240, 0.2556693718314817][270, 0.22710138015576076] 类别型变量归一化使聚类结果有了可贵的进步，但聚类结果还有进一步提升的空间。比如说，几个特征由于不是数值型就被去掉了，于是这些特征里有价值的信息也被丢掉了。如果将这些信息以某种形式加回来，我们应该能得到更好的聚类。 类别型特征可以用 one-hot 编码转换为几个二元特征，这几个二元特征可以看成数值型维度。举个例子，数据集的第二列代表协议类型，取值可能是 tcp、udp 或 icmp。可以把它们看成 3 个特征，分别取名为 is_tcp、is_udp 和 is_icmp。这样，特征值 tcp 就变成1,0,0，udp 对应 0,1,0，icmp 对应 0,0,1，以此类推。 123456789101112131415161718192021222324from pyspark.ml.feature import OneHotEncoder, StringIndexer# 类别型特征可以用 one-hot 编码转换为几个二元特征，这几个二元特征可以看成数值型维度。def oneHotPipeline(inputCol): indexer = StringIndexer(inputCol=inputCol,outputCol=inputCol + \"_indexed\").setHandleInvalid(\"keep\") encoder = OneHotEncoder(inputCol=inputCol + \"_indexed\",outputCol=inputCol + \"_vec\") pipeline = Pipeline().setStages([indexer, encoder]) return (pipeline, inputCol + \"_vec\")def clusteringScore3(data,k): protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\") serviceEncoder, serviceVecCol = oneHotPipeline(\"service\") flagEncoder, flagVecCol = oneHotPipeline(\"flag\") assembleCols = (set(data.columns)-set([\"label\", \"protocol_type\", \"service\", \"flag\"])).union(set([protoTypeVecCol, serviceVecCol, flagVecCol])) assembler = VectorAssembler(inputCols=list(assembleCols), outputCol=\"featureVector\") scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False) kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(pipelineModel.transform(data)) / data.count() return Srorefor k in range(60, 300, 30): print([k, clusteringScore3(dataDF, k)]) 输出： 12345678[60, 38.01382297522162][90, 16.419330083446177][120, 3.2093992442174235][150, 2.1454678299121843][180, 1.6142523558430413][210, 1.3533093788147306][240, 1.0616778921723296][270, 0.9068134376554267] 局部放大： 这些样本结果表明，从 k=180 这个点开始，评分值的变化趋于平缓。至少现在聚类使用了所有的输入特征。 利用标号的熵信息标签告诉我们每个数据点的真实性质。好的聚类应该和人工标签保持一致，大部分情况 下，标签相同的数据点应聚在一起，而标签不同的数据点不应该在一起，并且簇内的数据 点标签相同。 良好的聚类结果簇中样本类别大体相同，因而熵值较低。我们可以对各个簇的熵加权平均，将结果作为聚类得分： 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpydef entropy(x): ent = 0.0 x_value_list = [x[i] for i in range(x.shape[0])] n=sum(x_value_list) for x_value in x_value_list: p = float(x_value) / n ent -= p * numpy.log(p) return entdef fitPipeline4(data, k): protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\") serviceEncoder, serviceVecCol = oneHotPipeline(\"service\") flagEncoder, flagVecCol = oneHotPipeline(\"flag\") assembleCols = (set(data.columns)-set([\"label\", \"protocol_type\", \"service\", \"flag\"])).union(set([protoTypeVecCol, serviceVecCol, flagVecCol])) assembler = VectorAssembler(inputCols=list(assembleCols), outputCol=\"featureVector\") scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False) kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans]) pipelineModel = pipeline.fit(data) return pipelineModel# 良好的聚类结果簇中样本类别大体相同，因而熵值较低。对各个簇的熵加权平均，将结果作为聚类得分def clusteringScore4(data, k): pipelineModel = fitPipeline4(data, k) clusterLabel = pipelineModel.transform(data).select(\"cluster\", \"label\") pd=clusterLabel.toPandas() Sum=0 for name, group in pd.groupby(\"cluster\"): labelsize=group.count()[0] a=numpy.array(group.groupby('label').count()) b=[] for i in range(len(a)): for j in range(len(a[i])): b.append(a[i][j]) One=labelsize*entropy(numpy.array(b)) Sum=Sum+One return Sum/data.count()for k in range(60, 300, 30): print([k, clusteringScore4(dataDF, k)]) 输出结果： 12345678[60, 0.038993775215004474][90, 0.02985377476611417][120, 0.02266161774992263][150, 0.020766076760220943][180, 0.017547365257679748][210, 0.012974819022593053][240, 0.007150061376894767][270, 0.00833981903044443] 跟以前一样，可以根据上面的分析结果大致看出 k 的合适取值。随着 k 的增加，熵不一定会减小，因此我们找到的可能是一个局部最小值。这里结果同样表明，k 取 240 可能比较合理，因为它的得分实际上低于 210 以及 270。 聚类实战取 k=180 123pipelineModel = fitPipeline4(dataDF, 180)countByClusterLabel = pipelineModel.transform(dataDF).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")countByClusterLabel.show() 这里我们同样把每个簇的标号打印出来。聚类的结果中大部分属于同一簇，以及其他的少部分簇。 12345678910111213141516171819202122232425+-------+----------+-------+ |cluster| label| count|+-------+----------+-------+| 0| neptune.| 362876|| 0|portsweep.| 1|| 1| ipsweep.| 40|| 1| nmap.| 6|| 1| normal.| 3421|| 1|portsweep.| 2|| 1| satan.| 11|| 1| smurf.|2807886|| 2| neptune.| 1038|| 2|portsweep.| 13|| 2| satan.| 3|| 3| ipsweep.| 13|| 3| neptune.| 1046|| 3| normal.| 38|| 3|portsweep.| 11|| 3| satan.| 3|| 4| neptune.| 1034|| 4| normal.| 4|| 4|portsweep.| 7|| 4| satan.| 4|+-------+----------+-------+only showing top 20 rows 现在可以建立一个真正的异常检测系统了。异常检测时需要度量新数据点到最近的簇质心 的距离。如果这个距离超过某个阈值，那么就表示这个新数据点是异常的。我们可以把阈 值设为已知数据中离中心最远的第 100 个点到中心的距离。 1234567891011import os, tempfilefrom pyspark.ml.linalg import Vector, VectorspipelineModel = fitPipeline4(dataDF, 180)kmeansModel = pipelineModel.stages[-1]kmeansModel.save(\"/model/3/kmeansModel\")pipelineModel.save(\"/model/3/pipelineModel\")centroids = kmeansModel.clusterCenters()clustered = pipelineModel.transform(dataDF)threshold=clustered.select(\"cluster\", \"scaledFeatureVector\").rdd.map(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector)).sortBy(lambda x: x).take(100)[-1]print(threshold) 输出阈值： 13.232811853048799e-05 最后一步就是在新数据点出现的时候使用阈值进行评估。在unlabled数据上进行测试找出异常流量记录，并计算正确率。 123456789clustered = pipelineModel.transform(testDF)anomalies = clustered.rdd.filter(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector) &gt;= threshold).collect()n=len(anomalies)v=0for i in anomalies: if i[\"label\"]!='normal.': v=v+1print(\"正确率:\"+str(float(v)/n)) 输出结果： 1正确率:0.8051841158484633 取 k=240 12345678910import os, tempfilefrom pyspark.ml.linalg import Vector, VectorspipelineModel = fitPipeline4(dataDF, 240)kmeansModel = pipelineModel.stages[-1]kmeansModel.save(\"/model/3/test/kmeansModel\")pipelineModel.save(\"/model/3/test/pipelineModel\")centroids = kmeansModel.clusterCenters()clustered = pipelineModel.transform(dataDF)threshold=clustered.select(\"cluster\", \"scaledFeatureVector\").rdd.map(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector)).sortBy(lambda x: x).take(100)[-1]print(threshold) 17.665805787851659e-06 123456789clustered = pipelineModel.transform(testDF)anomalies = clustered.rdd.filter(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector) &gt;= threshold).collect()n=len(anomalies)v=0for i in anomalies: if i[\"label\"]!='normal.': v=v+1print(\"正确率:\"+str(float(v)/n)) 1正确率:0.8050769488123118 可以看出K=180是在unlabled数据上进行测试找出异常流量记录，计算正确率比K=240有较好的结果。 缩短计算的步长： 12for k in range(150, 220, 10): print([k, clusteringScore3(dataDF, k)]) 得到评分结果： 1234567[150, 2.292921780026778][160, 4.917845778754763][170, 2.0016455721528015][180, 1.7177635513092788][190, 1.5766159846344556][200, 1.5550587983858675][210, 1.2785418817225693] 局部放大： 取 k=190 12345678910import os, tempfilefrom pyspark.ml.linalg import Vector, VectorspipelineModel = fitPipeline4(dataDF, 190)kmeansModel = pipelineModel.stages[-1]kmeansModel.save(\"/model/3/190/kmeansModel\")pipelineModel.save(\"/model/3/190/pipelineModel\")centroids = kmeansModel.clusterCenters()clustered = pipelineModel.transform(dataDF)threshold=clustered.select(\"cluster\", \"scaledFeatureVector\").rdd.map(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector)).sortBy(lambda x: x).take(100)[-1]print(threshold) 13.247829147436459e-05 123456789clustered = pipelineModel.transform(testDF)anomalies = clustered.rdd.filter(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector) &gt;= threshold).collect()n=len(anomalies)v=0for i in anomalies: if i[\"label\"]!='normal.': v=v+1print(\"正确率:\"+str(float(v)/n)) 1正确率:0.8051841158484633 可以看出K=190是在unlabled数据上进行测试找出异常流量记录，计算正确率比K=180有较好的结果。","categories":[{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"}]},{"title":"基于Audioscrobbler数据集的音乐推荐(pyspark)","slug":"bigdata/Spark/基于Audioscrobbler数据集的音乐推荐","date":"2020-08-02T07:34:13.000Z","updated":"2020-08-02T07:34:13.000Z","comments":true,"path":"posts/1e621a56.html","link":"","permalink":"https://blog.mhuig.top/posts/1e621a56.html","excerpt":"根据用户播放次数数据使用协同过滤算法完成音乐推荐。","text":"根据用户播放次数数据使用协同过滤算法完成音乐推荐。 数据集Audioscrobbler数据集 下载 Audioscrobbler 数据集 user_artist_data.txt它包含141000个用户和160万个艺术家，记录了约2420万条用户播放艺术家歌曲的信息，其中包括播放次数信息。播放次数较多意味着该用户更喜欢对应艺术家的作品。 userid artistid playcount 用户ID 艺术家ID 播放次数 1000002 1 55 artist_data.txt该文件包含两列： artistid artist_name 艺术家ID 艺术家名字。文件中给出了每个艺术家的 ID 和对应的名字。此文件用于ID与名字的转换。 artistid artist_name 艺术家ID 艺术家名 1134999 06Crazy Life artist_alias.txt该文件包含两列: badid, goodid 坏ID 好ID 。该文件包含已知错误拼写的艺术家ID及其对应艺术家的正规的，用于将拼写错误的艺术家 ID 或ID 变体对应到该艺术家正确的ID。 badid goodid 坏ID 好ID 1092764 1000311 算法交替最小二乘推荐算法 (Alternating Least Squares，ALS)人们虽然经常听音乐，但很少给音乐评分。因此 Audioscrobbler 数据集覆盖了更多的用户和艺术家，也包含了更多的总体信息，虽然单条记录的信息比较少。这种类型的数据通常被称为隐式反馈数据，因为用户和艺术家的关系是通过其他行动隐含体现出来的，而不是通过显式的评分或点赞得到的。 根据两个用户的相似行为判断他们有相同的偏好，学习算法不需要用户和艺术家的属性信息。这类算法通常称为协同过滤算法。 潜在因素模型：试图通过数据相对少的未被观察到的底层原因，来解释大量用户和产品之间可观察到的交互。因子分析方法背后的理论是，有关观测变量之间的相互依赖性的信息可以稍后用于减少数据集中的变量集。 矩阵分解模型：数学上，算法把用户和产品数据当成一个大矩阵 R，矩阵第 i 行和第 j 列上的元素有值，代表用户 i 播放过艺术家 j 的音乐。矩阵 R 是稀疏的：R中大多数元素都是 0，因为相对于所有可能的用户 - 艺术家组合，只有很少一部分组合会出现在数据中。算法将 R 分解为两个小矩阵 U 和P的乘积。矩阵 U 和矩阵P非常“瘦”。因为 A 有很多行和列，但 U 和 P 的行很多而列很少（列数用 k 表示）。这 k 个列就是潜在因素，用于解释数据中的交互关系。由于 k 的值小，矩阵分解算法只能是某种近似。 为了使低秩矩阵P和U尽可能的逼近R，可以通过最小化如下损失函数L来完成。 损失函数公式与上图对应：表示用户i的偏好隐含向量，表示艺术家j包含的隐含特征向量，表示用户i对艺术家j的评分，是用户i对艺术家j评分的近似。其中λ是正则化项的系数，损失函数一般需要加入正则化项来避免过拟合等问题。 于是就简化为一个最小化损失函数L的优化问题。用户-特征矩阵和特征-艺术家矩阵的乘积的结果是对整个稠密的用户-艺术家相互关系矩阵的完整估计。该乘积可以理解成艺术家与其属性之间的一个映射，然后按用户属性进行加权。 通常没有确切的解，因为U和P通常不够大，不足以完全表示R， 应该尽可能逼近R。然而不幸的是，想直接同时得到 U 和 P 的最优解是不可能的。 如果 P 已知，求U的最优解是非常容易的，反之亦然。但 P 和 U事先都是未知的。 虽然 P 是未知的，但可以把P初始化为随机行向量矩阵。接着运用简单的线性代数，就能在给定 R 和 P 的条件下求出 U 的最优解。实际上，U 的第 i 行是 R 的第 i 行和 P 的函数。因此可以很容易分开计算 U 的每一行。因为 U 的每一行可以分开计算，所以我们可以将其并行化，而并行化是大规模计算的一大优点。 ALS是求解的著名算法，固定P或U对其对应的隐含向量求偏导数并令导数为0，得到求解公式： 随机对P、Q初始化，随后交替进行优化直到收敛。收敛标准是均方误差小于预定义阈值，或者到达最大迭代次数。 推荐质量评价指标AUCAUC指标是一个[0,1]之间的实数，代表如果随机挑选一个正样本和一个负样本，分类算法将这个正样本排在负样本前面的概率。值越大，表示分类算法更有可能将正样本排在前面，也即算法准确性越好。 随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。正样本负样本在有M个正样本,N个负样本的数据集里。一共有M×N对样本（一对样本，一个正样本与一个负样本）。统计这M×N对样本里，正样本的预测概率大于负样本的预测概率的个数。正样本负样本其中， 实验过程数据预处理artist_data.txt文件 数据最终处理成以逗号分割 artist_data.txt文件 两列之间的间隔有的是空格有的是Tab，第二列数据中包含空格 因第二列数据中含有逗号和空格，数据最终处理成以Tab分割 去除第一列不是数字的行 artist_alias.txt 文件 将拼写错误的艺术家 ID 或 ID 变体对应到该艺术家的规范 ID 两列之间的间隔有的是空格有的是Tab 包含数据缺失的列 在数据处理时对拼写错误ID进行映射，用别名数据集将所有的艺术家 ID 转换成正规 ID。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748aa={}with open(\"/export/work/F/1/data/artist_alias.txt\") as f: line = f.readline() while line: if len(line.split())==2: aa[line.split()[0]]=line.split()[1] line = f.readline()f3=open(\"/export/work/F/1/data/user_artist_data.txt.data\",\"w+\")with open(\"/export/work/F/1/data/user_artist_data.txt\") as f2: line = f2.readline() while line: it=line.split() if it[1] in aa: it[1]=aa[it[1]] print(it[0]+\",\"+it[1]+\",\"+it[2],file=f3) line = f2.readline()f3.close()f5=open(\"/export/work/F/1/data/artist_data.txt.data\",\"w+\")with open(\"/export/work/F/1/data/artist_data.txt\") as f4: line = f4.readline() while line: it=line.split() s=\"\" for i in range(len(it)): s+=it[i] if i==0: s+=\" \" elif i==len(it)-1: s+=\"\" else: s+=\" \" print(s,file=f5) line = f4.readline()f7=open(\"/export/work/F/1/data/artist_data.txt.data2\",\"w+\")with open(\"/export/work/F/1/data/artist_data.txt.data\") as f6: line = f6.readline() while line: it=line.split(\" \") try: a=int(it[0]) print(str(a)+\" \"+it[1],file=f7,end=\"\") except: pass line = f6.readline() 预处理后得到的数据集 artist_data user_artist_data 获取数据文件，并上传至HDFS 读入数据，转换成DataFrame备用1234567891011121314151617181920212223242526from pyspark.sql.types import Rowfrom pyspark.sql.types import StructTypefrom pyspark.sql.types import StructFieldfrom pyspark.sql.types import StringType,IntegerTypefrom pyspark.conf import SparkConffrom pyspark import SparkContextfrom pyspark.sql.session import SparkSession# 转换成DataFramename1=[\"user\", \"item\", \"rating\"]name2=[\"id\",\"name\"]conf = SparkConf().setAppName(\"applicaiton\").set(\"spark.executor.heartbeatInterval\",\"500000\").set(\"spark.network.timeout\",\"500000\")sc = SparkContext.getOrCreate(conf)spark = SparkSession(sc)uaRDD = sc.textFile(\"/1/user_artist_data.txt.data\")fields = list(map( lambda fieldName : StructField(fieldName, IntegerType(), nullable = True), name1))schema = StructType(fields)rowRDD = uaRDD.map(lambda line : line.split(\",\")).map(lambda attr : Row(int(attr[0]),int(attr[1]),int(attr[2])))uaDF = spark.createDataFrame(rowRDD, schema)aRDD = sc.textFile(\"/1/artist_data.txt.data2\")fields = list(map( lambda fieldName : StructField(fieldName, IntegerType(), nullable = True) if fieldName==\"id\" else StructField(fieldName, StringType(), nullable = True) , name2))schema = StructType(fields)rowRDD =aRDD.map(lambda line : line.split(\" \")).map(lambda attr : Row(int(attr[0]),attr[1]))aDF = spark.createDataFrame(rowRDD, schema) 展示数据格式基本统计信息数据格式 123456789101112131415161718192021222324252627uaDF.show()+-------+-------+------+| user| item|rating|+-------+-------+------+|1000002| 1| 55||1000002|1000006| 33||1000002|1000007| 8||1000002|1000009| 144||1000002|1000010| 314||1000002|1000013| 8||1000002|1000014| 42||1000002|1000017| 69||1000002|1000024| 329||1000002|1000025| 1||1000002|1000028| 17||1000002|1000031| 47||1000002|1000033| 15||1000002|1000042| 1||1000002|1000045| 1||1000002|1000054| 2||1000002|1000055| 25||1000002|1000056| 4||1000002|1000059| 2||1000002|1000062| 71|+-------+-------+------+only showing top 20 rows 基本统计信息用户数 1234a=uaDF.select(uaDF.user).distinct().count()print(a)148111 艺术家数目 1234b=uaDF.select(uaDF.item).distinct().count()print(b)1568126 每用户平均播放次数 123456789101112131415161718192021222324252627uaDF.drop(\"item\").groupBy(\"user\").agg({\"rating\":\"mean\"}).show()+-------+------------------+| user| avg(rating)|+-------+------------------+|1000190|55.355432780847146||1001043|6.0131578947368425||1001129| 12.32748538011696||1001139| 8.652557319223986||1002431|12.833333333333334||1002605|3.5392670157068062||1004666| 9.79409594095941||1005158|1.9245283018867925||1005439|28.333333333333332||1005697|11.733333333333333||1005853| 2.5||1007007| 2.443396226415094||1007847|14.333333333333334||1008081|31.232876712328768||1008233| 90.0||1008804| 9.0||1009408| 4.666666666666667||1012261|3.2887640449438202||1015587| 9.46||1016416| 8.241935483870968|+-------+------------------+only showing top 20 rows 每艺术家平均播放次数 123456789101112131415161718192021222324252627uaDF.drop(\"user\").groupBy(\"item\").agg({\"rating\":\"mean\"}).show()+-------+------------------+| item| avg(rating)|+-------+------------------+|1001129|10.578309692671395||1003373|2.3333333333333335||1007972|18.156831042845596||1029443| 20.54196642685851||1076507| 2.969264544456641||1318111|5.6902654867256635|| 833| 9.483282674772036||1239413| 3.821794871794872||1000636| 2.0||1002431|1.7142857142857142||1005697| 3.5||1040360| 1.0||1043263|1.9166666666666667||1245208|19.613390928725703|| 463| 34.3479262672811||1043126|14.580645161290322||1001601| 3.573529411764706||1091589| 2.5||1004021| 6.96403785488959||1012885| 4.744927536231884|+-------+------------------+only showing top 20 rows 构建ALS模型构建ALS模型，并记录所耗时间。初始参数：Rank 10, maxiter 15, RegParm 0.01 Alpha 1.0。 123456789from pyspark.ml.recommendation import ALS,ALSModelimport randomimport timestart = time.time()als = ALS(rank=10,maxIter=15,regParam=0.01,alpha=1.0,seed=int(random.random()*100))model=als.fit(uaDF)end = time.time()print (\"时间:\"+str(end-start)) 输出结果： 1时间:785.1817960739136 这样我们就构建了一个ALSModel 模型。 模型用两个不同的DataFrame，它们分别表示“用户 - 特征”和“产品 - 特征”这两个大型矩阵。 检查推荐结果依据构建的模型，选择部分ID检查推荐结果。 看看模型给出的艺术家推荐直观上是否合理，检查一下用户播放过的艺术家，然后看看模型向用户推荐的艺术家。具体来看看用户 2093760 的例子。 12userID = 2093760a=uaDF.rdd.filter(lambda x:x[0]==userID).collect() 查看用户输出结果： 1234567[Row(user=2093760, item=1180, rating=1), Row(user=2093760, item=1255340, rating=3), Row(user=2093760, item=378, rating=1), Row(user=2093760, item=813, rating=2), Row(user=2093760, item=942, rating=7)] 获取艺术家ID： 123artistid=[]for i in a: artistid.append(i.item) 输出结果： 1[1180, 1255340, 378, 813, 942] 要提取该用户收听过的艺术家 ID 并打印他们的名字，这意味着先在输入数据中搜索该用户收听过的艺术家的 ID，然后用这些 ID 对艺术家集合进行过滤，这样我们就可以获取并按序打印这些艺术家的名字： 1b=aDF.rdd.filter(**lambda** x: x[0] **in** artistid).collect() 输出结果： 123456789[Row(id=1180, name='David Gray'), Row(id=378, name='Blackalicious'), Row(id=813, name='Jurassic 5'), Row(id=1255340, name='The Saw Doctors'), Row(id=942, name='Xzibit')] 用户播放过的艺术家既有大众流行音乐风格的也有嘻哈风格的。 使用Spark2.4.6自带的recommendForUserSubset方法，对所有艺术家评分，并返回向用户2093760推荐其中分值最高的前5位。 12345d=sc.parallelize([(2093760,1)]).toDF(['user']) t=model.recommendForUserSubset(d,5) t.show() 输出结果： 12345+-------+--------------------+ | user| recommendations|+-------+--------------------+|2093760|[[6674945, 4997.0...|+-------+--------------------+ 遍历打印一下： 1t.select(\"recommendations\").rdd.foreach(**lambda** x:**print**(x)) 输出： 1234567Row(recommendations=[ Row(item=6674945, rating=4997.056640625), Row(item=1170225, rating=1805.596435546875), Row(item=1153293, rating=1753.0908203125), Row(item=6730413, rating=1233.61767578125), Row(item=183, rating=1169.90234375)]) 结果全部是嘻哈风格。能看出，这些推荐都不怎么样。虽然推荐的艺术家都受人欢迎，但好像并没有针对用户的收听习惯进行个性化。 训练-验证切分训练-验证切分，采用初始参数，重新训练模型。 为了利用输入数据，需要把它分成训练集和验证集。训练集只用于训练 ALS 模型，验证集用于评估模型。这里将 90% 的数据用于训练，剩余的 10% 用于交叉验证： 12345train,test=uaDF.randomSplit([0.9,0.1])als = ALS(rank=10,maxIter=15,regParam=0.01,alpha=1.0,seed=int(random.random()*100),implicitPrefs=True)model=als.fit(train)train.cache()test.cache() 计算AUC接受一个交叉验证集和一个预测函数，交叉验证集代表每个用户对应的“正面的”或“好的”艺术家。预测函数把每个包含“用户 - 艺术家”对的 DataFrame 转换为一个同时包含“用户 - 艺术家”和“预测”的 DataFrame，“预测”表示“用户”与“艺术家”之间关联的强度值，这个值越高，代表推荐的排名越高。 1234567891011121314151617181920212223242526272829303132333435363738394041424344allArtistIDs = uaDF.select(\"item\").distinct().collect()import numpyallArtistID = []for i in range(len(allArtistIDs)): allArtistID.append(allArtistIDs[i][\"item\"])def f(a,b): posItemIDSet = set(list(b)) negative = [] i = 0 while (i &lt; len(allArtistID)) and (len(negative) &lt; len(posItemIDSet)): artistID = allArtistID[numpy.random.randint(1, high=len(allArtistID), size=None, dtype='l')] if artistID not in posItemIDSet: negative.append(artistID) i += 1 s=list() for i in negative: s.append((a,i)) return s# 计算AUCimport pyspark.sql.functions as funcdef areaUnderCurve(positiveData,allArtistIDs,predictFunction): positivePredictions = predictFunction(positiveData.select(\"user\", \"item\")).withColumnRenamed(\"prediction\", \"positivePrediction\") negativeDatatmp = positiveData.select(\"user\", \"item\").rdd.groupByKey().map(lambda x: f(x[0],x[1])).collect() negativeDatalist=[] for i in negativeDatatmp: for j in i: negativeDatalist.append(j) negativeData=spark.createDataFrame(negativeDatalist,['user','item']) negativePredictions = predictFunction(negativeData.select(\"user\", \"item\")).withColumnRenamed(\"prediction\", \"negativePrediction\") joinedPredictions = positivePredictions.join(negativePredictions, \"user\").select(\"user\", \"positivePrediction\", \"negativePrediction\") allCounts = joinedPredictions.groupBy(\"user\").agg(func.count(func.lit(1)).alias(\"total\")).select(\"user\", \"total\") correctCounts = joinedPredictions.filter(joinedPredictions[\"positivePrediction\"] &gt; joinedPredictions[\"negativePrediction\"]).groupBy(\"user\").agg(func.count(\"user\").alias(\"correct\")).select(\"user\", \"correct\") meanAUCtemp = allCounts.join(correctCounts, \"user\", \"left_outer\") meanAUC = meanAUCtemp.select(\"user\", (meanAUCtemp[\"correct\"] / meanAUCtemp[\"total\"]).alias(\"auc\")).agg(func.mean(\"auc\")).first() try: joinedPredictions.unpersist() except: pass return meanAUC mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform)print(mostListenedAUC) 输出结果： 1Row(avg(auc)=0.9098560946043145) 有必要把上述方法和一个更简单方法做一个基准比对。举个例子，考虑下面的推荐方法：向每个用户推荐播放最多的艺术家。这个策略一点儿都不个性化，但它很简单，也可能有效。定义这个简单预测函数并评估它的 AUC 得分： 1234567def predictMostListened(data): listenCounts = train.groupBy(\"item\").agg({\"rating\":\"sum\"}).withColumnRenamed(\"sum(rating)\", \"prediction\").select(\"item\", \"prediction\") uaDF.join(listenCounts, [\"item\"], \"left_outer\").select(\"user\", \"item\", \"prediction\") listenCounts = uaDF.groupBy(\"item\").agg({\"rating\":\"sum\"}).withColumnRenamed(\"sum(rating)\", \"prediction\").select(\"item\", \"prediction\") return data.join(listenCounts, [\"item\"], \"left_outer\").select(\"user\", \"item\", \"prediction\")mostListenedAUC = areaUnderCurve(test, allArtistIDs, predictMostListened)print(mostListenedAUC) 输出结果： 1Row(avg(auc)=0.9578054887285846) 结果得分大约是 0.96。这意味着，对 AUC 这个指标，非个性化的推荐表现已经不错了。然而，我们想要的是得分更高，也就是更为“个性化”的推荐。显然这个模型还有待改进。调整超参数，使推荐结果更合理。 选择超参数Rank可选（5,30）RegParam可选（4.0,0.0001）,alpha可选（1.0,40.0）。合计8种参数组合。 可以把 rank、regParam 和 alpha 看作模型的超参数。（maxIter 更像是对分解过程使用的资源的一种约束。）这些值不会体现在 ALSModel 的内部矩阵中，这些矩阵只是参数，其值由算法选定。超参数则是构建过程本身的参数。 123456789def TrainALS(rank,regParam,alpha,dir): als = ALS(rank=rank,maxIter=15,regParam=regParam,alpha=alpha,seed=int(random.random()*100),implicitPrefs=True) model=als.fit(train) model.save(\"/model/ALS/Try2/\"+str(dir)) try: model.userFactors.unpersist() model.itemFactors.unpersist() except: pass 构建模型 123456dir=0for rank in [5,30]: for regParam in [4.0,0.0001]: for alpha in [1.0,40.0]: dir=dir+1 TrainALS(rank,regParam,alpha,dir) 加载模型计算AUC 得分： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748try: model=ALSModel.load(\"/model/ALS/Try2/1\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,4.0,1.0)))except: print((\"ERROR\",(5,4.0,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/2\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,4.0,40.0)))except: print((\"ERROR\",(5,4.0,40.0)))try: model=ALSModel.load(\"/model/ALS/Try2/3\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,0.0001,1.0)))except: print((\"ERROR\",(5,0.0001,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/4\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,0.0001,40.0)))except: print((\"ERROR\",(5,0.0001,40.0)))try: model=ALSModel.load(\"/model/ALS/Try2/5\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,4.0,1.0)))except: print((\"ERROR\",(30,4.0,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/6\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,4.0,40.0)))except: print((\"ERROR\",(30,4.0,40.0)))try: model=ALSModel.load(\"/model/ALS/Try2/7\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,0.0001,1.0)))except: print((\"ERROR\",(30,0.0001,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/8\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,0.0001,40.0)))except: print((\"ERROR\",(30,0.0001,40.0))) 输出结果： 12345678(Row(avg(auc)=0.9122637924972641), (5, 4.0, 1.0))(Row(avg(auc)=0.9154223563144587), (5, 4.0, 40.0))(Row(avg(auc)=0.9057761909633262), (5, 0.0001, 1.0))(Row(avg(auc)=0.9146676967584815), (5, 0.0001, 40.0))(Row(avg(auc)=0.9230010545570864), (30, 4.0, 1.0))(Row(avg(auc)=0.9275148741094371), (30, 4.0, 40.0))(Row(avg(auc)=0.9125799456221803), (30, 0.0001, 1.0))(Row(avg(auc)=0.9265221600644649), (30, 0.0001, 40.0)) 可以看出rank=30，regParam=4.0，alpha=40.0时取得了最优的结果avg(auc)=0.9275148741094371. 虽然这些值的绝对差很小，但对于 AUC 值来说，仍然具有一定的意义。有意思的是，参数 alpha 取 40 的时候看起来总是比取 1 表现好。这说明了模型在强调用户听过什么时的表现要比强调用户没听过什么时要好。 产生推荐选取10个用户展示推荐结果 1234model=ALSModel.load(\"/model/ALS/Try2/6\")d=sc.parallelize([(2093760,1),(1000002,1),(1006277,1),(1006282,1),(1006283,1),(1006285,1),(1041207,1),(1071489,1),(2025005,1),(2025007,1),]).toDF(['user'])t=model.recommendForUserSubset(d,5)t.select(\"recommendations\").rdd.foreach(lambda x:print(x)) 输出推荐结果： 12345678910Row(recommendations=[Row(item=1010991, rating=1.1815389394760132), Row(item=1245226, rating=1.139704942703247), Row(item=4629, rating=1.1092422008514404), Row(item=1113701, rating=1.1066040992736816), Row(item=1019715, rating=1.10056471824646)])Row(recommendations=[Row(item=1010921, rating=1.225757122039795), Row(item=1166169, rating=1.1972994804382324), Row(item=1183949, rating=1.184556007385254), Row(item=1082446, rating=1.178223729133606), Row(item=3892, rating=1.1580229997634888)])Row(recommendations=[Row(item=1028958, rating=1.146733283996582), Row(item=1086774, rating=1.1434733867645264), Row(item=1037761, rating=1.1272671222686768), Row(item=1184419, rating=1.1093372106552124), Row(item=1148170, rating=1.1065270900726318)])Row(recommendations=[Row(item=1010295, rating=1.2554553747177124), Row(item=3722, rating=1.2187168598175049), Row(item=1024674, rating=1.2044183015823364), Row(item=1009445, rating=1.099776268005371), Row(item=1018746, rating=1.0894378423690796)])Row(recommendations=[Row(item=1002068, rating=0.9575374126434326), Row(item=1005288, rating=0.9533681273460388), Row(item=2430, rating=0.9476644992828369), Row(item=1002270, rating=0.9428261518478394), Row(item=3909, rating=0.9334102869033813)])Row(recommendations=[Row(item=1034635, rating=0.606441080570221), Row(item=1000107, rating=0.6010327935218811), Row(item=1000024, rating=0.59807950258255), Row(item=4154, rating=0.5966558456420898), Row(item=1000157, rating=0.5944067239761353)])Row(recommendations=[Row(item=1034635, rating=0.290438175201416), Row(item=930, rating=0.2857869565486908), Row(item=4267, rating=0.2853849530220032), Row(item=1205, rating=0.2830250561237335), Row(item=1000113, rating=0.2829856276512146)])Row(recommendations=[Row(item=1002909, rating=1.2324668169021606), Row(item=1653, rating=1.1938585042953491), Row(item=988, rating=1.1880080699920654), Row(item=1003367, rating=1.1807997226715088), Row(item=1009545, rating=1.1761128902435303)])Row(recommendations=[Row(item=1017017, rating=0.8724791407585144), Row(item=1032349, rating=0.8424116373062134), Row(item=1028433, rating=0.8259942531585693), Row(item=1240603, rating=0.8185747265815735), Row(item=1029602, rating=0.8147093653678894)])Row(recommendations=[Row(item=1013187, rating=1.2516499757766724), Row(item=1098360, rating=1.2394661903381348), Row(item=1289948, rating=1.2353206872940063), Row(item=1129243, rating=1.2332189083099365), Row(item=1245184, rating=1.1997216939926147)]) 按照用户顺序将最喜欢的推荐结果输出到文件 123456def getp(x): f=open(\"/export/work/result\",\"a+\") print(x,file=f)u=uaDF.select(\"user\").distinct()t=model.recommendForUserSubset(u,1)t.rdd.foreach(lambda x:getp(x)) 输出详见result文件，以下为部分输出： 12345Row(user=1000092, recommendations=[Row(item=1002400, rating=1.2386927604675293)]) Row(user=1000144, recommendations=[Row(item=5221, rating=1.1728830337524414)]) Row(user=3175, recommendations=[Row(item=1022207, rating=0.282743901014328)]) Row(user=1000164, recommendations=[Row(item=1034635, rating=0.36578264832496643)]) Row(user=7340, recommendations=[Row(item=1007903, rating=0.8722475171089172)])","categories":[{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"}]},{"title":"Spark RDD编程","slug":"bigdata/Spark/Spark RDD编程","date":"2020-05-20T07:45:29.000Z","updated":"2020-05-20T07:45:29.000Z","comments":true,"path":"posts/32971e43.html","link":"","permalink":"https://blog.mhuig.top/posts/32971e43.html","excerpt":"Spark RDD 编程","text":"Spark RDD 编程 RDD 编程基础RDD 创建从文件系统中加载数据创建 RDDSpark 采用 textFile()方法来从文件系统中加载数据创建 RDD该方法把文件的 URI 作为参数，这个 URI 可以是 本地文件系统的地址 分布式文件系统 HDFS 的地址 AmazonS3 的地址 等等 从本地文件系统中加载数据创建 RDD123456&gt;&gt;&gt; lines = sc.textFile(&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;)&gt;&gt;&gt; lines.foreach(print)Hadoop is goodSpark is fastSpark is better 从分布式文件系统 HDFS 中加载数据123&gt;&gt;&gt; lines = sc.textFile(&quot;hdfs://localhost:9000/user/hadoop/word.txt&quot;)&gt;&gt;&gt; lines = sc.textFile(&quot;/user/hadoop/word.txt&quot;)&gt;&gt;&gt; lines = sc.textFile(&quot;word.txt&quot;) 通过并行集合（列表）创建 RDD可以调用 SparkContext 的 parallelize 方法，在 Driver 中一个已经存在的集合（列表）上创建。 12345678&gt;&gt;&gt; array = [1,2,3,4,5]&gt;&gt;&gt; rdd = sc.parallelize(array)&gt;&gt;&gt; rdd.foreach(print)12345 RDD 操作转换操作对于 RDD 而言，每一次转换操作都会产生不同的 RDD，供给下一个“转换”使用.转换得到的 RDD 是惰性求值的，也就是说，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会发生真正的计算，开始从血缘关系源头开始，进行物理的转换操作. 操作 含义 filter(func) 筛选出满足函数 func 的元素，并返回一个新的数据集 map(func) 将每个元素传递到函数 func 中，并将结果返回为一个新的数据集 flatMap(func) 与 map()相似，但每个输入元素都可以映射到 0 或多个输出结果 groupByKey() 应用于(K,V)键值对的数据集时，返回一个新的(K,Iterable)形式的数据集 reduceByKey(func) 应用于(K,V)键值对的数据集时，返回一个新的(K,V)形式的数据集，其中每个值是将每个 key 传递到函数 func 中进行聚合后的结果 filter(func)筛选出满足函数 func 的元素，并返回一个新的数据集 12345&gt;&gt;&gt; lines = sc.textFile(&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;)&gt;&gt;&gt; linesWithSpark = lines.filter(lambda line: &quot;Spark&quot; in line)&gt;&gt;&gt; linesWithSpark.foreach(print)Spark is betterSpark is fast map(func)map(func)操作将每个元素传递到函数 func 中，并将结果返回为一个新的数据集 123456789&gt;&gt;&gt; data = [1,2,3,4,5]&gt;&gt;&gt; rdd1 = sc.parallelize(data)&gt;&gt;&gt; rdd2 = rdd1.map(lambda x:x+10)&gt;&gt;&gt; rdd2.foreach(print)1113121415 flatMap(func)12&gt;&gt;&gt; lines = sc.textFile(&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;)&gt;&gt;&gt; words = lines.flatMap(lambda line:line.split(&quot; &quot;)) groupByKey()应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集 12345678910&gt;&gt;&gt; words = sc.parallelize([(&quot;Hadoop&quot;,1),(&quot;is&quot;,1),(&quot;good&quot;,1), \\... (&quot;Spark&quot;,1),(&quot;is&quot;,1),(&quot;fast&quot;,1),(&quot;Spark&quot;,1),(&quot;is&quot;,1),(&quot;better&quot;,1)])&gt;&gt;&gt; words1 = words.groupByKey()&gt;&gt;&gt; words1.foreach(print)(&#x27;Hadoop&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552c88&gt;)(&#x27;better&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552e80&gt;)(&#x27;fast&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552c88&gt;)(&#x27;good&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552c88&gt;)(&#x27;Spark&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552f98&gt;)(&#x27;is&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552e10&gt;) reduceByKey(func)应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个 key 传递到函数 func 中进行聚合后得到的结果 12345678910&gt;&gt;&gt; words = sc.parallelize([(&quot;Hadoop&quot;,1),(&quot;is&quot;,1),(&quot;good&quot;,1),(&quot;Spark&quot;,1), \\... (&quot;is&quot;,1),(&quot;fast&quot;,1),(&quot;Spark&quot;,1),(&quot;is&quot;,1),(&quot;better&quot;,1)])&gt;&gt;&gt; words1 = words.reduceByKey(lambda a,b:a+b)&gt;&gt;&gt; words1.foreach(print) (&#x27;good&#x27;, 1)(&#x27;Hadoop&#x27;, 1)(&#x27;better&#x27;, 1)(&#x27;Spark&#x27;, 2)(&#x27;fast&#x27;, 1)(&#x27;is&#x27;, 3) 行动操作行动操作是真正触发计算的地方。Spark 程序执行到行动操作时，才会执行真正的计算，从文件中加载数据，完成一次又一次转换操作，最终，完成行动操作得到结果。 操作 含义 count() 返回数据集中的元素个数 collect() 以数组的形式返回数据集中的所有元素 first() 返回数据集中的第一个元素 take(n) 以数组的形式返回数据集中的前 n 个元素 reduce(func) 通过函数 func（输入两个参数并返回一个值）聚合数据集中的元素 foreach(func) 将数据集中的每个元素传递到函数 func 中运行 惰性机制所谓的“惰性机制”是指，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会触发“从头到尾”的真正的计算这里给出一段简单的语句来解释 Spark 的惰性机制 1234&gt;&gt;&gt; lines = sc.textFile(&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;)&gt;&gt;&gt; lineLengths = lines.map(lambda s:len(s))&gt;&gt;&gt; totalLength = lineLengths.reduce(lambda a,b:a+b)&gt;&gt;&gt; print(totalLength) 持久化在 Spark 中，RDD 采用惰性求值的机制，每次遇到行动操作，都会从头开始执行计算。每次调用行动操作，都会触发一次从头开始的计算。这对于迭代计算而言，代价是很大的，迭代计算经常需要多次重复使用同一组数据 123456&gt;&gt;&gt; list = [&quot;Hadoop&quot;,&quot;Spark&quot;,&quot;Hive&quot;]&gt;&gt;&gt; rdd = sc.parallelize(list)&gt;&gt;&gt; print(rdd.count()) //行动操作，触发一次真正从头到尾的计算3&gt;&gt;&gt; print(&#x27;,&#x27;.join(rdd.collect())) //行动操作，触发一次真正从头到尾的计算Hadoop,Spark,Hive 可以通过持久化（缓存）机制避免这种重复计算的开销 可以使用 persist()方法对一个 RDD 标记为持久化 之所以说“标记为持久化”，是因为出现 persist()语句的地方，并不会马上计算生成 RDD 并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化 持久化后的 RDD 将会被保留在计算节点的内存中被后面的行动操作重复使用 1234567&gt;&gt;&gt; list = [&quot;Hadoop&quot;,&quot;Spark&quot;,&quot;Hive&quot;]&gt;&gt;&gt; rdd = sc.parallelize(list)&gt;&gt;&gt; rdd.cache() #会调用persist(MEMORY_ONLY)，但是，语句执行到这里，并不会缓存rdd，因为这时rdd还没有被计算生成&gt;&gt;&gt; print(rdd.count()) #第一次行动操作，触发一次真正从头到尾的计算，这时上面的rdd.cache()才会被执行，把这个rdd放到缓存中3&gt;&gt;&gt; print(&#x27;,&#x27;.join(rdd.collect())) #第二次行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存中的rddHadoop,Spark,Hive 分区RDD 是弹性分布式数据集，通常 RDD 很大，会被分成很多个分区，分别保存在不同的节点上RDD 分区的一个原则是使得分区的个数尽量等于集群中的 CPU 核心（core）数目 键值对 RDD键值对 RDD 的创建从文件中加载可以采用多种方式创建键值对 RDD，其中一种主要方式是使用 map()函数来实现 1234567&gt;&gt;&gt; lines = sc.textFile(&quot;file:///usr/local/spark/mycode/pairrdd/word.txt&quot;)&gt;&gt;&gt; pairRDD = lines.flatMap(lambda line:line.split(&quot; &quot;)).map(lambda word:(word,1))&gt;&gt;&gt; pairRDD.foreach(print)(&#x27;I&#x27;, 1)(&#x27;love&#x27;, 1)(&#x27;Hadoop&#x27;, 1)…… 通过并行集合（列表）创建 RDD12345678&gt;&gt;&gt; list = [&quot;Hadoop&quot;,&quot;Spark&quot;,&quot;Hive&quot;,&quot;Spark&quot;]&gt;&gt;&gt; rdd = sc.parallelize(list)&gt;&gt;&gt; pairRDD = rdd.map(lambda word:(word,1))&gt;&gt;&gt; pairRDD.foreach(print)(Hadoop,1)(Spark,1)(Hive,1)(Spark,1) 常用的键值对 RDD 转换操作reduceByKey(func)使用 func 函数合并具有相同键的值 12345&gt;&gt;&gt; pairRDD = sc.parallelize([(&quot;Hadoop&quot;,1),(&quot;Spark&quot;,1),(&quot;Hive&quot;,1),(&quot;Spark&quot;,1)])&gt;&gt;&gt; pairRDD.reduceByKey(lambda a,b:a+b).foreach(print)(&#x27;Spark&#x27;, 2)(&#x27;Hive&#x27;, 1)(&#x27;Hadoop&#x27;, 1) groupByKey()对具有相同键的值进行分组 1234567&gt;&gt;&gt; list = [(&quot;spark&quot;,1),(&quot;spark&quot;,2),(&quot;hadoop&quot;,3),(&quot;hadoop&quot;,5)]&gt;&gt;&gt; pairRDD = sc.parallelize(list)&gt;&gt;&gt; pairRDD.groupByKey()PythonRDD[27] at RDD at PythonRDD.scala:48&gt;&gt;&gt; pairRDD.groupByKey().foreach(print)(&#x27;hadoop&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7f2c1093ecf8&gt;)(&#x27;spark&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7f2c1093ecf8&gt;) sortByKey()返回一个根据键排序的 RDD 123456789101112&gt;&gt;&gt; list = [(&quot;Hadoop&quot;,1),(&quot;Spark&quot;,1),(&quot;Hive&quot;,1),(&quot;Spark&quot;,1)]&gt;&gt;&gt; pairRDD = sc.parallelize(list)&gt;&gt;&gt; pairRDD.foreach(print)(&#x27;Hadoop&#x27;, 1)(&#x27;Spark&#x27;, 1)(&#x27;Hive&#x27;, 1)(&#x27;Spark&#x27;, 1)&gt;&gt;&gt; pairRDD.sortByKey().foreach(print)(&#x27;Hadoop&#x27;, 1)(&#x27;Hive&#x27;, 1)(&#x27;Spark&#x27;, 1)(&#x27;Spark&#x27;, 1) mapValues(func)对键值对 RDD 中的每个 value 都应用一个函数，但是，key 不会发生变化 12345678&gt;&gt;&gt; list = [(&quot;Hadoop&quot;,1),(&quot;Spark&quot;,1),(&quot;Hive&quot;,1),(&quot;Spark&quot;,1)]&gt;&gt;&gt; pairRDD = sc.parallelize(list)&gt;&gt;&gt; pairRDD1 = pairRDD.mapValues(lambda x:x+1)&gt;&gt;&gt; pairRDD1.foreach(print)(&#x27;Hadoop&#x27;, 2)(&#x27;Spark&#x27;, 2)(&#x27;Hive&#x27;, 2)(&#x27;Spark&#x27;, 2) joinjoin 就表示内连接。对于内连接，对于给定的两个输入数据集(K,V1)和(K,V2)，只有在两个数据集中都存在的 key 才会被输出，最终得到一个(K,(V1,V2))类型的数据集。 1234567&gt;&gt;&gt; pairRDD1 = sc. \\... parallelize([(&quot;spark&quot;,1),(&quot;spark&quot;,2),(&quot;hadoop&quot;,3),(&quot;hadoop&quot;,5)])&gt;&gt;&gt; pairRDD2 = sc.parallelize([(&quot;spark&quot;,&quot;fast&quot;)])&gt;&gt;&gt; pairRDD3 = pairRDD1.join(pairRDD2)&gt;&gt;&gt; pairRDD3.foreach(print)(&#x27;spark&#x27;, (1, &#x27;fast&#x27;))(&#x27;spark&#x27;, (2, &#x27;fast&#x27;))","categories":[{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"}]},{"title":"Spark环境部署（Ubuntu20.04）","slug":"bigdata/Spark/Spark环境部署（Ubuntu20.04）","date":"2020-05-07T11:06:38.000Z","updated":"2020-05-07T11:06:38.000Z","comments":true,"path":"posts/e846a0cc.html","link":"","permalink":"https://blog.mhuig.top/posts/e846a0cc.html","excerpt":"Spark 在 Ubuntu20.04 中的配置","text":"Spark 在 Ubuntu20.04 中的配置 实验环境 实验环境 Ubuntu20.04 LTSHadoop 2.6.0-cdh5.14.0Java 1.8.0_141Python3.8.2(default)Spark 3.0.0-preview2 配置 java 环境解压安装 jdk 1tar -zxvf jdk-8u141-linux-x64.tar.gz -C ../servers/ 配置环境变量 123nano /etc/profileexport JAVA_HOME=/export/servers/jdk1.8.0_141export PATH=:$JAVA_HOME/bin:$PATH 修改完成之后记得 reboot -h now 或 source/etc/profile 生效 验证 1jps 配置 Hadoop 环境下载解压Hadoop 2 可以通过 https://mirrors.cnnic.cn/apache/hadoop/common/ 下载 将 Hadoop 安装至 /usr/local/ 中： 1234sudo tar -zxf hadoop-2.6.0.tar.gz -C /usr/local # 解压到/usr/local中cd /usr/local/sudo mv ./hadoop-2.6.0/ ./hadoop # 将文件夹名改为hadoopsudo chown -R hadoop ./hadoop # 修改文件权限 Hadoop 伪分布式配置伪分布式需要修改 2 个配置文件 core-site.xml 和 hdfs-site.xml core-site.xml1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 JAVA_HOME到 hadoop 的安装目录修改配置文件“/usr/local/hadoop/etc/hadoop/hadoop-env.sh”，在里面找到“export JAVA_HOME=${JAVA_HOME}”这行，然后，把它修改成 JAVA 安装路径的具体地址 NameNode 格式化12cd /usr/local/hadoop./bin/hdfs namenode -format 开启 NameNode 和 DataNode 守护进程12cd /usr/local/hadoop./sbin/start-dfs.sh 安装 Spark打开浏览器，访问Spark 官方下载地址由于我们已经自己安装了 Hadoop，所以，在“Choose a package type”后面需要选择“Pre-build with user-provided Hadoop将 spark 解压到/usr/local,并重命名为 spark修改 Spark 的配置文件 spark-env.sh 12cd /usr/local/sparkcp ./conf/spark-env.sh.template ./conf/spark-env.sh 编辑 spark-env.sh 文件，在第一行添加以下配置信息: 1export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath) 修改环境变量 12345export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoopexport SPARK_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;sparkexport PYTHONPATH&#x3D;$SPARK_HOME&#x2F;python:$SPARK_HOME&#x2F;python&#x2F;lib&#x2F;py4j-0.10.4-src.zip:$PYTHONPATHexport PYSPARK_PYTHON&#x3D;python3export PATH&#x3D;$HADOOP_HOME&#x2F;bin:$SPARK_HOME&#x2F;bin:$PATH 运行 Spark 自带的示例，验证 Spark 是否安装成功 使用 Spark 计算 PI（3.1415926….） 12cd /usr/local/sparkbin/run-example SparkPi grep 命令进行过滤 1bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is&quot;","categories":[{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"}],"tags":[{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"}]},{"title":"Ubuntu环境安装","slug":"Linux/Ubuntu环境安装","date":"2020-05-06T08:30:53.000Z","updated":"2020-05-06T08:30:53.000Z","comments":true,"path":"posts/2844bf39.html","link":"","permalink":"https://blog.mhuig.top/posts/2844bf39.html","excerpt":"Ubuntu 20.04 LTS 在虚拟机中的安装配置","text":"Ubuntu 20.04 LTS 在虚拟机中的安装配置 Ubuntu 20.04 LTS 的支持周期长达 5 年，同时适用于 Ubuntu Desktop、Ubuntu Server、Ubuntu Cloud 和 Ubuntu Core，其安全和维护更新直到 2025 年 4 月才到期。其余 flavour 的支持也长达 3 年，更多详细信息请参考 Ubuntu 20.04 LTS 发行说明。 下载Ubuntu镜像文件官方下载地址（不推荐） https://www.ubuntu.com/download 中国官网（推荐） https://cn.ubuntu.com/ VMware 安装配置 创建新的虚拟机 桌面版系统安装配置 安装过程完成后，单击「现在重启」以完成整个过程，然后卸下安装介质并按「回车」键以重新引导系统。 服务器版系统安装配置 选择系统语言-English键盘设置-English网卡设置，默认。代理服务设置，无代理不填写镜像地址设置，建议换成国内镜像：http://mirrors.aliyun.com/ubuntu/空格选中SSH安装。环境，无需选择。开始安装，等待出现重启选项。安装完成，选择重启。 启用root用户 启用root用户 设置root用户使用sudo passwd root来设置root密码设置root密码1sudo passwd root然后使用su root命令，再输入密码，测试是否可以进入root用户进入root1su root修改/root/.profile文件运行vim /root/.profile命令修改文件，但是发现系统没有安装vim，可以使用apt install vim命令自动安装vim安装成功后，使用vim /root/.profile打开该文件（你也可以使用nano）找到最后一行：mesg n || true，先注释掉，增加tty -s &amp;&amp; mesg n || true这行修改/etc/pam.d/目录下文件运行cd /etc/pam.d/，里面有两个要修改的文件，即gdm-autologin和gdm-password运行vim gdm-autologin，注释掉下面一行运行vim gdm-password，注释掉下面一行重启系统或者虚拟机输入用户名root，然后输入设置的root密码，使完成用root登录 更换国内源 更换国内源 备份/etc/apt/sources.list1sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak修改Ubuntu的源列表修改 /etc/apt/sources.list 文件下的列表清华大学开源软件镜像站更新apt1sudo apt-get update 安装SSH、配置SSH无密码登陆 配置SSH 集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：1sudo apt-get install openssh-server安装后，可以使用如下命令登陆本机：1ssh localhost利用 ssh-keygen 生成密钥，并将密钥加入到授权中：123cd ~&#x2F;.ssh&#x2F;ssh-keygen -t rsa # 会有提示，都按回车就可以cat .&#x2F;id_rsa.pub &gt;&gt; .&#x2F;authorized_keys # 加入授权如果没有问题可能是ssh-server的配置文件设置了拒绝以root用户登录的模式：1nano &#x2F;etc&#x2F;ssh&#x2F;sshd_config1PermitRootLogin yes重启ssh-server1sudo &#x2F;etc&#x2F;init.d&#x2F;ssh restart","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"Ubuntu","slug":"linux/ubuntu","permalink":"https://blog.mhuig.top/categories/linux/ubuntu/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"Ubuntu","slug":"ubuntu","permalink":"https://blog.mhuig.top/tags/ubuntu/"}]},{"title":"Travellings 开往下一个地方","slug":"pen/Travellings开往下一个地方","date":"2020-05-04T11:47:43.000Z","updated":"2020-05-04T11:47:43.000Z","comments":true,"path":"posts/4bd2472f.html","link":"","permalink":"https://blog.mhuig.top/posts/4bd2472f.html","excerpt":"互联网将人与人之间的距离大大减小，却还是形成了大大小小的孤岛。只有熟人间才知道彼此，而陌生人永远只能是陌生人。","text":"互联网将人与人之间的距离大大减小，却还是形成了大大小小的孤岛。只有熟人间才知道彼此，而陌生人永远只能是陌生人。 什么是开往-友链接力开往-友链助力是传统友链的增强，我们不必互相知道了解彼此，标准的审查让友好的朋友加入我们，只需要一个徽标，占用一块位置，我们所有人都联系在了一起，简单而又强大。 开往-友链接力 旅行愉快日常大家可以点击banner或者footer栏中的“Travellings”图标去参观其他博主的博客，祝大家旅行愉快。 站台这里是下一站的站台，来场星际旅行吧~。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"哈勃望远镜在你生日那天看到了啥","slug":"pen/哈勃望远镜在你生日那天看到了啥","date":"2020-05-04T08:50:43.000Z","updated":"2020-05-04T08:50:43.000Z","comments":true,"path":"posts/6af346a8.html","link":"","permalink":"https://blog.mhuig.top/posts/6af346a8.html","excerpt":"","text":"About1990年，NASA将哈勃望远镜送进太空。多年来哈勃望远镜一直在探索宇宙，拍摄了许多珍贵的图像。 2020年是哈勃望远镜服役的第30年，为此NASA公开了哈勃望远镜拍摄的366张珍贵图像。 哈勃望远镜每周 7 天，每天 24 小时不间断地探索宇宙。 这意味着它在一年中的每一天都观察到了一些迷人的宇宙奇观。 What did Hubble look at on your birthday? 可以在 NASA 网站 中输入月份和日期来查找。 What Did Hubble See on Your Birthday?大图警告 点击查看","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"PDF测试","slug":"others/Test/PDF测试","date":"2020-05-02T13:02:43.000Z","updated":"2020-05-02T13:02:43.000Z","comments":true,"path":"posts/323ed1b8.html","link":"","permalink":"https://blog.mhuig.top/posts/323ed1b8.html","excerpt":"","text":"PDF测试 源码 源码文件","categories":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}],"tags":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}]},{"title":"大数据技术概述","slug":"bigdata/大数据技术概述","date":"2020-05-01T00:02:39.000Z","updated":"2020-05-01T00:02:39.000Z","comments":true,"path":"posts/376025fe.html","link":"","permalink":"https://blog.mhuig.top/posts/376025fe.html","excerpt":"","text":"大数据时代 第三次信息化浪潮根据IBM前首席执行官郭士纳的观点，IT领域每隔十五年就会迎来一次重大变革 信息化浪潮 发生时间 标志 解决问题 代表企业 第一次浪潮 1980年前后 个人计算机 信息处理 Intel、AMD、IBM、苹果、微软、联想、戴尔、惠普等 第二次浪潮 1995年前后 互联网 信息传输 雅虎、谷歌、阿里巴巴、百度、腾讯等 第三次浪潮 2010年前后 物联网、云计算和大数据 信息爆炸 将涌现出一批新的市场标杆企业 信息科技为大数据时代提供技术支撑 存储设备容量不断增加 CPU处理能力大幅提升 网络带宽不断增加 数据产生方式的变革促成大数据时代的来临 大数据的特征及数据科学面临的挑战 大数据概念 数据量大 数据类型繁多 处理速度快 价值密度低 大数据的影响图灵奖获得者、著名数据库专家Jim Gray 博士观察并总结人类自古以来，在科学研究上，先后历经了实验、理论、计算和数据四种范式 在思维方式方面，大数据完全颠覆了传统的思维方式： 全样而非抽样 效率而非精确 相关而非因果 大数据关键技术 技术层面 功能 数据采集 利用ETL工具将分布的、异构数据源中的数据如关系数据、平面数据文件等，抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础；或者也可以把实时采集的数据作为流计算系统的输入，进行实时处理分析 数据存储和管理 利用分布式文件系统、数据仓库、关系数据库、NoSQL数据库、云数据库等，实现对结构化、半结构化和非结构化海量数据的存储和管理 数据处理与分析 利用分布式并行编程模型和计算框架，结合机器学习和数据挖掘算法，实现对海量数据的处理和分析；对分析结果进行可视化呈现，帮助人们更好地理解数据、分析数据 数据隐私和安全 在从大数据中挖掘潜在的巨大商业价值和学术价值的同时，构建隐私数据保护体系和数据安全体系，有效保护个人隐私和数据安全 两大核心技术 大数据计算模式 代表性大数据技术Hadoop Hadoop—MapReduce MapReduce将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数：Map和Reduce 编程容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量数据的计算 MapReduce采用“分而治之”策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片（split），这些分片可以被多个Map任务并行处理 Hadoop—YARNYARN的目标就是实现“一个集群多个框架”，为什么？ 一个企业当中同时存在各种不同的业务应用场景，需要采用不同的计算框架 MapReduce实现离线批处理 使用Impala实现实时交互式查询分析 使用Storm实现流式数据实时分析 使用Spark实现迭代计算 这些产品通常来自不同的开发团队，具有各自的资源调度管理机制为了避免不同类型应用之间互相干扰，企业就需要把内部的服务器拆分成多个集群，分别安装运行不同的计算框架，即“一个框架一个集群” 导致问题 集群资源利用率低 数据无法共享 维护代价高 YARN的目标就是实现“一个集群多个框架”，即在一个集群上部署一个统一的资源调度管理框架YARN，在YARN之上可以部署其他各种计算框架由YARN为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩可以实现一个集群上的不同应用负载混搭，有效提高了集群的利用率不同计算框架可以共享底层存储，避免了数据集跨集群移动 Spark Flink Beam","categories":[{"name":"BigData","slug":"bigdata","permalink":"https://blog.mhuig.top/categories/bigdata/"}],"tags":[{"name":"BigData","slug":"bigdata","permalink":"https://blog.mhuig.top/tags/bigdata/"}]},{"title":"Mac Code Test","slug":"others/Test/macCodeTest","date":"2020-04-30T06:06:43.000Z","updated":"2020-04-30T06:06:43.000Z","comments":true,"path":"posts/91953e39.html","link":"","permalink":"https://blog.mhuig.top/posts/91953e39.html","excerpt":"","text":"本项测试可能已失效 代码块全屏测试 How To Use? 导入库文件即可12&lt;link rel&#x3D;&#39;stylesheet&#39; href&#x3D;&#39;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;MHuiG&#x2F;blog-cdn@1.1.12&#x2F;css&#x2F;me.css&#39;&gt;&lt;script src&#x3D;&#39;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;MHuiG&#x2F;blog-cdn@1.1.12&#x2F;js&#x2F;me.js&#39;&gt;&lt;&#x2F;script&gt; cpp code12345678#include &lt;iostream&gt;using namespace std; int main() &#123; cout &lt;&lt; &quot;Hello, World!&quot;; return 0;&#125; python code123#!/usr/bin/pythonprint (&quot;Hello, Python!&quot;) code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &lt;cstdio&gt;#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;const int SIZE = 5e6 + 1;struct edge&#123; int to_node, id; edge(int t, int i): to_node(t), id(i) &#123;&#125; ~edge() = default;&#125;;vector&lt;int&gt; edges[SIZE];vector&lt;edge&gt; querys[SIZE];int father[SIZE], mark[SIZE], ans[SIZE];int n, m, s;int getfa(int x)&#123; if (father[x] == x) return x; else return father[x] = getfa(father[x]);&#125;void tarjan(int x)&#123; mark[x] = 1; for (auto i = edges[x].begin(); i != edges[x].end(); i++) &#123; if (mark[*i]) continue; tarjan(*i); father[*i] = x; &#125; for (auto i = querys[x].begin(); i != querys[x].end(); i++) &#123; int y = (*i).to_node, id = (*i).id; if (mark[y] == 2) ans[id] = getfa(y); &#125; mark[x] = 2;&#125;int main()&#123; int u, v; scanf(&quot;%d%d%d&quot;, &amp;n, &amp;m, &amp;s); for (int i = 1; i &lt;= n; i++) &#123; father[i] = i; mark[i] = 0; &#125; for (int i = 1; i &lt; n; i++) &#123; scanf(&quot;%d%d&quot;, &amp;u, &amp;v); edges[u].emplace_back(v); edges[v].emplace_back(u); &#125; int x, y; for (int i = 1; i &lt;= m; i++) &#123; scanf(&quot;%d%d&quot;, &amp;x, &amp;y); if (x == y) ans[i] = 0; else &#123; querys[x].emplace_back(edge(y, i)); querys[y].emplace_back(edge(x, i)); &#125; &#125; tarjan(s); for (int i = 1; i &lt;= m; i++) printf(&quot;%d\\n&quot;, ans[i]); return 0;&#125;","categories":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}],"tags":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}]},{"title":"图片墙","slug":"others/Test/图片墙","date":"2020-04-28T04:05:43.000Z","updated":"2020-04-28T04:05:43.000Z","comments":false,"path":"posts/e9fadccb.html","link":"","permalink":"https://blog.mhuig.top/posts/e9fadccb.html","excerpt":"","text":"* { margin: 0; padding: 0;} .list { width: 100%; margin: 0 auto; overflow: hidden; zoom: 1;} .list .ul { float: left; width: calc((100% - 100px)/4); margin: 0 10px;} .list .li { margin-bottom: 20px;} .list img { width: 100%; border-radius: 5px; vertical-align: top;} function URL(d){ d+=new Date().getTime() var s = \"https://picsum.photos/seed/\"+d+\"/200/300\" return s } var i=0; function WriteHtml(){ document.getElementsByClassName('list')[0].innerHTML+=` ` } WriteHtml(); WriteHtml(); WriteHtml(); WriteHtml(); function getScrollTop() { var scrollPos; if (window.pageYOffset) { scrollPos = window.pageYOffset; } else if (document.compatMode && document.compatMode != 'BackCompat') { scrollPos = document.documentElement.scrollTop; } else if (document.body) { scrollPos = document.body.scrollTop; } return scrollPos; } function WriteHtmlListener(){ var scrollTop = getScrollTop(); var scrollHeight = document.documentElement.scrollHeight var windowHeight = window.innerHeight; if(scrollTop + windowHeight > scrollHeight - 2000){ WriteHtml() } } window.addEventListener(\"scroll\",WriteHtmlListener,false) volantis.EventListener.list.push(new volantisEventListener(\"scroll\",WriteHtmlListener,window)) // $(window).scroll(function(){ // var scrollTop = $(this).scrollTop(); // var scrollHeight = $(document).height(); // var windowHeight = $(this).height(); // if(scrollTop + windowHeight > scrollHeight - 2000){ // WriteHtml() // } // });","categories":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}],"tags":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}]},{"title":"时间之箭","slug":"pen/时间之箭","date":"2020-04-22T01:42:43.000Z","updated":"2020-04-22T01:42:43.000Z","comments":true,"path":"posts/564714f9.html","link":"","permalink":"https://blog.mhuig.top/posts/564714f9.html","excerpt":"时间就像一只箭，射向未知的前方，把过去永远留在后面。Time is like an arrow, shooting towards the unknown, leaving the past behind forever.","text":"时间就像一只箭，射向未知的前方，把过去永远留在后面。Time is like an arrow, shooting towards the unknown, leaving the past behind forever. 每个人 你所热爱的一切Every piece of everyone, of everything you love, 你所憎恨的一切 of everything you hate, 你所拥有的最宝贵的东西 of everything you hold most precious, 在宇宙生命中最为伊始的几分钟内 was assembled by the forces of nature 由自然的力量合成 in the first few minutes of the universe, 在恒星的中心转化 transformed in the hearts of starts 或者在它们燃烧的消亡中诞生 or created in their fiery deaths 而当你去世的时候 And when you die, 这些碎片将回到宇宙中 those pieces will be returned to the universe 进入无限的死亡又重生的轮回之中 in the endless cycle of death and rebirth. 太阳的命运也是所有恒星的命运 The fate of the sun is the same as for all starts. 终有一天 他们都会消亡 One day, they must all eventually die 宇宙将会陷入永无止境的黑暗之中 and the cosmos will be plunged into eternal night. 这便是时间箭头最深远的影响 And this is the most profound consequence of the arrow of time. 我刚用相机捕捉到的 The light that I’ve just captured 这个光点二百五十万年前踏上了旅程 in my camera began its journey 2.5 million years ago. 那时地球上还没有人类 At that time, on Earth, there were no humans. 远古的祖先能人 Homo habilis, our distant ancestors, 正在非洲广袤的平原上漫步 wereroaming the plaints of Africa, 就是在那些光线 and as those light rays travelled 平行于无垠宇宙的同时 through the vastness of space. 人类不断进化 our species evolved,and thousands 一代又一代的生老病死 and thousands and thousands of qenerations of humans lived 周而复始 and died, 旅途开始的二百五十万年后 and then 2.5 million years after their journey began, 这些远道而来的信使 these messengers from the depths of space 穿越漫长的时间 and from way back in our past, 映入现在我们的眼帘 arrived here on Earth. 我们与那些遥远星系息息相关 we are in a very real sense connected to these qalaxies, 无论它们是如何与我们天各一方 no matter how far away they are across the universe, 那些经历过数十亿年旅行到达地球的光线 connected by the light 终究会把我们联系在一起 that’s journeved billions of ears to reach us. “生星时代” The Stelliferous Era - 恒星漫天的时代 the age of the starts. 我们的太阳只是银河系两千亿颗恒星中的一颗 Our sun is just one of 200 billion starts in our qalaxy. 我们的星系也只是可观测的宇宙范围内的 Our qalaxy is one of 100 bllion 一百亿个星系中的一个 in the observable universe. 数不尽的星球上有数不胜数的岛屿 And countless islands of countless stars. 当我们注意太空的时候 When we look out into space, 我们也是在寻找自己的起源 we are looking into our own origins. 我们的故事就是宇宙的故事 Our story is the story of the universe. 因为我们是恒星真正的孩子 Beacuse we are truly children of the starts. 注入进我们的身体的 and written into every atom 每一个原子和分子 and every molecule of our bodies 就是宇宙从大爆炸 is the entire history of the universe 到现在全部的历史 from the Big Bang to the present day. 对我们来说 像婚戒黄金一样的珍宝 It’s quite a thought that something as 实际上也可以在一颗遥远的 数百万光年 precious to us as the gold in a wedding ring was 甚至数十亿光年远的恒星上产生actually forged in the death of a distant star. 从宇宙起源到最后一个黑洞消失的这个过程中 as measured from its beainning to the evappration of the last black hole. 生命 正如我们所知life as we konw it, is only possible 只有百分之千亿分之for one thousandth of a billion billion billionth 千亿分之千分之一的可能性billion billion billionth billion billion billionth of a percent. 所以对于我来说 And that’s why for me 宇宙中最迷人的奇迹不是恒星 the most astonishing wonder of the universe isn’t a star 不是行星 也不是星系 or a planet or a galaxy 甚至根本不是一个物质 It isn’t a thing at all 而是时间里的一瞬间 It’s an instant in time 那个瞬间 就是现在 And that time is now. 当我们仰望天空 You see, when we look up into the sky, 望向遥远的恒星和星系时 at distant starts and galaxies, 我们其实是在仰望过去 then we’re looking back in time 因为光从那些遥远天体到达地球需要时间 because the light takes time to journey from them to us. 而光从那个红点处传播到我们这里 And the light from that red dot has been travelling to us 差不多经历了整个宇宙史 for almost the entire history of the universe. 我们看到的是一百三十亿年前 You see, what we’re looking at here is an event that happened 发生的事件 13 billion years ago. 我们看到的是宇宙初期的一颗恒星 What we’re looking at here is the explosive death 爆炸灭亡的景象 of one of the first starts in the universe. 一日为二十四个小时 A day on Earth is the 24 hours 即地球绕轴自转一周 it takes our planet to rotate once on its axis. 一月为二十九天半 Our months are based on the 29-and-a-half days 即月亮在夜空完成盈亏圆缺 it takes the moon to wax and wane in the night sky. 一年为三百六十五天又四分之一天 And a year is the 365-and-a-quarter days 即地球绕太阳公转一周 it takes us to orbit once around the sun. 人类的生命便消逝在这些熟悉的时间量程之内 These familiar timescales mark the passing of our lives. 这是宇宙无法避免的真相 It’s an inescapable fact of the universe, 也被写入了物理学基本定律 written into the fundamental laws of physics, 整个宇宙将消亡 The entire cosmos will die, 银河系中的两千亿恒星将全部消亡 Every single one of the 200 billion starts in our galaxy will go out. 如同太阳末日 便是地球末日 And just as the death of the sun means the end of life on our planet, 每一颗恒星的灭亡 so the death of every star 都可能预示着宇宙中其他某种生命的灭亡 will extinguish any possibility of life in the universe. 永恒的变化是人类生命中最基本的部分 Permanent change is a fundamental part of what it means to be human. 随着时间流逝 我们都会变老We all age as the years pass by. 人们出生 成长 死亡People are born, they live, they die. 我想这只是生命中悲悲喜喜的一部分I suppose it’s part of the joy and tragedy of our lives. 但纵观宇宙But out there in the universe, 那些宏伟的如史诗般的循环好像永恒不变those grand and epic cycles appear eternal and unchanging. 然而这只是错觉But that’s an illlusion. 宇宙长河就如同我们的生命一样You see, in the life of the universe, just in our lives, 一切都在不可逆转地变化everything is irreversibly changing. 我们从没见过浪花离开过湖面We never see waves travelling across lakes. 聚集在一起 组成巨大的冰块重回冰川coming together and bouncing chunks of ice back onto glaciers. 我们被迫前往将来We are compelled to travel into the future. 那是因为And that’s because 时间箭头规定 随着时间流逝the arrow of time dictates that as each moment passes, 万物也在发生变化things change. 变化一旦发生 就无法更改And once these changes have happend, they are never undone. 这些循环看似永恒不变These cycles seem eternal and unchanging, 但随着时间之书徐徐展开but as the story of time unfolds, 一条真理跃然眼前afundamental truth is revealed. 没有什么能够永恒Nothing lates forever.","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Time","slug":"随笔/time","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/time/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"Time","slug":"time","permalink":"https://blog.mhuig.top/tags/time/"}]},{"title":"Hello World","slug":"pen/Hello World","date":"2020-04-19T06:24:29.000Z","updated":"2020-04-19T06:24:29.000Z","comments":true,"path":"posts/4a17b156.html","link":"","permalink":"https://blog.mhuig.top/posts/4a17b156.html","excerpt":"你好，世界！","text":"你好，世界！ 全新的主题，熟悉的代码，怎能不再次心动？ 这是 MHuiG 的又一个博客！ 原博客数据不再迁移此处，将其作为该博客的子站我的 NoteBook: https://mhuig.github.io/NoteBook/ 后注: 原博客数据已迁移此处 【NoteBook已闭源】","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Hello","slug":"随笔/hello","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/hello/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"Hello","slug":"hello","permalink":"https://blog.mhuig.top/tags/hello/"}]},{"title":"理解卷积","slug":"others/ml/理解卷积","date":"2020-03-09T12:05:42.000Z","updated":"2020-03-09T12:05:42.000Z","comments":true,"path":"posts/xa6297acc.html","link":"","permalink":"https://blog.mhuig.top/posts/xa6297acc.html","excerpt":"本文的目的是深入的理解卷积,通过一些示例,卷积将会成为一个非常简单的想法.","text":"本文的目的是深入的理解卷积,通过一些示例,卷积将会成为一个非常简单的想法. 抛球的经验想象一下，我们将一个球从某个高度掉落到地面上，球在该地面只能在一个方向上运动,如果您将球放下然后从其着陆点上方再次放下,球可能会前进的距离是多少?让我们分解一下。在第一次下落之后，它将以概率降落距起点个单位，其中是概率分布。现在，在第一次下落之后，我们将球捡起并从其首次着陆点上方的另一个高度下落。球从新起点滚动单位的概率为,其中如果从不同高度掉落，则可能是不同的概率分布。 如果我们确定了第一个的结果，我们就知道了球的移动距离，对于球的总距离为，第二个球的移动距离也固定为，其中。因此发生这种情况的可能性就是。 让我们考虑一个具体的离散示例。 我们希望总距离为。如果它第一次滚动，则第二次它必须滚动,才能达到我们的总距离。 这个概率是。 但是，这不是我们达到的总距离的唯一方法。球第一次可以滚动个单位，第二次可以滚动个单位。 或第一次为，第二次为。 只要将和加和等于，它就可以取任意值。 这个概率分别是和。 为了找出球到达总距离的总可能性，我们不能仅考虑一种到达的可能方式。 相反，我们考虑将划分为两个球和的所有可能方法，并对每种方法的概率求和。我们已经知道，对于每种情况，的概率就是。 因此，对的每个解求和，我们可以将总似然表示为：事实证明，我们正在做卷积! 特别地，定义在处的和的卷积定义为：如果我们用代替，我们得到：这是卷积的标准定义。为了更具体一点，我们可以考虑球可能着陆的位置。 在第一次下降之后，它将以概率降落在中间位置。 如果它降落在处，则它有概率降落在位置处。 为了得到卷积，我们考虑所有中间位置。 可视化卷积有一个很好的技巧，可以帮助人们更轻松地思考卷积。 首先，观察。 假设一个球从其起点降落一定距离的概率为。 然后，此后，它从着陆点开始的距离为的概率为。 如果我们知道球在第二次下降后落在位置上，那么前一个位置是的概率是多少？ 因此，先前位置为的概率为。 现在，考虑每个中间位置有助于球最终降落在的概率。 我们知道第一滴将球放到中间位置的概率是。 我们还知道，如果它降落在上，它进入的概率为。 对所有求和，我们得到了卷积。 这种方法的优点是，它使我们可以在单个图片中可视化卷积在值处的评估。 通过移动下半部分，我们可以评估其他值的卷积。 这使我们能够从整体上理解卷积。 例如，我们可以看到分布对齐时达到峰值。 且随着分布之间的交点变小而缩小。 过在动画中使用此技巧，实际上可以从视觉上理解卷积。 下面，我们可以看到两个框函数的卷积： 有了这种观点，很多事情就会变得更加直观。 让我们考虑一个非概率示例。 卷积有时在音频处理中使用。 例如，一个函数可能会使用其中有两个尖峰但在其他所有地方为零的函数来创建回波。 当我们的双尖峰功能滑动时，一个尖峰首先击中一个时间点，将该信号添加到输出声音中，然后又出现另一个尖峰，并添加第二个延迟副本。 高维卷积卷积是一个非常笼统的想法。我们还可以将它们用于更大的尺寸。让我们再次考虑一个落球的例子。现在，随着位置的下降，它的位置不仅在一维，而在二维。 卷积与以前相同：除了，现在，和是向量。更明确地说， 或在标准定义中：就像一维卷积一样，我们可以将二维卷积视为将一个函数滑动到另一个函数之上，相乘和相加。一种常见的应用是图像处理。我们可以将图像视为二维函数。许多重要的图像转换都是卷积，您可以在其中将图像函数与一个非常小的局部函数（称为“卷积核”）进行卷积。 卷积核滑动到图像的每个位置，并计算一个新像素作为其浮动像素的加权和。例如，通过平均3x3像素矩阵，我们可以使图像模糊。 为此，我们的内核在框中的每个像素上取值， 我们还可以通过在两个相邻像素上取值和，在其他所有位置取零来检测边缘。 也就是说，我们减去两个相邻像素。 当并排像素相似时，这大约等于零。 然而，在边缘上，相邻像素在垂直于边缘的方向上有很大不同。 卷积神经网络那么，卷积与卷积神经网络有何关系？ 考虑一个具有输入和输出的一维卷积层，就像这样： 正如我们观察到的，我们可以用输入来描述输出：通常，A将是多个神经元。 但是假设它只是一个神经元。回想一下，神经网络中的典型神经元描述为：其中，是输入。 权重，描述了神经元如何连接到其输入。 负权重表示输入抑制神经元的激活，而正权重则鼓励神经元激活。 权重是神经元的心脏，控制神经元的行为。说多个神经元是相同的，这与说权重相同是一回事。卷积将为我们处理的是神经元的这种连线，描述了所有权重以及哪些权重相同。通常，我们一次而不是单独描述一个层中的所有神经元。 诀窍是要有一个权重矩阵：例如，我们得到：矩阵的每一行都描述了将神经元连接到其输入的权重。但是，返回到卷积层，因为同一神经元有多个副本，所以许多权重出现在多个位置。 对应于等式：因此，通常，权重矩阵会将每个输入连接到具有不同权重的每个神经元：像上面的卷积层的矩阵看起来很不一样。 相同的权重出现在多个位置中。 而且由于神经元没有连接到许多可能的输入，因此存在很多零。与上述矩阵相乘与与，，卷积相同。 滑动到不同位置的功能对应于在那些位置具有神经元。二维卷积层呢？ 二维卷积层的布线对应于二维卷积。考虑上面的示例，通过使用卷积来检测图像边缘，方法是在周围滑动一个内核并将其应用于每个面片。 就像这样，卷积层会将神经元应用于图像的每个patch。 结论我们在此博客文章中介绍了许多数学机制，但是获得的结果可能并不明显。 卷积显然是概率论和计算机图形学中的有用工具，但是从卷积的角度说卷积神经网络有什么好处呢？ 第一个优点是我们拥有一些非常强大的语言来描述网络的布线。 到目前为止，我们所处理的示例还不够复杂，以至于无法清楚地看到这种好处，但是通过卷积可以使我们摆脱大量令人不快的簿记工作。 其次，卷积具有明显的实施优势。 许多库提供高效的卷积例程。 此外，尽管卷积天真地看起来是运算，但使用一些相当深的数学见解，就有可能创建实现。 我们将在以后的文章中对此进行更详细的讨论。 实际上，在GPU上使用高效并行卷积实现对于计算机视觉的最新进展至关重要。 参考文献[English] Understanding Convolutions","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"},{"name":"概率","slug":"math/概率","permalink":"https://blog.mhuig.top/categories/math/%E6%A6%82%E7%8E%87/"}],"tags":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"},{"name":"概率","slug":"概率","permalink":"https://blog.mhuig.top/tags/%E6%A6%82%E7%8E%87/"},{"name":"神经网络","slug":"神经网络","permalink":"https://blog.mhuig.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"JavaScript反调试","slug":"web/JavaScript反调试","date":"2020-02-26T02:36:40.000Z","updated":"2020-02-26T02:36:40.000Z","comments":true,"path":"posts/bfe2ff2a.html","link":"","permalink":"https://blog.mhuig.top/posts/bfe2ff2a.html","excerpt":"总结一下关于JavaScript反调试技巧方面的内容。本文的目的是收集与JavaScript中的反调试有关的小窍门（其中一些已经被恶意软件或商业产品使用）。","text":"总结一下关于JavaScript反调试技巧方面的内容。本文的目的是收集与JavaScript中的反调试有关的小窍门（其中一些已经被恶意软件或商业产品使用）。 对于JavaScript来说，你只需要花一点时间进行调试和分析，你就能够了解到JavaScript代码段的功能逻辑。而我们所要讨论的内容，可以给那些想要分析你JavaScript代码的人增加一定的难度。不过我们的技术跟代码混淆无关，我们主要针对的是如何给代码主动调试增加困难。本文所要介绍的技术方法大致如下： 检测未知的执行环境（我们的代码只想在浏览器中被执行）； 检测调试工具（例如DevTools）； 代码完整性控制； 流完整性控制； 反模拟； 简而言之，如果我们检测到了“不正常”的情况，程序的运行流程将会改变，并跳转到伪造的代码块，并“隐藏”真正的功能代码。 函数重定义这是一种最基本也是最常用的代码反调试技术了。在JavaScript中，我们可以对用于收集信息的函数进行重定义。比如说，console.log() 函数可以用来收集函数和变量等信息，并将其显示在控制台中。如果我们重新定义了这个函数，我们就可以修改它的行为，并隐藏特定信息或显示伪造的信息。我们可以直接在DevTools中运行这个函数来了解其功能： 1234console.log(&quot;HelloWorld&quot;);var fake = function() &#123;&#125;;window[&#x27;console&#x27;][&#x27;log&#x27;]= fake;console.log(&quot;Youcan&#x27;t see me!&quot;); 运行后我们将会看到： 1VM127:1 HelloWorld 你会发现第二条信息并没有显示，因为我们重新定义了这个函数，即“禁用”了它原本的功能。但是我们也可以让它显示伪造的信息。比如说这样： 123456789101112131415161718192021console.log(&quot;Normalfunction&quot;);//First we save a reference to the original console.log functionvar original = window[&#x27;console&#x27;][&#x27;log&#x27;];//Next we create our fake function//Basicly we check the argument and if match we call original function with otherparam.// If there is no match pass the argument to the original functionvar fake = function(argument) &#123; if (argument === &quot;Ka0labs&quot;) &#123; original(&quot;Spoofed!&quot;); &#125; else &#123; original(argument); &#125;&#125;// We redefine now console.log as our fake functionwindow[&#x27;console&#x27;][&#x27;log&#x27;]= fake;//Then we call console.log with any argumentconsole.log(&quot;Thisis unaltered&quot;);//Now we should see other text in console different to &quot;Ka0labs&quot;console.log(&quot;Ka0labs&quot;);//Aaaand everything still OKconsole.log(&quot;Byebye!&quot;); 如果一切正常的话： 1234VM84:1 NormalfunctionVM84:11 Thisis unalteredVM84:9 Spoofed!VM84:11 Byebye! 实际上，为了控制代码的执行方式，我们还能够以更加聪明的方式来修改函数的功能。比如说，我们可以基于上述代码来构建一个代码段，并重定义 eval 函数。我们可以把JavaScript代码传递给 eval 函数，接下来代码将会被计算并执行。如果我们重定义了这个函数，我们就可以运行不同的代码了： 123456789101112131415161718//Just a normal evaleval(&quot;console.log(&#x27;1337&#x27;)&quot;);//Now we repat the process...var original = eval;var fake = function(argument) &#123; // If the code to be evaluated contains1337... if (argument.indexOf(&quot;1337&quot;) !==-1) &#123; // ... we just execute a different code original(&quot;for (i = 0; i &lt; 10;i++) &#123; console.log(i);&#125;&quot;); &#125; else &#123; original(argument); &#125;&#125;eval= fake;eval(&quot;console.log(&#x27;Weshould see this...&#x27;)&quot;);//Now we should see the execution of a for loop instead of what is expectedeval(&quot;console.log(&#x27;Too1337 for you!&#x27;)&quot;); 运行结果如下： 123456789101112VM171:1 1337VM172:1 Weshould see this...VM173:1 0VM173:1 1VM173:1 2VM173:1 3VM173:1 4VM173:1 5VM173:1 6VM173:1 7VM173:1 8VM173:1 9 通过这种方式修改程序流是一个很酷的技巧，但是正如我们在一开始所说的那样，它是最基本的技巧，很容易被发现并被击败。这是因为在JavaScript中，每个函数都有一个方法toString（或Firefox中的toSource）返回其自己的代码。因此，仅需要检查所需函数的代码是否已更改。当然，我们可以重新定义方法toString / toSource，但是我们陷入了同样的情况：function.toString.toString() 。 断点为了帮助我们了解代码的功能，JavaScript调试工具（例如DevTools）都可以通过设置断点的方式阻止脚本代码执行，而断点也是代码调试中最基本的了。如果你研究过调试器或者x86架构，你可能会比较熟悉0xCC指令。在JavaScript中，我们有一个名叫debugger的类似指令。当我们在代码中声明了debugger函数后，脚本代码将会在debugger指令这里停止运行。比如说： 123console.log(&quot;Seeme!&quot;);debugger;console.log(&quot;Seeme!&quot;); 很多商业产品会在代码中定义一个无限循环的debugger指令，不过某些浏览器会屏蔽这种代码，而有些则不会。这种方法的主要目的就是让那些想要调试你代码的人感到厌烦，因为无限循环意味着代码会不断地弹出窗口来询问你是否要继续运行脚本代码： 1setTimeout(function()&#123;while (true) &#123;eval(&quot;debugger&quot;)&#125;&#125;) 1setInterval(function() &#123;var a = new Date(); debugger; return new Date() - a &gt; 100;&#125;, 100); 时间差异这是一种从传统反逆向技术那里借鉴过来的基于时间的反调试技巧。当脚本在DevTools等工具环境下执行时，运行速度会非常慢（时间久），所以我们就可以根据运行时间来判断脚本当前是否正在被调试。比如说，我们可以通过测量代码中两个设置点之间的运行时间，然后用这个值作为参考，如果运行时间超过这个值，说明脚本当前在调试器中运行。演示代码如下： 1234567891011setInterval(function()&#123; var startTime = performance.now(), check,diff; for (check = 0; check &lt; 1000; check++)&#123; console.log(check); console.clear(); &#125; diff = performance.now() - startTime; if (diff &gt; 200)&#123; alert(&quot;Debugger detected!&quot;); &#125;&#125;,500); DevTools检测（I）[Chrome]：getter这项技术利用的是div元素中的id属性，当div元素被发送至控制台（例如 console.log(div) ）时，浏览器会自动尝试获取其中的元素id。如果代码在调用了 console.log 之后又调用了 getter 方法，说明控制台当前正在运行。简单的概念验证代码如下： 123456789let div = document.createElement(&#x27;div&#x27;);let loop = setInterval(() =&gt; &#123; console.log(div); console.clear();&#125;);Object.defineProperty(div,&quot;id&quot;, &#123;get: () =&gt; &#123; clearInterval(loop); alert(&quot;Dev Tools detected!&quot;);&#125;&#125;); DevTools检测（II）[Chrome]：大小更改如果打开了DevTools（除非将其取消对接打开），则 window.outerWidth / Height 和 window.innerWidth / Height 之间的差异将发生变化，因此可以循环检测。Devtools-detect使用此技巧： 123const widthThreshold = window.outerWidth - window.innerWidth &gt; threshold;const heightThreshold = window.outerHeight - window.innerHeight &gt; threshold;const orientation = widthThreshold ? &#x27;vertical&#x27; : &#x27;horizontal&#x27;; 隐式流完整性控制当我们尝试对JavaScript代码段进行模糊处理时，第一步就是开始重命名一些变量和函数，以阐明源代码。您只需将代码拆分为较小的代码块，然后开始在此处和此处重命名。在JavaScript中，我们可以检查函数名称是否已更改或保持相同的名称。或更准确地说，我们可以检查堆栈跟踪是否包含原始名称和原始顺序。使用arguments.callee.caller，我们可以创建堆栈跟踪，以保存先前执行的函数。我们可以使用此信息来生成一个哈希，该哈希将成为用于生成用于解密JavaScript其他部分的密钥的种子。这样，我们就可以对流的完整性进行隐式控制，因为如果重命名功能或要执行的功能顺序稍有不同，则创建的哈希将完全不同。如果哈希不同，则生成的密钥也将不同。如果密钥不同，则无法解密代码。为了更好地理解它，请参见下一个示例： 123456789101112131415161718192021function getCallStack() &#123; var stack = &quot;#&quot;, total = 0, fn =arguments.callee; while ( (fn = fn.caller) ) &#123; stack = stack + &quot;&quot; +fn.name; total++ &#125; return stack&#125;function test1() &#123; console.log(getCallStack());&#125;function test2() &#123; test1();&#125;function test3() &#123; test2();&#125;function test4() &#123; test3();&#125;test4(); 执行此代码时，您将看到字符串 #test1test2test3test4 。如果我们修改（我邀请您这样做）任何函数的名称，返回的字符串也将不同。我们可以使用该字符串计算安全哈希，然后将其用作种子，以得出用于解密其他代码块的密钥。有趣的是，如果由于密钥无效（分析人员更改了函数名称）而无法解密下一个代码块，则可以捕获异常并将执行流重定向到伪路径。 1VM50:10 #test1test2test3test4 请记住，此技巧需要与强大的混淆功能结合在一起才能使用。 隐式代码完整性控制 在“ 函数重新定义”部分的结尾，我们提到可以使用toString（）方法检索JavaScript中函数的代码。就像我们说过的那样，这对于检查函数是否已重新定义很有用，实际上，可以使用相同的想法来知道函数的代码是否被修改。 效果较差的方法是计算函数或代码块的哈希并将其与已知表进行比较。但是这种方法确实很愚蠢。一种更现实，更有效的方法可以重复使用我们之前在堆栈跟踪中使用的相同策略。我们可以计算代码块的哈希值，并将其用作解密其他代码块的密钥。 创建隐式完整性控件的最漂亮方法是在md5中使用冲突。基本上，我们可以创建在自己的函数中测试其自己的md5的函数。为了在功能内执行检查，我们需要进行碰撞处理（我们想创建类似的东西 function(){ if (md5(arguments.callee.toString() === &#39;&lt;md5&gt;&#39;) code_function; } )。 该技术背后的概念与用于生成图像文件的概念相同，在自己的图片中显示了md5校验和。这是一个经典示例：显示自己的md5校验和的gif。 (注:本站的图片处理策略更改了图片md5值,点击查看原图=&gt; 显示自己的md5校验和的gif) 关于如何产生这种冲突，有大量的文章（甚至在PoC || GTFO中出现了一些示例），但是我阅读并可以复制的第一个文章是使用PHP编写的。您可以非常快速地预先计算生成碰撞所需的块。实际上，这是@cgvwzq创建的示例，通过这种方式检查了函数内容的完整性。 如前所述，我们需要对这种技术进行强力混淆。 代理对象(old,已弃用)代理对象是目前JavaScript中最有用的一个工具，这种对象可以帮助我们了解代码中的其他对象，包括修改其行为以及触发特定环境下的对象活动。比如说，我们可以创建一个嗲哩对象并跟踪每一次document.createElement调用，然后记录下相关信息： 123456789const handler = &#123; // Our hook to keep the track apply: function (target, thisArg, args)&#123; console.log(&quot;Intercepted a call tocreateElement with args: &quot; + args); return target.apply(thisArg, args) &#125;&#125; document.createElement= new Proxy(document.createElement, handler) // Create our proxy object withour hook ready to interceptdocument.createElement(&#x27;div&#x27;); 接下来，我们可以在控制台中记录下相关参数和信息： 1VM216:3 Intercepted a call tocreateElement with args: div 我们可以利用这些信息并通过拦截某些特定函数来调试代码，但是本文的主要目的是为了介绍反调试技术，那么我们如何检测“对方”是否使用了代理对象呢？其实这就是一场“猫抓老鼠”的游戏，比如说，我们可以使用相同的代码段，然后尝试调用toString方法并捕获异常： 123456//Call a &quot;virgin&quot; createElement:try &#123; document.createElement.toString();&#125;catch(e)&#123; console.log(&quot;I saw your proxy!&quot;);&#125; 信息如下： 1&quot;function createElement() &#123; [native code] &#125;&quot; 但是当我们使用了代理之后： 123456789101112131415//Then apply the hookconst handler = &#123; apply: function (target, thisArg, args)&#123; console.log(&quot;Intercepted a call tocreateElement with args: &quot; + args); return target.apply(thisArg, args) &#125;&#125;document.createElement= new Proxy(document.createElement, handler); //Callour not-so-virgin-after-that-party createElementtry &#123; document.createElement.toString();&#125;catch(e) &#123; console.log(&quot;I saw your proxy!&quot;);&#125; 没错，我们确实可以检测到代理： 1VM391:13 I saw your proxy! 我们还可以添加toString方法： 1234567891011121314const handler = &#123; apply: function (target, thisArg, args)&#123; console.log(&quot;Intercepted a call tocreateElement with args: &quot; + args); return target.apply(thisArg, args) &#125;&#125;document.createElement= new Proxy(document.createElement, handler);document.createElement= Function.prototype.toString.bind(document.createElement); //Add toString//Callour not-so-virgin-after-that-party createElementtry &#123; document.createElement.toString();&#125;catch(e) &#123; console.log(&quot;I saw your proxy!&quot;);&#125; 现在我们就没办法检测到了： 1&quot;function createElement() &#123; [native code] &#125;&quot; 代理对象异常把戏不能再使用了。幸运的是，我们仍然可以通过toString长度检测代理对象的使用。例如，document.createElement的大小为42（Chrome）： 12document.createElement.toString().length42 另一方面，当我们创建代理时，此值将更改： 1234567891011const handler = &#123; apply: function(target, thisArg, args) &#123; console.log(&quot;Intercepted call&quot;); return target.apply(thisArg, args); &#125;&#125;document.createElement = new Proxy(document.createElement, handler);document.createElement.toString().length29 因此，我们可以执行以下操作： 123456if (document.createElement.toString().length &lt; 30) &#123; console.log(&quot;I saw your proxy&quot;);&#125;else &#123; console.log(&quot;Not a proxy&quot;);&#125; 此技巧不能在windoww对象中使用，但仍然有用。 限制环境如引言中所述，我们想要做的一件事就是尝试检测代码是否在正确的环境中执行。我们所谓的“正确的环境”是： 该代码正在浏览器（不是仿真器，不是NodeJS等）中执行。 该代码正在指定给它的域/资源中执行（不是本地服务器） 例如，我们可以用来证明代码是否在本地执行的简单检查是： 1234// Pretty stupid idea found in commercial softwareif (location.hostname === &quot;localhost&quot; || location.hostname === &quot;127.0.0.1&quot; || location.hostname === &quot;&quot;) &#123; console.log(&quot;Don&#x27;t run me here!&quot;)&#125; 如果我们在本地html中运行此JavaScript代码段，则会看到以下消息： 1VM28:3 Don&#x27;t run me here! 按照这个想法，另一个检查选项是用于打开文档的处理程序（类似if (location.protocol == ‘file:’){…}），或者尝试通过HTTP请求进行测试，以确定是否有其他资源（图像，css等）可用。当然，所有这些方法都非常容易被绕过。 如果代码是在NodeJS中执行的（或者正如我们在本文中提到的：将流更改为伪造的路径），则可以避免执行代码。这很危险，但是我在野外看到使用NodeJS来解决JavaScript挑战并绕过反暴力缓解措施。我们可以尝试检测仅存在于浏览器上下文中的对象的存在： 12345678//Under NodeJS try &#123; console.log(window); &#125; catch(e)&#123; console.log(&quot;NodeJS detected!!!!&quot;); &#125; NodeJS detected!!!! 反之亦然：在NodeJS中，我们具有浏览器上下文中不存在的对象。 1234567891011//Under the browserconsole.log(global)VM104:1 Uncaught ReferenceError: global is not defined at &lt;anonymous&gt;:1:13//Under NodeJS console.log(global)&#123; console: Console &#123; log: [Function: bound log],... ... 我们可以搜索仅存在于浏览器中的大量元数据。我们可以检索到的一些此类想法可以在Panopticlick Project中看到。 相关文献javascript-antidebugging","categories":[{"name":"JavaScript","slug":"javascript","permalink":"https://blog.mhuig.top/categories/javascript/"}],"tags":[{"name":"JavaScript","slug":"javascript","permalink":"https://blog.mhuig.top/tags/javascript/"},{"name":"反调试","slug":"反调试","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E8%B0%83%E8%AF%95/"}]},{"title":"Python如何调用C","slug":"Python/Python如何调用C","date":"2020-02-24T07:45:30.000Z","updated":"2020-02-24T07:45:30.000Z","comments":true,"path":"posts/b6ffaac2.html","link":"","permalink":"https://blog.mhuig.top/posts/b6ffaac2.html","excerpt":"Python语言特点：简单，明确，优雅，高效率，同时Python语言的可扩展性和可嵌入性很强，又被成为“胶水语言”。但是Python语言有一个最大的缺点，便是运行速度慢，所以当你对速度有要求时，你可以用C语言来编写你的关键代码，或者当你希望某些算法不公开时，也可以把你的程序用C编写，然后在你的Python程序中使用它们。本文将介绍在Python程序中如何调用C…","text":"Python语言特点：简单，明确，优雅，高效率，同时Python语言的可扩展性和可嵌入性很强，又被成为“胶水语言”。但是Python语言有一个最大的缺点，便是运行速度慢，所以当你对速度有要求时，你可以用C语言来编写你的关键代码，或者当你希望某些算法不公开时，也可以把你的程序用C编写，然后在你的Python程序中使用它们。本文将介绍在Python程序中如何调用C… 编写C语言代码一个简单的c语言程序，实现了两个整数的加法运算 12345#include &lt;stdio.h&gt;int sum(int a,int b)&#123; return a + b;&#125; 生成so库文件使用命令： 1gcc -fPIC -shared main.c -o lib.so so库文件不能跨平台使用，如果你在Windows下面生成的，便只能够在Windows下面使用，使用命令以后，生成后缀为.so的库文件 编写Python程序来调用C语言 把so库文件放入我们的Python项目中 使用ctypes库中的CDLL来加载库 lib_main = CDLL(‘so库文件路径’) 调用C sum_value = lib_main.sum(10, 20) 123456789# ctypes的库from ctypes import *# 加载so库lib_main = CDLL(&#x27;./lib.so&#x27;) # CDLL加载库sum_value = lib_main.sum(10, 20)print(sum_value) 最终得到结果30 ctypes库是Python提供的一个外部函数库，提供C语言兼容集中数据类型，可以允许调用C编译好的库，已下附上ctypes库官方文档：https://docs.python.org/3/library/ctypes.html","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"}]},{"title":"Npm更换源","slug":"web/npm更换源","date":"2020-02-23T11:02:43.000Z","updated":"2020-02-23T11:02:43.000Z","comments":true,"path":"posts/ba036091.html","link":"","permalink":"https://blog.mhuig.top/posts/ba036091.html","excerpt":"由于npm的源在国外，所以国内用户使用起来有很多不方便，比如拖慢下载速度。","text":"由于npm的源在国外，所以国内用户使用起来有很多不方便，比如拖慢下载速度。 如何使用有很多方法来配置npm registry地址，下面根据不同情境列出几种比较常用的方法。以淘宝npm为例 临时使用1npm --registry https://registry.npm.taobao.org install express 持久使用1npm config set registry https://registry.npm.taobao.org 配置后可通过下面方式来验证是否成功： 1npm config get registry 或者 1npm info express 更换为淘宝npm源1npm install -g cnpm --registry=https://registry.npm.taobao.org 通过cnpm更新模块 1cnpm install expresstall express 使用官方镜像1npm config set registry https://registry.npmjs.org/ 总结更换为淘宝npm源 1npm install -g cnpm --registry=https://registry.npm.taobao.org 使用官方镜像 1npm config set registry https://registry.npmjs.org/ NPM检查并更新项目依赖的版本 12345678# 安装npm install -g npm-check-updates# 检查当前目录下可更新的依赖项ncu# 升级 package.jsonncu -u# 根据更新的 package.json 安装新版本npm install","categories":[{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/categories/npm/"}],"tags":[{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/tags/npm/"}]},{"title":"LBS 球面距离公式","slug":"math/LBS 球面距离公式","date":"2020-02-23T01:40:32.000Z","updated":"2020-02-23T01:40:32.000Z","comments":true,"path":"posts/5e7ad437.html","link":"","permalink":"https://blog.mhuig.top/posts/5e7ad437.html","excerpt":"","text":"维基百科推荐使用公式，理由是公式用到了大量余弦函数， 而两点间距离很短时（比如地球表面上相距几百米的两点），余弦函数会得出0.999…的结果， 会导致较大的舍入误差。而公式采用了正弦函数，即使距离很小，也能保持足够的有效数字。 以前采用三角函数表计算时的确会有这个问题，但经过实际验证，采用计算机来计算时，两个公式的区别不大。 稳妥起见，这里还是采用公式。 Haversine公式⊿ 其中 为地球半径，可取平均值, 表示两点的纬度； 表示两点经度的差值。 距离计算函数下面就是计算球面间两点之间距离的函数。 12345678910111213141516171819202122from math import sin, asin, cos, radians, fabs, sqrt EARTH_RADIUS=6371 # 地球平均半径，6371km def hav(theta): s = sin(theta / 2) return s * s def get_distance_hav(lat0, lng0, lat1, lng1): \"用haversine公式计算球面两点间的距离。\" # 经纬度转换成弧度 lat0 = radians(lat0) lat1 = radians(lat1) lng0 = radians(lng0) lng1 = radians(lng1) dlng = fabs(lng0 - lng1) dlat = fabs(lat0 - lat1) h = hav(dlat) + cos(lat0) * cos(lat1) * hav(dlng) distance = 2 * EARTH_RADIUS * asin(sqrt(h)) return distance 相关文献LBS 球面距离公式","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"},{"name":"距离","slug":"距离","permalink":"https://blog.mhuig.top/tags/%E8%B7%9D%E7%A6%BB/"}]},{"title":"NoSQL-Neo4j","slug":"bigdata/NoSql/NoSQLNeo4j","date":"2020-02-09T02:41:51.000Z","updated":"2020-02-09T02:41:51.000Z","comments":true,"path":"posts/9b624d43.html","link":"","permalink":"https://blog.mhuig.top/posts/9b624d43.html","excerpt":"NoSQL Neo4j PDF","text":"NoSQL Neo4j PDF GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"NoSQL","slug":"大数据/nosql","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/nosql/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"NoSQL","slug":"nosql","permalink":"https://blog.mhuig.top/tags/nosql/"},{"name":"Neo4j","slug":"neo4j","permalink":"https://blog.mhuig.top/tags/neo4j/"}]},{"title":"NoSQL-MongoDB","slug":"bigdata/NoSql/NoSQLMongoDB","date":"2020-02-09T02:41:50.000Z","updated":"2020-02-09T02:41:50.000Z","comments":true,"path":"posts/ec477027.html","link":"","permalink":"https://blog.mhuig.top/posts/ec477027.html","excerpt":"NoSQL MongoDB PDF","text":"NoSQL MongoDB PDF GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"NoSQL","slug":"大数据/nosql","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/nosql/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"NoSQL","slug":"nosql","permalink":"https://blog.mhuig.top/tags/nosql/"},{"name":"MongoDB","slug":"mongodb","permalink":"https://blog.mhuig.top/tags/mongodb/"}]},{"title":"NoSQL-HBase","slug":"bigdata/NoSql/NoSQLHBase","date":"2020-02-09T02:41:49.000Z","updated":"2020-02-09T02:41:49.000Z","comments":true,"path":"posts/cdec63d.html","link":"","permalink":"https://blog.mhuig.top/posts/cdec63d.html","excerpt":"NoSQL HBase PDF","text":"NoSQL HBase PDF GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"NoSQL","slug":"大数据/nosql","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/nosql/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"NoSQL","slug":"nosql","permalink":"https://blog.mhuig.top/tags/nosql/"},{"name":"HBase","slug":"hbase","permalink":"https://blog.mhuig.top/tags/hbase/"}]},{"title":"NoSQL-Cassandra","slug":"bigdata/NoSql/NoSQLCassandra","date":"2020-02-09T02:41:48.000Z","updated":"2020-02-09T02:41:48.000Z","comments":true,"path":"posts/e3dcc811.html","link":"","permalink":"https://blog.mhuig.top/posts/e3dcc811.html","excerpt":"NoSQL Cassandra PDF","text":"NoSQL Cassandra PDF GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"NoSQL","slug":"大数据/nosql","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/nosql/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"NoSQL","slug":"nosql","permalink":"https://blog.mhuig.top/tags/nosql/"},{"name":"Cassandra","slug":"cassandra","permalink":"https://blog.mhuig.top/tags/cassandra/"}]},{"title":"NoSQL","slug":"bigdata/NoSql/NoSQL","date":"2020-02-09T02:41:47.000Z","updated":"2020-02-09T02:41:47.000Z","comments":true,"path":"posts/f0940727.html","link":"","permalink":"https://blog.mhuig.top/posts/f0940727.html","excerpt":"NoSQL PDF","text":"NoSQL PDF GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"NoSQL","slug":"大数据/nosql","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/nosql/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"NoSQL","slug":"nosql","permalink":"https://blog.mhuig.top/tags/nosql/"}]},{"title":"如何加密你的 Python 代码","slug":"Python/如何加密你的 Python 代码","date":"2020-02-05T05:08:17.000Z","updated":"2020-02-05T05:08:17.000Z","comments":true,"path":"posts/b190dcb.html","link":"","permalink":"https://blog.mhuig.top/posts/b190dcb.html","excerpt":"讲述了如何通过修改 Python 解释器达到加解密 Python 代码的目的。","text":"讲述了如何通过修改 Python 解释器达到加解密 Python 代码的目的。 前言本文将首先介绍下现有源码加密方案的思路、方法、优点与不足，进而介绍如何通过定制 Python 解释器来达到更好地加解密源码的目的。 现有加密方案由于 Python 的动态特性和开源特点，导致 Python 代码很难做到很好的加密。社区中的一些声音认为这样的限制是事实，应该通过法律手段而不是加密源码达到商业保护的目的；而还有一些声音则是不论如何都希望能有一种手段来加密。于是乎，人们想出了各种或加密、或混淆的方案，借此来达到保护源码的目的。 常见的源码保护手段有如下几种： 发行 .pyc 文件 代码混淆 使用 py2exe 使用 Cython 下面来简单说说这些方案。 发行 .pyc 文件思路大家都知道，Python 解释器在执行代码的过程中会首先生成 .pyc 文件，然后解释执行 .pyc 文件中的内容。当然了，Python 解释器也能够直接执行 .pyc 文件。而 .pyc 文件是二进制文件，无法直接看出源码内容。如果发行代码到客户环境时都是 .pyc 而非 .py 文件的话，那岂不是能达到保护 Python 代码的目的？ 方法把 .py 文件编译为 .pyc 文件，是件非常轻松地事情，可不需要把所有代码跑一遍，然后去捞生成的 .pyc 文件。 事实上，Python 标准库中提供了一个名为 compileall 的库，可以轻松地进行编译。 执行如下命令能够将遍历 目录下的所有 .py 文件，将之编译为 .pyc 文件： 1python -m compileall &lt;src&gt; 然后删除 目录下所有 .py 文件就可以打包发布了： 1find &lt;src&gt; -name &#39;*.py&#39; -type f -print -exec rm &#123;&#125; \\; 优点 简单方便，提高了一点源码破解门槛 平台兼容性好，.py 能在哪里运行，.pyc 就能在哪里运行 不足 解释器兼容性差，.pyc 只能在特定版本的解释器上运行 有现成的反编译工具，破解成本低 python-uncompyle6 就是这样一款反编译工具，效果出众。 执行如下命令，即可将 .pyc 文件反编译为 .py 文件： 1uncompyle6 *compiled-python-file-pyc-or-pyo* 代码混淆如果代码被混淆到一定程度，连作者看着都费劲的话，是不是也能达到保护源码的目的呢？ 思路既然我们的目的是混淆，就是通过一系列的转换，让代码逐渐不那么让人容易明白，那就可以这样下手： 移除注释和文档。没有这些说明，在一些关键逻辑上就没那么容易明白了。 改变缩进。完美的缩进看着才舒服，如果缩进忽长忽短，看着也一定闹心。 在tokens中间加入一定空格。这就和改变缩进的效果差不多。 重命名函数、类、变量。命名直接影响了可读性，乱七八糟的名字可是阅读理解的一大障碍。 在空白行插入无效代码。这就是障眼法，用无关代码来打乱阅读节奏。 方法方法一：使用 oxyry 进行混淆http://pyob.oxyry.com/ 是一个在线混淆 Python 代码的网站，使用它可以方便地进行混淆。 假定我们有这样一段 Python 代码，涉及到了类、函数、参数等内容： 1234567891011121314151617181920212223# coding: utf-8class A(object): &quot;&quot;&quot; Description &quot;&quot;&quot; def __init__(self, x, y, default=None): self.z = x + y self.default = default def name(self): return &#x27;No Name&#x27;def always(): return Truenum = 1a = A(num, 999, 100)a.name()always() 经过 Oxyry 的混淆，得到如下代码： 12345678910111213class A (object ):#line:4 &quot;&quot;#line:7 def __init__ (O0O0O0OO00OO000O0 ,OO0O0OOOO0000O0OO ,OO0OO00O00OO00OOO ,OO000OOO0O000OOO0 =None ):#line:9 O0O0O0OO00OO000O0 .z =OO0O0OOOO0000O0OO +OO0OO00O00OO00OOO #line:10 O0O0O0OO00OO000O0 .default =OO000OOO0O000OOO0 #line:11 def name (O000O0O0O00O0O0OO ):#line:13 return &#x27;No Name&#x27;#line:14def always ():#line:17 return True #line:18num =1 #line:21a =A (num ,999 ,100 )#line:22a .name ()#line:23always () 混淆后的代码主要在注释、参数名称和空格上做了些调整，稍微带来了点阅读上的障碍。 方法二：使用 pyobfuscate 库进行混淆pyobfuscate 算是一个颇具年头的 Python 代码混淆库了，但却是“老当益壮”了。 对上述同样一段 Python 代码，经 pyobfuscate 混淆后效果如下： 123456789101112131415161718192021222324# coding: utf-8if 64 - 64: i11iIiiIiiif 65 - 65: O0 / iIii1I11I1II1 % OoooooooOO - i1IIiclass o0OO00 ( object ) : if 78 - 78: i11i . oOooOoO0Oo0O if 10 - 10: IIiI1I11i11 if 54 - 54: i11iIi1 - oOo0O0Ooo if 2 - 2: o0 * i1 * ii1IiI1i % OOooOOo / I11i / Ii1I def __init__ ( self , x , y , default = None ) : self . z = x + y self . default = default if 48 - 48: iII111i % IiII + I1Ii111 / ooOoO0o * Ii1I def name ( self ) : return &#x27;No Name&#x27; if 46 - 46: ooOoO0o * I11i - OoooooooOO if 30 - 30: o0 - O0 % o0 - OoooooooOO * O0 * OoooooooOOdef Oo0o ( ) : return True if 60 - 60: i1 + I1Ii111 - I11i / i1IIi if 40 - 40: oOooOoO0Oo0O / O0 % ooOoO0o + O0 * i1IIiI1Ii11I1Ii1i = 1Ooo = o0OO00 ( I1Ii11I1Ii1i , 999 , 100 )Ooo . name ( )Oo0o ( ) # dd678faae9ac167bc83abf78e5cb2f3f0688d3a3 相比于方法一，方法二的效果看起来更好些。除了类和函数进行了重命名、加入了一些空格，最明显的是插入了若干段无关的代码，变得更加难读了。 优点 简单方便，提高了一点源码破解门槛 兼容性好，只要源码逻辑能做到兼容，混淆代码亦能 不足 只能对单个文件混淆，无法做到多个互相有联系的源码文件的联动混淆 代码结构未发生变化，也能获取字节码，破解难度不大 使用 py2exe思路py2exe 是一款将 Python 脚本转换为 Windows 平台上的可执行文件的工具。其原理是将源码编译为 .pyc 文件，加之必要的依赖文件，一起打包成一个可执行文件。 如果最终发行由 py2exe 打包出的二进制文件，那岂不是达到了保护源码的目的？ 方法使用 py2exe 进行打包的步骤较为简便。 1.编写入口文件。本示例中取名为 hello.py： 1print &#x27;Hello World&#x27; 2.编写 setup.py： 1234from distutils.core import setupimport py2exesetup(console=[&#x27;hello.py&#x27;]) 3.生成可执行文件 1python setup.py py2exe 生成的可执行文件位于 dist\\hello.exe。 优点 能够直接打包成 exe，方便分发和执行 破解门槛比 .pyc 更高一些 不足 兼容性差，只能运行在 Windows 系统上 生成的可执行文件内的布局是明确、公开的，可以找到源码对应的 .pyc 文件，进而反编译出源码 使用 Cython思路虽说 Cython 的主要目的是带来性能的提升，但是基于它的原理：将 .py/.pyx 编译为 .c 文件，再将 .c 文件编译为 .so(Unix) 或 .pyd(Windows)，其带来的另一个好处就是难以破解。 方法使用 Cython 进行开发的步骤也不复杂。 1.编写文件 hello.pyx 或 hello.py： 12def hello(): print(&#x27;hello&#x27;) 2.编写 setup.py： 12345from distutils.core import setupfrom Cython.Build import cythonizesetup(name=&#x27;Hello World app&#x27;, ext_modules=cythonize(&#x27;hello.pyx&#x27;)) 3.编译为 .c，再进一步编译为 .so 或 .pyd： 1python setup.py build_ext --inplace 执行 python -c “from hello import hello;hello()” 即可直接引用生成的二进制文件中的 hello() 函数。 优点 生成的二进制 .so 或 .pyd 文件难以破解 同时带来了性能提升 不足 兼容性稍差，对于不同版本的操作系统，可能需要重新编译 虽然支持大多数 Python 代码，但如果一旦发现部分代码不支持，完善成本较高 定制 Python 解释器考虑前文所述的几个方案，均是从源码的加工入手，或多或少都有些不足。假设我们从解释器的改造入手，会不会能够更好的保护代码呢？ 由于发行商业 Python 程序到客户环境时通常会包含一个 Python 解释器，如果改造解释器能解决源码保护的问题，那么也是可选的一条路。 假定我们有一个算法，能够加密原始的 Python 代码，这些加密后代码随发行程序一起，可被任何人看到，却难以破解。另一方面，有一个定制好的 Python 解释器，它能够解密这些被加密的代码，然后解释执行。而由于 Python 解释器本身是二进制文件，人们也就无法从解释器中获取解密的关键数据。从而达到了保护源码的目的。 要实现上述的设想，我们首先需要掌握基本的加解密算法，其次探究 Python 执行代码的方式从而了解在何处进行加解密，最后禁用字节码用以防止通过 .pyc 反编译。 加解密算法对称密钥加密算法对称密钥加密（Symmetric-key algorithm）又称为对称加密、私钥加密、共享密钥加密，是密码学中的一类加密算法。这类算法在加密和解密时使用相同的密钥，或是使用两个可以简单地相互推算的密钥。 对称加密算法的特点是算法公开、计算量小、加密速度快、加密效率高。 常见的对称加密算法有：DES、3DES、AES、Blowfish、IDEA、RC5、RC6 等。 对称密钥加解密过程如下： 明文通过密钥加密成密文，密文也可通过相同的密钥解密为明文。 通过 openssl 工具，我们能够方便选择对称加密算法进行加解密。下面我们以 AES 算法为例，介绍其用法。 AES 加密指定密码进行对称加密 1openssl enc -aes-128-cbc -in test.py -out entest.py -pass pass:123456 指定文件进行对称加密 1openssl enc -aes-128-cbc -in test.py -out entest.py -pass file:passwd.txt 指定环境变量进行对称加密 1openssl enc -aes-128-cbc -in test.py -out entest.py -pass env:passwd AES 解密指定密码进行对称解密 1openssl enc -aes-128-cbc -d -in entest.py -out test.py -pass pass:123456 指定文件进行对称解密 1openssl enc -aes-128-cbc -d -in entest.py -out test.py -pass file:passwd.txt 指定环境变量进行对称解密 1openssl enc -aes-128-cbc -d -in entest.py -out test.py -pass env:passwd 非对称密钥加密算法密钥加密（英语：public-key cryptography，又译为公开密钥加密），也称为非对称加密（asymmetric cryptography），一种密码学算法类型，在这种密码学方法中，需要一对密钥，一个是私钥，另一个则是公钥。这两个密钥是数学相关，用某用户公钥加密后所得的信息，只能用该用户的私钥才能解密。 非对称加密算法的特点是算法强度复杂、安全性依赖于算法与密钥但是由于其算法复杂，而使得加密解密速度没有对称加密解密的速度快。 常见的对称加密算法有：RSA、Elgamal、背包算法、Rabin、D-H、ECC 等。 非对称密钥加解密过程如下： 明文通过公钥加密成密文，密文通过与公钥对应的私钥解密为明文。 通过 openssl 工具，我们能够方便选择非对称加密算法进行加解密。下面我们以 RSA 算法为例，介绍其用法。 生成私钥、公钥辅以 AES-128 算法，生成 2048 比特长度的私钥 1openssl genrsa -aes128 -out private.pem 2048 根据私钥来生成公钥 1openssl rsa -in private.pem -outform PEM -pubout -out public.pem RSA 加密使用公钥进行加密 1openssl rsautl -encrypt -in passwd.txt -inkey public.pem -pubin -out enpasswd.txt RSA 解密使用私钥进行解密 1openssl rsautl -decrypt -in enpasswd.txt -inkey private.pem -out passwd.txt 基于加密算法实现源码保护对称加密适合加密源码文件，而非对称加密适合加密密钥。如果将两者结合，就能达到加解密源码的目的。 在构建环境进行加密我们发行出去安装包中，源码应该是被加密过的，那么就需要在构建阶段对源码进行加密。加密的过程如下： 1.随机生成一个密钥。这个密钥实际上是一个用于对称加密的密码。 2.使用该密钥对源代码进行对称加密，生成加密后的代码。 3.使用公钥（生成方法见 非对称密钥加密算法）对该密钥进行非对称加密，生成加密后的密钥。 不论是加密后的代码还是加密后的密钥，都会放在安装包中。它们能够被用户看到，却无法被破译。而 Python 解释器该如何执行加密后的代码呢？ Python 解释器进行解密假定我们发行的 Python 解释器中内置了与公钥相对应的私钥，有了它就有了解密的可能。而由于 Python 解释器本身是二进制文件，所以不需要担心内置的私钥会被看到。解密的过程如下： 1.Python 解释器执行加密代码时需要被传入指示加密密钥的参数，通过这个参数，解释器获取到了加密密钥 2.Python 解释器使用内置的私钥，对该加密密钥进行非对称解密，得到原始密钥 3.Python 解释器使用原始密钥对加密代码进行对称解密，得到原始代码 4.Python 解释器执行这段原始代码 可以看到，通过改造构建环节、定制 Python 解释器的执行过程，便可以实现保护源码的目的。改造构建环节是容易的，但是如何定制 Python 解释器呢？我们需要深入了解解释器执行脚本和模块的方式，才能在特定的入口进行控制。 脚本、模块的执行与解密执行 Python 代码的几种方式为了找到 Python 解释器执行 Python 代码时的所有入口，我们需要首先执行 Python 解释器都能以怎样的方式执行代码。 直接运行脚本1python test.py 直接运行语句1python -c &quot;print &#39;hello&#39;&quot; 直接运行模块1python -m test 导入、重载模块123python&gt;&gt;&gt; import test # 导入模块&gt;&gt;&gt; reload(test) # 重载模块 直接运行语句 的方式接收的就是明文的代码，我们也无需对这种方式做额外处理。直接运行模块和导入、重载模块这两种方式在流程上是殊途同归的，所以接下来会一起来看。因此我们将分两种情况：运行脚本和加载模块来进一步探究各自的过程和解密方式。 运行脚本时解密运行脚本的过程Python 解释器在运行脚本时的代码调用逻辑如下： 12345678910 main WinMain[Modules&#x2F;python.c] [PC&#x2F;WinMain.c] \\ &#x2F; \\ &#x2F; \\ &#x2F; \\ &#x2F; \\ &#x2F; Py_Main [Moduls&#x2F;main.c] Python 解释器运行脚本的入口函数因操作系统而异，在 Linux/Unix 系统上，主入口函数是 Modules/python.c 中的 main 函数，在 Windows系统上，则是 PC/WinMain.c 中的 WinMain 函数。不过这两个函数最终都会调用 Moduls/main.c 中的 Py_Main 函数。 我们不妨来看看 Py_Main 函数中的相关逻辑： 1234567891011121314151617[Modules/Main.c]--------------------------------------intPy_Main(int argc, char **argv)&#123; if (command) &#123; // 处理 python -c &lt;command&gt; &#125; else if (module) &#123; // 处理 python -m &lt;module&gt; &#125; else &#123; // 处理 python &lt;file&gt; ... fp = fopen(filename, &quot;r&quot;); ... &#125; 处理和的部分我们暂且先不管，在处理文件（通过直接运行脚本的方式）的逻辑中，可以看到解释打开了文件，获得了文件指针。那么如果我们把这里的 fopen 换成是自定义的 decrypt_open 函数，这个函数用来打开一个加密文件，然后进行解密，并返回一个文件指针，这个指针指向解密后的文件。那么，不就可以实现解密脚本的目的了吗？ 自定义 decrypt_open我们不妨新增一个 Modules/crypt.c 文件，用来存放一些自定义的加解密函数。 decrypt_open 函数大概实现如下： 123456789101112131415161718192021222324[Modules/crypt.c]--------------------------------------/* 以解密方式打开文件 */FILE *decrypt_open(const char *filename, const char *mode)&#123; int plainlen = -1; char *plaintext = NULL; FILE *fp = NULL; if (aes_passwd == NULL) fp = fopen(filename, &quot;r&quot;); else &#123; plainlen = aes_decrypt(filename, aes_passwd, &amp;plaintext); // 如果无法解密，返回源文件描述符 if (plainlen &lt; 0) fp = fopen(filename, &quot;r&quot;); // 否则，转换为内存文件描述符 else fp = fmemopen(plaintext, plainlen, &quot;r&quot;); &#125; return fp;&#125; 这里的 aes_passwd 是一个全局变量，代表对称加密算法中的密钥。我们暂时假定已经获取该密钥了，后文会说明如何获得。而 aes_decrypt 是自定义的一个使用AES算法进行对称解密的函数，限于篇幅，此函数的实现不再贴出。 decrypt_open 逻辑如下： 判断是否获得了对称密钥，如果没获得，直接打开该文件并返回文件指针 如果获得了，则尝试使用对称算法进行解密 如果解密失败，可能就是一段非加密的脚本，直接打开该文件并返回文件指针 如果解密成功，我们通过解密后的内容创建一个内存文件对象，并返回该文件指针实现了上述这些函数后，我们就能够实现在直接运行脚本时，解密执行被加密代码的目的。 加载模块时解密加载模块的过程加载模块的逻辑主要实现在 Python/import.c 文件中，其过程如下： 1234567891011121314 Py_Main [Moduls&#x2F;main.c] | builtin___import__ RunModule | |PyImport_ImportModuleLevel &lt;----┐ PyImport_ImportModule | | | import_module_level └------- PyImport_Import | load_next builtin_reload | | import_submodule PyImport_ReloadModule | | find_module &lt;---------------------------┘ 通过 python -m 的方式来加载模块时，其入口函数是 Py_Main 函数 通过 import 的方式来加载模块时，其入口函数是 builtin___import__ 函数 通过 reload() 的方式来加载模块时，其入口函数是 builtin_reload 函数 但不论是哪种方式，最终都会调用 find_module 函数，我们看看这个函数中是否暗藏乾坤呢？ 1234567891011[Python/import.c]--------------------------------------static struct filedescr *find_module(char *fullname, char *subname, PyObject *path, char *buf, size_t buflen, FILE **p_fp, PyObject **p_loader)&#123; ... fp = fopen(buf, filemode); ...&#125; 我们在 find_module 函数中找到了打开文件的逻辑，如果直接改成前文实现的 decrypt_open，岂不是就能达成加载模块时解密的目的了？ 总体思路是这样的，但有个细节需要注意，buf 不一定就是 .py 文件，也可能是 .pyc 文件，我们只对 .py 文件做改动，则可以这么写： 12345678910111213141516[Python/import.c]--------------------------------------static struct filedescr *find_module(char *fullname, char *subname, PyObject *path, char *buf, size_t buflen, FILE **p_fp, PyObject **p_loader)&#123; ... if (fdp-&gt;type == PY_SOURCE) &#123; fp = decrypt_open(buf, filemode); &#125; else &#123; fp = fopen(buf, filemode); &#125; ...&#125; 经过上述改动，就实现了加载模块时解密的目的了。 支持指定密钥文件前文中还留有一个待解决的问题：我们一开始是假定解释器已获取到了密钥内容并存放在了全局变量 aes_passwd 中，那么密钥内容怎么获取呢？ 我们需要 Python 解释器能支持一个新的参数选项，通过它来指定已加密的密钥文件，然后再通过非对称算法进行解密，得到 aes_passed。 假定这个参数选项是 -k ，则可使用如 python -k enpasswd.txt 的方式来告知解释器加密密钥的文件路径。其实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[Modules/main.c]--------------------------------------/* 命令行选项，注意k:是新增的内容 */#define BASE_OPTS &quot;3bBc:dEhiJk:m:OQ:RsStuUvVW:xX?&quot;.../* Long usage message, split into parts &lt; 512 bytes */static char *usage_1 = &quot;\\...-k key : decrypt source file by using key file\\n\\...&quot;;...intPy_Main(int argc, char **argv)&#123; ... char *keyfilename = NULL; ... while ((c = _PyOS_GetOpt(argc, argv, PROGRAM_OPTS)) != EOF) &#123; ... case &#x27;k&#x27;: keyfilename = (char *)malloc(strlen(_PyOS_optarg) + 1); if (keyfilename == NULL) Py_FatalError( &quot;not enough memory to copy -k argument&quot;); strcpy(keyfilename, _PyOS_optarg); keyfilename[strlen(_PyOS_optarg)] = &#x27;\\0&#x27;; break; ... &#125; ... if (keyfilename != NULL) &#123; int passwdlen; char *passwd = NULL; passwdlen = rsa_decrypt(keyfilename, &amp;passwd); set_aes_passwd(passwd); if (passwdlen &lt; 0) &#123; fprintf(stderr, &quot;%s: parsing key file &#x27;%s&#x27; error\\n&quot;, argv[0], keyfilename); free(keyfilename); return 2; &#125; else &#123; free(keyfilename); &#125; &#125; ...&#125; 其逻辑如下： k:中的 k 表示支持 -k 选项；: 表示选项后跟一个参数，即这里的已加密密钥文件的路径 解释器在处理到 -k 参数时，获取其后所跟的文件路径，记录在 keyfilename 中 使用自定义的 rsa_decrypt 函数（限于篇幅，不列出如何实现的逻辑）对已加密密钥文件进行非对称解密，获得密钥的原始内容 将该密钥内容写入到 aes_passwd 中 由此，通过显示地指定已加密密钥文件，解释器获得了原始密钥，进而通过该密钥解密已加密代码，再执行原始代码。但是，这里面还潜藏着一个风险：执行代码的过程中会生成 .pyc 文件，通过它反编译出的 .py 文件是未加密的。换句话说，恶意用户可以通过这种手段绕过限制。所以，我们需要禁用字节码 禁用字节码不生成 .pyc 文件首先要做的就是不生成 .pyc 文件，这样，恶意用户就没法直接根据 .pyc 文件来得到源码。 我们知道，通过 -B 选项可以告知 Python 解释器不生成 .pyc 文件。既然定制的 Python 解释器就不生成 .pyc 我们干脆禁用这个选项： 123456789101112131415161718192021[Modules/main.c]--------------------------------------/* 命令行选项，注意移除了B */#define BASE_OPTS &quot;3bc:dEhiJm:OQ:RsStuUvVW:xX?&quot;.../* Long usage message, split into parts &lt; 512 bytes */static char *usage_1 = &quot;\\...//-B : don&#x27;t write .py[co] files on import; also PYTHONDONTWRITEBYTECODE=x\\n\\...&quot;;...intPy_Main(int argc, char **argv)&#123; ... // 不生成 py[co] Py_DontWriteBytecodeFlag++; ...&#125; 除此以外，Python 解释器还会从环境变量中获取是否不生成 .pyc 文件，因此也需要做处理： 123456789101112131415161718[Python/pythonrun.c]--------------------------------------voidPy_InitializeEx(int install_sigs)&#123; ... f ((p = Py_GETENV(&quot;PYTHONDEBUG&quot;)) &amp;&amp; *p != &#x27;\\0&#x27;) Py_DebugFlag = add_flag(Py_DebugFlag, p); if ((p = Py_GETENV(&quot;PYTHONVERBOSE&quot;)) &amp;&amp; *p != &#x27;\\0&#x27;) Py_VerboseFlag = add_flag(Py_VerboseFlag, p); if ((p = Py_GETENV(&quot;PYTHONOPTIMIZE&quot;)) &amp;&amp; *p != &#x27;\\0&#x27;) Py_OptimizeFlag = add_flag(Py_OptimizeFlag, p); // 移除对 PYTHONDONTWRITEBYTECODE 的处理 if ((p = Py_GETENV(&quot;PYTHONDONTWRITEBYTECODE&quot;)) &amp;&amp; *p != &#x27;\\0&#x27;) Py_DontWriteBytecodeFlag = add_flag(Py_DontWriteBytecodeFlag, p); ...&#125; 禁止访问字节码对象 co_code仅仅是不生成 .pyc 文件还是不够的，恶意用户已然可以访问对象的 co_code 属性来获取字节码，进而通过反编译的手段获取到源码。因此，我们也需要禁止用户访问字节码对象： 1234567891011121314151617181920[Objects/codeobject.c]--------------------------------------static PyMemberDef code_memberlist[] = &#123; &#123;&quot;co_argcount&quot;, T_INT, OFF(co_argcount), READONLY&#125;, &#123;&quot;co_nlocals&quot;, T_INT, OFF(co_nlocals), READONLY&#125;, &#123;&quot;co_stacksize&quot;,T_INT, OFF(co_stacksize), READONLY&#125;, &#123;&quot;co_flags&quot;, T_INT, OFF(co_flags), READONLY&#125;, // &#123;&quot;co_code&quot;, T_OBJECT, OFF(co_code), READONLY&#125;, &#123;&quot;co_consts&quot;, T_OBJECT, OFF(co_consts), READONLY&#125;, &#123;&quot;co_names&quot;, T_OBJECT, OFF(co_names), READONLY&#125;, &#123;&quot;co_varnames&quot;, T_OBJECT, OFF(co_varnames), READONLY&#125;, &#123;&quot;co_freevars&quot;, T_OBJECT, OFF(co_freevars), READONLY&#125;, &#123;&quot;co_cellvars&quot;, T_OBJECT, OFF(co_cellvars), READONLY&#125;, &#123;&quot;co_filename&quot;, T_OBJECT, OFF(co_filename), READONLY&#125;, &#123;&quot;co_name&quot;, T_OBJECT, OFF(co_name), READONLY&#125;, &#123;&quot;co_firstlineno&quot;, T_INT, OFF(co_firstlineno), READONLY&#125;, &#123;&quot;co_lnotab&quot;, T_OBJECT, OFF(co_lnotab), READONLY&#125;, &#123;NULL&#125; /* Sentinel */&#125;; 到此，一个定制的 Python 解释器完成了。 演示运行脚本通过 -k 选项执行已加密密钥文件，Python 解释器可以运行已加密和未加密的 Python 文件。 加载模块可以通过 -m 的方式加载已加密和未加密的模块，也可以通过 import 的方式来加载已加密和未加密的模块。 禁用字节码通过禁用字节码，我们达到以下效果： 不会生成 .pyc 文件 可以访问函数的 func_code 无法访问代码对象的 co_code，即本示例中的 f.func_code.co_code 无法使用dis模块来获取字节码 异常堆栈信息尽管代码是加密的，但是不会影响异常时的堆栈信息。 调试加密的代码也是允许调试的，但是输出的代码内容会是加密的，这正是我们所期望的。 思考 1.如何防止通过内存操作的方式找到对象的co_code? 2.如何进一步提升私钥被逆向工程探知的难度？ 3.如何能在调试并希望看到源码的时候看到? 参考文献5种方法，加密你的Python代码 如何加密你的Python 代码 附录: gpg 命令行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# 生成 gpg 密钥gpg --gen-key# 生成吊销证书gpg --gen-revoke 735C4581G2442686# 列出所有 gpg 公钥gpg --list-keys# 列出所有 gpg 私钥gpg --list-secret-keys# 删除 gpg 公钥gpg --delete-keys 735C4581G2442686# 删除 gpg 私钥gpg --delete-secret-keys 735C4581G2442686# 输出 gpg 公钥 asciigpg --armor --output public.key --export 735C4581G2442686# 输出 gpg 私钥 asciigpg --armor --output private.key --export-secret-keys 735C4581G2442686# 上传 gpg 公钥gpg --send-keys 735C4581G2442686 --keyserver # 查看 gpg 公钥指纹gpg --fingerprint 735C4581G2442686# 导入 gpg 密钥(导入私钥时会自动导入公钥)gpg --import private.key# 加密文件gpg --recipient 735C4581G2442686 --output encrypt.file --encrypt origin.file# 解密文件gpg --output origin.file --decrypt encrypt.file# 文件签名，生成二进制的 gpg 文件gpg --sign file.txt# 文件签名，生成文本末尾追加 ASCII 签名的 asc 文件gpg --clearsign file.txt# 文件签名，生成二进制的 sig 文件gpg --detach-sign file.txt# 文件签名，生成 ASCII 格式的 asc 文件gpg --detach-sign file.txt# 签名并加密gpg --local-user 735C4581G2442686 --recipient 735C4581G2442686 --armor --sign --encrypt file.txt# 验证签名gpg --verify file.txt.asc file.txt# 延期# gpg 也是使用主密钥和子密钥结合加密的# pub 和 sub 分别是主公钥和子公钥# sec 和 ssb 分别是主私钥和子私钥# 如果有多个子密钥，会显示更多的 sub 和 ssb# 一个主密钥可以绑定多个子密钥，平时加密解密使用的都是子密钥gpg --edit-key admin@XXXXXXXX.comsec rsa4096/6E22BF79E2586289 创建于：2019-02-11 有效至：9012-12-08 可用于：SC 信任度：未知 有效性：未知ssb rsa4096/A1FB112628F3B06C 创建于：2019-02-11 有效至：9012-12-08 可用于：E[ 未知 ] (1). Wildlife &lt;admin@XXXXXXXX.com&gt;# 指定子密钥，不指定则为主密钥gpg&gt; key 1sec rsa4096/6E22BF79E2586289 创建于：2019-02-11 有效至：9012-12-08 可用于：SC 信任度：未知 有效性：未知ssb* rsa4096/A1FB112628F3B06C 创建于：2019-02-11 有效至：9012-12-08 可用于：E[ 未知 ] (1). Wildlife &lt;admin@XXXXXXXX.com&gt;# 更新过期时间gpg&gt; expire将要变更子密钥的过期时间。请设定这个密钥的有效期限。 0 = 密钥永不过期 &lt;n&gt; = 密钥在 n 天后过期 &lt;n&gt;w = 密钥在 n 周后过期 &lt;n&gt;m = 密钥在 n 月后过期 &lt;n&gt;y = 密钥在 n 年后过期密钥的有效期限是？(0) 2y密钥于 9014年12月08日 星期二 12时53分35秒 CST 过期这些内容正确吗？ (y/N) ysec rsa4096/6E22BF79E2586289 创建于：2019-02-11 有效至：9012-12-08 可用于：SC 信任度：未知 有效性：未知ssb* rsa4096/A1FB112628F3B06C 创建于：2019-02-11 有效至：2022-01-25 可用于：E[ 未知 ] (1). Wildlife &lt;admin@XXXXXXXX.com&gt;gpg&gt; save","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"解释器","slug":"python/解释器","permalink":"https://blog.mhuig.top/categories/python/%E8%A7%A3%E9%87%8A%E5%99%A8/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"},{"name":"解释器","slug":"解释器","permalink":"https://blog.mhuig.top/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"源码保护","slug":"源码保护","permalink":"https://blog.mhuig.top/tags/%E6%BA%90%E7%A0%81%E4%BF%9D%E6%8A%A4/"}]},{"title":"Windows安装gcc","slug":"win/windows安装gcc","date":"2020-02-05T04:57:02.000Z","updated":"2020-02-05T04:57:02.000Z","comments":true,"path":"posts/e6f6b883.html","link":"","permalink":"https://blog.mhuig.top/posts/e6f6b883.html","excerpt":"最近测试一下windows上vs编译和gcc编译的区别，同时比较ubuntu上gcc编译的却别，主要在内存上，做了一个小测试，现在写下安装gcc的过程。","text":"最近测试一下windows上vs编译和gcc编译的区别，同时比较ubuntu上gcc编译的却别，主要在内存上，做了一个小测试，现在写下安装gcc的过程。 下载先去官网下载安装包，http://www.mingw.org， 进入官网找到download： 单击就可以直接下载了。 安装双击运行下载的exe，然后点install，然后就是下一步到底就行了，最后选择安装gcc-g++的就可以了。 注意下面这个要选中 其他需要的也可以自行选择，安装完之后，也可以通过安装目录下bin目录的 安装其他东西，可以自行去了解。 配置安装完成后就是配置环境变量了 然后打开控制台，输入: 1gcc -v 1gcc --version 我们可以写一个例子试一下，经典例子hello world出来吧！ 123456#include &lt;stdio.h&gt;int main()&#123; printf(&quot;Hello world!&quot;); return 0;&#125; 1gcc -o test main.cpp MinGW-w64Window系统下的MinGW，总是编译为32位代码。因为MinGW只支持32位代码。 Window系统下的MinGW-w64（例如安装了TDM-GCC，选择MinGW-w64），默认是编译为64位代码，包括在32位的Windows系统下。","categories":[{"name":"windows","slug":"windows","permalink":"https://blog.mhuig.top/categories/windows/"},{"name":"gcc","slug":"windows/gcc","permalink":"https://blog.mhuig.top/categories/windows/gcc/"}],"tags":[{"name":"windows","slug":"windows","permalink":"https://blog.mhuig.top/tags/windows/"},{"name":"gcc","slug":"gcc","permalink":"https://blog.mhuig.top/tags/gcc/"}]},{"title":"AES加解密","slug":"Python/密码/AES加解密","date":"2020-02-03T05:14:17.000Z","updated":"2020-02-03T05:14:17.000Z","comments":true,"path":"posts/357dc334.html","link":"","permalink":"https://blog.mhuig.top/posts/357dc334.html","excerpt":"AES加密解密实现","text":"AES加密解密实现 Python语言实现直接安装Crypto是不好使的。因为历史原因导致的比较混乱，引用外部博友的解释内容如下： pycrypto、pycrytodome和crypto是一个东西，crypto在python上面的名字是pycrypto，它是一个第三方库，但是已经停止更新三年了，所以不建议安装这个库；这个时候pycryptodome就来了，它是pycrypto的延伸版本，用法和pycrypto是一模一样的；所以，我现在告诉大家一种解决方法–直接安装：pip install pycryptodome但是，在使用的时候导入模块是有问题的，这个时候只要修改一个文件夹的名称就可以完美解决这个问题，Python\\Python36\\Lib\\site-packages，找到这个路径，下面有一个文件夹叫做crypto,将小写c改成大写C就ok了。 为了安装方便，可以直接使用下面的命令： 123pip install crypto pycryptodomepip uninstall crypto pycryptodomepip install pycryptodome 而如果你是linux环境，则直接安装pycryptodome即可： 1pip install pycryptodome 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import base64from Crypto.Cipher import AESdef pkcs7padding(text): bs = AES.block_size # 16 length = len(text) bytes_length = len(bytes(text, encoding=&#x27;utf-8&#x27;)) # tips：utf-8编码时，英文占1个byte，而中文占3个byte padding_size = length if(bytes_length == length) else bytes_length padding = bs - padding_size % bs # tips：chr(padding)看与其它语言的约定，有的会使用&#x27;\\0&#x27; padding_text = chr(padding) * padding return text + padding_textdef pkcs7unpadding(text): length = len(text) unpadding = ord(text[length-1]) return text[0:length-unpadding]def encrypt(key, content): &quot;&quot;&quot; AES加密 key,iv使用同一个 模式cbc 填充pkcs7 :param key: 密钥 :param content: 加密内容 :return: &quot;&quot;&quot; key_bytes = bytes(key, encoding=&#x27;utf-8&#x27;) iv = key_bytes cipher = AES.new(key_bytes, AES.MODE_CBC, iv) # 处理明文 content_padding = pkcs7padding(content) # 加密 encrypt_bytes = cipher.encrypt(bytes(content_padding, encoding=&#x27;utf-8&#x27;)) # 重新编码 result = str(base64.b64encode(encrypt_bytes), encoding=&#x27;utf-8&#x27;) return resultdef decrypt(key, content): &quot;&quot;&quot; AES解密 key,iv使用同一个 模式cbc 去填充pkcs7 :param key: :param content: :return: &quot;&quot;&quot; key_bytes = bytes(key, encoding=&#x27;utf-8&#x27;) iv = key_bytes cipher = AES.new(key_bytes, AES.MODE_CBC, iv) # base64解码 encrypt_bytes = base64.b64decode(content) # 解密 decrypt_bytes = cipher.decrypt(encrypt_bytes) # 重新编码 result = str(decrypt_bytes, encoding=&#x27;utf-8&#x27;) # 去除填充内容 result = pkcs7unpadding(result) return resultaes_key = &#x27;1234567812345678&#x27;# 加密source_en = &#x27;1111111111111111&#x27;encrypt_en = encrypt(aes_key, source_en)print(encrypt_en)# 解密decrypt_en = decrypt(aes_key, encrypt_en)print(decrypt_en)print(source_en == decrypt_en) C语言实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336#include &lt;stdio.h&gt;#include &lt;ctype.h&gt;void aes(char*, char*, char*, int);void aes_detail(int[4][4], int[4][4], int);void subBytes(int [4][4], int);void shiftRows(int [4][4], int);void mixColumns(int [4][4], int);void addRoundKey(int [4][4], int[4][4]);int aes_multiple(int, int);void keyExpansion(int key[4][4], int w[11][4][4]);int c2i(char );/** * S盒 */static const int S_BOX[16][16] = &#123; 0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76, 0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0, 0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0, 0xb7, 0xfd, 0x93, 0x26, 0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15, 0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2, 0xeb, 0x27, 0xb2, 0x75, 0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0, 0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84, 0x53, 0xd1, 0x00, 0xed, 0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf, 0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f, 0x50, 0x3c, 0x9f, 0xa8, 0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5, 0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2, 0xcd, 0x0c, 0x13, 0xec, 0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73, 0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14, 0xde, 0x5e, 0x0b, 0xdb, 0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c, 0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79, 0xe7, 0xc8, 0x37, 0x6d, 0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08, 0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f, 0x4b, 0xbd, 0x8b, 0x8a, 0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e, 0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e, 0xe1, 0xf8, 0x98, 0x11, 0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf, 0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68, 0x41, 0x99, 0x2d, 0x0f, 0xb0, 0x54, 0xbb, 0x16 &#125;;/** * 逆S盒 */static const int INVERSE_S_BOX[16][16] = &#123; 0x52, 0x09, 0x6a, 0xd5, 0x30, 0x36, 0xa5, 0x38, 0xbf, 0x40, 0xa3, 0x9e, 0x81, 0xf3, 0xd7, 0xfb, 0x7c, 0xe3, 0x39, 0x82, 0x9b, 0x2f, 0xff, 0x87, 0x34, 0x8e, 0x43, 0x44, 0xc4, 0xde, 0xe9, 0xcb, 0x54, 0x7b, 0x94, 0x32, 0xa6, 0xc2, 0x23, 0x3d, 0xee, 0x4c, 0x95, 0x0b, 0x42, 0xfa, 0xc3, 0x4e, 0x08, 0x2e, 0xa1, 0x66, 0x28, 0xd9, 0x24, 0xb2, 0x76, 0x5b, 0xa2, 0x49, 0x6d, 0x8b, 0xd1, 0x25, 0x72, 0xf8, 0xf6, 0x64, 0x86, 0x68, 0x98, 0x16, 0xd4, 0xa4, 0x5c, 0xcc, 0x5d, 0x65, 0xb6, 0x92, 0x6c, 0x70, 0x48, 0x50, 0xfd, 0xed, 0xb9, 0xda, 0x5e, 0x15, 0x46, 0x57, 0xa7, 0x8d, 0x9d, 0x84, 0x90, 0xd8, 0xab, 0x00, 0x8c, 0xbc, 0xd3, 0x0a, 0xf7, 0xe4, 0x58, 0x05, 0xb8, 0xb3, 0x45, 0x06, 0xd0, 0x2c, 0x1e, 0x8f, 0xca, 0x3f, 0x0f, 0x02, 0xc1, 0xaf, 0xbd, 0x03, 0x01, 0x13, 0x8a, 0x6b, 0x3a, 0x91, 0x11, 0x41, 0x4f, 0x67, 0xdc, 0xea, 0x97, 0xf2, 0xcf, 0xce, 0xf0, 0xb4, 0xe6, 0x73, 0x96, 0xac, 0x74, 0x22, 0xe7, 0xad, 0x35, 0x85, 0xe2, 0xf9, 0x37, 0xe8, 0x1c, 0x75, 0xdf, 0x6e, 0x47, 0xf1, 0x1a, 0x71, 0x1d, 0x29, 0xc5, 0x89, 0x6f, 0xb7, 0x62, 0x0e, 0xaa, 0x18, 0xbe, 0x1b, 0xfc, 0x56, 0x3e, 0x4b, 0xc6, 0xd2, 0x79, 0x20, 0x9a, 0xdb, 0xc0, 0xfe, 0x78, 0xcd, 0x5a, 0xf4, 0x1f, 0xdd, 0xa8, 0x33, 0x88, 0x07, 0xc7, 0x31, 0xb1, 0x12, 0x10, 0x59, 0x27, 0x80, 0xec, 0x5f, 0x60, 0x51, 0x7f, 0xa9, 0x19, 0xb5, 0x4a, 0x0d, 0x2d, 0xe5, 0x7a, 0x9f, 0x93, 0xc9, 0x9c, 0xef, 0xa0, 0xe0, 0x3b, 0x4d, 0xae, 0x2a, 0xf5, 0xb0, 0xc8, 0xeb, 0xbb, 0x3c, 0x83, 0x53, 0x99, 0x61, 0x17, 0x2b, 0x04, 0x7e, 0xba, 0x77, 0xd6, 0x26, 0xe1, 0x69, 0x14, 0x63, 0x55, 0x21, 0x0c, 0x7d &#125;;int RC[10] = &#123;0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1b, 0x36&#125;;int main()&#123; int method = 1;//1表示加密， 0表示解密 //待加密/解密文件存放路径 char * source_path = &quot;0.py&quot;; // 加密/解密后文件存放路径 char *des_path = &quot;1.py&quot;; // 32位16进制密钥 char * password = &quot;dd67f79831571c947d9e85b76a7f6835&quot;; aes(source_path, des_path, password, method); printf(&quot;success!!!!!!!!!!&quot;); method = 0;//1表示加密， 0表示解密 //待加密/解密文件存放路径 source_path = &quot;1.py&quot;; // 加密/解密后文件存放路径 des_path = &quot;2.py&quot;; aes(source_path, des_path, password, method); printf(&quot;success!!!!!!!!!!&quot;);&#125;void aes(char* source_path, char* des_path, char* password, int method)&#123; //将密钥转换成4*4数组 int p[4][4]; for (int m = 0; m &lt; 4; ++m) &#123; for (int i = 0; i &lt; 4; ++i) &#123; int indx = 4 * i + m; p[i][m] = 16 * c2i(password[indx]) + c2i(password[indx + 1]); &#125; &#125; FILE *file = fopen(source_path, &quot;r&quot;); //获取文件的指针 fseek(file, 0, SEEK_END); //移动文件的指针到文件结尾 int len = ftell(file); //获取文件的长度 rewind(file); //将文件指针移动回文件开始 // 如果文件长度不是128位（16字节）的整数倍，则补齐 int size = len; if (len % 16 != 0) &#123; size = (len / 16 + 1) * 16; &#125; unsigned char content[size]; //读取文件内容赋值给content fread(content, 1, len, file); for (int j = len; j &lt; size; ++j) &#123; content[j] = 0; &#125; fclose(file); //存储结果 unsigned char encry[size]; //将文件转换成16字节的int型数组加密、解密 for (int i = 0; i &lt; size / 16; ++i) &#123; int content_to_int[4][4]; for (int j = 0; j &lt; 4; ++j) &#123; for (int k = 0; k &lt; 4; ++k) &#123; content_to_int[j][k] = content[j * 4 + k + 16 * i]; &#125; &#125; aes_detail(content_to_int, p, method); for (int j = 0; j &lt; 4; ++j) &#123; for (int k = 0; k &lt; 4; ++k) &#123; encry[j * 4 + k + 16 * i] = content_to_int[j][k]; &#125; &#125; &#125; FILE *file1 = fopen(des_path, &quot;w&quot;); fwrite(encry, size, 1, file1); fflush(file1); fclose(file1);&#125;void aes_detail(int content[4][4], int password[4][4], int encode)&#123; int p[11][4][4]; keyExpansion(password, p); if (encode) &#123; addRoundKey(content, p[0]); for (int i = 1; i &lt;= 10; ++i) &#123; subBytes(content, encode); shiftRows(content, encode); if (i != 10) &#123; mixColumns(content, encode); &#125; addRoundKey(content, p[i]); &#125; &#125;else &#123; addRoundKey(content, p[10]); for (int i = 9; i &gt;= 0; --i) &#123; shiftRows(content, encode); subBytes(content, encode); addRoundKey(content, p[i]); if (i != 0) &#123; mixColumns(content, encode); &#125; &#125; &#125;&#125;void subBytes(int a[4][4], int encode)&#123; // encode 为1 代表字节替代，为0代表逆向字节替代 for (int i = 0; i &lt; 4; ++i) &#123; for (int j = 0; j &lt; 4; ++j) &#123; int temp = a[i][j]; int row = temp / 16; int column = temp % 16; if (encode) a[i][j] = S_BOX[row][column]; else a[i][j] = INVERSE_S_BOX[row][column]; &#125; &#125;&#125;void shiftRows(int a[4][4], int encode)&#123; //encode 为1代表行移位，为0代表逆向行移位 for (int i = 0; i &lt; 4; ++i) &#123; for (int j = 0; j &lt; i; ++j) &#123; if (encode) &#123; int temp = a[i][0]; a[i][0] = a[i][1]; a[i][1] = a[i][2]; a[i][2] = a[i][3]; a[i][3] = temp; &#125; else&#123; int temp = a[i][3]; a[i][3] = a[i][2]; a[i][2] = a[i][1]; a[i][1] = a[i][0]; a[i][0] = temp; &#125; &#125; &#125;&#125;void mixColumns(int a[4][4], int encode)&#123; //encode 为1代表列混淆，为0代表逆向列混淆 for (int i = 0; i &lt; 4; ++i) &#123; int temp0 = a[0][i]; int temp1 = a[1][i]; int temp2 = a[2][i]; int temp3 = a[3][i]; if (encode) &#123; a[0][i] = aes_multiple(temp0, 2) ^ aes_multiple(temp1, 3) ^ temp2 ^ temp3; a[1][i] = temp0 ^ (aes_multiple(temp1, 2)) ^ (temp2 ^ aes_multiple(temp2, 2)) ^ temp3; a[2][i] = temp0 ^ temp1 ^ (aes_multiple(temp2, 2)) ^ (temp3 ^ aes_multiple(temp3, 2)); a[3][i] = temp0 ^ (aes_multiple(temp0, 2)) ^ temp1 ^ temp2 ^ aes_multiple(temp3, 2); &#125;else&#123; a[0][i] = aes_multiple(temp0, 14) ^ aes_multiple(temp1, 11) ^ aes_multiple(temp2, 13) ^ aes_multiple(temp3, 9); a[1][i] = aes_multiple(temp0, 9) ^ aes_multiple(temp1, 14) ^ aes_multiple(temp2, 11) ^ aes_multiple(temp3, 13); a[2][i] = aes_multiple(temp0, 13) ^ aes_multiple(temp1, 9) ^ aes_multiple(temp2, 14) ^ aes_multiple(temp3, 11); a[3][i] = aes_multiple(temp0, 11) ^ aes_multiple(temp1, 13) ^ aes_multiple(temp2, 9) ^ aes_multiple(temp3, 14); &#125; &#125;&#125;void addRoundKey(int a[4][4], int k[4][4])&#123; // 由于用w[11][4][4]表示W[44]导致行列转置，所以在进行异或操作的时候应该是a[i，j] 异或 k[j,i] for (int i = 0; i &lt; 4; ++i) &#123; for (int j = 0; j &lt; 4; ++j) &#123; a[i][j] = a[i][j] ^ k[j][i]; &#125; &#125;&#125;//AES乘法计算int aes_multiple(int a, int le)&#123; int thr = le &amp; 0x8; int sec = le &amp; 0x4; int fir = le &amp; 0x2; int fir_mod = le % 2; int result = 0; if (thr)&#123; int b = a; for (int i = 1; i &lt;=3 ; ++i) &#123; b = b&lt;&lt;1; if (b &gt;= 256) b = b ^ 0x11b; &#125; b = b % 256; result = result ^ b; &#125; if (sec)&#123; int b = a; for (int i = 1; i &lt;=2 ; ++i) &#123; b = b&lt;&lt;1; if (b &gt;= 256) b = b ^ 0x11b; &#125; b = b % 256; result = result ^ b; &#125; if (fir)&#123; int b = a &lt;&lt; 1; if (b &gt;= 256) b = b ^ 0x11b; b = b % 256; result = result ^ b; &#125; if (fir_mod) result = result ^ a; return result;&#125;void keyExpansion(int key[4][4], int w[11][4][4])&#123; for (int i = 0; i &lt; 4; ++i) &#123; for (int j = 0; j &lt; 4; ++j) &#123; w[0][i][j] = key[j][i]; &#125; &#125; for (int i = 1; i &lt; 11; ++i)&#123; for (int j = 0; j &lt; 4; ++j) &#123; int temp[4]; if (j == 0)&#123; temp[0] = w[i-1][3][1]; temp[1] = w[i-1][3][2]; temp[2] = w[i-1][3][3]; temp[3] = w[i-1][3][0]; for (int k = 0; k &lt; 4; ++k) &#123; int m = temp[k]; int row = m / 16; int column = m % 16; temp[k] = S_BOX[row][column]; if (k == 0)&#123; temp[k] = temp[k] ^ RC[i-1]; &#125; &#125; &#125; else&#123; temp[0] = w[i][j-1][0]; temp[1] = w[i][j-1][1]; temp[2] = w[i][j-1][2]; temp[3] = w[i][j-1][3]; &#125; for (int l = 0; l &lt; 4; ++l) &#123; w[i][j][l] = w[i-1][j][l] ^ temp[l]; &#125; &#125; &#125;&#125;/*int isdigit(char ch) &#123; if (ch&gt;=&#x27;0&#x27;&amp;&amp;ch&lt;=&#x27;9&#x27;) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;int isalpha(char ch) &#123; if ((ch&gt;=&#x27;a&#x27;&amp;&amp;ch&lt;=&#x27;z&#x27;)||(ch&gt;=&#x27;A&#x27;&amp;&amp;ch&lt;=&#x27;Z&#x27;)) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;int isupper(char ch) &#123; if (ch&gt;=&#x27;A&#x27;&amp;&amp;ch&lt;=&#x27;Z&#x27;) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;*///将字符转换为数值int c2i(char ch) &#123; // 如果是数字，则用数字的ASCII码减去48, 如果ch = &#x27;2&#x27; ,则 &#x27;2&#x27; - 48 = 2 if(isdigit(ch)) return ch - 48; // 如果是字母，但不是A~F,a~f则返回 if( ch &lt; &#x27;A&#x27; || (ch &gt; &#x27;F&#x27; &amp;&amp; ch &lt; &#x27;a&#x27;) || ch &gt; &#x27;z&#x27; ) return -1; // 如果是大写字母，则用数字的ASCII码减去55, 如果ch = &#x27;A&#x27; ,则 &#x27;A&#x27; - 55 = 10 // 如果是小写字母，则用数字的ASCII码减去87, 如果ch = &#x27;a&#x27; ,则 &#x27;a&#x27; - 87 = 10 if(isalpha(ch)) return isupper(ch) ? ch - 55 : ch - 87; return -1;&#125;","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"AES","slug":"aes","permalink":"https://blog.mhuig.top/tags/aes/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"WebSocket Maven配置模板","slug":"others/code/WebSocketmaven配置模板","date":"2020-01-12T05:59:18.000Z","updated":"2020-01-12T05:59:18.000Z","comments":true,"path":"posts/4a71123b.html","link":"","permalink":"https://blog.mhuig.top/posts/4a71123b.html","excerpt":"maven配置模板","text":"maven配置模板 WebSocketTemplate源码 GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn&lt;/groupId&gt; &lt;artifactId&gt;WebSocketTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!-- WebSocket --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-websocket&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-messaging&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.miemiedev&lt;/groupId&gt; &lt;artifactId&gt;mybatis-paginator&lt;/artifactId&gt; &lt;version&gt;1.2.15&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- JSP相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.41&lt;/version&gt; &lt;/dependency&gt; &lt;!-- junit--&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 配置Tomcat插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;path&gt;/&lt;/path&gt; &lt;port&gt;8080&lt;/port&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt;&lt;/project&gt;","categories":[{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"maven","slug":"模板/maven","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/maven/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://blog.mhuig.top/tags/maven/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"WebSocket","slug":"websocket","permalink":"https://blog.mhuig.top/tags/websocket/"}]},{"title":"SSM Maven配置模板","slug":"others/code/SSM maven配置模板","date":"2020-01-12T05:59:17.000Z","updated":"2020-01-12T05:59:17.000Z","comments":true,"path":"posts/4a71133b.html","link":"","permalink":"https://blog.mhuig.top/posts/4a71133b.html","excerpt":"maven配置模板","text":"maven配置模板 SSMTemplate源码 GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn&lt;/groupId&gt; &lt;artifactId&gt;WebSocketTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!-- Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.miemiedev&lt;/groupId&gt; &lt;artifactId&gt;mybatis-paginator&lt;/artifactId&gt; &lt;version&gt;1.2.15&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- JSP相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.41&lt;/version&gt; &lt;/dependency&gt; &lt;!-- junit--&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 配置Tomcat插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;path&gt;/&lt;/path&gt; &lt;port&gt;8080&lt;/port&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt;&lt;/project&gt;","categories":[{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"maven","slug":"模板/maven","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/maven/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://blog.mhuig.top/tags/maven/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"SSM","slug":"ssm","permalink":"https://blog.mhuig.top/tags/ssm/"}]},{"title":"Flink常用的算子","slug":"bigdata/Filnk/Flink常用的算子","date":"2020-01-11T06:12:29.000Z","updated":"2020-01-11T06:12:29.000Z","comments":true,"path":"posts/714668f9.html","link":"","permalink":"https://blog.mhuig.top/posts/714668f9.html","excerpt":"Flink 数据处理模型 Flink 算子 Operator","text":"Flink 数据处理模型 Flink 算子 Operator Flink 数据处理模型在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是下图这种模型，有数据源（source），有数据下游（sink） Source: 数据源 基于本地集合、基于文件、基于网络套接字 自定义的 source Apache kafka、RabbitMQ Transformation: 数据转换 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project Sink: 接收器 写入文件、打印出来、写入 Socket 、自定义的 Sink 自定义的 Sink Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、HDFS Flink 算子 OperatorMap获取一个元素并生成一个元素 FlatMap获取一个元素并生成零个、一个或多个元素 filter KeyByKeyBy 在逻辑上是基于 key 对流进行分区，相同的 Key 会被分到一个分区 AggregationsDataStream API 支持各种聚合， 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合 常用的方法有 min、minBy、max、minBy、sum max 和 maxBy 之间的区别在于 max 返回流中的最大值，但 maxBy 返回具有最大值的键， min 和 minBy 同理 WindowWindow 函数允许按时间或其他条件对现有 KeyedStream 进行分组 10 秒的时间窗口的和（聚合） 1socketStream.keyBy(0).window(Time.seconds(10)).sum(1) UnionUnion 函数将两个或多个数据流结合在一起, 这样后面在使用的时候就只需使用一个数据流就行了 1inputStream.union(inputStream1, inputStream2, ...) 123val socketStream = env.socketTextStream(&quot;localhost&quot;, 9000, &#x27;\\n&#x27;)val textStream = env.readTextFile(&quot;/word.txt&quot;)socketStream.union(textStream) Window Join通过一些 key 将同一个 window 的两个数据流 join 起来 12345stream.join(otherStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(&lt;WindowAssigner&gt;) .apply(&lt;JoinFunction&gt;) 1234inputStream.join(inputStream1) .where(0).equalTo(1) .window(Time.seconds(5)) .apply (new JoinFunction () &#123;...&#125;); Split根据条件将流拆分为两个或多个流 Select从拆分流中选择特定流，那么就得搭配使用 Select 算子 通常搭配Split算子一起使用","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flink","slug":"大数据/flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"}],"tags":[{"name":"Flink","slug":"flink","permalink":"https://blog.mhuig.top/tags/flink/"}]},{"title":"Flink Window","slug":"bigdata/Filnk/Flink Window","date":"2020-01-11T01:12:29.000Z","updated":"2020-01-11T01:12:29.000Z","comments":true,"path":"posts/38a07d35.html","link":"","permalink":"https://blog.mhuig.top/posts/38a07d35.html","excerpt":"什么是 Window？ Window 有什么作用？ Flink 中的三种Window","text":"什么是 Window？ Window 有什么作用？ Flink 中的三种Window Flink Window Demo 源码GitHub 什么是 Window？统计经过某红绿灯的汽车数量之和？ 假设在一个红绿灯处，统计通过此红绿灯的汽车数量 可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合） 这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？ 这个问题，就相当于一个定义了一个 Window（窗口），Window 的界限是 1 分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图： 再考虑一种情况，每 30 秒统计一次过去 1 分钟的汽车数量之和 此时，Window 出现了重合。这样，1 个小时内会有 120 个 Window。 Window 指定时间范围内的所有数据 滚动窗口 各个窗口之间的数据不重叠（不重复） 滑动窗口 各个窗口之间的数据重叠（重复） Window 有什么作用？通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。 Flink 中的三种 WindowFlink 在 KeyedStream（DataStream 的继承类） 中提供了下面几种 Window： 以时间驱动的 Time Window 以事件数量驱动的 Count Window 以会话间隔驱动的 Session Window Time Window 正如命名那样，Time Window 根据时间来聚合流数据。 例如：一分钟的时间窗口就只会收集一分钟的数据，并在一分钟过后对窗口中的所有数据应用于下一个算子。 在 Flink 中使用 Time Window 非常简单，输入一个时间参数，这个时间参数可以利用 Time 这个类来控制，如果事前没指定 TimeCharacteristic 类型的话，则默认使用的是 ProcessingTime 123dataStream.keyBy(1).timeWindow(Time.minutes(1)) //time Window 每分钟统计一次数量和.sum(1); 123dataStream.keyBy(1).timeWindow(Time.minutes(1), Time.seconds(30)) //隔 30s 统计过去1m和.sum(1); Count WindowApache Flink 还提供计数窗口功能，如果计数窗口的值设置的为 3 ，那么将会在窗口中收集 3 个事件，并在添加第 3 个事件时才会计算窗口中所有事件的值。 123dataStream.keyBy(1).countWindow(3) //统计每 3 个元素的数量之和.sum(1); 123dataStream.keyBy(1) .countWindow(4, 3) //每隔 3 个元素统计过去 4 个元素的数量之和.sum(1); Session WindowApache Flink 还提供了会话窗口，是什么意思呢？使用该窗口的时候你可以传入一个时间参数（表示某种数据维持的会话持续时长），如果超过这个时间，就代表着超出会话时长。 123dataStream.keyBy(1).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))//表示如果 5s 内没出现数据则认为超出会话时长，然后计算这个窗口的和.sum(1);","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flink","slug":"大数据/flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"}],"tags":[{"name":"Flink","slug":"flink","permalink":"https://blog.mhuig.top/tags/flink/"}]},{"title":"CentOS7重启之后无法联网重启network发现报错","slug":"Linux/CentOS/CentOS7重启之后无法联网重启network发现报错","date":"2020-01-10T12:12:29.000Z","updated":"2020-01-10T12:12:29.000Z","comments":true,"path":"posts/5cc46d49.html","link":"","permalink":"https://blog.mhuig.top/posts/5cc46d49.html","excerpt":"虚拟机里边的CentOS7重启之后无法联网，重启network发现报错。","text":"虚拟机里边的CentOS7重启之后无法联网，重启network发现报错。 解决方式：禁用NetworkManager 123systemctl stop NetworkManagersystemctl disable NetworkManager 然后重启网络服务，能正常联网了！ 1service network restart","categories":[{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/categories/centos7/"}],"tags":[{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/tags/centos7/"},{"name":"network","slug":"network","permalink":"https://blog.mhuig.top/tags/network/"}]},{"title":"Flink 时间语义","slug":"bigdata/Filnk/Flink 时间语义","date":"2020-01-10T07:12:29.000Z","updated":"2020-01-10T07:12:29.000Z","comments":true,"path":"posts/594433a.html","link":"","permalink":"https://blog.mhuig.top/posts/594433a.html","excerpt":"Flink 的三种时间语义Flink Time使用场景","text":"Flink 的三种时间语义Flink Time使用场景 Flink的三种时间语义 Processing Time：事件被处理时机器的系统时间 Event Time：事件自身的时间 Ingestion Time：事件进入 Flink 的时间 Process Time事件处理时间 即事件被处理时机器的系统时间 特点 最简单的 Time 概念 最好的性能和最低的延迟 分布式和异步环境下，不能提供确定性（不能保证结果数据的准确性） 容易受到事件到达系统的速度(如消息队列)、事件在系统内操作流动的速度和中断的影响 Event Time事件自身的时间，一般就是数据本身携带的时间 特点 数据本身携带，时间取决于数据 事件到达Flink之前就已经确定 必须指定如何生成WaterMarks，用来表示Event Time进度的机制 无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果 Ingestion Time事件进入Flink的时间 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳 特点 事件在进入数据源（Flink Source）时的时间作为时间戳（） 介于Event Time 和 Processing Time 之间 Time生成的位置 Flink Time使用场景Time的使用场景一般来说在生产环境中使用 Processing Time 和 Event Time 比较多，Ingestion Time 一般用的较少 Processing Time使用场景Processing Time使用场景 用户不关心事件时间，只关心这个时间窗口要有数据进来 Processing Time 的几种应用场景举例 淘宝双十一晚会大屏幕的下单总金额 Event Time使用场景Event Time使用场景 业务需求需要时间这个字段 Event Time 的几种应用场景举例 购物时先有下单事件、再有支付事件 机器异常检测出发的警告也需要具体的事件展示出来 商品广告及时精准推荐给用户依赖的就是用户在浏览器的时间段/频率/时长等信息 可能出现的情况影响事件到达不一定及时、乱序、延迟 网络抖动 服务可用性 消息队列的分区数据堆积 但是使用事件时间的话，就可能有这样的情况：数据源采集的数据往消息队列中发送时可能因为网络抖动、服务可用性、消息队列的分区数据堆积的影响而导致数据到达的不一定及时，可能会出现数据出现一定的乱序、延迟几分钟等，庆幸的是 Flink 支持通过 WaterMark 机制来处理这种延迟的数据如何设置 Time 策略？1234val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flink","slug":"大数据/flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"}],"tags":[{"name":"Flink","slug":"flink","permalink":"https://blog.mhuig.top/tags/flink/"}]},{"title":"Windows环境下netcat的安装及使用","slug":"win/windows环境下netcat的安装及使用","date":"2020-01-10T02:10:47.000Z","updated":"2020-01-10T02:10:47.000Z","comments":true,"path":"posts/d70eaced.html","link":"","permalink":"https://blog.mhuig.top/posts/d70eaced.html","excerpt":"windows环境下netcat的安装及使用","text":"windows环境下netcat的安装及使用 下载netcat下载地址：https://eternallybored.org/misc/netcat/ 解压文件夹将文件夹中的所有内容复制到C:\\Windows\\System32的文件夹下打开命令界面：Windows+R cmd 输入nc 命令即可.","categories":[{"name":"windows","slug":"windows","permalink":"https://blog.mhuig.top/categories/windows/"},{"name":"netcat","slug":"windows/netcat","permalink":"https://blog.mhuig.top/categories/windows/netcat/"}],"tags":[{"name":"windows","slug":"windows","permalink":"https://blog.mhuig.top/tags/windows/"},{"name":"netcat","slug":"netcat","permalink":"https://blog.mhuig.top/tags/netcat/"}]},{"title":"Flink实时处理Socket数据","slug":"bigdata/Filnk/Flink实时处理Socket数据","date":"2020-01-10T01:53:11.000Z","updated":"2020-01-10T01:53:11.000Z","comments":true,"path":"posts/fc610c2d.html","link":"","permalink":"https://blog.mhuig.top/posts/fc610c2d.html","excerpt":"Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能Flink实时处理Socket数据","text":"Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能Flink实时处理Socket数据 Flink Socket 源码GitHub 通过 Maven Archetype 创建项目创建项目1234mvn archetype:generate \\-DarchetypeGroupId=org.apache.flink \\-DarchetypeArtifactId=flink-quickstart-scala \\-DarchetypeVersion=1.9.0 通过以上Maven 命令进行项目创建的过程中，命令会交互式地提示用户对项目的 groupId、artifactId、version、package 等信息进行定义，且部分选项有默认值，直接回车即可。如图如果创建项目成功之后，客户端会有相应提示。 这里我们分别指定 groupId、artifactId 的信息分别如下，其余参数使用默认值 groupId：com.qst artifactId：flink-socket 检查项目对于使用 Maven 创建的项目，我们可以看到的项目结构如下所示 以上项目结构可以看出，该项目是一个 Scala 代码的项目，分别是 BatchJob.java 和StreamingJob.java 两个文件，分别赌赢 Flink 批量接口 DataSet 的实例代码和流式接口的实例代码。 将项目导入IDE项目经过上述步骤创建后，Flink 官网推荐使用 Intellij IDEA 进行后续项目开发。 编译项目项目经过上述步骤创建后，可以使用 Maven Command 命令 mvn clean package 对项目进行编译，编译完成后会在项目同级目录下生成 target/-.jar 文件，此jar文件就可以通过 Web 客户端提交到集群上运行。 开发环境配置这里我们使用官网推荐的 IntelliJ IDEA 作为应用的开发的 IDE。 下载 IntelliJ IDEA用户可以通过 IntelliJ IDEA 官方地址下载安装程序，根据操作系统选择相应的程序包进行安装。 安装 Scala Plugins安装完 IntelliJ IDEA 默认是不支持 Scala 开发环境的，需要安装 Scala 插件进行支持。一下说明在 IDEA 中进行 Scala 插件的安装。 打开 IDEA IDE 后，在 IntelliJ IDEA 菜单栏中选择 Preferences选项，然后选择 Plugins 子选项，最后在页面中选择 Marketplace，在搜索框中输入 Scala 进行搜索 在检索出来的选项列表中选择和安装 Scala插件  点击安装后重启IDE，Scala 编程环境即可生效 导入 Flink 项目 启动 IntelliJ IDEA，选择 Import Project，在文件选项框中选择创建好的项目，点击确定。 导入项目中选择 Import project from external mode 中的 Maven 后续选项使用默认值即可。 Flink Socket 应用程序编写 Flink Socket 应用程序代码123456789101112131415161718192021import org.apache.flink.streaming.api.scala._object StreamingJob &#123; def main(args: Array[String]) &#123; //设置环境变量 val env = StreamExecutionEnvironment.getExecutionEnvironment //指定数据源，读取socket val socketStream = env.socketTextStream(&quot;localhost&quot;, 9000, &#x27;\\n&#x27;) //对数据集指定转换操作逻辑 val count = socketStream .flatMap(_.toLowerCase.split(&quot;\\\\W+&quot;)) .filter(_.nonEmpty) .map((_, 1)) .keyBy(0) .sum(1) //将计算结果打印到控制台 count.print() //指定任务名称并触发流式任务 env.execute(&quot;Socket Stream&quot;) &#125;&#125; 在 IDE 中测试代码在代码文件中右键运行程序 此时会报如下错误 这时我们需要在 IDEA 的 Run/Debug Configuration 中将 “Include dependencies with “Provided” scope ”选项勾选，这时我们就可以在本地 IDE 运行了 在本地测试代码首先在命令行我们现在终端开启监听端口9000，在命令行中执行如下命令 1nc -l 9000 然后在 IDE 中 右键运行 StreamingJob 类的 main 方法，运行结果如下 在 Web 客户端中运行 Job首先在项目所在目录执行 mvn clean package 进行打包，在项目的 target 目录下生成一个 flink-socket-1.0-SNAPSHOT.jar 文件在命令行我们现在终端开启监听端口9000，在命令行中执行如下命令 1nc -l 9000 在浏览器中打开 Flink Web 监控页面，在左侧选择 Submit New Job 选项，点击 右上角的 Add New 选择我们编译好的 flink-socket-1.0-SNAPSHOT.jar 文件，点击 Submit 按钮提交Job 选择 Task Managers 选择列表中的对应 Job 点击 Stdout选项查看执行结果","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flink","slug":"大数据/flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"}],"tags":[{"name":"Flink","slug":"flink","permalink":"https://blog.mhuig.top/tags/flink/"}]},{"title":"Flink 编程模型","slug":"bigdata/Filnk/Flink 编程模型","date":"2020-01-10T01:19:47.000Z","updated":"2020-01-10T01:19:47.000Z","comments":true,"path":"posts/4920f048.html","link":"","permalink":"https://blog.mhuig.top/posts/4920f048.html","excerpt":"Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能","text":"Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能 Flink 环境准备 Flink编写程序需要依赖Java——JDK 项目使用Maven管理依赖——Maven 开发工具使用IDEA——IntelliJ IDEA JDK 8https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html Mavenhttp://maven.apache.org/download.cgi IntelliJ IDEAhttps://www.jetbrains.com/idea/download/#section=windows 下载Flinkhttps://flink.apache.org/downloads.html 安装Scala Plugins点击 File -&gt; Settings 菜单 , 或Ctrl + Alt + S 快捷键 . 打开设置面板 . 并切换到Plugins插件视图搜索 Scala 点击 Install Flink 项目模版基于Java的项目模版Flink WordCount Java 源码GitHub 在命令行使用maven创建Flink项目1234mvn archetype:generate \\-DarchetypeGroupId&#x3D;org.apache.flink \\-DarchetypeArtifactId&#x3D;flink-quickstart-java \\-DarchetypeVersion&#x3D;1.8.3 根据提示输入groupId、artifactId groupId：com.qst（所在公司、学校、组织官网网址的反写） artifactId：wordcount-java（项目名称） 项目目录结构 使用mvn命令创建项目后我们会得到一个如下结构的项目目录 编译项目 在项目所在目录执行“mvn clean package”命令对项目进行编译 这时maven会下载Flink项目需要的依赖包并编译项目 编译完成后产生一个 target/-.jar 基于 Scala 的项目模版1234mvn archetype:generate \\-DarchetypeGroupId&#x3D;org.apache.flink \\-DarchetypeArtifactId&#x3D;flink-quickstart-scala \\-DarchetypeVersion&#x3D;1.8.3 Flink WordCountFlink WordCount scala 源码GitHub 创建WrodCount项目在命令行使用maven创建Flink项目1234mvn archetype:generate \\-DarchetypeGroupId&#x3D;org.apache.flink \\-DarchetypeArtifactId&#x3D;flink-quickstart-scala \\-DarchetypeVersion&#x3D;1.8.3 根据提示在输入groupId、artifactId groupId：com.qst（所在公司、学校、组织官网网址的反写） artifactId：wordcount-scala（项目名称） 其它选项使用默认值 将项目导入IDEA 在IDEA中将 flink-wordcount 项目导入 选择 Import Project 找到 wordcount-scala 所在目录将项目导入IDEA 开发WordCount程序第一步：设定执行环境 运行Flink程序的第一步就是获得相应的执行环境，执行环境决定了程序在什么环境执行（本地/集群） 不同的运行环境也决定了程序的类型 批处理 ExecutionEnvironment 流处理 StreamExecutionEnvironment第二步：指定数据源 读取数据 定义执行环境后需要获得需要处理的数据，将外部数据转换成 DateStream 或 DataSet如下方法读取所示使用 readTextFile() 方法读取文件中的数据并转换成 DataStream 数据集1val source = env.readTextFile(&quot;/word.txt&quot;) 第三步：对数据集执行转换操作 Flink 中的 Transformation 操作通过不同的 Operator 来实现对数据的操作 Operator 内部通过 Function 接口完成数据处理 在 DataStream API 和 DataSet API 中提供了大量的转换操作flatMap、map、filter、keyBy 123456source.flatMap(line =&gt; line.toLowerCase.split(&quot;\\\\W+&quot;)) //将文本转换成数组.filter(_.length &gt; 0) //过滤空字符串.map(word =&gt; (word, 1)) //转换成 key-value 接口.keyBy(0) //按照指定字段（key）对数据进行分区.sum(1) //执行求和运算 第四步：输出结果经过转换后形成了最终结果，通常需要将结果数据输出到外部系统中 12345678source.flatMap(line =&gt; line.toLowerCase.split(&quot;\\\\W+&quot;)) //将文本转换成数组.filter(_.length &gt; 0) //过滤空字符串.map(word =&gt; (word, 1)) //转换成 key-value 接口.keyBy(0) //按照指定字段（key）对数据进行分区.sum(1) //执行求和运算.print() //输出到控制台//.writeAsText(&quot;/word_out.txt&quot;) //写入外部文件 第五步：触发程序执行所有的计算逻辑完成之后，需要调用 StreamExecutionEnvironment 的 execute 方法来触发应用程序的执行 12env.execute(&quot;Streaming Scala WordCount&quot;) 运行&amp;编译 WordCount 程序编译 WordCount 应用程序 在程序根目录执行“mvn clean package”命令进行编译 这时maven会下载Flink项目需要的依赖包并编译项目 编译完成后产生一个 target/-.jar","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flink","slug":"大数据/flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"}],"tags":[{"name":"Flink","slug":"flink","permalink":"https://blog.mhuig.top/tags/flink/"}]},{"title":"Flink 概述","slug":"bigdata/Filnk/Flink 概述","date":"2020-01-09T06:58:17.000Z","updated":"2020-01-09T06:58:17.000Z","comments":true,"path":"posts/72cd4c87.html","link":"","permalink":"https://blog.mhuig.top/posts/72cd4c87.html","excerpt":"Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能","text":"Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能 Flink是什么 Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能 Apache Flink 的前身是柏林理工大学一个研究性项目， 在 2014 被 Apache 孵化器所接受，然后迅速地成为了Apache Software Foundation的顶级项目之一 代码主要由Java实现，部分代码是Scala Flink主要处理的场景就是流数据，批处理只是流数据的一个极限特例 数据类型有界流（bounded stream） 批量数据 有界流有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。 有界流通常被称为有界数据集，数据的特点为有限不会改变的数据集合 常见的有界流 T+1的销售数据 11月的汽车销售数量 2018年全国电影票房 无界流（unbounded stream） 实时数据 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取数据，例如事件发生的顺序，以便能够推断结果的完整性。 无界流通常被称为无穷数据集，数据的特点为无穷集成的数据集合 常见的无界流 用户与客户断的实时交互数据 应用时产生的日志 金融市场的实时交易记录 有界流和无界流 数据运算模型流式计算 只要数据一直在产生，计算就持续的进行 处理无界数据集批处理 在预定义的时间内运行计算，当计算完成时释放计算机资源 处理有界数据集 Flink组件栈 Deploy本地 Local 一个Java虚拟机 Single JVM（IDE中直接运行）集群 Cluster Standalone（start-cluster.sh） YARN MESOS K8s云 Cloud GCE google AWS/EC2 amazon MapR Aliyun Program Code Flink应用程序代码Job Client 任务执行起点，负责接受用户的程序代码、创建数据流、提交数据流给Job Manager、返回结果Job Manager 作业管理器协调管理程序Task Manager 从Job Manager接受需要部署的Task RuntimeRuntime层提供了支持Flink计算的全部核心实现，比如：支持分布式Stream处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务 API&amp;Libaries 核心APIs DataSet API：批处理，处理有界的数据集 DataStream API：流式处理，处理有界或无界的数据集Table API 以表为中心声明的DSL select、project、join、group-by、aggregate操作 支持与DataStream/DataSet混合使用SQL Flink提供的最高级抽象 支持与DataStream/DataSet混合使用面向批处理的Lib FlinkML 机器学习 Gelly 图处理面向流处理的类库 CEP 复杂事件处理 SQL-Like Table的关系操作Flink的基本编程模型 Source 数据输入 基于文件 基于本地集合 基于网络套接字 自定义：Apache Kafka、RabbitMQTransformation 数据转换 Map、FlatMap、Filter、Reduce、WindowSink 数据输出 写文件 打印 socket 自定义：Apache Kafka、HDFS、MySQL 大数据框架对比（流式/实时数据处理） 大数据Lamdba框架","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flink","slug":"大数据/flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"}],"tags":[{"name":"Flink","slug":"flink","permalink":"https://blog.mhuig.top/tags/flink/"}]},{"title":"删除注释自动化","slug":"others/code/删除注释自动化","date":"2019-12-29T03:14:41.000Z","updated":"2019-12-29T03:14:41.000Z","comments":true,"path":"posts/53d0a404.html","link":"","permalink":"https://blog.mhuig.top/posts/53d0a404.html","excerpt":"实现批量删除 python java C CPP JS CSS html xml php sql 注释","text":"实现批量删除 python java C CPP JS CSS html xml php sql 注释 源码见GitHub PythonPython中的注释有单行注释和多行注释： 井号（#） Python中单行注释以 # 开头，例如： 123# 这是一个注释print(&quot;Hello, World!&quot;)多行注释用三个单引号 &#x27;&#x27;&#x27; 或者三个双引号 &quot;&quot;&quot; 将注释括起来，例如: 单引号（’’’） 1234567#!/usr/bin/python3 &#x27;&#x27;&#x27;这是多行注释，用三个单引号这是多行注释，用三个单引号 这是多行注释，用三个单引号&#x27;&#x27;&#x27;print(&quot;Hello, World!&quot;) 双引号（”””）1234567#!/usr/bin/python3 &quot;&quot;&quot;这是多行注释，用三个双引号这是多行注释，用三个双引号 这是多行注释，用三个双引号&quot;&quot;&quot;print(&quot;Hello, World!&quot;) java 单行注释 1// 注释内容 多行注释 12345/*... 注释内容....... 注释内容....... 注释内容....*/ 文档注释 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.io.*; /*** 这个类演示了文档注释* @author Ayan Amhed* @version 1.2*/public class SquareNum &#123; /** * This method returns the square of num. * This is a multiline description. You can use * as many lines as you like. * @param num The value to be squared. * @return num squared. */ public double square(double num) &#123; return num * num; &#125; /** * This method inputs a number from the user. * @return The value input as a double. * @exception IOException On input error. * @see IOException */ public double getNumber() throws IOException &#123; InputStreamReader isr = new InputStreamReader(System.in); BufferedReader inData = new BufferedReader(isr); String str; str = inData.readLine(); return (new Double(str)).doubleValue(); &#125; /** * This method demonstrates square(). * @param args Unused. * @return Nothing. * @exception IOException On input error. * @see IOException */ public static void main(String args[]) throws IOException &#123; SquareNum ob = new SquareNum(); double val; System.out.println(&quot;Enter value to be squared: &quot;); val = ob.getNumber(); val = ob.square(val); System.out.println(&quot;Squared value is &quot; + val); &#125;&#125; C语言 以//开始、以换行符结束的单行注释1const double pi = 3.1415926536; // pi是—个常量 以/开始、以/结束的块注释1int open( const char *name, int mode, … /* int permissions */ ); html 标签 123&lt;!--这是一段注释。--&gt;&lt;p&gt;这是一段普通的段落。&lt;/p&gt; php // 单行注释1// 单行注释 井号（#） 单行注释1# 单行注释 /* */多行注释块12345/*这是多行注释块它横跨了多行*/","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"删除注释","slug":"删除注释","permalink":"https://blog.mhuig.top/tags/%E5%88%A0%E9%99%A4%E6%B3%A8%E9%87%8A/"}]},{"title":"适用于Linux的windows子系统","slug":"win/windows10下安装kali子系统","date":"2019-11-23T01:23:29.000Z","updated":"2019-11-23T01:23:29.000Z","comments":true,"path":"posts/f70539ce.html","link":"","permalink":"https://blog.mhuig.top/posts/f70539ce.html","excerpt":"适用于Linux的windows子系统方法案例","text":"适用于Linux的windows子系统方法案例 开启wsl首先：为了win10能运行适用于Linux的windows子系统，我们需要开启wsl第一种方法： 开启wsl，开启步骤：按win + x进入Windows Power Shell，输入下面的命令开启， 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 开启后重启系统。 第二种 方法步骤 安装kali进入应用商店，搜索kali，直接安装 安装VIM1sudo apt-get install vim 更新源1vi &#x2F;etc&#x2F;apt&#x2F;sources.list 1234567891011121314151617181920#阿里云deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kali kali-rolling main non-free contribdeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kali kali-rolling main non-free contrib#清华大学deb http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;kali kali-rolling main contrib non-freedeb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;kali kali-rolling main contrib non-free#浙大deb http:&#x2F;&#x2F;mirrors.zju.edu.cn&#x2F;kali kali-rolling main contrib non-freedeb-src http:&#x2F;&#x2F;mirrors.zju.edu.cn&#x2F;kali kali-rolling main contrib non-free#中科大deb http:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;kali kali-rolling main non-free contribdeb-src http:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;kali kali-rolling main non-free contrib#官方源deb http:&#x2F;&#x2F;http.kali.org&#x2F;kali kali-rolling main non-free contribdeb-src http:&#x2F;&#x2F;http.kali.org&#x2F;kali kali-rolling main non-free contrib 1sudo apt-get update &amp;&amp; sudo apt-get dist-upgrade 1apt-get install apt-transport-https 配置SSH在Linux子系统默认命令端输入，查看ip地址 1ifconfig 配置SSH服务 12sudo apt-get remove --purge openssh-server ## 先删sshsudo apt-get install openssh-server ## 在安装ssh 12sudo rm &#x2F;etc&#x2F;ssh&#x2F;ssh_config ## 删配置文件sudo service ssh --full-restart 修改sshd_config文件 1vi &#x2F;etc&#x2F;ssh&#x2F;sshd_config 将#PasswordAuthentication no的注释去掉，并且将NO修改为YES，kali中默认是yes 1PasswordAuthentication yes 将PermitRootLogin without-password修改为 1PermitRootLogin yes 使用xshell登录 上面命令执行完之后，在xshell中输入用户名和ip就可以通过xshell登录自己电脑的Linux。 配置永久解决方案 通过上面的方法，我们可以通过xshell登录自己电脑的Linux。但是断开之后重新开机，我们又需要重新配置SSH。因此，我们需要配置以下命令下，一劳永逸。 1sudo service ssh --full-restart ## 将该命令保存为service.sh，存在home目录下 配置好之后，下次开机，只需要在Linux子系统的默认终端运行sh service.sh命令后，关掉终端改用xshell登录即可。 图形界面123456789sudo apt-get install vnc4server tightvncserversudo apt-get install xrdpsudo sed -i &#39;s&#x2F;port&#x3D;3389&#x2F;port&#x3D;3390&#x2F;g&#39; &#x2F;etc&#x2F;xrdp&#x2F;xrdp.ini&#x2F;&#x2F;apt-get install kali-defaults kali-root-login desktop-base kde-fullsudo apt-get install xorgsudo apt-get install xfce4&#x2F;&#x2F;apt-get install kali-defaults kali-root-login desktop-base xfce4 xfce4-places-plugin xfce4-goodiessudo echo xfce4-session &gt;~&#x2F;.xsessionsudo service xrdp restart 安装工具包1apt install kali-linux-full win下关闭kali1net stop LxssManager ubuntu下神奇的多线程apt-get安装axel1sudo apt-get install axel 下载脚本apt-fast.sh下载地址 http://www.mattparnell.com/linux/apt-fast/apt-fast.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# !/bin/sh# apt-fast v0.03 by Matt Parnell http://www.mattparnell.com, this thing is fully open-source# if you do anything cool with it, let me know so I can publish or host it for you# contact me at admin@mattparnell.com# Special thanks# Travis/travisn000 - support for complex apt-get commands# Allan Hoffmeister - aria2c support# Abhishek Sharma - aria2c with proxy support# Richard Klien - Autocompletion, Download Size Checking (made for on ubuntu, untested on other distros)# Patrick Kramer Ruiz - suggestions - see Suggestions.txt# Sergio Silva - test to see if axel is installed, root detection/sudo autorun# Use this just like apt-get for faster package downloading.# Check for proper priveliges[ &quot;`whoami`&quot; = root ] || exec sudo &quot;$0&quot; &quot;$@&quot;# Test if the axel is installedif [ ! -x /usr/bin/axel ]then echo &quot;axel is not installed, perform this?(y/n)&quot; read ops case $ops in y) if apt-get install axel -y --force-yes then echo &quot;axel installed&quot; else echo &quot;unable to install the axel. you are using sudo?&quot; ; exit fi ;; n) echo &quot;not possible usage apt-fast&quot; ; exit ;; esacfi# If the user entered arguments contain upgrade, install, or dist-upgradeif echo &quot;$@&quot; | grep -q &quot;upgrade\\|install\\|dist-upgrade&quot;; then echo &quot;Working...&quot;; # Go into the directory apt-get normally puts downloaded packages cd /var/cache/apt/archives/; # Have apt-get print the information, including the URI&#x27;s to the packages # Strip out the URI&#x27;s, and download the packages with Axel for speediness # I found this regex elsewhere, showing how to manually strip package URI&#x27;s you may need...thanks to whoever wrote it apt-get -y --print-uris $@ | egrep -o -e &quot;(ht|f)tp://[^\\&#x27;]+&quot; &gt; apt-fast.list &amp;&amp; cat apt-fast.list | xargs -l1 axel -a # Perform the user&#x27;s requested action via apt-get apt-get $@; echo -e &quot;\\nDone! Verify that all packages were installed successfully. If errors are found, run apt-get clean as root and try again using apt-get directly.\\n&quot;;else apt-get $@;fi 安装apt-fast12sudo mv &#x2F;root&#x2F;apt-fast.sh &#x2F;usr&#x2F;bin&#x2F;apt-fastsudo chmod +x &#x2F;usr&#x2F;bin&#x2F;apt-fast 现在你已经可以使用apt-fast替代apt-get了试一下 123apt-fast updateapt-fast upgradeapt-fast install XXXXX 魔改axel设置脚本1sudo vim &#x2F;etc&#x2F;axelrc 找到 1num_connections &#x3D; 4 默认的4线程直接修改这个值例如：十线程 1num_connections &#x3D; 10 Linux下的vim配置文件1vi ~&#x2F;.vimrc 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180&quot; Vim config file.&quot; Global Settings: &#123;&#123;&#123;syntax on &quot; highlight syntaxfiletype plugin indent on &quot; auto detect file typeset nocompatible &quot; out of Vi compatible mode&quot;set number &quot; show line numberset numberwidth&#x3D;3 &quot; minimal culumns for line numbersset textwidth&#x3D;0 &quot; do not wrap words (insert)set nowrap &quot; do not wrap words (view)set showcmd &quot; show (partial) command in status lineset ruler &quot; line and column number of the cursor positionset wildmenu &quot; enhanced command completionset wildmode&#x3D;list:longest,full &quot; command completion modeset laststatus&#x3D;2 &quot; always show the status lineset mouse&#x3D; &quot; use mouse in all modeset foldenable &quot; fold linesset foldmethod&#x3D;marker &quot; fold as markerset noerrorbells &quot; do not use error bellset novisualbell &quot; do not use visual bellset t_vb&#x3D; &quot; do not use terminal bellset wildignore&#x3D;.svn,.git,*.swp,*.bak,*~,*.o,*.aset autowrite &quot; auto save before commands like :next and :makeset cursorlineset hidden &quot; enable multiple modified buffersset history&#x3D;1000 &quot; record recent used command historyset autoread &quot; auto read file that has been changed on diskset backspace&#x3D;indent,eol,start &quot; backspace can delete everythingset completeopt&#x3D;menuone,longest &quot; complete options (insert)set pumheight&#x3D;10 &quot; complete popup heightset scrolloff&#x3D;5 &quot; minimal number of screen lines to keep beyond the cursorset autoindent &quot; automatically indent new lineset cinoptions&#x3D;:0,l1,g0,t0,(0,(s &quot; C kind language indent optionsset clipboard+&#x3D;unnamed &quot; shared clipboardset noexpandtab &quot; do not use spaces instead of tabsset tabstop&#x3D;4 &quot; number of spaces in a tabset softtabstop&#x3D;4 &quot; insert and delete space of &lt;tab&gt;set shiftwidth&#x3D;4 &quot; number of spaces for indentset expandtab &quot; expand tabs into spacesset incsearch &quot; incremental searchset hlsearch &quot; highlight search matchset ignorecase &quot; do case insensitive matchingset smartcase &quot; do not ignore if search pattern has CAPSset nobackup &quot; do not create backup file&quot;set noswapfile &quot; do not create swap fileset backupcopy&#x3D;yes &quot; overwrite the original fileset encoding&#x3D;utf-8set termencoding&#x3D;utf-8set fileencoding&#x3D;utf-8set fileencodings&#x3D;gb2312,utf-8,gbk,gb18030set fileformat&#x3D;unixset background&#x3D;dark&quot;colorscheme SolarizedDark_modified&quot;colorscheme wombat_modified&quot; gui settingsif has(&quot;gui_running&quot;) set guioptions-&#x3D;T &quot; no toolbar set guioptions-&#x3D;r &quot; no right-hand scrollbar set guioptions-&#x3D;R &quot; no right-hand vertically scrollbar set guioptions-&#x3D;l &quot; no left-hand scrollbar set guioptions-&#x3D;L &quot; no left-hand vertically scrollbar autocmd GUIEnter * simalt ~x &quot; window width and height language messages zh_CN.utf-8 &quot; use chinese messages if hasendif&quot; Restore the last quit position when open file.autocmd BufReadPost * \\ if line(&quot;&#39;\\&quot;&quot;) &gt; 0 &amp;&amp; line(&quot;&#39;\\&quot;&quot;) &lt;&#x3D; line(&quot;$&quot;) | \\ exe &quot;normal g&#39;\\&quot;&quot; | \\ endif&quot;&#125;&#125;&#125;&quot; Key Bindings: &#123;&#123;&#123;let mapleader &#x3D; &quot;,&quot;let maplocalleader &#x3D; &quot;\\\\&quot;&quot; map : -&gt; &lt;space&gt;map &lt;Space&gt; :&quot; move between windowsnmap &lt;C-h&gt; &lt;C-w&gt;hnmap &lt;C-j&gt; &lt;C-w&gt;jnmap &lt;C-k&gt; &lt;C-w&gt;knmap &lt;C-l&gt; &lt;C-w&gt;l&quot; Don&#39;t use Ex mode, use Q for formattingmap Q gq&quot;make Y consistent with C and Dnnoremap Y y$&quot; toggle highlight trailing whitespacenmap &lt;silent&gt; &lt;leader&gt;l :set nolist!&lt;CR&gt;&quot; Ctrl-E to switch between 2 last buffersnmap &lt;C-E&gt; :b#&lt;CR&gt;&quot; ,e to fast finding files. just type beginning of a name and hit TABnmap &lt;leader&gt;e :e **&#x2F;&quot; Make shift-insert work like in Xtermmap &lt;S-Insert&gt; &lt;MiddleMouse&gt;map! &lt;S-Insert&gt; &lt;MiddleMouse&gt;&quot; ,n to get the next location (compilation errors, grep etc)nmap &lt;leader&gt;n :cn&lt;CR&gt;nmap &lt;leader&gt;p :cp&lt;CR&gt;&quot; Ctrl-N to disable search match highlightnmap &lt;silent&gt; &lt;C-N&gt; :silent noh&lt;CR&gt;&quot; center display after searchingnnoremap n nzznnoremap N Nzznnoremap * *zznnoremap # #zznnoremap g* g*zznnoremap g# g#z&quot;&#125;&#125;&#125;&quot; mrulet MRU_Window_Height &#x3D; 10nmap &lt;Leader&gt;r :MRU&lt;cr&gt;&quot; taglistlet g:Tlist_WinWidth &#x3D; 25let g:Tlist_Use_Right_Window &#x3D; 0let g:Tlist_Auto_Update &#x3D; 1let g:Tlist_Process_File_Always &#x3D; 1let g:Tlist_Exit_OnlyWindow &#x3D; 1let g:Tlist_Show_One_File &#x3D; 1let g:Tlist_Enable_Fold_Column &#x3D; 0let g:Tlist_Auto_Highlight_Tag &#x3D; 1let g:Tlist_GainFocus_On_ToggleOpen &#x3D; 1nmap &lt;Leader&gt;t :TlistToggle&lt;cr&gt;&quot; nerdtreelet g:NERDTreeWinPos &#x3D; &quot;right&quot;let g:NERDTreeWinSize &#x3D; 30let g:NERDTreeShowLineNumbers &#x3D; 1let g:NERDTreeQuitOnOpen &#x3D; 1nmap &lt;Leader&gt;f :NERDTreeToggle&lt;CR&gt;nmap &lt;Leader&gt;F :NERDTreeFind&lt;CR&gt;&quot;pastevmap &lt;C-c&gt; &quot;+ynmap &lt;C-v&gt; &quot;+pset pastetoggle&#x3D;&lt;F12&gt;&quot;C，C++ Java Compile and run by F5map &lt;F5&gt; :call CompileRunGcc()&lt;CR&gt;func! CompileRunGcc() exec &quot;w&quot; if &amp;filetype &#x3D;&#x3D; &#39;c&#39; exec &quot;!g++ % -o %&lt;&quot; exec &quot;! .&#x2F;%&lt;&quot; elseif &amp;filetype &#x3D;&#x3D; &#39;cpp&#39; exec &quot;!g++ % -o %&lt;&quot; exec &quot;! .&#x2F;%&lt;&quot; elseif &amp;filetype &#x3D;&#x3D; &#39;java&#39; exec &quot;!javac %&quot; exec &quot;!java %&lt;&quot; elseif &amp;filetype &#x3D;&#x3D; &#39;sh&#39; :!.&#x2F;% endifendfunc&quot;C,C++ debugmap &lt;F8&gt; :call Rungdb()&lt;CR&gt;func! Rungdb() exec &quot;w&quot; exec &quot;!g++ % -g -o %&lt;&quot; exec &quot;!gdb .&#x2F;%&lt;&quot;endfunc PS11PS1&#x3D;&#39;$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;31m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ &#39; bashrc123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# ~&#x2F;.bashrc: executed by bash(1) for non-login shells.# see &#x2F;usr&#x2F;share&#x2F;doc&#x2F;bash&#x2F;examples&#x2F;startup-files (in the package bash-doc)# for examples# If not running interactively, don&#39;t do anythingcase $- in *i*) ;; *) return;;esac# don&#39;t put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL&#x3D;ignoreboth# append to the history file, don&#39;t overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE&#x3D;1000HISTFILESIZE&#x3D;2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern &quot;**&quot; used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)#[ -x &#x2F;usr&#x2F;bin&#x2F;lesspipe ] &amp;&amp; eval &quot;$(SHELL&#x3D;&#x2F;bin&#x2F;sh lesspipe)&quot;# set variable identifying the chroot you work in (used in the prompt below)if [ -z &quot;$&#123;debian_chroot:-&#125;&quot; ] &amp;&amp; [ -r &#x2F;etc&#x2F;debian_chroot ]; then debian_chroot&#x3D;$(cat &#x2F;etc&#x2F;debian_chroot)fi# set a fancy prompt (non-color, unless we know we &quot;want&quot; color)case &quot;$TERM&quot; in xterm-color) color_prompt&#x3D;yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the promptforce_color_prompt&#x3D;yesif [ -n &quot;$force_color_prompt&quot; ]; then if [ -x &#x2F;usr&#x2F;bin&#x2F;tput ] &amp;&amp; tput setaf 1 &gt;&amp;&#x2F;dev&#x2F;null; then # We have color support; assume it&#39;s compliant with Ecma-48 # (ISO&#x2F;IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt&#x3D;yes else color_prompt&#x3D; fifiif [ &quot;$color_prompt&quot; &#x3D; yes ]; then PS1&#x3D;&#39;$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;31m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ &#39;else PS1&#x3D;&#39;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h:\\w\\$ &#39;fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase &quot;$TERM&quot; inxterm*|rxvt*) PS1&#x3D;&quot;\\[\\e]0;$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h: \\w\\a\\]$PS1&quot; ;;*) ;;esac# colored GCC warnings and errorsexport GCC_COLORS&#x3D;&#39;error&#x3D;01;31:warning&#x3D;01;35:note&#x3D;01;36:caret&#x3D;01;32:locus&#x3D;01:quote&#x3D;01&#39;# enable color support of ls and also add handy aliasesif [ -x &#x2F;usr&#x2F;bin&#x2F;dircolors ]; then test -r ~&#x2F;.dircolors &amp;&amp; eval &quot;$(dircolors -b ~&#x2F;.dircolors)&quot; || eval &quot;$(dircolors -b)&quot; alias ls&#x3D;&#39;ls --color&#x3D;auto&#39; alias dir&#x3D;&#39;dir --color&#x3D;auto&#39; alias vdir&#x3D;&#39;vdir --color&#x3D;auto&#39; alias grep&#x3D;&#39;grep --color&#x3D;auto&#39; alias fgrep&#x3D;&#39;fgrep --color&#x3D;auto&#39; alias egrep&#x3D;&#39;egrep --color&#x3D;auto&#39;fi# some more ls aliasesalias ll&#x3D;&#39;ls -l&#39;alias la&#x3D;&#39;ls -A&#39;alias l&#x3D;&#39;ls -la&#39;# Alias definitions.# You may want to put all your additions into a separate file like# ~&#x2F;.bash_aliases, instead of adding them here directly.# See &#x2F;usr&#x2F;share&#x2F;doc&#x2F;bash-doc&#x2F;examples in the bash-doc package.if [ -f ~&#x2F;.bash_aliases ]; then . ~&#x2F;.bash_aliasesfi# enable programmable completion features (you don&#39;t need to enable# this, if it&#39;s already enabled in &#x2F;etc&#x2F;bash.bashrc and &#x2F;etc&#x2F;profile# sources &#x2F;etc&#x2F;bash.bashrc).if ! shopt -oq posix; then if [ -f &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion ]; then . &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion elif [ -f &#x2F;etc&#x2F;bash_completion ]; then . &#x2F;etc&#x2F;bash_completion fifi","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"kali","slug":"linux/kali","permalink":"https://blog.mhuig.top/categories/linux/kali/"}],"tags":[{"name":"kali","slug":"kali","permalink":"https://blog.mhuig.top/tags/kali/"}]},{"title":"特征向量和特征值的几何本质","slug":"others/ml/特征向量和特征值的几何本质","date":"2019-11-17T01:52:45.000Z","updated":"2019-11-17T01:52:45.000Z","comments":true,"path":"posts/f0765214.html","link":"","permalink":"https://blog.mhuig.top/posts/f0765214.html","excerpt":"子矩阵的特征值编码了原矩阵特征向量的隐藏信息。","text":"子矩阵的特征值编码了原矩阵特征向量的隐藏信息。 为阶矩阵，若数和维非列向量满足，那么数称为的特征值，称为的对应于特征值的特征向量。 它的物理意义是： 一个矩阵乘以一个向量， 就相当于做了一个线性变换。 方向仍然保持不变， 只是拉伸或者压缩一定倍数。 特征向量和特征值的几何本质，其实就是： 空间矢量的旋转和缩放。 线性变换 A 对于特征空间只起到“扩张(或者压缩)”的作用（扩张后还是同样的特征空间） 求解特征向量按照传统解法： 计算特征多项式→求解特征值→求解齐次线性方程组，得出特征向量。 全新的方法： 其中: 为特征值对应特征向量的第个元素; 为矩阵的第个特征向量; 为矩阵的第个余子式,是该主子式的第个特征值. 通过删除原始矩阵的行和列，创建子矩阵。 子矩阵和原始矩阵的特征值组合在一起，就可以计算原始矩阵的特征向量。 简而言之，已知特征值，一个方程式就可以求得特征向量。 参考文献Eigenvectors from Eigenvalues Eigenvalues: the Rosetta Stone for Neutrino Oscillations in Matter","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"},{"name":"线性代数","slug":"math/线性代数","permalink":"https://blog.mhuig.top/categories/math/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"}],"tags":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"},{"name":"线性代数","slug":"线性代数","permalink":"https://blog.mhuig.top/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"特征向量","slug":"特征向量","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"}]},{"title":"Xrdp连接远程桌面","slug":"win/xrdp连接远程桌面","date":"2019-11-07T03:46:25.000Z","updated":"2019-11-07T03:46:25.000Z","comments":true,"path":"posts/ca7a6c92.html","link":"","permalink":"https://blog.mhuig.top/posts/ca7a6c92.html","excerpt":"在和远程服务器交互的过程中，除了最基础的ssh链接以外，更多人喜欢图形界面的操作，当然ssh+x11可以实现部分图形的使用，但是依然需要敲命令行，虽然看起来很酷（zhuang）炫 （bi）但是图形界面依然是很多人的习惯。所以介绍下xrdp访问远程CentOS的处理步骤","text":"在和远程服务器交互的过程中，除了最基础的ssh链接以外，更多人喜欢图形界面的操作，当然ssh+x11可以实现部分图形的使用，但是依然需要敲命令行，虽然看起来很酷（zhuang）炫 （bi）但是图形界面依然是很多人的习惯。所以介绍下xrdp访问远程CentOS的处理步骤 安装epel库，否则无法安装xrdp 1yum install epel-release 安装 xrdp 1yum install xrdp 安装tigervnc-server 1yum install tigervnc-server 设置xrdp服务，开机自动启动 12systemctl start xrdpsystemctl enable xrdp 查看xrdp是否启动 12systemctl status xrdp.servicess -antup|grep xrdp 启动window rdp连接 附录 centos系统xrdp登录失败 .bashrc里面修改过PATH环境变量，添加过anaconda/bin 1vi ~&#x2F;.bashrc 最后添加 1conda deactivate 1source .bashrc","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"xrdp","slug":"xrdp","permalink":"https://blog.mhuig.top/tags/xrdp/"}]},{"title":"Linux设置虚拟内存","slug":"Linux/Linux设置虚拟内存","date":"2019-11-07T02:53:04.000Z","updated":"2019-11-07T02:53:04.000Z","comments":true,"path":"posts/d53724bc.html","link":"","permalink":"https://blog.mhuig.top/posts/d53724bc.html","excerpt":"虚拟内存配置","text":"虚拟内存配置 查看内存1free -m -m是显示单位为MB，-g单位GB 创建一个文件1touch &#x2F;root&#x2F;swapfile 使用dd命令，来创建大小为2G的文件swapfile: 1dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;root&#x2F;swapfile bs&#x3D;1M count&#x3D;2048 命令执行完需要等待一段时间 if表示input_file输入文件 of表示output_file输出文件 bs表示block_size块大小 count表示计数。 这里，我采用了数据块大小为1M，数据块数目为2048，这样分配的空间就是2G大小。 格式化交换文件1mkswap &#x2F;root&#x2F;swapfile 启用交换文件1swapon &#x2F;root&#x2F;swapfile 开机自动加载虚拟内存1vi &#x2F;etc&#x2F;fstab 在/etc/fstab文件中加入如下命令： 1&#x2F;root&#x2F;swapfile swap swap defaults 0 0 重启后生效1reboot 删除交换分区和交换文件如果要删除交换分区和交换文件，逆着上面的顺序操作: 先删除/etc/fstab文件中添加的交换文件行停用交换文件 1swapoff &#x2F;root&#x2F;swapfile 删除交换文件 1rm -fr &#x2F;root&#x2F;swapfile","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"虚拟内存","slug":"虚拟内存","permalink":"https://blog.mhuig.top/tags/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/"}]},{"title":"视知觉整合的认知和神经机制研究","slug":"others/ml/视知觉整合的认知和神经机制研究","date":"2019-10-02T01:00:46.000Z","updated":"2019-10-02T01:00:46.000Z","comments":true,"path":"posts/63637a24.html","link":"","permalink":"https://blog.mhuig.top/posts/63637a24.html","excerpt":"本文主要探讨知觉整合研究的新视角、轮廓线整合与纹理整合的神经机制以及知觉整合机制待解决的研究问题。","text":"本文主要探讨知觉整合研究的新视角、轮廓线整合与纹理整合的神经机制以及知觉整合机制待解决的研究问题。 知觉整合研究新视角结构极简取向知觉整合理论研究中最有目共睹的研究成果是格式塔思想，其最核心的原则是结构最简化原则，该思想可根据结构极简原则推测出格式塔知觉组织的其他特定原则。 格式塔心理学家认为理解知觉组织原则的关键在于将视网膜图像中的所有结构识别为视觉系统所敏感的知觉结构。 所谓结构极简原则，是指视觉系统将所有可获得信息组合为最简化的表征方式。 Leeuwenberg提出了一套思想框架，视觉系统通过选择编码语言的最短表达式来描述刺激的可能组织，结构信息理论所负载的最短代码或者是最少信息即最短表达式。 生态学取向结构信息理论能够解释一些知觉组织现象，但是他不能回答的一个重要问题是，为什么视觉系统对某些特定的结构更为敏感。 为什么视觉系统对某些特定的结构敏感性有利于有机体发现外部世界的结构。 知觉整合生态学研究视角有一个普适性的基本原理：不论视觉系统通过哪种方式进行整合，确定哪些部分属于同一整体的判定，其结果更可能是对符合外部世界的真实状态的反应。 Kruer和Sigman等人发现自然场景图像共线、共圆以及平行排布的统计学规律。 相关研究发现自然场景中相邻边缘间最主要的排布方式是对齐分布。首尾相连的线段对出现的概率要远远高于边对边的线段对，而且他们在空间比例不变性维度上有质的差异。 计算模型取向一般来说，计算模型至少包括两个相关的计算理论水平：宏观水平的整体框架以及微观水平的特定机制。 在宏观水平，计算理论的目标是在各种结构类型中找出最适合用于计算观察者所看到结构的整体框架，从而服务于知觉整合分析。 在微观层面，计算理论主要聚焦于具体的计算元素以及元素间的相互关系。与知觉整合相关的一个计算模型来自于环路循环连接网络：一个类似神经元元素组成的前馈和反馈连接的构型。 有研究者进一步推测，大脑的物理格式塔是基于分布在皮层中的动态电磁场。 所谓对称环路循环网络，是指网络中任意单位对之间的双向连接方向都有相同的权重。该网络总会收敛到均衡状态，使得信息约束满足各向同性至物理最小能量。 神经机制研究取向知觉整合神经机制研究的目标是探究促使知觉整合发生的实际神经活动的本质。 以往关于初级视皮层简单神经元本质的研究说明功能和生理学研究相互依赖的最好例子。Hubel和Wiesel在对猫的研究中发现外侧膝状体和初级视皮层中的单个神经元对简单的刺激属性（如朝向、运动方向等）具有选择性反应，他们将此解释为“特征探测器”（如线条、边缘探测器）。 当研究者在经典感受野内呈现偏好朝向的随机运动点模式时，他们发现神经元反应与运动方向的倒U关系（最优朝向反应最强，随着与最优朝向顺时针或逆时针偏转越来越大时，神经元反应越来越小）。 结果表明神经元不仅仅对其感受野内的基本刺激属性有反应，单个神经元活动会受到周围神经元的影响，提示着神经元还表征格式塔的相关属性。 轮廓线整合的神经机制研究者发现初级视皮层中的神经元不仅受到经典感受野内刺激的影响，还受到感受野外刺激的影响，单个神经元的活动会受到神经元之间相互作用的调节。虽然感受野外的线段本身不会诱发神经元的反应，但是当感受野外的线段与感受野内线段成共线关系时，神经元的反应会增强，而且感受野外线段数量越多，其发放强度越大。 Field等人根据其系列结果提出了“联合野”概念，他们的理论认为具有相似朝向选择性的神经元之间会具有选择性的相互作用，当这些神经元的排布方式违反这种规则时，这种促进和抑制的链接使得大脑完成对轮廓线信息的编码。 视觉系统在长期的进化发展过程中受到环境的交互影响，外界环境优化了视觉系统对环境中具有最高统计概率的刺激或刺激模式的反应机制。 研究者提出轮廓整合的神经实现基础是通过初级视皮层神经元间长距离兴奋性连接网络组成的“联合野”所实现的，然而最新的研究认为实现“联合野”还可能需要来自高级皮层自上而下对初级皮层的反馈作用。 纹理整合的神经机制神经元感受野中的纹理朝向与外周非经典感受野内的纹理朝向成正交关系时，会使得具有朝向信息的纹理边界线被“朝向对抗”神经元探测到。 目前“朝向对抗”神经元还没有被证实。 研究提示，各层级视皮层的前馈和水平投射可以对感受野内进行中心-外周比较，从而使得早期视皮层实现对较小空间范围内的同向抑制，较高级视皮层实现对较大空间范围内的同向抑制；反馈连接则将较高级皮层的区域填充信号反馈传回到低级视皮层实现同向兴奋。 研究者在多分层的层级视觉框架中通过多个空间尺度的特征地图架构了纹理分隔计算模型，实现了对纹理分隔任务的加工。 关于图形背景分割两阶段理论：该理论提出的第一个阶段是边界探测，纹理定义的边界线首先由具有相似偏好神经元间的相互抑制机制所探测到，该理论得到了实验证据的支持，研究者发现初级和较高级视皮质表层的神经元在很短时间内对图形边界的反应更强。第二个阶段是区域填充，该模型的区域填充过程始于存在于视觉系统多个空间尺度中的特征探测器，然后这些神经元会将信号反馈回早期视皮层的神经元，区域填充的结果是初级视皮层以活动增强的方式表征图像区域。研究者提出通过NMDA受体以及存在反馈链接并投射到第1层和第5层深层和表层神经元的树突共同作用实现将调制信号限制在激活强度最大的神经元群体，即图像表征区域。通过两种机制的共同作用，视觉系统通过对图形增强背景抑制的编码模式实现对图形背景的分割，从而实现对图形的知觉以及准确完成行为任务。 知觉整合机制待解决的研究问题视知觉整合的加工时程视知觉整合与注意麻醉情况下的无意识状态不能进行轮廓整合 自下而上与自上而下初级视皮层是否表征轮廓信息受限于知觉学习状态，只有当轮廓任务被学习之后，初级视皮层才能表征轮廓信息。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"视知觉","slug":"视知觉","permalink":"https://blog.mhuig.top/tags/%E8%A7%86%E7%9F%A5%E8%A7%89/"}]},{"title":"视知觉整合","slug":"others/ml/视知觉整合","date":"2019-10-01T11:51:37.000Z","updated":"2019-10-01T11:51:37.000Z","comments":true,"path":"posts/4c2deb5e.html","link":"","permalink":"https://blog.mhuig.top/posts/4c2deb5e.html","excerpt":"为了满足生存和生活的需要，人类需随时对外界环境中的客体信息进行高效地识别并与之产生交互。然而，由于视觉系统的固有组织属性，视觉系统必须提供一个强有力的机制快速地从海量的碎片式信息中准确识别出目标客体，这是视觉系统面临的一大挑战。","text":"为了满足生存和生活的需要，人类需随时对外界环境中的客体信息进行高效地识别并与之产生交互。然而，由于视觉系统的固有组织属性，视觉系统必须提供一个强有力的机制快速地从海量的碎片式信息中准确识别出目标客体，这是视觉系统面临的一大挑战。 视知觉整合（visual perceptual grouping）是指视觉系统将场景中属于同一客体或模式的离散元素组合并与其他客体或模式及背景区分的过程。 视知觉整合通常被认为是低级感觉加工和高级知觉加工（如客体、场景或事件加工等）间的功能桥梁。 视知觉整合与知觉组织当视网膜上的信息经由外侧膝状体首次进入初级视觉皮层时，神经元群组会对落在其感受野内的局部信号进行表征，如客体的轮廓线、纹理，在大多数情况下，同一个客体的不同部分会由具有不同调谐属性的神经元来表征。这是由于初级视觉皮层单个神经元的感受野很小且仅编码特定特征，当客体的大小大于单个神经元的感受野范围时，同一个客体的不同部分会由不同神经元来表征。比如，同一个客体的轮廓线会以线段的方式在具有不同朝向调谐属性的神经元来表征。 理论上这会严重破坏视觉信息的完整性，但事实上人们始终能知觉到排列有序的不同客体和背景信息，而不是一堆没有组织结构的局部信息的集合。 在这个过程中，视觉系统所面临的第一个挑战是，人们如何将属于同一个客体的元素从嘈杂的背景中提取整合并与其他客体及背景信息区分开来，即大脑如何完成知觉整合过程。 知觉整合对客体识别及其与环境交互都很重要。视觉系统的内在结构属性使得外界信息始于碎片式表征，然而我们最终知觉到的是排布有序的外部世界。 研究表明灵长类动物在刺激出现后150ms内即可识别出自然场景中的客体，一种观点认为这是因为视觉系统具有一套强有力的机制能高效完成整合。 视觉系统面临的最首要的知觉组织问题是判断视网膜上的哪些色块或者亮度块集合属于同一个或同一群客体。 在视觉系统加工的过程中，视觉系统首先需要将输入的离散信号准确地组织为后续信息加工的整体单元，即知觉组织加工。知觉组织是后续客体识别、注意分配等高级加工的基础。 研究历史1923年，韦德海默提出了知觉组织和知觉整合问题，试图阐述清楚知觉组织最根本的定律。最具有普世性的核心定律是所谓的极简定律，即大脑具有看见最简单形状的倾向。 20世纪五六十年代，视觉科学有了革命性的发展，尤其是单细胞电生理记录技术和计算模型的发展。Hubel和Wiesel等人发现初级视皮层的神经元对基本视觉特征（如特定方向的边缘）具有选择性反应。 Campbell等人使用线性系统方法对视觉加工过程进行建模并取得了很大进展。 当代知觉组织研究还发展出了新的间接测量方法，并从实验心理学中借鉴了标准的测量范式，在测量指标上，借鉴了心理物理学的阈值和无偏差反应指标等。 当代视知觉整合研究进展共同区域律共同区域律是指观察者会倾向于将同一个边界范围内的元素知觉为整体。 具体来说，当离散元素处于同一个连续同质的颜色或纹理空间区域内，或处于同一个边界线内，这些离散元素会基于共同区域进行整合，从而被知觉为整体。 如果两个元素都处于同一个图像区域内，那么同偶然出现在一个空间区域内的元素相比，这两个元素属于同一个客体的概率更高。 元素连通律当两个元素间存在第三个元素将其联通时，观察者会倾向于将这两个元素知觉为整体，即所谓的元素连通律。 同步律亮度或运动方向的共同性会诱发整合。 研究还发现即便元素运动方向不相关，元素间也能根据他们在出现时间上的同步性进行整合。 同步律是新的整合原则，不能被已知的视觉机制所解释，并存在争议。 当代知觉整合范式的研究进展早期知觉整合研究通过简单图片分别研究整合的关键因素，然而日常生活中的视觉场景并不是如此简单。 外部世界的典型视觉场景投影在视网膜形成二维图像，该图像由不同的亮度、颜色、形状、纹理等大量图形元素组成。其中边界线为客体的二维和三维形状提供了至关重要的信息。对于连通的没有受遮挡的客体来说，客体的边缘线在视网膜上投射为简单的闭合曲线，该曲线本身即足以形成完整的二维至三维形状知觉。 然而，视觉场景中经常存在遮挡，或存在客体与背景的亮度或颜色对比度低等情况，导致投射在视网膜上的轮廓线线段就已经变得碎片化了，而且因为视觉系统自身的固有结构属性，这种零碎的信息在大脑中呈碎片化表征。为完成识别，视觉系统需将轮廓线线段进行整合。此外，作为替代方案，区域整合也为也为客体识别提供了重要的线索，即视觉系统将轮廓线内部的刺激信息根据相似性原则进行整合。 不论是轮廓边界线还是区块纹理，信息首先进入初级视觉皮层都是碎片化式的表征，那么视觉系统面临的一个根本的重要任务是如何将这些信息重组成我们所感知到的排列有序的客体知觉。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"视知觉","slug":"视知觉","permalink":"https://blog.mhuig.top/tags/%E8%A7%86%E7%9F%A5%E8%A7%89/"}]},{"title":"银行家算法","slug":"others/OS/银行家算法","date":"2019-09-26T08:24:03.000Z","updated":"2019-09-26T08:24:03.000Z","comments":true,"path":"posts/3bfd1783.html","link":"","permalink":"https://blog.mhuig.top/posts/3bfd1783.html","excerpt":"银行家算法（Banker’s Algorithm）是一个避免死锁（Deadlock）的著名算法，是由艾兹格·迪杰斯特拉在1965年为T.H.E系统设计的一种避免死锁产生的算法。它以银行借贷系统的分配策略为基础，判断并保证系统的安全运行。","text":"银行家算法（Banker’s Algorithm）是一个避免死锁（Deadlock）的著名算法，是由艾兹格·迪杰斯特拉在1965年为T.H.E系统设计的一种避免死锁产生的算法。它以银行借贷系统的分配策略为基础，判断并保证系统的安全运行。 背景在银行中，客户申请贷款的数量是有限的，每个客户在第一次申请贷款时要声明完成该项目所需的最大资金量，在满足所有贷款要求时，客户应及时归还。银行家在客户申请的贷款数量不超过自己拥有的最大值时，都应尽量满足客户的需要。在这样的描述中，银行家就好比操作系统，资金就是资源，客户就相当于要申请资源的进程。 进程123456 Allocation Max Available ＡＢＣＤ ＡＢＣＤ ＡＢＣＤP1 ００１４ ０６５６ １５２０ P2 １４３２ １９４２ P3 １３５４ １３５６P4 １０００ １７５０ 我们会看到一个资源分配表，要判断是否为安全状态，首先先找出它的Need，Need即Max(最多需要多少资源)减去Allocation(原本已经分配出去的资源)，计算结果如下： 123456 NEEDＡＢＣＤ０６４２ ０５１００００２０７５０ 然后加一个全都为false的字段 12345FINISHfalsefalsefalsefalse 接下来找出need比available小的(千万不能把它当成4位数 他是4个不同的数) 123456 NEED AvailableＡＢＣＤ ＡＢＣＤ０６４２ １５２００５１０&lt;-０００２０７５０ P2的需求小于能用的，所以配置给他再回收 123456 NEED AvailableＡＢＣＤ ＡＢＣＤ０６４２ １５２０００００ ＋１４３２０００２－－－－－－－０７５０ ２９５２ 此时P2 FINISH的false要改成true(己完成) 12345FINISHfalsetruefalsefalse 接下来继续往下找，发现P3的需求为0002，小于能用的2952，所以资源配置给他再回收 123456 NEED AvailableＡＢＣＤ Ａ Ｂ Ｃ Ｄ０６４２ ２ ９ ５ ２００００ ＋１ ３ ５ ４００００－－－－－－－－－－０７５０ ３ 12 10 6 依此类推，做完P4→P1，当全部的FINISH都变成true时，就是安全状态。 安全和不安全的状态如果所有过程有可能完成执行（终止），则一个状态（如上述范例）被认为是安全的。由于系统无法知道什么时候一个过程将终止，或者之后它需要多少资源，系统假定所有进程将最终试图获取其声明的最大资源并在不久之后终止。在大多数情况下，这是一个合理的假设，因为系统不是特别关注每个进程运行了多久（至少不是从避免死锁的角度）。此外，如果一个进程终止前没有获取其它能获取的最多的资源，它只是让系统更容易处理。 基于这一假设，该算法通过尝试寻找允许每个进程获得的最大资源并结束（把资源返还给系统）的进程请求的一个理想集合，来决定一个状态是否是安全的。不存在这个集合的状态都是不安全的。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"死锁","slug":"死锁","permalink":"https://blog.mhuig.top/tags/%E6%AD%BB%E9%94%81/"}]},{"title":"定时备份服务器/网站数据到Github私人仓库","slug":"Linux/CentOS/定时备份服务器-网站数据到Github私人仓库","date":"2019-09-24T14:07:00.000Z","updated":"2019-09-24T14:07:00.000Z","comments":true,"path":"posts/32ffa341.html","link":"","permalink":"https://blog.mhuig.top/posts/32ffa341.html","excerpt":"现在Github被微软收购后，私人仓库已经开始免费了，然后就可以拿来折腾下了，让其充分发挥下作用，这里我们可以用来备份下网站或者服务器一些数据。","text":"现在Github被微软收购后，私人仓库已经开始免费了，然后就可以拿来折腾下了，让其充分发挥下作用，这里我们可以用来备份下网站或者服务器一些数据。 配置Git SSH密钥由于本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以必须要让github仓库认证你SSH key，在操作之前，需要先在服务器上生成SSH key。 我们先去根目录下使用命令： 12cd ~ssh-keygen -t rsa 这里会要你命名密匙名称(这里建议使用默认名称)，然后连续按几次Enter，这时候会在/root/.ssh文件夹生成2个ssh密钥，然后我们查看公钥id_rsa.pub。 1cat ~&#x2F;.ssh&#x2F;id_rsa.pub 查看后，再复制下公钥，然后打开Github官网，进入 https://github.com/settings/ssh/new ，Title随便填，然后Key填入刚刚复制的密匙，最后点击Add SSH Key添加即可。 建立私人仓库我们需要先访问 https://github.com/new ，新建一个仓库用来存放备份文件，名称自己随意，记得下面一定要勾选Private，也就是私人仓库。 配置本地仓库由于博主是用来备份网站，所以需要备份文件夹为网站根目录/alidata/，也就是把该文件夹定为本地仓库，使用命令： 12345678#进入需要备份的文件夹cd &#x2F;alidata&#x2F;#安装gityum install git#初始化你的github仓库git init#关联到远程github仓库git remote add origin git@github.com:MHuiG&#x2F;BackupWebSite.git 关联仓库的时候，后面可以用HTTPS链接也可以用SSH，这里强烈建议选择SSH，安全性很高。 初次备份1234567891011#进入备份的文件夹cd &#x2F;alidata&#x2F;#忽略大于50.00 MB文件find . -size +50M&gt;.gitignoresed -i &#39;s&#x2F;.&#x2F;&#x2F;&#39; .gitignore#把目录下所有文件更改状况提交到暂存区，包括增，删，改。git add -A#提交更改的说明，说明随意了，这里为BackupWebSitegit commit -m &quot;BackupWebSite&quot;#开始推送到Githubgit push -u origin master 推送的时候可能会提示The authenticity of host ‘github.com’ can’t be established.信息，直进yes即可。然后可以看到仓库的备份文件了。 设置定时备份在根目录先新建一个bash脚本： 1nano ~/gitback.sh 代码如下： 123456789101112#!/bin/bash#进入到网站根目录，记得修改为自己的站点cd /alidata/#将数据库导入到该目录，这里以mysql为例，passwd为数据库密码，all.sql为备份的数据库文件mysqldump -uroot -ppasswd --events --all-databases&gt;all.sql#忽略大于50.00 MB文件find . -size +50M&gt;.gitignoresed -i &#x27;s/.//&#x27; .gitignoregit add -Agit commit -m &quot;BackupWebSite&quot;git push -u origin master 然后编辑好了后，使用ctrl+x，y保存退出。再测试下脚本，使用命令 1bash ~&#x2F;gitback.sh 脚本没问题的话，再设置为每天05:15执行一次： 1234#并将运行日志输出到根目录的siteback.log文件echo &quot;15 05 * * * bash ~&#x2F;gitback.sh &gt; ~&#x2F;siteback.log 2&gt;&amp;1 &amp;&quot; &gt; bt.croncrontab bt.cronrm -rf bt.cron 最后使用命令查看添加成功。 1crontab -l 附录crontab定时任务中提示command not found解决方案写了个脚本定时从MySQL中提取数据，但是crontab发邮件提示mysql command not found 很奇怪，因为直接执行此脚本不会报错，正常运行，但加入到crontab中就会报错， 经查，MySQL不在crontab执行的环境变量中 解决方案： 找到MySQL的安装路径： which mysql 假设找到的是:/home/user1/mysql/bin/mysql 建立软连接 cd /usr/bin &amp;&amp; ln -fs /home/user1/mysql/bin/mysql mysql","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"Github","slug":"github","permalink":"https://blog.mhuig.top/tags/github/"}]},{"title":"CPU调度","slug":"others/OS/CPU调度","date":"2019-09-24T13:11:01.000Z","updated":"2019-09-24T13:11:01.000Z","comments":true,"path":"posts/be57c430.html","link":"","permalink":"https://blog.mhuig.top/posts/be57c430.html","excerpt":"CPU调度","text":"CPU调度 基本概念CPU－I/O区间周期进程执行由CPU和I／O等待周期组成。进程在这两个状态之间切换。 CPU调度程序所谓CPU调度程序，其实就是：当CPU空闲时，操作系统如何从就绪队列中选择一个进程来执行的策略。 抢占调度 非抢占调度，一旦CPU分配给一个进程，那么该进程会一直使用CPU直到进程终止或切换到等待状态。 抢占调度，可能一个进程正在运行时，另一个新的进程也到来。而依据调度策略与当前各进程状态，新进程应该先执行。那么，新进程会抢占CPU进行执行，原进程切换到就绪状态。 分派程序分派程序是一个模块，用来将CPU的控制交给由短期调度程序选择的进程。 功能包括： 切换上下文 切换到用户模式 跳转到用户程序的合适位置，以重新启动程序 调度准则 CPU使用率 吞吐量：一个时间单元内所完成进程的数量 周转时间：从进程提交到进程完成的时间段称为周转时间。 等待时间：为在就绪队列中等待所花费时间之和 响应时间：开始响应所需要的时间，响应时间指从进程提交到被运行第一段代码的时间 调度算法1.先到先服务调度（FCFS）非抢占。 补充概念：护航效果：所有其他进程等待一个大进程释放CPU的状态 2.最短作业优先调度（SJF）这一算法将每个进程与其下一个CPU区间段相关联。当CPU为空闲时，它会赋给具有最短CPU区间的进程。 如果两个进程具有同样长度，那可以使用FCFS调度来处理。 SJF调度算法的平均等待时间最小。 SJF的难点就是如何得知下一个CPU区间的长度。书上采用，预测下一个CPU区间为以前CPU区间的测量长度的指数平均。 T（n＋1）＝åt（n）＋（1-å）T（n） 抢占SJF调度：最短剩余时间优先调度 也存在非抢占SJF 3.优先级调度SJF可作为通用优先级调度算法的一个特例 （书上默认）优先级越高，数值越小 即 优先级1比优先级2的优先级要高 优先级调度可以是抢占的或者非抢占的。 主要问题：无穷阻塞或饥饿＝》它可能会导致某个低优先级进程无线等待CPU 解决：使用老化技术，以逐渐增加在系统中等待很长时间的进程的优先级 4.轮转法调度（RR）定义一个较小时间单元，称为时间片。 将就绪队列保存为进程的FIFO队列。新进程增加到就绪队列的尾部。CPU调度程序就从就绪队列中选择第一个进程，设置定时器在一个时间片之后中断，再分排该进程。（1进程在时间片中运行完，进程自动释放CPU，下一个进程开始执行 2.进程未在时间片内执行完，定时器产生中断并产生操作系统中断，然后进行上下文切换，将进程加入到就绪队列的尾部，就绪队列中下一个进程开始执行） 该策略的平均等待时间通常较长 具体效率和时间片大小有关 适合分时（交互系统） 5.多级队列调度将就绪队列分成多个独立队列，每个队列有自己的调度算法，每个队列有自己的优先级 6.多级反馈队列调度在上面的基础上，允许等待时间过长的进程转移到更高优先级的队列","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"进程调度","slug":"进程调度","permalink":"https://blog.mhuig.top/tags/%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6/"}]},{"title":"进程同步之信号量机制","slug":"others/OS/信号量机制","date":"2019-09-24T12:28:47.000Z","updated":"2019-09-24T12:28:47.000Z","comments":true,"path":"posts/2ffcda9a.html","link":"","permalink":"https://blog.mhuig.top/posts/2ffcda9a.html","excerpt":"信号量（semaphore）的数据结构为一个值和一个指针，指针指向等待该信号量的下一个进程。信号量的值与相应资源的使用情况有关。","text":"信号量（semaphore）的数据结构为一个值和一个指针，指针指向等待该信号量的下一个进程。信号量的值与相应资源的使用情况有关。 信号量机制信号量机制即利用pv操作来对信号量进行处理。 什么是信号量？信号量（semaphore）的数据结构为一个值和一个指针，指针指向等待该信号量的下一个进程。信号量的值与相应资源的使用情况有关。 当它的值大于0时，表示当前可用资源的数量； 当它的值小于0时，其绝对值表示等待使用该资源的进程个数。 注意，信号量的值仅能由PV操作来改变。 一般来说，信号量S$\\ge$0时，S表示可用资源的数量。执行一次P操作意味着请求分配一个单位资源，因此S的值减1；当S&lt;0时，表示已经没有可用资源，请求者必须等待别的进程释放该类资源，它才能运行下去。而执行一个V操作意味着释放一个单位资源，因此S的值加1；若S£0，表示有某些进程正在等待该资源，因此要唤醒一个等待状态的进程，使之运行下去。 经典伪代码p操作（wait）：申请一个单位资源，进程进入 1234wait(semaphore *S)&#123; S-&gt;value--; if(S-&gt;value&lt;0) block(S-&gt;list);&#125; v操作（signal）：释放一个单位资源，进程出来 1234signal(semaphore *S)&#123; S-&gt;value++; if(S-&gt;value&lt;=0) wakeup(S-&gt;list);&#125; 综合训练专题","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"进程同步","slug":"进程同步","permalink":"https://blog.mhuig.top/tags/%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5/"}]},{"title":"自建Https证书","slug":"Linux/CentOS/自建https证书","date":"2019-09-23T11:53:44.000Z","updated":"2019-09-23T11:53:44.000Z","comments":true,"path":"posts/5bb73de.html","link":"","permalink":"https://blog.mhuig.top/posts/5bb73de.html","excerpt":"生成nginx的证书与配置chrome安全告警的问题","text":"生成nginx的证书与配置chrome安全告警的问题 安装openssl生成根证书1openssl req -x509 -nodes -days 1461 -newkey rsa:2048 -subj &quot;&#x2F;C&#x3D;CN&#x2F;ST&#x3D;MyProvince&#x2F;L&#x3D;MyCity&#x2F;O&#x3D;MyOrganization&quot; -keyout CA-private.key -out CA-certificate.crt -reqexts v3_req -extensions v3_ca 生成私钥1openssl genrsa -out private.key 2048 1openssl req -new -key private.key -subj &quot;&#x2F;C&#x3D;CN&#x2F;ST&#x3D;MyProvince&#x2F;L&#x3D;MyCity&#x2F;O&#x3D;MyOrganization&#x2F;CN&#x3D;xxx.xxx.xxx.xxx&quot; -sha256 -out private.csr 解决 Chrome 安全警告按照上面的流程，需要注意的是，在默认情况下生成的证书一旦选择信任，在 Edge, Firefox 等浏览器都显示为安全，但是 Chrome 仍然会标记为不安全并警告拦截，这是因为 Chrome 需要证书支持扩展 Subject Alternative Name, 因此生成时需要特别指定 SAN 扩展并添加相关参数。SAN Extension 所需配置文件关键属性： req_distinguished_name: 一节的内容与上面 -subj 一样都是证书的附加信息subjectAltName: 是最关键的属性，取值有两种情况，除前缀外值应与上一步 -subj 中指定的 CN 参数值相同：如果是为某一域名签发证书，则其值可为 DNS:www.example.com 或者使用通配符 DNS:*.example.com；如果为 IP 地址颁发证书，则应该使用 IP:xxx.xxx.xxx.xxx 的形式。 123456789101112131415[ req ]default_bits &#x3D; 2048distinguished_name &#x3D; req_distinguished_namereq_extensions &#x3D; sanextensions &#x3D; san[ req_distinguished_name ]countryName &#x3D; CNstateOrProvinceName &#x3D; MyProvincelocalityName &#x3D; MyCityorganizationName &#x3D; MyOrganization[SAN]authorityKeyIdentifier&#x3D;keyid,issuerbasicConstraints&#x3D;CA:FALSEkeyUsage &#x3D; digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentsubjectAltName &#x3D; IP:xxx.xxx.xxx.xxx 将上述内容放到一个文件中,命名为private.ext执行命令,生成证书1openssl x509 -req -days 1461 -in private.csr -CA CA-certificate.crt -CAkey CA-private.key -CAcreateserial -sha256 -out private.crt -extfile private.ext -extensions SAN nginx中配置如下: 1234567server &#123; listen 443; server_name localhost; ssl on; ssl_certificate /alidata/ssl/private.crt; ssl_certificate_key /alidata/ssl/private.key;&#125; 使用证书生成的具体域名证书和私钥可在 nginx 中使用，然后再客户端所在电脑导入根证书： Windows 需要添加根证书至 受信任的根证书颁发机构macOS 将其导入 钥匙串访问 并选择信任另外 Windows 快捷安装根证书脚本如下(需要管理员权限)： 1certutil -addstore -f -enterprise -user root &quot;.\\CA-certificate.crt&quot; 在window或者mac上安装private.crt文件后，nginx上页面或者接口就可以正常访问了。","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Nginx","slug":"web/nginx","permalink":"https://blog.mhuig.top/categories/web/nginx/"}],"tags":[{"name":"Nginx","slug":"nginx","permalink":"https://blog.mhuig.top/tags/nginx/"},{"name":"https","slug":"https","permalink":"https://blog.mhuig.top/tags/https/"}]},{"title":"Nginx配置","slug":"Linux/Nginx/Nginx配置","date":"2019-09-23T08:46:19.000Z","updated":"2019-09-23T08:46:19.000Z","comments":true,"path":"posts/ad720447.html","link":"","permalink":"https://blog.mhuig.top/posts/ad720447.html","excerpt":"Nginx 基础配置 安全性配置","text":"Nginx 基础配置 安全性配置 Nginx配置error_page 404 500等自定义的错误页面 1.创建自己的404.html页面 2.更改nginx.conf在http定义区域加入： 12345http&#123; ... fastcgi_intercept_errors on; ...&#125; 3.更改nginx.conf(或单独网站配置文件)中在server 区域加入： 12345678server&#123; ... error_page 400 401 402 403 404 405 408 410 412 413 414 415 500 501 502 503 504 506 /404.html; location = /404.html &#123; root /alidata/www/phpwind/error; &#125; ...&#125; 4.更改后重启nginx,测试nginx.conf正确性：1nginx -t 5.502 等错误可以用同样的方法来配置。 12345678server&#123; ... error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /alidata/www/phpwind/error; &#125; ...&#125; Nginx隐藏版本号的安全性与方法隐藏原因：Nginx某些版本有漏洞，暴露出来容易被攻击者利用，隐藏起来更安全 隐藏版本号nginx.conf中去掉下面注释，或者添加这一行 12345http&#123; ... server_tokens off ...&#125; 如果是转发给php－fpm ，需要编辑fastcgi.conf，一般在nginx.conf 同层找到： 1fastcgi_param SERVER_SOFTWARE nginx/$nginx_version; 改为： 1fastcgi_param SERVER_SOFTWARE nginx; 编译源码返回自定义的server修改src/http/ngx_http_header_filter_module.c 中的48行 1static char ngx_http_server_string[] = &quot;Server: nginx&quot; CRLF; 把其中的nginx改为我们自己想要的文字即可，笔者就改为了GFW. 修改src/core/nginx.h 定位到13-14行 123#define nginx_version 2000000#define NGINX_VERSION &quot;2.0&quot;#define NGINX_VER &quot;GFW/&quot; NGINX_VERSION Server返回的就是常量NGINX_VER 重新编译 1make &amp;&amp; make install 控制缓冲区溢出攻击编辑nginx.conf，为所有客户端设置缓冲区的大小限制。 编辑和设置所有客户端缓冲区的大小限制如下： 12345678910http &#123; ...## Start: Size Limits &amp; Buffer Overflows ## client_body_buffer_size 1K; client_header_buffer_size 1k; client_max_body_size 1k; large_client_header_buffers 2 1k;## END: Size Limits &amp; Buffer Overflows ## ...&#125; 解释： 1、client_body_buffer_size 1k-（默认8k或16k）这个指令可以指定连接请求实体的缓冲区大小。如果连接请求超过缓存区指定的值，那么这些请求实体的整体或部分将尝试写入一个临时文件。 2、client_header_buffer_size 1k-指令指定客户端请求头部的缓冲区大小。绝大多数情况下一个请求头不会大于1k，不过如果有来自于wap客户端的较大的cookie它可能会大于1k，Nginx将分配给它一个更大的缓冲区，这个值可以在large_client_header_buffers里面设置。 3、client_max_body_size 1k-指令指定允许客户端连接的最大请求实体大小，它出现在请求头部的Content-Length字段。如果请求大于指定的值，客户端将收到一个”Request Entity Too Large” (413)错误。记住，浏览器并不知道怎样显示这个错误。 4、large_client_header_buffers-指定客户端一些比较大的请求头使用的缓冲区数量和大小。请求字段不能大于一个缓冲区大小，如果客户端发送一个比较大的头，nginx将返回”Request URI too large” (414) 同样，请求的头部最长字段不能大于一个缓冲区，否则服务器将返回”Bad request” (400)。缓冲区只在需求时分开。默认一个缓冲区大小为操作系统中分页文件大小，通常是4k或8k，如果一个连接请求最终将状态转换为keep-alive，它所占用的缓冲区将被释放。你还需要控制超时来提高服务器性能并与客户端断开连接。按照如下编辑： 12345678910http &#123; ...## Start: Timeouts ## client_body_timeout 10; client_header_timeout 10; keepalive_timeout 5 5; send_timeout 10;## End: Timeouts ## ...&#125; 1、client_body_timeout 10;-指令指定读取请求实体的超时时间。这里的超时是指一个请求实体没有进入读取步骤，如果连接超过这个时间而客户端没有任何响应，Nginx将返回一个”Request time out” (408)错误。 2、client_header_timeout 10;-指令指定读取客户端请求头标题的超时时间。这里的超时是指一个请求头没有进入读取步骤，如果连接超过这个时间而客户端没有任何响应，Nginx将返回一个”Request time out” (408)错误。 3、keepalive_timeout 5 5; – 参数的第一个值指定了客户端与服务器长连接的超时时间，超过这个时间，服务器将关闭连接。参数的第二个值（可选）指定了应答头中Keep-Alive: timeout=time的time值，这个值可以使一些浏览器知道什么时候关闭连接，以便服务器不用重复关闭，如果不指定这个参数，nginx不会在应答头中发送Keep-Alive信息。（但这并不是指怎样将一个连接“Keep-Alive”）参数的这两个值可以不相同。 4、send_timeout 10; 指令指定了发送给客户端应答后的超时时间，Timeout是指没有进入完整established状态，只完成了两次握手，如果超过这个时间客户端没有任何响应，nginx将关闭连接。 限制可用的请求方法GET和POST是互联网上最常用的方法。 Web服务器的方法被定义在RFC 2616。如果Web服务器不要求启用所有可用的方法，它们应该被禁用。下面的指令将过滤只允许GET，HEAD和POST方法： 123456789server &#123; ...## Only allow these request methods ## if ($request_method !~ ^(GET|HEAD|POST)$ ) &#123; return 444; &#125;## Do not accept DELETE, SEARCH and other methods ## ...&#125; 更多关于HTTP方法的介绍 GET方法是用来请求，如文件https://www.centos.bz/index.php。HEAD方法是一样的，除非该服务器的GET请求无法返回消息体。POST方法可能涉及到很多东西，如储存或更新数据，或订购产品，或通过提交表单发送电子邮件。这通常是使用服务器端处理，如PHP，Perl和Python等脚本。如果你要上传的文件和在服务器处理数据，你必须使用这个方法。 拒绝一些User-Agents你可以很容易地阻止User-Agents,如扫描器，机器人以及滥用你服务器的垃圾邮件发送者。Nginx的444状态比较特殊，如果返回444那么客户端将不会收到服务端返回的信息，就像是网站无法连接一样 123456789server &#123; ...## Block download agents ## if ($http_user_agent ~* LWP::Simple|BBBike|wget|curl) &#123; return 444; &#125;## ...&#125; 阻止Soso和有道的机器人： 123456789server &#123; ...## Block some robots ## if ($http_user_agent ~* Sosospider|YodaoBot) &#123; return 403; &#125;## ...&#125; Header头设置通过以下设置可有效防止XSS攻击 123add_header X-Frame-Options &quot;SAMEORIGIN&quot;;add_header X-XSS-Protection &quot;1; mode=block&quot;;add_header X-Content-Type-Options &quot;nosniff&quot;; X-Frame-Options： 响应头表示是否允许浏览器加载frame等属性，有三个配置DENY禁止任何网页被嵌入,SAMEORIGIN只允许本网站的嵌套,ALLOW-FROM允许指定地址的嵌套 X-XSS-Protection： 表示启用XSS过滤（禁用过滤为X-XSS-Protection: 0），mode=block表示若检查到XSS攻击则停止渲染页面 X-Content-Type-Options： 响应头用来指定浏览器对未指定或错误指定Content-Type资源真正类型的猜测行为，nosniff 表示不允许任何猜测 在通常的请求响应中，浏览器会根据Content-Type来分辨响应的类型，但当响应类型未指定或错误指定时，浏览会尝试启用MIME-sniffing来猜测资源的响应类型，这是非常危险的 例如一个.jpg的图片文件被恶意嵌入了可执行的js代码，在开启资源类型猜测的情况下，浏览器将执行嵌入的js代码，可能会有意想不到的后果 另外还有几个关于请求头的安全配置需要注意 Content-Security-Policy： 定义页面可以加载哪些资源， 1add_header Content-Security-Policy &quot;default-src &#x27;self&#x27;&quot;; 上边的配置会限制所有的外部资源，都只能从当前域名加载，其中default-src定义针对所有类型资源的默认加载策略，self允许来自相同来源的内容 Strict-Transport-Security： 会告诉浏览器用HTTPS协议代替HTTP来访问目标站点 1add_header Strict-Transport-Security &quot;max-age=31536000; includeSubDomains&quot;; 上边的配置表示当用户第一次访问后，会返回一个包含了Strict-Transport-Security响应头的字段，这个字段会告诉浏览器，在接下来的31536000秒内，当前网站的所有请求都使用https协议访问，参数includeSubDomains是可选的，表示所有子域名也将采用同样的规则 经过多层CDN之后取得原始用户的IP地址，nginx 配置根据用户的真实 IP 做连接限制123456789101112131415161718192021222324252627http &#123; ...##############map $http_x_forwarded_for $clientRealIp &#123; ## 没有通过代理，直接用 remote_addr &quot;&quot; $remote_addr; ## 用正则匹配，从 x_forwarded_for 中取得用户的原始IP ## 例如 X-Forwarded-For: 202.123.123.11, 208.22.22.234, 192.168.2.100,... ## 这里第一个 202.123.123.11 是用户的真实 IP，后面其它都是经过的 CDN 服务器 ~^(?P&lt;firstAddr&gt;[0-9\\.]+),?.*$ $firstAddr;&#125;## 通过 map 指令，我们为 nginx 创建了一个变量 $clientRealIp ，这个就是 原始用户的真实 IP 地址，## 不论用户是直接访问，还是通过一串 CDN 之后的访问，我们都能取得正确的原始IP地址################### 针对原始用户 IP 地址做限制limit_conn_zone $clientRealIp zone=TotalConnLimitZone:20m ;limit_conn TotalConnLimitZone 50;limit_conn_log_level notice;## 针对原始用户 IP 地址做限制limit_req_zone $clientRealIp zone=ConnLimitZone:20m rate=10r/s;limit_req zone=ConnLimitZone burst=10 nodelay;limit_req_log_level notice;###################### ...&#125; nginx日志按天保存1234log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; 1234log_format main &#x27;$remote_addr - $remote_user [$time_iso8601] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; 将原来的time_local修改为time_iso8601，该格式日期为“2017-01-19T09:10:52+08:00”，也可以其他格式，看个人习惯 注意层次关系，这段脚本一定要加到server配置内部，且if要在access_log前面，否则set的变量将无法引用 12345678server&#123;...if ($time_iso8601 ~ &#x27;(\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;)&#x27;) &#123; set $tttt $1; &#125; access_log logs/access-$tttt.log main;...&#125; 按yyyy-mm-dd格式截取字符串，写入指定日志文件中 执行 nginx -s reload 后则配置生效 123456789101112131415http &#123; .... log_format main &#x27;$remote_addr - $remote_user [$time_iso8601] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; server &#123; if ($time_iso8601 ~ &#x27;(\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;)&#x27;) &#123; set $tttt $1; &#125; access_log logs/$tttt.access.log main; .... &#125; ....&#125;","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Nginx","slug":"web/nginx","permalink":"https://blog.mhuig.top/categories/web/nginx/"}],"tags":[{"name":"Nginx","slug":"nginx","permalink":"https://blog.mhuig.top/tags/nginx/"},{"name":"Web安全","slug":"web安全","permalink":"https://blog.mhuig.top/tags/web%E5%AE%89%E5%85%A8/"}]},{"title":"LaTeX数学符号语法速查表","slug":"math/LaTeX数学符号语法速查表","date":"2019-09-22T11:31:47.000Z","updated":"2019-09-22T11:31:47.000Z","comments":true,"path":"posts/96f894a7.html","link":"","permalink":"https://blog.mhuig.top/posts/96f894a7.html","excerpt":"最近写一些报告的时候经常需要使用 LaTeX 语法输入数学公式，每次用的时候都去网上搜资料实在是太麻烦了，所以花了点时间整理了一些常用的 LaTeX 数学符号语法供自己查阅。","text":"最近写一些报告的时候经常需要使用 LaTeX 语法输入数学公式，每次用的时候都去网上搜资料实在是太麻烦了，所以花了点时间整理了一些常用的 LaTeX 数学符号语法供自己查阅。 PS： 本文中所有的符号都是我手动敲进去的，如发现错误请联系我做出修改。更多符号使用可以查看 LaTeX:Symbols 加减乘除 符号 语法 + - \\times \\div 幂运算 符号 语法 a^x a^{xyz} \\sqrt{x} \\sqrt[n]{x} 逻辑运算 符号 语法 \\oplus \\vee \\wedge 关系运算 符号 语法 = \\not= \\approx &gt; &lt; 符号 语法 \\equiv \\le \\ge \\ll \\gg 集合 符号 语法 \\in \\ni \\subset \\supset \\subseteq \\supseteq 存在 符号 语法 \\exists \\forall 希腊字母要输入希腊字母只要用反斜杠 *\\* 加上相应字母的拼写即可。大写字母将对应拼写的首字母大写即可，这里仅列出一部分作为参考。 符号（小写） 语法 \\phi \\omega \\delta \\gamma 符号（大写） 语法 \\Phi \\Omega \\Delta \\Gamma 箭头 符号 语法 \\gets \\to \\Leftarrow \\Rightarrow \\Leftrightarrow 省略号 符号 语法 \\dots \\cdots \\vdots \\ddots 头顶符号 符号 语法 \\hat{x} \\bar{x} \\vec{x} \\dot{x} \\ddot{x} 标准括号 符号 语法 ( ) [ ] 取整括号（函数） 符号 语法 \\lfloor \\rfloor \\lceil \\rceil 空格LaTex 默认会忽略掉空格，要显示空格的话需要自己用命令输入（mu是一个数学单位）。 效果 说明 语法 空格宽度是当前字宽(18mu) \\quad 空格宽度是3mu \\, 空格宽度是4mu \\: 空格宽度是5mu \\; 空格宽度是-3mu(向左缩) \\! 空格宽度是标准空格键效果 在 \\ 后面敲一个空格 空格宽度是36mu \\qquad 上标与下标使用 ^ 和 _ 来表示上下标，使用 {} 来限定上下标的所属关系，下面是一些使用示例。 符号 语法 x^i a_i x^{a_i} x^a_i x^{a^i} x_{i+1} 上划线下划线 符号 语法 \\overline{a+bi} \\underline{xyz} 分式分式有两种尺寸表示，分别用 frac 和 dfrac 关键字表示 尺寸 较小 较小 适中 适中 符号 语法 \\frac{1}{2} \\frac{1+\\frac{1}{x}}{3x + 2} \\dfrac{1}{2} \\dfrac{1+\\frac{1}{x}}{3x + 2} 连续嵌套使用时用：\\cfrac 符号显示 语法 \\cfrac{1+\\cfrac{2}{1+\\cfrac{2}{1+\\cfrac{2}{1}}}}{2} 根式 符号 语法 \\sqrt{x+y} \\sqrt{x} \\sqrt[n]{x} 三角函数直接反斜杠 *\\* 加正常书写的符号即可，这里只列举几个。 符号 语法 \\cos \\sin \\arccos 符号 语法 \\cos^2 x +\\sin^2 x = 1 \\cos 90^\\circ = 0 求和 求积 求极限 符号 语法 \\sum \\prod \\lim 符号 语法 \\sum_{i=1}^{\\infty}\\frac{1}{i} \\prod_{n=1}^5\\frac{n}{n-1} \\lim_{x\\to\\infty}\\frac{1}{x} 求积分 偏导 符号 语法 \\int \\oint \\partial^2y 符号 语法 \\frac{d}{dx}\\left(x^2\\right) = 2x \\int 2x\\ dx = x^2+C \\frac{\\partial^2U}{\\partial x^2} + \\frac{\\partial^2U}{\\partial y^2} 绝对值直接插入竖线 | 即可，可使用 \\left 、 \\right 标签来指定竖线的垂直长度与那对应字符块匹配 12直接插入竖线： |a^x|指定垂直长度相匹配： \\left|a\\right|^\\left|x\\right| 直接插入竖线：指定垂直长度相匹配： 注：所有成对出现的符号均可以像上面那样使用 \\left 、 \\right 标签来指定其大小匹配的字符块。 矩阵和行列式所有的矩阵都是使用 \\begin{matrix} 开始， \\end{matrix} 结束。其中的 matrix 还可以改为 pmatrix 、 bmatrix 、 Bmatrix 、 vmatrix 、 Vmatrix 。 在每一行中使用 &amp; 分隔元素，行末用双反斜杠 *\\* 表示换行。 ##基础格式 对于下面的公式，修改大括号内的关键字分别为 matrix 、 pmatrix 、 bmatrix 、 Bmatrix 、 vmatrix 、 Vmatrix 时对应的情况如下所示。 12345\\begin{matrix}A &amp; B &amp; C\\\\D &amp; E &amp; F\\\\G &amp; H &amp; I\\\\\\end{matrix} matrix pmatrix bmatrix Bmatrix vmatrix Vmatrix 带省略号的矩阵这里使用 bmatrix 做示范，其他的类似。 如上面“省略号”所在小节所示，时使用 \\cdots 表示水平方向省略号， \\vdots 表示竖直方向省略号， \\ddots 表示对角线方向省略号（我这里为了美观把公式按 &amp; 对齐了，这并不是必需的）。 123456\\begin{bmatrix}A &amp; B &amp; \\cdots &amp; C \\\\D &amp; E &amp; \\cdots &amp; F \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\G &amp; H &amp; \\cdots &amp; I \\\\\\end{bmatrix} 矩阵方程（函数）使用 \\begin{equation} 作为整个公式块的开始，以 \\end{equation} 结束。在里面再配合其他符号的语法使用即可。 一个简单的例子如下（这里使用 bmatrix 做示范，其他的类似）。 123456789\\begin{equation}H_x=\\frac{1}{3}\\times{\\begin{bmatrix}A &amp; B &amp; \\cdots &amp; C \\\\D &amp; E &amp; \\cdots &amp; F \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\G &amp; H &amp; \\cdots &amp; I \\\\\\end{bmatrix}}\\end{equation} 其他符号 符号 语法 \\infty \\triangle \\angle \\checkmark \\nabla","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"},{"name":"LaTeX","slug":"math/latex","permalink":"https://blog.mhuig.top/categories/math/latex/"}],"tags":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"},{"name":"LaTeX","slug":"latex","permalink":"https://blog.mhuig.top/tags/latex/"}]},{"title":"Nginx隐藏版本号信息","slug":"Linux/Nginx/Nginx隐藏版本号信息","date":"2019-09-22T09:35:17.000Z","updated":"2019-09-22T09:35:17.000Z","comments":true,"path":"posts/f3b50546.html","link":"","permalink":"https://blog.mhuig.top/posts/f3b50546.html","excerpt":"当我们使用 apt 或者其他包管理工具安装完 Nginx 之后，访问网站时 Header 里面会默认携带 Nginx 的版本号信息。","text":"当我们使用 apt 或者其他包管理工具安装完 Nginx 之后，访问网站时 Header 里面会默认携带 Nginx 的版本号信息。 命令行下可以使用命令查看： 1curl -I http:&#x2F;&#x2F;your-domain 123456HTTP&#x2F;2 200server: nginx&#x2F;1.16.1 #这里带有版本号信息date: Thu, 12 Sep 2019 03:06:23 GMTcontent-type: text&#x2F;html; charset&#x3D;cache-control: publiccontent-language: auto 而软件漏洞往往都是跟版本绑定的，在管理员没有及时更新修复漏洞的情况下，一旦攻击者知道了你用的 Nginx 版本就能轻松利用已知漏洞实现入侵。 这无疑是一个安全隐患，所以对版本号进行隐藏就一定必要了（当然，更新修复漏洞才是解决问题的根本途径）。 解决方法要隐藏 Nginx 版本号其实很简单，稍微修改一下配置文件即可。这里使用 vim 编辑器： 1sudo vim &#x2F;etc&#x2F;nginx&#x2F;nginx.conf 在 http{} 段中添加一行 server_tokens off; 12345http &#123; ...... server_tokens off; ......&#125; 之后保存文件，测试 Nginx 配置文件是否正常后重载配置即可 12sudo nginx -t #测试配置文件是否正常sudo nginx -s reload #重载nginx配置 测试效果配置好之后可以再用 curl 测试一下，会发现不再显示 Nginx 版本号了。 123456HTTP&#x2F;2 200server: nginx #版本号信息没有了date: Thu, 12 Sep 2019 03:32:16 GMTcontent-type: text&#x2F;html; charset&#x3D;cache-control: publiccontent-language: auto","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Nginx","slug":"web/nginx","permalink":"https://blog.mhuig.top/categories/web/nginx/"}],"tags":[{"name":"Nginx","slug":"nginx","permalink":"https://blog.mhuig.top/tags/nginx/"}]},{"title":"Django带文件的表单上传","slug":"web/Django/Django带文件的表单上传","date":"2019-09-22T07:12:52.000Z","updated":"2019-09-22T07:12:52.000Z","comments":true,"path":"posts/4dd55455.html","link":"","permalink":"https://blog.mhuig.top/posts/4dd55455.html","excerpt":"带文件的表单如何上传","text":"带文件的表单如何上传 带文件的表单上传首先在表单form中必须要添加这个属性1enctype=&quot;multipart/form-data&quot; 然后在js中添加下列代码 1234567891011121314151617181920212223242526272829303132333435//选中需要上传的表单，并且进行格式化处理var formData=new FormData($(&quot;#formdata&quot;)[0]);console.log(formData)//获取用户名var user=document.getElementById(&quot;username&quot;).innerHTML;var date=new Date();var month=date.getMonth()+1;//把数据追加到表单formData.append(&quot;username&quot;,user);formData.append(&quot;date&quot;,date.getFullYear()+&quot;-&quot;+month+&quot;-&quot;+date.getDate());formData.append(&quot;sign&quot;,date.getTime());$.ajax(&#123; type:&quot;post&quot;, url:&quot;http://127.0.0.1:8000/tour/sendDay&quot;, async:true, data:formData, timeout:5000, dataType:&quot;json&quot;, cache:false, //提交表单必须增加的属性 contentType:false, processData:false, success:function(data)&#123; alert(data); console.log(data) if(data.code == &quot;1&quot;) &#123; alert(&quot;发布成功&quot;) &#125; if(data.code == &quot;2&quot;)&#123; alert(&quot;发布失败&quot;) &#125; &#125;, error:function(xhr,textState)&#123; alert(&quot;请求失败！&quot;) &#125;&#125;); 在models中添加以下类12345678class Tours(models.Model): username = models.CharField(max_length=20) date = models.CharField(max_length=20) times = models.CharField(max_length=100) desc = models.CharField(max_length=255) photoname = models.FileField(upload_to=&quot;photo&quot;,null=True,blank=True) musicname = models.FileField(upload_to=&quot;music&quot;,null=True,blank=True) isDelete = models.BooleanField(default=False) 在settings.py文件中添加如下代码12MEDIA_URL = &#x27;/media/&#x27;MEDIA_ROOT = os.path.join(BASE_DIR,&#x27;media&#x27;) 在urls.py文件中创建路由12345678910111213from django.views.static import servefrom App import viewsfrom tourdemo import settingsfrom tourdemo.settings import MEDIA_ROOTurlpatterns = [ url(r&#x27;^admin/&#x27;, admin.site.urls), url(r&quot;^tour/sendDay&quot;,views.tourSendDay), #加载media文件需要的路由 url(r&#x27;^media/(?P&lt;path&gt;.*)/$&#x27;, serve, &#123;&quot;document_root&quot;: MEDIA_ROOT&#125;),] 在views.py文件中添加tourSendDay函数1234567891011121314151617181920212223242526272829303132def tourSendDay(request): try: tour = Tours() username = request.POST.get(&quot;username&quot;) tour.username = username date = request.POST.get(&quot;date&quot;) tour.date = date times = request.POST.get(&quot;sign&quot;) tour.times = times music = request.FILES.get(&quot;music&quot;) tour.musicname = music img = request.FILES.get(&quot;photo&quot;) tour.photoname = img desc = request.POST.get(&quot;desc&quot;) # print(desc) tour.desc = desc # print(&quot;desc&quot;,tour.desc) tour.save() tourdic = &#123;&quot;code&quot;:&quot;1&quot;,&quot;id&quot;: tour.id, &quot;photoname&quot;: tour.photoname.url, &quot;musicname&quot;: tour.musicname.url, &quot;times&quot;: tour.times, &quot;username&quot;: tour.username, &quot;date&quot;: tour.date, &quot;desc&quot;: tour.desc&#125; response = HttpResponse(json.dumps(tourdic)) except Exception as e: print(e) response = HttpResponse(json.dumps(&#123;&quot;code&quot;: &quot;2&quot;&#125;)) response[&quot;Access-Control-Allow-Origin&quot;] = &quot;*&quot; response[&quot;Access-Control-Allow-Methods&quot;] = &quot;POST, GET, OPTIONS&quot; response[&quot;Access-Control-Max-Age&quot;] = &quot;1000&quot; response[&quot;Access-Control-Allow-Headers&quot;] = &quot;*&quot; return response 注意若出现存储中文失败则需要在创建的的时候指定编码格式1create database tourdb charset&#x3D;&#39;utf8&#39;;","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"}]},{"title":"Django将发布内容动态显示到页面上","slug":"web/Django/Django将发布内容动态显示到页面上","date":"2019-09-22T07:11:42.000Z","updated":"2019-09-22T07:11:42.000Z","comments":true,"path":"posts/5bfe6261.html","link":"","permalink":"https://blog.mhuig.top/posts/5bfe6261.html","excerpt":"Django如何将发布内容动态显示到页面上","text":"Django如何将发布内容动态显示到页面上 将发布内容动态显示到页面上在settings.py中配置12345678910111213141516171819202122TEMPLATES = [ &#123; &#x27;BACKEND&#x27;: &#x27;django.template.backends.django.DjangoTemplates&#x27;, &#x27;DIRS&#x27;: [os.path.join(BASE_DIR,&quot;Templates&quot;)], &#x27;APP_DIRS&#x27;: True, &#x27;OPTIONS&#x27;: &#123; &#x27;context_processors&#x27;: [ &#x27;django.template.context_processors.debug&#x27;, &#x27;django.template.context_processors.request&#x27;, &#x27;django.contrib.auth.context_processors.auth&#x27;, &#x27;django.contrib.messages.context_processors.messages&#x27;, &#x27;django.template.context_processors.media&#x27;, # 新添加的 ], &#125;, &#125;,]#已经配置过的MEDIA_URL = &#x27;/media/&#x27;MEDIA_ROOT = os.path.join(BASE_DIR,&#x27;media&#x27;) 在urls.py中配置路由123456789from tour.settings import MEDIA_ROOTfrom django.views.static import serve #注意包不能导错urlpatterns = [ url(r&#x27;^admin/&#x27;, admin.site.urls), url(r&#x27;^media/(?P&lt;path&gt;.*)/$&#x27;, serve, &#123;&quot;document_root&quot;: MEDIA_ROOT&#125;),#加载media文件的时候需要的路由 url(r&quot;^gettour&quot;,views.gettour),#获取tour.html页面的路由 url(r&quot;^sendtour&quot;,views.sendTour),#发布动态的路由] 在views.py文件中创建sendTour的函数12345678910111213141516171819202122232425def sendTour(request): try: #从请求中将表单中的数据取出，并且存储到数据库中 tour = Tour() tour.username= request.POST.get(&quot;username&quot;) tour.times = request.POST.get(&quot;times&quot;) tour.sendtime = request.POST.get(&quot;sendtime&quot;) tour.sendtxt = request.POST.get(&quot;sendtxt&quot;) tour.phonename = request.FILES.get(&quot;imgfile&quot;) tour.musicname = request.FILES.get(&quot;musicfile&quot;) tour.save() #若存储成功，则将对象转为字典 tourdict = &#123;&quot;code&quot;:&quot;1&quot;,&quot;username&quot;:tour.username,&quot;times&quot;:tour.times, &quot;sendtime&quot;:tour.sendtime,&quot;phonename&quot;:tour.phonename.url ,&quot;musicname&quot;:tour.musicname.url,&quot;sendtxt&quot;:tour.sendtxt&#125; # 再字典转为json字符串，返回到前端页面中 response = HttpResponse(json.dumps(tourdict)) except Exception as e: print(e) response = HttpResponse(json.dumps(&#123;&#x27;code&#x27;: &quot;0&quot;&#125;)) response[&quot;Access-Control-Allow-Origin&quot;] = &quot;*&quot; response[&quot;Access-Control-Allow-Methods&quot;] = &quot;POST, GET, OPTIONS&quot; response[&quot;Access-Control-Max-Age&quot;] = &quot;1000&quot; response[&quot;Access-Control-Allow-Headers&quot;] = &quot;*&quot; return response 在前端页面中添加函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869function sendData()&#123;//选中dataform表单对其进行格式化处理var dataform = new FormData($(&quot;#datafrom&quot;)[0])console.log(dataform)var user = document.getElementById(&quot;user&quot;).innerHTML; //将数据追加到表单中 dataform.append(&quot;username&quot;,user)var dates = new Date()var y = dates.getFullYear()var m = dates.getMonth() + 1var d = dates.getDate()dataform.append(&quot;sendtime&quot;,y+&quot;/&quot;+m+&quot;/&quot;+d)//追加一个时间戳dataform.append(&quot;times&quot;,dates.getTime())$.ajax(&#123; type:&quot;post&quot;, url:&quot;http://127.0.0.1:9000/sendtour&quot;, data:dataform, async:true, dataType:&quot;json&quot;, timeout:5000, cache:false, //提交表单的时候需要的参数 contentType:false, processData:false, success:function(data)&#123;//请求成功的时候返回的参数 alert(data) if(data.code==&quot;1&quot;)&#123;//判断数据是否成功存入数据库 alert(&quot;发布成功&quot;) // 将我们返回的数据显示到页面上来 addhistory(data) &#125; if(data.code==&quot;0&quot;)&#123;//数据没存储成功，则显示发布失败！ alert(&quot;发布失败&quot;) &#125; &#125;, error:function()&#123; alert(&quot;请求异常&quot;) &#125;&#125;)&#125;//将得到的数据动态的添加到html页面中function addhistory(data) &#123; //打印data数据console.log(data) //找到存放li的大盒子var $ul = document.getElementById(&quot;ulitem&quot;); //创建一个livar $li = document.createElement(&quot;li&quot;)//将li添加到ul中$ul.appendChild($li) //给li添加一个class属性$li.className = &quot;item&quot;; //给li添加标签$li.innerHTML = &#x27;&lt;img class=&quot;item-img&quot; src=&quot;&#x27;+data.phonename+&#x27;&quot;/&gt;&#x27; + &#x27;&lt;div class=&quot;item-right&quot;&gt;&#x27; + &#x27;&lt;a class=&quot;delete&quot; href=&quot;#&quot;&gt;删除&lt;/a&gt;&#x27; + &#x27;&lt;p class=&quot;itemtxt&quot;&gt;&#x27;+data.sendtxt+&#x27;&lt;/p&gt;&#x27;+ &#x27;&lt;div class=&quot;userbox&quot;&gt;&lt;img class=&quot;icon-img&quot; src=&quot;/static/img/a1.png&quot;/&gt; &#x27; + &#x27;&lt;span class=&quot;username&quot;&gt;&#x27;+data.username+&#x27;&lt;/span&gt;&lt;span class=&quot;sendtime&quot;&gt;&#x27;+data.sendtime +&#x27;&lt;/span&gt;&lt;/div&gt;&#x27; + &#x27;&lt;div&gt;&lt;a class=&quot;musicname&quot; href=&quot;#&quot;&gt;&#x27;+data.musicname+&#x27;&lt;/a&gt;&lt;/div&gt;&#x27; + &#x27;&lt;/div&gt;&#x27;&#125;","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"}]},{"title":"Django中引用静态文件","slug":"web/Django/Django中引用静态文件","date":"2019-09-22T07:09:59.000Z","updated":"2019-09-22T07:09:59.000Z","comments":true,"path":"posts/1f519fd3.html","link":"","permalink":"https://blog.mhuig.top/posts/1f519fd3.html","excerpt":"Django中如何引用静态文件","text":"Django中如何引用静态文件 Django中引用静态文件 当我们将我们的html文件放到Templates文件中的时候，这时候此html我们可以直接引用， 若出现这个html文件，它还引用了其他的一些文件【js，css，img】，这是就需要引用django中静态的文件 需要在Templates的同级目录下创建一个static目录需要在setting文件中添加代码123456#默认自带的STATIC_URL = &#x27;/static/&#x27;#添加代码STATICFILES_DIRS = ( os.path.join(BASE_DIR, &#x27;static&#x27;),) 将html需要用到的资源，放在static目录下 在html中引用静态资源 1234567&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=&quot;/static/js/jquery-2.1.0.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/static/css/style.css&quot;/&gt;&lt;/head&gt; 配置路由，在urls.py文件中配置1234urlpatterns = [ url(r&#x27;^admin/&#x27;, admin.site.urls), url(r&quot;^getlogin&quot;,views.getlogin)] 需要在views.py文件中创建getlogin函数1234def getlogin(request): #返回登录的页面 return render(request,&quot;login.html&quot;) 启动服务1python manage.py runserver 127.0.0.1:9000 如何请求接口1http:&#x2F;&#x2F;127.0.0.1:9000&#x2F;getlogin","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"}]},{"title":"Django添加新的路由","slug":"web/Django/Django添加新的路由","date":"2019-09-22T07:09:06.000Z","updated":"2019-09-22T07:09:06.000Z","comments":true,"path":"posts/a79b4f5b.html","link":"","permalink":"https://blog.mhuig.top/posts/a79b4f5b.html","excerpt":"Django如何添加新的路由","text":"Django如何添加新的路由 Django添加新的路由首先urls.py文件添加路由1234567urlpatterns = [ url(r&#x27;^admin/&#x27;, admin.site.urls), # alt+enter url(r&quot;^login/&quot;,views.login), url(r&quot;^app/addStu/&quot;,views.addStu), url(r&quot;^register&quot;,views.register)] 在views.py中创建函数register12345678910111213def register(request): print(&quot;register&quot;) user = request.POST.get(&quot;user&quot;) print(user) psd = request.POST.get(&quot;psd&quot;) print(psd) #解决跨域问题 response = HttpResponse(user) response[&quot;Access-Control-Allow-Origin&quot;] = &quot;*&quot; response[&quot;Access-Control-Allow-Methods&quot;] = &quot;POST, GET, OPTIONS&quot; response[&quot;Access-Control-Max-Age&quot;] = &quot;1000&quot; response[&quot;Access-Control-Allow-Headers&quot;] = &quot;*&quot; return response 由于ajax跨域的问题，也需要在settings.py文件中设置，将选中的模块注释12345678910MIDDLEWARE = [ &#x27;django.middleware.security.SecurityMiddleware&#x27;, &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;, &#x27;django.middleware.common.CommonMiddleware&#x27;, # &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;, &#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;, &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;, &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;,] 我们的ajax请求在前端中使用的，在使用ajax之前，我们需要将jquery链接到我们的项目中12&lt;script src=&quot;js/jquery-2.1.0.js&quot; type=&quot;text/javascript&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;src:jquery的链接地址 调用ajax请求12345678910111213$.ajax(&#123; type:&quot;post&quot;,//请求的类型 url:&quot;http://127.0.0.1:8000/register/&quot;,//请求的地址[路由] async:true, //声明异步请求 data:&#123;&quot;user&quot;:user,&quot;psd&quot;:psd&#125;,//将参数传递到后台 dataType:&quot;text&quot;, //声明返回的数据的类型，json success:function (data) &#123; //请求成功的时候调用的函数，data：后台返回给我们的数据 alert(data) &#125;, error:function () &#123; //请求的失败的时候，打印 alert(&quot;请求失败&quot;) &#125; &#125;)","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"}]},{"title":"Mysql与pycharm的设置","slug":"web/Django/mysql与pycharm的设置","date":"2019-09-22T07:07:16.000Z","updated":"2019-09-22T07:07:16.000Z","comments":true,"path":"posts/30b27542.html","link":"","permalink":"https://blog.mhuig.top/posts/30b27542.html","excerpt":"mysql与pycharm的设置","text":"mysql与pycharm的设置 mysql与pycharm的设置 保证mysql已经安装成功 使用终端在mysql中创建一个数据库 ```pythonmysql -u root -p #连接数据库mysql&gt; show databases;#查看当前数据库mysql&gt; create database tour;#创建数据库 tour：数据库名，可以自己命名 1234567891011121314151617- 找到setting.py文件，并在添加如下代码&#96;&#96;&#96;pythonDATABASES &#x3D; &#123; &#39;default&#39;: &#123; # &#39;ENGINE&#39;: &#39;django.db.backends.sqlite3&#39;, # &#39;NAME&#39;: os.path.join(BASE_DIR, &#39;db.sqlite3&#39;), &#39;ENGINE&#39;: &#39;django.db.backends.mysql&#39;, &#39;NAME&#39;: &#39;tour&#39;, #数据库名字 &quot;USER&quot;:&quot;root&quot;, #数据库用户名 &quot;PASSWORD&quot;:&quot;root&quot;, #数据库的密码 &quot;HOST&quot;:&quot;127.0.0.1&quot;, #ip地址 &quot;PORT&quot;:&quot;3306&quot;,# 端口号 &#125;&#125; 注意：在使用数据库的时候，必须保证数据库的服务是开启的状态 net start mysql 找到mysql的安装地址，找到bin文件夹，到bin文件夹下面找到mysqld.exe，双击执行 到与setting.py同目录的__init__.py文件下，添加以下代码 ```pythonimport pymysql pymysql.install_as_MySQLdb() 1234567**注意** 若没有安装pymysql模块，则会报错，需要将pymsql模块安装&#96;&#96;&#96;python1.使用pycharm安装2.使用pip安装pip install pymsql 当项目创建之后，配置完成之后，我们执行一下迁移【因为只有执行迁移的时候，才会在数据库中生成表】 ```pythonpython manage.py migrate 123456789- 需要在models.py文件中创建一个类，并且这个类必须要继承models.Model- &#96;&#96;&#96;python class User(models.Model): username &#x3D; models.CharField(max_length&#x3D;20) password &#x3D; models.CharField(max_length&#x3D;20) #CharField 指定字段的类型 #max_length 指定字段的最大长度 生成迁移文件 ```pyhthonpython manage.py makemigrations 12345- 执行迁移文件- &#96;&#96;&#96;python python manage.py migrate","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.mhuig.top/tags/mysql/"}]},{"title":"Django的环境配置","slug":"web/Django/Django的环境配置","date":"2019-09-22T07:06:15.000Z","updated":"2019-09-22T07:06:15.000Z","comments":true,"path":"posts/42ecd398.html","link":"","permalink":"https://blog.mhuig.top/posts/42ecd398.html","excerpt":"Django环境配置","text":"Django环境配置 Django的环境配置python环境是ok的pip是可用的 pip用来安装第三方包的 创建虚拟环境【可以先不写】 linux/mac windows 安装Django 1pip install django==1.11.7 django安装成功之后，创建项目 创建项目之前首先新建一个目录【文件夹】 进入这个目录之后执行 12django-admin startproject projectname#django-admin startproject 项目名 使用pycharm打开项目的时候，要在manage.py的上一级打开 manage所在的文件夹 当进入pychram之后，我们可以使用自带终端来创建app1python manage.py startapp appname 当app创建完成之后，需要在setting.py文件中配置123456789INSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;, &#x27;django.contrib.auth&#x27;, &#x27;django.contrib.contenttypes&#x27;, &#x27;django.contrib.sessions&#x27;, &#x27;django.contrib.messages&#x27;, &#x27;django.contrib.staticfiles&#x27;, &quot;App&quot;,#添加我们创建的app] 在setting.py文件中 ```pythonALLOWED_HOSTS = [“*”]#允许所有人访问12345678### 在setting.py文件中 &#96;&#96;&#96;python #设置语言 LANGUAGE_CODE &#x3D; &#39;zh-hans&#39; #设置时区 TIME_ZONE &#x3D; &#39;Asia&#x2F;Shanghai&#39; 运行当前项目1python manage.py runserver 运行成功，在浏览器访问12http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;#会显示正常工作 添加一个路由，在urls.py文件中添加12345urlpatterns = [ url(r&#x27;^admin/&#x27;, admin.site.urls), # alt+enter 添加,需要导包，App下面的views url(r&quot;^login/&quot;,views.login),] 需要在app中的views.py去创建视图函数login 123def login(request): #必须返回的是httpResponse对象 return HttpResponse(&quot;你真是一个小机灵鬼！！！&quot;) 执行python manage.py runserver 将服务器重新部署 在浏览器访问的时候，这时候需要使用 1http:&#x2F;&#x2F;127.0.0.1:8000&#x2F;login","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"}]},{"title":"Mysql的使用","slug":"web/Django/mysql的使用","date":"2019-09-22T07:04:38.000Z","updated":"2019-09-22T07:04:38.000Z","comments":true,"path":"posts/606a512e.html","link":"","permalink":"https://blog.mhuig.top/posts/606a512e.html","excerpt":"mysql的简单配置使用","text":"mysql的简单配置使用 mysql的使用 免安装版本 解压mysql压缩包【记得解压的文件路径】 进行环境变量的配置 我的电脑–》属性—》高级环境变量设置–》找到path –》新建–》将mysql的路径【bin的路径】直接复制粘贴 启动数据库 先进入mysql的解压文件–》找到bin文件夹–》双击执行mysqld.exe文件 连接数据库 mysql -u root -p root【默认密码】 数据库连接成功之后，可以查看数据库 show databases; //查看数据库 use 数据库名 ; //使用某个指定的数据库 show tables; //查看所有的表 create database 数据库名; //创建数据库 drop database 数据库名; //删除数据库","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"}],"tags":[{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.mhuig.top/tags/mysql/"}]},{"title":"Frp内网穿透","slug":"Linux/CentOS/Frp内网穿透","date":"2019-09-22T06:30:50.000Z","updated":"2019-09-22T06:30:50.000Z","comments":true,"path":"posts/48a2d97d.html","link":"","permalink":"https://blog.mhuig.top/posts/48a2d97d.html","excerpt":"frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp 协议，为 http 和 https 应用协议提供了额外的能力，且尝试性支持了点对点穿透。","text":"frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp 协议，为 http 和 https 应用协议提供了额外的能力，且尝试性支持了点对点穿透。 github 启动 frps 123cd &#x2F;Main&#x2F;frp_024.1_server&#x2F;chmod -Rf 777 .&#x2F;*.&#x2F;frps -c frps.ini 相关配置 12345678# frps.ini[common]bind_port &#x3D; 7000token &#x3D; yourtokendashboard_port &#x3D; 7500dashboard_user &#x3D; usernamedashboard_pwd &#x3D; yourpasswordvhost_http_port &#x3D; 9000 #设置 http 访问端口 1234567891011121314# frpc.ini[common]server_addr &#x3D; x.x.x.x #假设 frps 所在服务器的公网 IP 为 x.x.x.xserver_port &#x3D; 7000 #与frps.ini bind_port一致token &#x3D; yourtoken #与frps.ini token一致#[ssh]#type &#x3D; tcp#local_ip &#x3D; 127.0.0.1#local_port &#x3D; 22#remote_port &#x3D; 8080[web]type &#x3D; httplocal_port &#x3D; 8000 #本地机器上 web 服务对应的端口custom_domains &#x3D; www.yourdomain.com #绑定自定义域名或serverip 关闭防火墙 12systemctl stop firewalld.servicesystemctl disable firewalld.service 设置开机自启动 1vim &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;frps.service 1234567891011121314151617181920212223#frps.service[Unit]Description&#x3D;fraps serviceAfter&#x3D;network.target syslog.targetWants&#x3D;network.target[Service]Type&#x3D;simple#启动服务的命令（此处写你的frps的实际安装目录）ExecStart&#x3D;&#x2F;Main&#x2F;frp_024.1_server&#x2F;frps -c &#x2F;Main&#x2F;frp_024.1_server&#x2F;frps.ini[Install]WantedBy&#x3D;multi-user.target 然后就启动frps 1sudo systemctl start frps 再打开自启动 1sudo systemctl enable frps 如果要重启应用，可以这样 1sudo systemctl restart frps 如果要停止应用，可以输入 1sudo systemctl stop frps 如果要查看应用的日志，可以输入 1sudo systemctl status frps","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"内网穿透","slug":"web/内网穿透","permalink":"https://blog.mhuig.top/categories/web/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"}],"tags":[{"name":"内网穿透","slug":"内网穿透","permalink":"https://blog.mhuig.top/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"Frp","slug":"frp","permalink":"https://blog.mhuig.top/tags/frp/"}]},{"title":"CentOS7设置Jupyter","slug":"Python/CentOS7设置Jupyter","date":"2019-09-22T06:05:46.000Z","updated":"2019-09-22T06:05:46.000Z","comments":true,"path":"posts/c2c98f74.html","link":"","permalink":"https://blog.mhuig.top/posts/c2c98f74.html","excerpt":"Jupyter Notebook（前身是IPython Notebook）是一个基于Web的交互式计算环境，用于创建Jupyter Notebook文档。Notebook一词可以通俗地引用许多不同的实体，主要是Jupyter Web应用程序、Jupyter Python Web服务器或Jupyter文档格式（取决于上下文）。Jupyter Notebook文档是一个JSON文档，遵循版本化模式，包含一个有序的输入/输出单元格列表，这些单元格可以包含代码、文本（使用Markdown语言）、数学、图表和富媒体，通常以“.ipynb”结尾扩展。","text":"Jupyter Notebook（前身是IPython Notebook）是一个基于Web的交互式计算环境，用于创建Jupyter Notebook文档。Notebook一词可以通俗地引用许多不同的实体，主要是Jupyter Web应用程序、Jupyter Python Web服务器或Jupyter文档格式（取决于上下文）。Jupyter Notebook文档是一个JSON文档，遵循版本化模式，包含一个有序的输入/输出单元格列表，这些单元格可以包含代码、文本（使用Markdown语言）、数学、图表和富媒体，通常以“.ipynb”结尾扩展。 启动1jupyter notebook --allow-root --ip 0.0.0.0 --port 9999 默认不允许/不建议root启动jupyter,如果非要用，加上–allow-root–ip ip填写0.0.0.0 或者本机ip–port 端口号 启动后，浏览器访问对应ip和端口就行，需要输入token,token在启动界面有输出 生产配置文件每次记住token，复制再登录不现实 1jupyter notebook --generate-config 生成的配置文件位于 1~&#x2F;.jupyter&#x2F;jupyter_notebook_config.py 1jupyter-notebook password 输入两遍密码 启动，就可以 以固定密码登录了 1jupyter notebook --allow-root --ip 0.0.0.0 --port 999 设置浏览器打开jupyter默认路径1vim ~&#x2F;.jupyter&#x2F;jupyter_notebook_config.py 填写自己想要的服务器路径1c.NotebookApp.notebook_dir&#x3D;&#39;&#x2F;&#39; 设置jupyter开机启动systemctl脚本目录：/usr/lib/systemd/系统服务目录：/usr/lib/systemd/system/用户服务目录：/usr/lib/systemd/system/ 1cd &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F; 1vim myjupyter.service 12345678910111213141516171819202122232425262728293031[UNIT]#服务描述Description&#x3D;python jupyter Service#指定了在systemd在执行完那些target之后再启动该服务After&#x3D;network.target[Service]#定义Service的运行类型，一般是forking(后台运行) #Type&#x3D;forking 这个会卡住啊,不写Type 或者 如下Type&#x3D;simple#定义systemctl start|stop|reload *.service 的执行方法（具体命令需要写绝对路径）#注：ExecStartPre为启动前执行的命令# ExecStartPre&#x3D;&#x2F;usr&#x2F;bin&#x2F;test &quot;x$&#123;NETWORKMANAGER&#125;&quot; &#x3D; xyesExecStart&#x3D;&#x2F;root&#x2F;anaconda3&#x2F;bin&#x2F;jupyter notebook --allow-root --ip 0.0.0.0 --port 9999#ExecReload&#x3D;# ExecStop&#x3D;&#x2F;home&#x2F;mobileoa&#x2F;apps&#x2F;shMediaManager.sh -stop#创建私有的内存临时空间PrivateTmp&#x3D;True[Install]#多用户WantedBy&#x3D;multi-user.target 1vi &#x2F;root&#x2F;.jupyter&#x2F;jupyter_notebook_config.py 12345678# Set ip to &#39;*&#39; to bind on all interfaces (ips) for the public serverc.NotebookApp.ip &#x3D; &#39;*&#39;# It is a good idea to set a known, fixed port for server accessyc.NotebookApp.port &#x3D; 9999# 是否打开浏览器c.NotebookApp.open_browser &#x3D; False#设置工作路径c.NotebookApp.notebook_dir &#x3D; &#39;&#x2F;&#39; 重载系统服务1systemctl daemon-reload 设置开机启动1systemctl enable myjupyter.service 启动服务1systemctl start myjupyter.service 停止服务1systemctl stop myjupyter.service 重启服务1systemctl restart myjupyter.service Notebook支持虚拟运行环境为了让Jupyter Notebook支持虚拟运行环境，需要在Anaconda里安装一个插件。回到终端下面，用C-c退出目前正在运行的Jupyter Notebook Server，然后执行： 1conda install nb_conda 再重新开启 1Jupyter Notebook 或者(better) 安装 ipykernel 首先切换到想要在 jupyter notebook 里使用的虚拟环境： 1conda activate 环境名称 安装 ipykernel： 1conda install ipykernel 写入 jupyter 的 kernel 在当前虚拟环境里执行： 1python -m ipykernel install --user --name 环境名称 --display-name &quot;Python (环境名称)&quot; “环境名称”为当前虚拟环境的名称，最后面引号内的字符串是该虚拟环境显示在 jupyter notebook 界面的名字，可以随意修改。 删除 kernel 环境 上面写入 kernel 的配置并不会随虚拟环境的删除而删除。也就是说即使删除了该虚拟环境，jupyter notebook 的界面上仍会有它的选项，只是无法正常使用。 此时就需要去手动删除 kernel 环境了： 1jupyter kernelspec remove 环境名称 jupyter中用notedown插件来读取md文档1pip install https:&#x2F;&#x2F;github.com&#x2F;mli&#x2F;notedown&#x2F;tarball&#x2F;master 1vi &#x2F;root&#x2F;.jupyter&#x2F;jupyter_notebook_config.py 1c.NotebookApp.contents_manager_class &#x3D; &#39;notedown.NotedownContentsManager&#39; Jupyter Notebook 自定义主题安装好了Jupyter Notebook和Python之后，我们就已经搭建好啦运行和笔记环境，可以愉快的开始学习了。 于是本着爱折腾的精神和护眼的需求，我搜索了Jupyter Notebook的themes，也就是自定义主题。果然Github上有人一早解决了这个问题。 github 12345# install jupyterthemespip install jupyterthemes# upgrade to latest versionpip install --upgrade jupyterthemes 这时候，你就可以在terminal里面调用已经安装好的themes啦～ 例如，在terminal中输入 1jt -l 就会返回所有你安装好的主题的名词列表，这样你就知道了你安装了哪些主题。最终，我的选择是 1jt -t chesterish -T -N 表示我选择了chesterish这个主题，同时希望打开顶部的工具栏（Toolbar），显示笔记本的名字（Name） Jupyter 扩展配置器（ Jupyter NbExtensions Configurator）可以通过 coda 安装： 12conda install -c conda-forge jupyter_contrib_nbextensionsconda install -c conda-forge jupyter_nbextensions_configurator 也可以使用 pip 安装 1234pip install jupyter_nbextensions_configuratorjupyter_contrib_nbextensionsjupyter contrib nbextension install --userjupyter nbextensions_configurator enable --user 1.标题折叠 Collapsible headings 当你在处理一个大型的 notebooks 时，这项扩展非常有用，它可以让你隐藏部分内容。 通知 Notify当你长时间运行一个任务程序的时候，程序运行结束后，此扩展功能会自动提醒你。 如需使用此扩展，你需要勾选其对应得选择框，并点击 Notify 按钮来选择一个最短通知时间，即 notebook 最少持续运行多久后进行提醒。（需要注意的是，这个扩展只有在 notebook 被浏览器正常打开的情况下才能正常工作。） 代码折叠 Code folding 进度条 tqdm_notebook tqdm 本质上不是一个 notebook 的扩展，它是 Python 中的一个进度条库。 但是此库有时在 jupyter notebooks 会无法正常工作。 Randy Olson 给出一个小小的提醒： tqdm 是一个 Python 的进度条库，在 jupyter notebook 中则被称之为 “tqdm_notebook”。自从在 nootbook 中加入了 tqdm_notebook 扩展功能，你再也不用担心其引发的混乱问题了。 （Randy Olson 2018 年 3 月 2 日） %debug这个本质上也不是 notebook 的一个扩展，而是 IPython 中的一个魔法命令。为了加深你的理解，建议你读一读 Radek Osmulski 的发布 twitter 上的推文。 %debug 魔法命令 得到了一个异常 重新插入一个新的输入框，输入 %debug，然后运行它交互式的调试方法可以打开并显示代码出现异常的语句，方便你联系前后程序查看具体情况。(Radek 2017年12月26日) 其他小的拓展与技巧 %Ismagic ：在输入框中运行这个命令，列出所有可用的 IPython 魔法命令 zen mode 扩展： 隐藏菜单栏，让你更专注于代码 Execute time 扩展：显示程序块运行的时间 autoreload：在不重启 notebook 的情况下，自动载入外部文件，从而修改代码，具体操作如下： %load_ext autoreload %autoreload 2 JUPYTER 服务的 NGINX 配置jupyter 配置配置文件在 1&#x2F;home&#x2F;&#123;user&#125;&#x2F;.jupyter&#x2F;jupyter_notebook_config.py 配置 jupyter 的路径 1c.NotebookApp.base_url &#x3D; &#39;&#x2F;jupyter&#x2F;&#39; nginx 配置jupyter 使用了 websocket 协议，所以需要配置支持 websocket。 123456789101112131415location &#x2F;jupyter&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;jupyter; proxy_set_header Host $host; proxy_set_header X-Real-Scheme $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # WebSocket support proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_read_timeout 120s; proxy_next_upstream error;&#125; jupyter notebook 用到了 websocket, 所以需要配置 12proxy_set_header Upgrade $http_upgrade;proxy_set_header Connection &quot;upgrade&quot;;","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"CentOS7","slug":"linux/centos7","permalink":"https://blog.mhuig.top/categories/linux/centos7/"},{"name":"Python","slug":"linux/centos7/python","permalink":"https://blog.mhuig.top/categories/linux/centos7/python/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/tags/centos7/"},{"name":"Jupyter","slug":"jupyter","permalink":"https://blog.mhuig.top/tags/jupyter/"}]},{"title":"Centos7安装Anaconda3","slug":"Python/Centos7安装Anaconda3","date":"2019-09-22T03:14:48.000Z","updated":"2019-09-22T03:14:48.000Z","comments":true,"path":"posts/6009a9d8.html","link":"","permalink":"https://blog.mhuig.top/posts/6009a9d8.html","excerpt":"Anaconda是一个免费开源的Python和R语言的发行版本，用于计算科学（数据科学、机器学习、大数据处理和预测分析），Anaconda致力于简化包管理和部署。","text":"Anaconda是一个免费开源的Python和R语言的发行版本，用于计算科学（数据科学、机器学习、大数据处理和预测分析），Anaconda致力于简化包管理和部署。 安装下载Anaconda方式一：官方网站 方式二：清华大学开源软件镜像站 可以下载到本地，然后通过xftp上传到Contos上 1bash Anaconda3-4.4.0-Linux-x86_64.sh 该按enter按，该yes|no的yes。 1source ~&#x2F;.bashrc 然后重启终端，然后输入python Anaconda虚拟环境创建环境1conda create -n envname python&#x3D;3.6 删除环境1conda remove -n envname --all 激活环境1source activate envname 退出环境1source deactivate Anaconda 换源添加清华源12345conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;conda-forgeconda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;msys2&#x2F;conda config --set show_channel_urls yes 删源1conda config --remove-key channels 附录清华大学开源软件镜像站 12345channels: - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F; - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F; - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;conda-forge&#x2F;ssl_verify: true 上海交通大学开源镜像站 12345channels: - https:&#x2F;&#x2F;mirrors.sjtug.sjtu.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F; - https:&#x2F;&#x2F;mirrors.sjtug.sjtu.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F; - https:&#x2F;&#x2F;mirrors.sjtug.sjtu.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;conda-forge&#x2F;ssl_verify: true 中国科学技术大学 USTC Mirror 12345channels: - https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F; - https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F; - https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;conda-forge&#x2F;ssl_verify: true .bashrc里面修改过PATH环境变量，添加过anaconda/bin 1vi ~&#x2F;.bashrc 最后添加 1conda deactivate 1source .bashrc","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"CentOS7","slug":"linux/centos7","permalink":"https://blog.mhuig.top/categories/linux/centos7/"},{"name":"Python","slug":"linux/centos7/python","permalink":"https://blog.mhuig.top/categories/linux/centos7/python/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/tags/centos7/"},{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"Anaconda","slug":"anaconda","permalink":"https://blog.mhuig.top/tags/anaconda/"}]},{"title":"CentOS7安装Python3","slug":"Python/CentOS7安装Python3","date":"2019-09-22T02:57:06.000Z","updated":"2019-09-22T02:57:06.000Z","comments":true,"path":"posts/eeb47c66.html","link":"","permalink":"https://blog.mhuig.top/posts/eeb47c66.html","excerpt":"centos7 自带有 python，但是却是 python2 版本的 python，如果你想安装个python3怎么办呢？难道要从github上把源码clone下来进行编译安装么？没错！因为 yum 源中并没有现成的 python3 程序，所以必须要自己手动编译安装。","text":"centos7 自带有 python，但是却是 python2 版本的 python，如果你想安装个python3怎么办呢？难道要从github上把源码clone下来进行编译安装么？没错！因为 yum 源中并没有现成的 python3 程序，所以必须要自己手动编译安装。 首先，你要知道系统现在的python的位置在哪儿： 1whereis python 进入Python安装目录 1ll python* 添加epel扩展源 1yum -y install epel-release 安装pip 1yum install python-pip 用pip装wget 1pip install wget 用wget下载python3的源码包 1wget https:&#x2F;&#x2F;www.python.org&#x2F;ftp&#x2F;python&#x2F;3.6.4&#x2F;Python-3.6.4.tar.xz 编译python3源码包 解压 12xz -d Python-3.6.4.tar.xztar -xf Python-3.6.4.tar 进入解压后的目录，依次执行下面命令进行手动编译 12.&#x2F;configure prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;python3make &amp;&amp; make install 添加软链接将原来的链接备份 1mv &#x2F;usr&#x2F;bin&#x2F;python &#x2F;usr&#x2F;bin&#x2F;python.bak 添加python3的软链接 1ln -s &#x2F;usr&#x2F;local&#x2F;python3&#x2F;bin&#x2F;python3.6 &#x2F;usr&#x2F;bin&#x2F;python 测试是否安装成功了 1python -V 更改yum配置，因为其要用到python2才能执行，否则会导致yum不能正常使用 1vi &#x2F;usr&#x2F;bin&#x2F;yum 把#! /usr/bin/python修改为#! /usr/bin/python2 1vi &#x2F;usr&#x2F;libexec&#x2F;urlgrabber-ext-down 把#! /usr/bin/python 修改为#! /usr/bin/python2 加上pip的修改 12mv &#x2F;usr&#x2F;bin&#x2F;pip &#x2F;usr&#x2F;bin&#x2F;pip.bakln -s &#x2F;usr&#x2F;local&#x2F;python3&#x2F;bin&#x2F;pip3 &#x2F;usr&#x2F;bin&#x2F;pip 1pip -V 修改环境变量 1vi &#x2F;etc&#x2F;profile 12export PYTHON_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;python3export PATH&#x3D;:$PYTHON_HOME&#x2F;bin:$PATH 1source &#x2F;etc&#x2F;profile","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"CentOS7","slug":"linux/centos7","permalink":"https://blog.mhuig.top/categories/linux/centos7/"},{"name":"Python","slug":"linux/centos7/python","permalink":"https://blog.mhuig.top/categories/linux/centos7/python/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/tags/centos7/"},{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"}]},{"title":"Linux安装PHP-EXIF扩展模块","slug":"Linux/Linux安装PHP-EXIF扩展模块","date":"2019-09-22T02:29:41.000Z","updated":"2019-09-22T02:29:41.000Z","comments":true,"path":"posts/d7edfddd.html","link":"","permalink":"https://blog.mhuig.top/posts/d7edfddd.html","excerpt":"您可以使用exif相关的函数从文件头读取数码相机拍摄的JPEG和TIFF格式的图像文件元数据。","text":"您可以使用exif相关的函数从文件头读取数码相机拍摄的JPEG和TIFF格式的图像文件元数据。 安装编译PHP时安装使用–enable-exif选项配置PHP来启用exif支持。 Windows用户必须在php.ini中启用php_mbstring.dll和php_exif.dll扩展。请确保在php.ini中保持正确的顺序：php_mbstring.dll必须在php_exif.dll之前加载。 源码安装在PHP源码中可以找到EXIF扩展源码，然后编译安装到当前的PHP环境中 1234cd &#x2F;Main&#x2F;sh-1.5.5&#x2F;php-5.5.7&#x2F;ext&#x2F;exif&#x2F;alidata&#x2F;server&#x2F;php-5.5.7&#x2F;bin&#x2F;phpize.&#x2F;configure --with-php-config&#x3D;&#x2F;alidata&#x2F;server&#x2F;php-5.5.7&#x2F;bin&#x2F;php-configmake &amp;&amp; make install cd进入/alidata/server/php-5.5.7/etc文件夹，修改php.ini，添加exif.so扩展 1extension = exif.so 重启Apache服务器1service httpd restart Nginx的服务器Nginx服务器重启之前，需要先重启php-fpm 12service php-fpm restartnginx -s reload 查看使用phpinfo()查看PHP环境，安装配置成功。","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"php","slug":"php","permalink":"https://blog.mhuig.top/tags/php/"}]},{"title":"云服务器部署可道云(KodExplorer)","slug":"Linux/云服务器部署可道云-KodExplorer","date":"2019-09-22T01:29:24.000Z","updated":"2019-09-22T01:29:24.000Z","comments":true,"path":"posts/9ef6ff16.html","link":"","permalink":"https://blog.mhuig.top/posts/9ef6ff16.html","excerpt":"在做一些项目的时候，经常有一些文档交流，修改之后的文档在QQ或微信上发来发去，还要下载，我们在这里部署KodExplorer可道云。 kodexplorer可道云是目前国内有代表性、美观易用性好的私有云软件，本文介绍在阿里云的云服务器上如何部署kodexplorer可道云，搭建私有网盘。","text":"在做一些项目的时候，经常有一些文档交流，修改之后的文档在QQ或微信上发来发去，还要下载，我们在这里部署KodExplorer可道云。 kodexplorer可道云是目前国内有代表性、美观易用性好的私有云软件，本文介绍在阿里云的云服务器上如何部署kodexplorer可道云，搭建私有网盘。 注意：云服务器部署和普通的Ubuntu上部署有一些区别，因为云服务器上只能使用命令行，没有界面。 官方下载页面:https://kodcloud.com/download/。其中有Linux获取最新版可道云的相关命令。 下载命令： 1wget http:&#x2F;&#x2F;static.kodcloud.com&#x2F;update&#x2F;download&#x2F;kodexplorer4.40.zip 创建目录： 1sudo mkdir cloud 解压命令： 1unzip kodexplorer4.40.zip -d .&#x2F;cloud 进入对应文件夹，并设置权限： 1234chmod -Rf 777 .&#x2F;cloudcd .&#x2F;cloudchmod -Rf 777 .&#x2F;*","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"cloud","slug":"cloud","permalink":"https://blog.mhuig.top/tags/cloud/"}]},{"title":"一键安装Ecs服务器的web环境(阿里云)","slug":"Linux/CentOS/一键安装ecs服务器的web环境-阿里云","date":"2019-09-22T00:27:11.000Z","updated":"2019-09-22T00:27:11.000Z","comments":true,"path":"posts/c91f449a.html","link":"","permalink":"https://blog.mhuig.top/posts/c91f449a.html","excerpt":"阿里云Linux一键安装web环境使用教程","text":"阿里云Linux一键安装web环境使用教程 教程1.准备工具阿里云linux一键安装web环境 2.将安装包上传到服务器上ftp,putty等. 3.解压安装包进行安装1234unzip -o -d . sh-1.5.5.zipchmod -R 777 sh-1.5.5cd sh-1.5.5&#x2F;.&#x2F;install.sh 4.Mysql选择的5.5.40版本，其他版本会出现问题;php选择5.5.7版本;5.安装完成查看自己安装的信息1netstat -tunpl 正在运行状态的服务及端口 9000 端口：php进程服务(apache没有9000端口，因为nginx+php集成方式与apache+php集成方式不同） 3306端口：mysql服务 80端口：httpd或者nginx服务 21端口：ftp服务 6.查看ftp和mysql用户名和密码1cat account.log 7.修改ftp的密码使用root身份执行如下命令： 1passwd www 8.修改mysql的密码：1mysqladmin -uroot -p旧密码 password 新密码 注：-p 和旧密码之间没有空格，password 和新密码之间有空格 另外，我们也可以在在/alidata/website-info.log文件中查看到安装软件的版本信息 9.清空phpwind文件夹12cd &#x2F;alidata&#x2F;www&#x2F;phpwindrm -rf &#x2F;alidata&#x2F;www&#x2F;phpwind&#x2F;* 10.安装phpMyAdmin下载数据库管理软件:phpMyAdmin,不要下载带有“betal”字样的版本，那是测试版。排序规则选：utf8_general_ci 12wget https:&#x2F;&#x2F;files.phpmyadmin.net&#x2F;phpMyAdmin&#x2F;4.9.1&#x2F;phpMyAdmin-4.9.1-all-languages.tar.gztar -zxvf phpMyAdmin-4.9.1-all-languages.tar.gz 附录Linux下的解压命令小结 unzip filename. zip tar -zxvf filename. tar.gz tar -Jxvf filename. tar.xz tar -Zxvf filename. tar.Z tar –help tar -xvf filename. tar.gz tar -xvf filename phpMyAdmin配置文件现在需要一个短语密码解决方法phpMyAdmin登陆之后，在其下方会出现配置文件现在需要一个短语密码的提示。 解决方法： 1、将 phpMyAdmin/libraries/config.default.php中的$cfg[‘blowfish_secret’] = ‘’; 改成 $cfg[‘blowfish_secret’] = ‘thepie.top’; (注：其中的’thepie.top′为随意的长字符串) 2、在phpMyAdmin目录中，打开config.sample.inc.php，17行 $cfg[‘blowfish_secret’] = ‘’; 改成 $cfg[‘blowfish_secret’] = ‘thepie.top’; (注：其中的’thepie.top′为随意的长字符串) 这个密码用于Cookies的加密，以免多个PhpMyAdmin或者和其他程序共用Cookies时搞混。 变量 $cfg[‘TempDir’] （./tmp/）无法访问。phpMyAdmin无法缓存模板文件，所以会运行缓慢。出现这个的原因是 phpmyadmin的安装目录， tmp目录不存在，或者存在但是权限不对。解决的方法就是没有创建一下这个目录，给予正确的读写权限即可。进入phpmyadmin的安装目录然后执行 12mkdir tmpchmod 777 tmp phpmyadmin 提示的很清楚，这是个缓存目录，可以加快phpmyadmin的运行，即使不理睬这个警告信息，也不会影响程序的执行，就是执行的慢点。","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/tags/web/"}]},{"title":"CentOS7安装GUI图形界面","slug":"Linux/CentOS/CentOS7安装GUI图形界面","date":"2019-09-21T08:23:03.000Z","updated":"2019-09-21T08:23:03.000Z","comments":true,"path":"posts/bbe38b68.html","link":"","permalink":"https://blog.mhuig.top/posts/bbe38b68.html","excerpt":"当你安装CentOS7服务器版本的时候，系统默认是不会安装GUI的图形界面程序，这个需要手动安装CentOS7 Gnome GUI包。","text":"当你安装CentOS7服务器版本的时候，系统默认是不会安装GUI的图形界面程序，这个需要手动安装CentOS7 Gnome GUI包。 在安装Gnome包之前，需要检查一下安装源(yum)是否正常，因为需要在yum命令来安装gnome包。 第一步：先检查yum 是否安装，以及网络是否有网络。如果这两者都没有，先解决网络，在解决yum的安装。 第二步：在命令行下 输入下面的命令来安装Gnome包。 1yum groupinstall &quot;GNOME Desktop&quot; &quot;Graphical Administration Tools&quot; 第三步：更新系统的运行级别。 1ln -sf /lib/systemd/system/runlevel5.target /etc/systemd/system/default.target 第四步：重启机器。启动默认进入图形界面。 1reboot Linux查看端口状态 1netstat -ntulp |grep 8000 杀进程 1kill -9 id","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"CentOS7","slug":"linux/centos7","permalink":"https://blog.mhuig.top/categories/linux/centos7/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/tags/centos7/"}]},{"title":"大数据处理技术-数据可视化Echarts介绍","slug":"bigdata/hadoop/大数据处理技术 - 数据可视化Echarts介绍","date":"2019-09-19T03:25:51.000Z","updated":"2019-09-19T03:25:51.000Z","comments":true,"path":"posts/f4e10e30.html","link":"","permalink":"https://blog.mhuig.top/posts/f4e10e30.html","excerpt":"大数据处理技术 PDF 数据可视化Echarts介绍","text":"大数据处理技术 PDF 数据可视化Echarts介绍 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Echarts","slug":"大数据/大数据处理技术/echarts","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/echarts/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Echarts","slug":"echarts","permalink":"https://blog.mhuig.top/tags/echarts/"}]},{"title":"大数据处理技术-工作流调度器Azkaban","slug":"bigdata/hadoop/大数据处理技术 -工作流调度器azkaban","date":"2019-09-19T03:25:50.000Z","updated":"2019-09-19T03:25:50.000Z","comments":true,"path":"posts/8485750c.html","link":"","permalink":"https://blog.mhuig.top/posts/8485750c.html","excerpt":"大数据处理技术 PDF 工作流调度器azkaban","text":"大数据处理技术 PDF 工作流调度器azkaban GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Azkaban","slug":"大数据/大数据处理技术/azkaban","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/azkaban/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Azkaban","slug":"azkaban","permalink":"https://blog.mhuig.top/tags/azkaban/"}]},{"title":"大数据处理技术-Sqoop数据迁移","slug":"bigdata/hadoop/大数据处理技术 - sqoop数据迁移","date":"2019-09-19T03:25:49.000Z","updated":"2019-09-19T03:25:49.000Z","comments":true,"path":"posts/bfbc2e0b.html","link":"","permalink":"https://blog.mhuig.top/posts/bfbc2e0b.html","excerpt":"大数据处理技术 PDF sqoop数据迁移","text":"大数据处理技术 PDF sqoop数据迁移 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Sqoop","slug":"大数据/大数据处理技术/sqoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/sqoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Sqoop","slug":"sqoop","permalink":"https://blog.mhuig.top/tags/sqoop/"}]},{"title":"大数据处理技术-Kafka-Manager监控工具的使用","slug":"bigdata/hadoop/大数据处理技术 - kafka-manager监控工具的使用","date":"2019-09-19T03:25:48.000Z","updated":"2019-09-19T03:25:48.000Z","comments":true,"path":"posts/10cabcd3.html","link":"","permalink":"https://blog.mhuig.top/posts/10cabcd3.html","excerpt":"大数据处理技术 PDF kafka-manager监控工具的使用","text":"大数据处理技术 PDF kafka-manager监控工具的使用 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Flume与kafka的整合","slug":"bigdata/hadoop/大数据处理技术 - flume与kafka的整合","date":"2019-09-19T03:25:47.000Z","updated":"2019-09-19T03:25:47.000Z","comments":true,"path":"posts/a4a39746.html","link":"","permalink":"https://blog.mhuig.top/posts/a4a39746.html","excerpt":"大数据处理技术 PDF flume与kafka的整合","text":"大数据处理技术 PDF flume与kafka的整合 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-Kafka的配置文件的说明","slug":"bigdata/hadoop/大数据处理技术 -kafka的配置文件的说明","date":"2019-09-19T03:25:46.000Z","updated":"2019-09-19T03:25:46.000Z","comments":true,"path":"posts/4e6873c5.html","link":"","permalink":"https://blog.mhuig.top/posts/4e6873c5.html","excerpt":"大数据处理技术 PDF kafka的配置文件的说明","text":"大数据处理技术 PDF kafka的配置文件的说明 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Kafka的数据的分区","slug":"bigdata/hadoop/大数据处理技术 - kafka的数据的分区","date":"2019-09-19T03:25:45.000Z","updated":"2019-09-19T03:25:45.000Z","comments":true,"path":"posts/c8fc3430.html","link":"","permalink":"https://blog.mhuig.top/posts/c8fc3430.html","excerpt":"大数据处理技术 PDF kafka的数据的分区","text":"大数据处理技术 PDF kafka的数据的分区 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-kafka的javaAPI的使用","slug":"bigdata/hadoop/大数据处理技术 - kafka的javaAPI的使用","date":"2019-09-19T03:25:44.000Z","updated":"2019-09-19T03:25:44.000Z","comments":true,"path":"posts/32c5d0d9.html","link":"","permalink":"https://blog.mhuig.top/posts/32c5d0d9.html","excerpt":"大数据处理技术 PDF kafka的javaAPI的使用","text":"大数据处理技术 PDF kafka的javaAPI的使用 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Kafka的命令行的管理使用","slug":"bigdata/hadoop/大数据处理技术 - kafka的命令行的管理使用","date":"2019-09-19T03:25:43.000Z","updated":"2019-09-19T03:25:43.000Z","comments":true,"path":"posts/aa02a436.html","link":"","permalink":"https://blog.mhuig.top/posts/aa02a436.html","excerpt":"大数据处理技术 PDF kafka的命令行的管理使用","text":"大数据处理技术 PDF kafka的命令行的管理使用 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Kafka的安装","slug":"bigdata/hadoop/大数据处理技术 -kafka的安装","date":"2019-09-19T03:25:42.000Z","updated":"2019-09-19T03:25:42.000Z","comments":true,"path":"posts/f77eee2d.html","link":"","permalink":"https://blog.mhuig.top/posts/f77eee2d.html","excerpt":"大数据处理技术 PDF kafka的安装","text":"大数据处理技术 PDF kafka的安装 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Flume的负载均衡loadbalancer","slug":"bigdata/hadoop/大数据处理技术 - flume的负载均衡loadbalancer","date":"2019-09-19T03:25:41.000Z","updated":"2019-09-19T03:25:41.000Z","comments":true,"path":"posts/c89a329f.html","link":"","permalink":"https://blog.mhuig.top/posts/c89a329f.html","excerpt":"大数据处理技术 PDF flume的负载均衡loadbalancer","text":"大数据处理技术 PDF flume的负载均衡loadbalancer GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-Kafka的介绍","slug":"bigdata/hadoop/大数据处理技术 - kafka的介绍","date":"2019-09-19T03:25:41.000Z","updated":"2019-09-19T03:25:41.000Z","comments":true,"path":"posts/b9b7baa7.html","link":"","permalink":"https://blog.mhuig.top/posts/b9b7baa7.html","excerpt":"大数据处理技术 PDF kafka的介绍","text":"大数据处理技术 PDF kafka的介绍 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-高可用Flume","slug":"bigdata/hadoop/大数据处理技术 - 高可用Flume","date":"2019-09-19T03:25:40.000Z","updated":"2019-09-19T03:25:40.000Z","comments":true,"path":"posts/a105df5c.html","link":"","permalink":"https://blog.mhuig.top/posts/a105df5c.html","excerpt":"大数据处理技术 PDF 高可用Flume","text":"大数据处理技术 PDF 高可用Flume GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-Flume更多source和sink组件","slug":"bigdata/hadoop/大数据处理技术 - Flume更多source和sink组件","date":"2019-09-19T03:25:39.000Z","updated":"2019-09-19T03:25:39.000Z","comments":true,"path":"posts/801b51ef.html","link":"","permalink":"https://blog.mhuig.top/posts/801b51ef.html","excerpt":"大数据处理技术 PDF Flume更多source和sink组件","text":"大数据处理技术 PDF Flume更多source和sink组件 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-Flume两个agent级联","slug":"bigdata/hadoop/大数据处理技术 -Flume两个agent级联","date":"2019-09-19T03:25:38.000Z","updated":"2019-09-19T03:25:38.000Z","comments":true,"path":"posts/d8795dab.html","link":"","permalink":"https://blog.mhuig.top/posts/d8795dab.html","excerpt":"大数据处理技术 PDF Flume两个agent级联","text":"大数据处理技术 PDF Flume两个agent级联 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-Flume采集案例监控目录变化","slug":"bigdata/hadoop/大数据处理技术 -Flume采集案例监控目录变化","date":"2019-09-19T03:25:37.000Z","updated":"2019-09-19T03:25:37.000Z","comments":true,"path":"posts/8ce758a7.html","link":"","permalink":"https://blog.mhuig.top/posts/8ce758a7.html","excerpt":"大数据处理技术 PDF Flume采集案例监控目录变化","text":"大数据处理技术 PDF Flume采集案例监控目录变化 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-Flume的安装部署","slug":"bigdata/hadoop/大数据处理技术 - Flume的安装部署","date":"2019-09-19T03:25:36.000Z","updated":"2019-09-19T03:25:36.000Z","comments":true,"path":"posts/75ee5fa.html","link":"","permalink":"https://blog.mhuig.top/posts/75ee5fa.html","excerpt":"大数据处理技术 PDF Flume的安装部署","text":"大数据处理技术 PDF Flume的安装部署 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-Flume介绍","slug":"bigdata/hadoop/大数据处理技术 - Flume介绍","date":"2019-09-19T03:25:35.000Z","updated":"2019-09-19T03:25:35.000Z","comments":true,"path":"posts/a5eec80c.html","link":"","permalink":"https://blog.mhuig.top/posts/a5eec80c.html","excerpt":"大数据处理技术 PDF Flume介绍","text":"大数据处理技术 PDF Flume介绍 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"}]},{"title":"大数据处理技术-关于Yarn常用参数设置","slug":"bigdata/hadoop/大数据处理技术 - 关于yarn常用参数设置","date":"2019-09-19T03:25:34.000Z","updated":"2019-09-19T03:25:34.000Z","comments":true,"path":"posts/780b9de2.html","link":"","permalink":"https://blog.mhuig.top/posts/780b9de2.html","excerpt":"大数据处理技术 PDF 关于yarn常用参数设置","text":"大数据处理技术 PDF 关于yarn常用参数设置 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Yarn","slug":"大数据/大数据处理技术/yarn","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/yarn/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Yarn","slug":"yarn","permalink":"https://blog.mhuig.top/tags/yarn/"}]},{"title":"大数据处理技术-Yarn资源调度","slug":"bigdata/hadoop/大数据处理技术 - Yarn资源调度","date":"2019-09-19T03:25:33.000Z","updated":"2019-09-19T03:25:33.000Z","comments":true,"path":"posts/c5520104.html","link":"","permalink":"https://blog.mhuig.top/posts/c5520104.html","excerpt":"大数据处理技术 PDF Yarn资源调度","text":"大数据处理技术 PDF Yarn资源调度 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Yarn","slug":"大数据/大数据处理技术/yarn","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/yarn/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Yarn","slug":"yarn","permalink":"https://blog.mhuig.top/tags/yarn/"}]},{"title":"大数据处理技术-Hive语句综合练习","slug":"bigdata/hadoop/大数据处理技术 -hive语句综合练习","date":"2019-09-19T03:25:32.000Z","updated":"2019-09-19T03:25:32.000Z","comments":true,"path":"posts/b4fa1b4e.html","link":"","permalink":"https://blog.mhuig.top/posts/b4fa1b4e.html","excerpt":"大数据处理技术 PDF hive语句综合练习","text":"大数据处理技术 PDF hive语句综合练习 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"大数据/大数据处理技术/hive","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"hive","permalink":"https://blog.mhuig.top/tags/hive/"}]},{"title":"大数据处理技术-Hive创建数据库表","slug":"bigdata/hadoop/大数据处理技术 - Hive创建数据库表","date":"2019-09-19T03:25:31.000Z","updated":"2019-09-19T03:25:31.000Z","comments":true,"path":"posts/48a224d1.html","link":"","permalink":"https://blog.mhuig.top/posts/48a224d1.html","excerpt":"大数据处理技术 PDF Hive创建数据库表","text":"大数据处理技术 PDF Hive创建数据库表 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"大数据/大数据处理技术/hive","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"hive","permalink":"https://blog.mhuig.top/tags/hive/"}]},{"title":"大数据处理技术-Hive基本操作之创建数据库","slug":"bigdata/hadoop/大数据处理技术 - Hive基本操作之创建数据库","date":"2019-09-19T03:25:30.000Z","updated":"2019-09-19T03:25:30.000Z","comments":true,"path":"posts/7cb85200.html","link":"","permalink":"https://blog.mhuig.top/posts/7cb85200.html","excerpt":"大数据处理技术 PDF Hive基本操作之创建数据库","text":"大数据处理技术 PDF Hive基本操作之创建数据库 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"大数据/大数据处理技术/hive","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"hive","permalink":"https://blog.mhuig.top/tags/hive/"}]},{"title":"大数据处理技术-Hive基本概念","slug":"bigdata/hadoop/大数据处理技术 - Hive基本概念","date":"2019-09-19T03:25:29.000Z","updated":"2019-09-19T03:25:29.000Z","comments":true,"path":"posts/f4524359.html","link":"","permalink":"https://blog.mhuig.top/posts/f4524359.html","excerpt":"大数据处理技术 PDF Hive基本概念","text":"大数据处理技术 PDF Hive基本概念 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"大数据/大数据处理技术/hive","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hive","slug":"hive","permalink":"https://blog.mhuig.top/tags/hive/"}]},{"title":"大数据处理技术-索引建立","slug":"bigdata/hadoop/大数据处理技术 - 索引建立","date":"2019-09-19T03:25:28.000Z","updated":"2019-09-19T03:25:28.000Z","comments":true,"path":"posts/cc03340e.html","link":"","permalink":"https://blog.mhuig.top/posts/cc03340e.html","excerpt":"大数据处理技术 PDF 索引建立","text":"大数据处理技术 PDF 索引建立 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-MapReduce Shuffle过程","slug":"bigdata/hadoop/大数据处理技术 - MapReduceshuffle过程","date":"2019-09-19T03:25:27.000Z","updated":"2019-09-19T03:25:27.000Z","comments":true,"path":"posts/c81f7fcf.html","link":"","permalink":"https://blog.mhuig.top/posts/c81f7fcf.html","excerpt":"大数据处理技术 PDF MapReduce shuffle过程","text":"大数据处理技术 PDF MapReduce shuffle过程 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-MapTask运行机制详解以及Map任务的并行度","slug":"bigdata/hadoop/大数据处理技术 - MapTask运行机制详解以及Map任务的并行度","date":"2019-09-19T03:25:25.000Z","updated":"2019-09-19T03:25:25.000Z","comments":true,"path":"posts/d178b062.html","link":"","permalink":"https://blog.mhuig.top/posts/d178b062.html","excerpt":"大数据处理技术 PDF MapTask运行机制详解以及Map任务的并行度","text":"大数据处理技术 PDF MapTask运行机制详解以及Map任务的并行度 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-ReduceTask工作机制以及reduceTask的并行度","slug":"bigdata/hadoop/大数据处理技术 - ReduceTask工作机制以及reduceTask的并行度","date":"2019-09-19T03:25:25.000Z","updated":"2019-09-19T03:25:25.000Z","comments":true,"path":"posts/ea7b2ec0.html","link":"","permalink":"https://blog.mhuig.top/posts/ea7b2ec0.html","excerpt":"大数据处理技术 PDF ReduceTask工作机制以及reduceTask的并行度","text":"大数据处理技术 PDF ReduceTask工作机制以及reduceTask的并行度 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-MapReduce的分区与reduceTask的数量","slug":"bigdata/hadoop/大数据处理技术 - MapReduce的分区与reduceTask的数量","date":"2019-09-19T03:25:23.000Z","updated":"2019-09-19T03:25:23.000Z","comments":true,"path":"posts/a5d678be.html","link":"","permalink":"https://blog.mhuig.top/posts/a5d678be.html","excerpt":"大数据处理技术 PDF MapReduce的分区与reduceTask的数量","text":"大数据处理技术 PDF MapReduce的分区与reduceTask的数量 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-MapReduce编程模型-WordCount实例分析","slug":"bigdata/hadoop/大数据处理技术 - MapReduce编程模型-WordCount实例分析","date":"2019-09-19T03:25:22.000Z","updated":"2019-09-19T03:25:22.000Z","comments":true,"path":"posts/695a6c4b.html","link":"","permalink":"https://blog.mhuig.top/posts/695a6c4b.html","excerpt":"大数据处理技术 PDF MapReduce编程模型-WordCount实例分析","text":"大数据处理技术 PDF MapReduce编程模型-WordCount实例分析 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-WordCount示例编写本地模式","slug":"bigdata/hadoop/大数据处理技术 -WordCount示例编写本地模式","date":"2019-09-19T03:25:21.000Z","updated":"2019-09-19T03:25:21.000Z","comments":true,"path":"posts/46b39789.html","link":"","permalink":"https://blog.mhuig.top/posts/46b39789.html","excerpt":"大数据处理技术 PDF WordCount示例编写本地模式","text":"大数据处理技术 PDF WordCount示例编写本地模式 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-MapReduce编程规范及示例编写","slug":"bigdata/hadoop/大数据处理技术 - MapReduce编程规范及示例编写","date":"2019-09-19T03:25:20.000Z","updated":"2019-09-19T03:25:20.000Z","comments":true,"path":"posts/610da529.html","link":"","permalink":"https://blog.mhuig.top/posts/610da529.html","excerpt":"大数据处理技术 PDF MapReduce编程规范及示例编写","text":"大数据处理技术 PDF MapReduce编程规范及示例编写 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-MapReduce框架结构","slug":"bigdata/hadoop/大数据处理技术 - MapReduce框架结构","date":"2019-09-19T03:25:19.000Z","updated":"2019-09-19T03:25:19.000Z","comments":true,"path":"posts/7def3950.html","link":"","permalink":"https://blog.mhuig.top/posts/7def3950.html","excerpt":"大数据处理技术 PDF MapReduce框架结构","text":"大数据处理技术 PDF MapReduce框架结构 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-HadoopMapReduce设计构思","slug":"bigdata/hadoop/大数据处理技术 - HadoopMapReduce设计构思","date":"2019-09-19T03:25:18.000Z","updated":"2019-09-19T03:25:18.000Z","comments":true,"path":"posts/b0ef01f5.html","link":"","permalink":"https://blog.mhuig.top/posts/b0ef01f5.html","excerpt":"大数据处理技术 PDF HadoopMapReduce设计构思","text":"大数据处理技术 PDF HadoopMapReduce设计构思 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-理解MapReduce思想","slug":"bigdata/hadoop/大数据处理技术 - 理解MapReduce思想","date":"2019-09-19T03:25:17.000Z","updated":"2019-09-19T03:25:17.000Z","comments":true,"path":"posts/7d6b1f3c.html","link":"","permalink":"https://blog.mhuig.top/posts/7d6b1f3c.html","excerpt":"大数据处理技术 PDF 理解MapReduce思想","text":"大数据处理技术 PDF 理解MapReduce思想 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"}]},{"title":"大数据处理技术-HDFS的JavaAPI操作","slug":"bigdata/hadoop/大数据处理技术 - HDFS的JavaAPI操作","date":"2019-09-19T03:25:16.000Z","updated":"2019-09-19T03:25:16.000Z","comments":true,"path":"posts/a3ed1548.html","link":"","permalink":"https://blog.mhuig.top/posts/a3ed1548.html","excerpt":"大数据处理技术 PDF HDFS的JavaAPI操作","text":"大数据处理技术 PDF HDFS的JavaAPI操作 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"},{"name":"JavaAPI","slug":"javaapi","permalink":"https://blog.mhuig.top/tags/javaapi/"}]},{"title":"大数据处理技术-HDFS的文件读取过程","slug":"bigdata/hadoop/大数据处理技术 - HDFS的文件读取过程","date":"2019-09-19T03:25:15.000Z","updated":"2019-09-19T03:25:15.000Z","comments":true,"path":"posts/6e430c9c.html","link":"","permalink":"https://blog.mhuig.top/posts/6e430c9c.html","excerpt":"大数据处理技术 PDF HDFS的文件读取过程","text":"大数据处理技术 PDF HDFS的文件读取过程 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-HDFS的文件写入过程","slug":"bigdata/hadoop/大数据处理技术 - HDFS的文件写入过程","date":"2019-09-19T03:25:14.000Z","updated":"2019-09-19T03:25:14.000Z","comments":true,"path":"posts/4edeccca.html","link":"","permalink":"https://blog.mhuig.top/posts/4edeccca.html","excerpt":"大数据处理技术 PDF HDFS的文件写入过程","text":"大数据处理技术 PDF HDFS的文件写入过程 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-HDFS的元数据信息FSimage以及edits和secondaryNN的作用","slug":"bigdata/hadoop/大数据处理技术 - HDFS的元数据信息FSimage以及edits和secondaryNN的作用","date":"2019-09-19T03:25:13.000Z","updated":"2019-09-19T03:25:13.000Z","comments":true,"path":"posts/fec1c989.html","link":"","permalink":"https://blog.mhuig.top/posts/fec1c989.html","excerpt":"大数据处理技术 PDF HDFS的元数据信息FSimage以及edits和secondaryNN的作用","text":"大数据处理技术 PDF HDFS的元数据信息FSimage以及edits和secondaryNN的作用 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"}]},{"title":"大数据处理技术-Hdfs的架构之文件的文件副本机制","slug":"bigdata/hadoop/大数据处理技术 -hdfs的架构之文件的文件副本机制","date":"2019-09-19T03:25:12.000Z","updated":"2019-09-19T03:25:12.000Z","comments":true,"path":"posts/787a8307.html","link":"","permalink":"https://blog.mhuig.top/posts/787a8307.html","excerpt":"大数据处理技术 PDF hdfs的架构之文件的文件副本机制","text":"大数据处理技术 PDF hdfs的架构之文件的文件副本机制 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"}]},{"title":"大数据处理技术-HDFS的架构图之基础架构","slug":"bigdata/hadoop/大数据处理技术 - HDFS的架构图之基础架构","date":"2019-09-19T03:25:11.000Z","updated":"2019-09-19T03:25:11.000Z","comments":true,"path":"posts/bb7c60d.html","link":"","permalink":"https://blog.mhuig.top/posts/bb7c60d.html","excerpt":"大数据处理技术 PDF HDFS的架构图之基础架构","text":"大数据处理技术 PDF HDFS的架构图之基础架构 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"}]},{"title":"大数据处理技术-HDFS的来源","slug":"bigdata/hadoop/大数据处理技术 - HDFS的来源","date":"2019-09-19T03:25:10.000Z","updated":"2019-09-19T03:25:10.000Z","comments":true,"path":"posts/e981c79e.html","link":"","permalink":"https://blog.mhuig.top/posts/e981c79e.html","excerpt":"大数据处理技术 PDF HDFS的来源","text":"大数据处理技术 PDF HDFS的来源 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"}]},{"title":"大数据处理技术-HDFS分布式文件系统设计目标","slug":"bigdata/hadoop/大数据处理技术 - HDFS分布式文件系统设计目标","date":"2019-09-19T03:25:09.000Z","updated":"2019-09-19T03:25:09.000Z","comments":true,"path":"posts/6acc7206.html","link":"","permalink":"https://blog.mhuig.top/posts/6acc7206.html","excerpt":"大数据处理技术 PDF HDFS分布式文件系统设计目标","text":"大数据处理技术 PDF HDFS分布式文件系统设计目标 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"}]},{"title":"大数据处理技术-分布式文件系统详细介绍","slug":"bigdata/hadoop/大数据处理技术 -分布式文件系统详细介绍","date":"2019-09-19T03:25:08.000Z","updated":"2019-09-19T03:25:08.000Z","comments":true,"path":"posts/72b49ca2.html","link":"","permalink":"https://blog.mhuig.top/posts/72b49ca2.html","excerpt":"大数据处理技术 PDF 分布式文件系统详细介绍","text":"大数据处理技术 PDF 分布式文件系统详细介绍 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"}]},{"title":"大数据处理技术-CDH伪分布式环境搭建","slug":"bigdata/hadoop/大数据处理技术 - CDH伪分布式环境搭建","date":"2019-09-19T03:25:07.000Z","updated":"2019-09-19T03:25:07.000Z","comments":true,"path":"posts/b387b8d5.html","link":"","permalink":"https://blog.mhuig.top/posts/b387b8d5.html","excerpt":"大数据处理技术 PDF CDH伪分布式环境搭建","text":"大数据处理技术 PDF CDH伪分布式环境搭建 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-HDFS的命令行使用","slug":"bigdata/hadoop/大数据处理技术 - HDFS的命令行使用","date":"2019-09-19T03:25:06.000Z","updated":"2019-09-19T03:25:06.000Z","comments":true,"path":"posts/fb97a8a5.html","link":"","permalink":"https://blog.mhuig.top/posts/fb97a8a5.html","excerpt":"大数据处理技术 PDF HDFS的命令行使用","text":"大数据处理技术 PDF HDFS的命令行使用 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-HDFS入门介绍","slug":"bigdata/hadoop/大数据处理技术 -HDFS入门介绍","date":"2019-09-19T03:25:05.000Z","updated":"2019-09-19T03:25:05.000Z","comments":true,"path":"posts/87e9dc75.html","link":"","permalink":"https://blog.mhuig.top/posts/87e9dc75.html","excerpt":"大数据处理技术 PDF HDFS入门介绍","text":"大数据处理技术 PDF HDFS入门介绍 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"}]},{"title":"大数据处理技术-Hadoop集群初体验","slug":"bigdata/hadoop/大数据处理技术 - hadoop集群初体验","date":"2019-09-19T03:25:04.000Z","updated":"2019-09-19T03:25:04.000Z","comments":true,"path":"posts/980d646e.html","link":"","permalink":"https://blog.mhuig.top/posts/980d646e.html","excerpt":"大数据处理技术 PDF hadoop集群初体验","text":"大数据处理技术 PDF hadoop集群初体验 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Hadoop三种架构介绍（高可用分布式环境介绍以及安装）","slug":"bigdata/hadoop/大数据处理技术 - hadoop三种架构介绍（高可用分布式环境介绍以及安装）","date":"2019-09-19T03:25:03.000Z","updated":"2019-09-19T03:25:03.000Z","comments":true,"path":"posts/1e1e5118.html","link":"","permalink":"https://blog.mhuig.top/posts/1e1e5118.html","excerpt":"大数据处理技术 PDF hadoop三种架构介绍（高可用分布式环境介绍以及安装）","text":"大数据处理技术 PDF hadoop三种架构介绍（高可用分布式环境介绍以及安装） GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术","slug":"bigdata/hadoop/大数据处理技术 -hadoop三种架构介绍（伪分布介绍以及安装）","date":"2019-09-19T03:25:02.000Z","updated":"2019-09-19T03:25:02.000Z","comments":true,"path":"posts/1009d3f6.html","link":"","permalink":"https://blog.mhuig.top/posts/1009d3f6.html","excerpt":"大数据处理技术 PDF hadoop三种架构介绍（伪分布介绍以及安装）","text":"大数据处理技术 PDF hadoop三种架构介绍（伪分布介绍以及安装） GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Apache hadoop三种架构介绍（standAlone)","slug":"bigdata/hadoop/大数据处理技术 - apache hadoop三种架构介绍（standAlone)","date":"2019-09-19T03:25:01.000Z","updated":"2019-09-19T03:25:01.000Z","comments":true,"path":"posts/e00c1bc.html","link":"","permalink":"https://blog.mhuig.top/posts/e00c1bc.html","excerpt":"大数据处理技术 PDF apache hadoop三种架构介绍（standAlone)","text":"大数据处理技术 PDF apache hadoop三种架构介绍（standAlone) GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-hadoop的架构模型（1.x，2.x的各种架构模型介绍）","slug":"bigdata/hadoop/大数据处理技术 - hadoop的架构模型（1.x，2.x的各种架构模型介绍）","date":"2019-09-19T03:25:00.000Z","updated":"2019-09-19T03:25:00.000Z","comments":true,"path":"posts/429246c5.html","link":"","permalink":"https://blog.mhuig.top/posts/429246c5.html","excerpt":"大数据处理技术 PDF hadoop的架构模型（1.x，2.x的各种架构模型介绍）","text":"大数据处理技术 PDF hadoop的架构模型（1.x，2.x的各种架构模型介绍） GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-三大公司发行版本介绍","slug":"bigdata/hadoop/大数据处理技术 - 三大公司发行版本介绍","date":"2019-09-19T03:24:59.000Z","updated":"2019-09-19T03:24:59.000Z","comments":true,"path":"posts/bfb9db00.html","link":"","permalink":"https://blog.mhuig.top/posts/bfb9db00.html","excerpt":"大数据处理技术 PDF 三大公司发行版本介绍","text":"大数据处理技术 PDF 三大公司发行版本介绍 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"}]},{"title":"大数据处理技术-Hadoop的介绍以及发展历史","slug":"bigdata/hadoop/大数据处理技术 - hadoop的介绍以及发展历史","date":"2019-09-19T03:24:58.000Z","updated":"2019-09-19T03:24:58.000Z","comments":true,"path":"posts/3098d714.html","link":"","permalink":"https://blog.mhuig.top/posts/3098d714.html","excerpt":"大数据处理技术 PDF hadoop的介绍以及发展历史","text":"大数据处理技术 PDF hadoop的介绍以及发展历史 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"历史","slug":"历史","permalink":"https://blog.mhuig.top/tags/%E5%8E%86%E5%8F%B2/"}]},{"title":"大数据处理技术-Zookeeper的介绍以及集群环境搭建","slug":"bigdata/hadoop/大数据处理技术 - zookeeper的介绍以及集群环境搭建","date":"2019-09-19T03:24:57.000Z","updated":"2019-09-19T03:24:57.000Z","comments":true,"path":"posts/f1ab62f1.html","link":"","permalink":"https://blog.mhuig.top/posts/f1ab62f1.html","excerpt":"大数据处理技术 PDF zookeeper的介绍以及集群环境搭建","text":"大数据处理技术 PDF zookeeper的介绍以及集群环境搭建 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Zookeeper","slug":"大数据/大数据处理技术/zookeeper","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/zookeeper/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Zookeeper","slug":"zookeeper","permalink":"https://blog.mhuig.top/tags/zookeeper/"}]},{"title":"大数据处理技术-分布式集群","slug":"bigdata/hadoop/大数据处理技术 - 分布式集群","date":"2019-09-19T03:24:56.000Z","updated":"2019-09-19T03:24:56.000Z","comments":true,"path":"posts/897831fa.html","link":"","permalink":"https://blog.mhuig.top/posts/897831fa.html","excerpt":"大数据处理技术 PDF 分布式集群","text":"大数据处理技术 PDF 分布式集群 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"大数据集群环境准备","slug":"大数据/大数据处理技术/大数据集群环境准备","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"大数据集群环境准备","slug":"大数据集群环境准备","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"}]},{"title":"大数据处理技术-大数据集群环境准备","slug":"bigdata/hadoop/大数据处理技术 -大数据集群环境准备","date":"2019-09-19T03:24:55.000Z","updated":"2019-09-19T03:24:55.000Z","comments":true,"path":"posts/c16b9d82.html","link":"","permalink":"https://blog.mhuig.top/posts/c16b9d82.html","excerpt":"大数据处理技术 PDF 大数据集群环境准备","text":"大数据处理技术 PDF 大数据集群环境准备 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"大数据集群环境准备","slug":"大数据/大数据处理技术/大数据集群环境准备","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"大数据集群环境准备","slug":"大数据集群环境准备","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"}]},{"title":"大数据处理技术-三台虚拟机创建并联网","slug":"bigdata/hadoop/大数据处理技术 - 三台虚拟机创建并联网","date":"2019-09-19T03:24:54.000Z","updated":"2019-09-19T03:24:54.000Z","comments":true,"path":"posts/e821aeca.html","link":"","permalink":"https://blog.mhuig.top/posts/e821aeca.html","excerpt":"大数据处理技术 PDF 三台虚拟机创建并联网","text":"大数据处理技术 PDF 三台虚拟机创建并联网 GitHub","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"大数据集群环境准备","slug":"大数据/大数据处理技术/大数据集群环境准备","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"大数据集群环境准备","slug":"大数据集群环境准备","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"}]},{"title":"大数据处理技术","slug":"bigdata/大数据处理技术","date":"2019-09-19T03:24:53.000Z","updated":"2019-09-19T03:24:53.000Z","comments":true,"path":"posts/1009d3f6.html","link":"","permalink":"https://blog.mhuig.top/posts/1009d3f6.html","excerpt":"大数据处理技术 PDF 实训资料汇总","text":"大数据处理技术 PDF 实训资料汇总 GitHub day01大数据集群环境准备&amp;zookeeper的介绍以及集群环境搭建三台虚拟机创建并联网 大数据集群环境准备 分布式集群 zookeeper的介绍以及集群环境搭建 day02大数据发展简史及环境安装hadoop的介绍以及发展历史 hadoop的历史版本介绍 三大公司发行版本介绍 hadoop的架构模型（1.x，2.x的各种架构模型介绍） apache hadoop三种架构介绍（standAlone) apache hadoop三种架构介绍（伪分布介绍以及安装） apache hadoop三种架构介绍（高可用分布式环境介绍以及安装） day03Hadoop集群初体验&amp;HDFS的命令行使用hadoop集群初体验 HDFS入门介绍 HDFS的命令行使用 CDH伪分布式环境搭建 day04分布式文件系统HDF分布式文件系统详细介绍 HDFS分布式文件系统设计目标 HDFS的来源 HDFS的架构图之基础架构 hdfs的架构之文件的文件副本机制 HDFS的元数据信息FSimage以及edits和secondaryNN的作用 HDFS的文件写入过程 HDFS的文件读取过程 HDFS的JavaAPI操作 day05MapReduce编程模型-WordCount实例分析理解MapReduce思想 HadoopMapReduce设计构思 MapReduce框架结构 MapReduce编程规范及示例编写 WordCount示例编写本地模式 MapReduce编程模型-WordCount实例分析 day06MapReduce的运行机制MapReduce的分区与reduceTask的数量 MapTask运行机制详解以及Map任务的并行度 ReduceTask工作机制以及reduceTask的并行度 MapReduceshuffle过程 索引建立 day07Yarn资源调度及Hive初步Hive基本概念 Hive的安装部署 Hive基本操作之创建数据库 创建数据库表 hive语句综合练习 Yarn资源调度 关于yarn常用参数设置 day08Flume数据采集Flume介绍 Flume的安装部署 采集案例监控目录变化 采集案例监控文件的变化 两个agent级联 更多source和sink组件 高可用Flume flume的负载均衡loadbalancer day09消息队列Kafkakafka的介绍 kafka的安装 kafka的命令行的管理使用 kafka的javaAPI的使用 kafka的数据的分区 kafka的配置文件的说明 flume与kafka的整合 kafka-manager监控工具的使用 CDH版本的zookeeper环境搭建 day10sqoop数据迁移sqoop day11工作流调度器azkaban&amp;数据可视化Echarts介绍azkaban 数据可视化Echarts介绍","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop","slug":"大数据/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"}]},{"title":"数学建模资料","slug":"math/数学建模资料","date":"2019-09-19T01:16:41.000Z","updated":"2019-09-19T01:16:41.000Z","comments":true,"path":"posts/6cfc95e.html","link":"","permalink":"https://blog.mhuig.top/posts/6cfc95e.html","excerpt":"汇总司守奎老师的讲义 程序 习题解答等","text":"汇总司守奎老师的讲义 程序 习题解答等 GitHub 封面 前言 目录 第一章 线性规划 第二章 整数规划 第三章 非线性规划 第四章 动态规划 第五章 图与网络 第六章 排队论 第七章 对策论 第八章 层次分析法 第九章 插值与拟合 第十章 数据的统计描述和分析 第十一章 方差分析 第十二章 回归分析 第十三章 微分方程建模 第十四章 稳定状态模型 第十五章 常微分方程的解法 第十六章 差分方程模型 第十七章 马氏链模型 第十八章 变分法模型 第十九章 神经网络模型 第二十章 偏微分方程的数值解 第二十一章 目标规划 第二十二章 模糊数学模型 第二十三章 现代优化算法 第二十四章 时间序列模型 第二十五章 灰色系统理论及其应用 第二十六章 多元分析 第二十七章 偏最小二乘回归分析 第二十八章 存贮论 第二十九章 经济与金融中的优化问题 第三十章 生产与服务运作管理中的优化问题 第三十一章 支持向量机 第三十二章 作业计划 附录一 Matlab入门 附录二 Matlab在线性代数中的应用 附录三 运筹学的LINGO软件 附录四 Excel在统计分析与数量方法中的应用 附录五 SPSS在统计分析中的应用 参考文献","categories":[{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"},{"name":"数学建模","slug":"math/数学建模","permalink":"https://blog.mhuig.top/categories/math/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}],"tags":[{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"},{"name":"数学建模","slug":"数学建模","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}]},{"title":"数据可视化PDF资料","slug":"others/ml/数据可视化","date":"2019-09-17T01:02:02.000Z","updated":"2019-09-17T01:02:02.000Z","comments":true,"path":"posts/98579906.html","link":"","permalink":"https://blog.mhuig.top/posts/98579906.html","excerpt":"汇总数据可视化PDF资料","text":"汇总数据可视化PDF资料 HTML的基本标签及语法 CSS语法规则 网页基本布局方式 自适应设计和响应式设计 js基本语法汇总 Ajax基本原理和概念 Ajax实现与后台服务通信 Echarts常用功能api","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据可视化","slug":"大数据/数据可视化","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"}]},{"title":"数据挖掘PDF资料","slug":"others/ml/数据挖掘","date":"2019-09-17T01:02:02.000Z","updated":"2019-09-17T01:02:02.000Z","comments":true,"path":"posts/b59b6a41.html","link":"","permalink":"https://blog.mhuig.top/posts/b59b6a41.html","excerpt":"汇总数据挖掘PDF资料","text":"汇总数据挖掘PDF资料 数据挖掘概念与特性 常用分类算法及原理 常用降维算法及原理 常用聚类算法及原理 关联分析及常用算法 常用推荐算法及原理","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据挖掘","slug":"大数据/数据挖掘","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}]},{"title":"特征和分类器","slug":"others/ml/特征和分类器","date":"2019-09-12T03:19:19.000Z","updated":"2019-09-12T03:19:19.000Z","comments":true,"path":"posts/483290b5.html","link":"","permalink":"https://blog.mhuig.top/posts/483290b5.html","excerpt":"特征提取和分类是典型计算机视觉系统的两个关键阶段。 视觉系统的准确性、稳健性和效率在很大程度上取决于图像特征，和分类器的质量。因此，目标是从输入图像中提取信息丰富的、可靠的特征，以便能够开发出很大程度上独立于领域理论的分类。","text":"特征提取和分类是典型计算机视觉系统的两个关键阶段。 视觉系统的准确性、稳健性和效率在很大程度上取决于图像特征，和分类器的质量。因此，目标是从输入图像中提取信息丰富的、可靠的特征，以便能够开发出很大程度上独立于领域理论的分类。 特征特征是任何独特的方面或特性，用于解决与特定应用相关的计算任务。 n个特征的组合可以表示成n维向量，称为特征向量。特征向量的质量取决于其区分不同类别的图像样本的能力。 来自同一类的图像样本应该有相似的特征值，来自不同类的图像应具有不同的特征值。 分类器分类器是现代计算机视觉和模式识别的核心。 分类器的任务是使用特征向量对图像或感兴趣区域（RoI）划分类别。 分类任务的困难程度取决于来自相同类别的图像的特征值的可变性，以及相对于来自不同类别图像的特征值的差异性。 但是，完美的分类性能通常是不可能的。这主要是因为： 噪声（以阴影、遮挡、透视扭曲等形式） 异常值 模糊性 缺少标签 仅有小训练样本可用 训练数据样本中正/负覆盖的不平衡 传统特征描述符传统（手工设计）特征提取方法可分为两大类： 全局 局部 全局特征提取方法定义了一组有效描述整个图像的全局特征。因此形状细节被忽略。全局特征也不适用于识别部分遮挡的对象。 局部特征提取方法提取关键点周围的局部区域，由此可以更好的处理遮挡。 检测关键点，并在他们周围构建描述符的方法： 局部描述符（如HOG、SIFT、SURF、FREAK、ORB、BRISK、BRIEF、LIOP） 方向梯度直方图HOG是一个特征描述符，用于自动检测图像中的对象。HOG描述符对图像中局部部分的梯度方向的分布进行编码。 HOG背后的想法是可以通过边缘方向的直方图来描述图像内的对象外观和形状。 1.梯度计算第一步是计算梯度值。在图像的水平和垂直方向上，执行一维中心点离散微分模板。具体的说，该方法需要用以下滤波器内核处理灰度图像： 因此给定一个图像，以下卷积操作（表示为 ）得出图像在和方向的导数： 因此，梯度的方向和梯度的大小计算如下： CNN也在层中使用卷积运算，然而主要区别在于不使用手工设计的滤波器，CNN使用可训练的滤波器，使其具有高度的自适应性。 CNN也在层中使用卷积运算，然而主要区别在于不使用手工设计的滤波器，CNN使用可训练的滤波器，使其具有高度的自适应性。 2.单元方向直方图第二步是计算单元直方图。首先将图像分成小的（通常是8X8像素）单元。每个单元都有固定数量的梯度方向区间，他们均匀分布在。或。之间，具体取决于梯度是有符号的还是无符号的。 单元内的每一个像素基于该像素处梯度的模对每一个梯度方向区间偷加权票。对于投票权重，可以是梯度大小，梯度大小的平方根或梯度大小的平方。 3.描述符块为了处理光照和对比度的变化，通过将单元组合在一起形成更大的空间上相连的块，局部的归一化梯度强度。然后，HOG描述符是来自所有块区域内的、归一化的单元直方图部件的向量。 4.块的归一化最后一步是块描述符的归一化。设v是包含给定块中所有直方图的非归一化向量，‖为它的(k)阶范数（(k=1,2) ），(\\epsilon)是一个小常量。归一化因子可以是如下之一： 范数或者 范数或者 范数平方根还有另一个归一化因子L2-Hys,它通过削减v的L2范数得到（将v的最大值限制为0.2），然后重新归一化。 最终的图像/RoI描述符是通过连接所有归一化的块描述符而形成的。 L2范数、L2-Hys和L1范数平方根（L1-sqrt）归一化方法提供了类似的性能，而L1范数提供了可靠性稍差的性能。 尺度不变特征变换SIFT[Lowe,2004]提供了一组对象的特征，这些特征对于对象缩放和旋转是健壮的。 SIFT算法由四个主要步骤组成。 1.尺度空间的极值侦测第一步旨在确定对缩放和方向不变的潜在关键点。 SIFT使用高斯差分（DoG）来检测尺度空间中关键点中的位置。 高斯差分是将两个不同尺度的图像（其中一个尺度为,另一个是其k倍，即）的高斯模糊进行差分得到的。 2.关键点精确定位3.方向定位4.关键点描述符","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器视觉","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"分类器","slug":"分类器","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E7%B1%BB%E5%99%A8/"},{"name":"特征","slug":"特征","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81/"}]},{"title":"图像导数","slug":"others/ml/图像导数","date":"2019-09-12T02:29:46.000Z","updated":"2019-09-12T02:29:46.000Z","comments":true,"path":"posts/9bc3b11e.html","link":"","permalink":"https://blog.mhuig.top/posts/9bc3b11e.html","excerpt":"在图像中，边缘可以看做是位于一阶导数较大的像素处，因此，我们可以求图像的一阶导数来确定图像的边缘，像sobel算子等一系列算子都是基于这个思想的。","text":"在图像中，边缘可以看做是位于一阶导数较大的像素处，因此，我们可以求图像的一阶导数来确定图像的边缘，像sobel算子等一系列算子都是基于这个思想的。 如下图a表示函数在边沿的时候关系，求导得b图，可知边沿可就是函数的极值点，对应二阶导数为0处，如图c的二阶导图。 关于导数总结如下： （1）一阶导数通常图像中产生较粗的边缘 （2）二阶导数对精细细节，如细线、孤立点和噪声有较强的响应 （3）二阶导数在灰度斜坡和灰度台阶过度处会产生双边沿响应 （4）二阶导数的符号可以确定边缘的过渡是从亮到暗还是从暗到亮 （5）选导数提取边沿之前最好是做下图像的平滑，导数对噪声比较敏感","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器视觉","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"图像","slug":"图像","permalink":"https://blog.mhuig.top/tags/%E5%9B%BE%E5%83%8F/"},{"name":"数学模型","slug":"数学模型","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B/"}]},{"title":"进程的描述","slug":"others/OS/进程的描述","date":"2019-09-12T01:54:49.000Z","updated":"2019-09-12T01:54:49.000Z","comments":true,"path":"posts/253269c2.html","link":"","permalink":"https://blog.mhuig.top/posts/253269c2.html","excerpt":"为了能使程序并发执行，并且可以对并发执行的程序加以描述和控制，人们引入了“进程”的概念。","text":"为了能使程序并发执行，并且可以对并发执行的程序加以描述和控制，人们引入了“进程”的概念。 进程的特征和定义结构特征进程实体是由程序段、数据段及进程、控制块（PCB）三部分组成. UNIX中将这三部分称为“进程映像”。 创建进程：创建进程实体中的进程控制块（PCB）。 撤销进程：撤销进程实体中的进程控制块（PCB）。 进程的特征 动态性 并发性 独立性 异步性","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"进程","slug":"进程","permalink":"https://blog.mhuig.top/tags/%E8%BF%9B%E7%A8%8B/"}]},{"title":"前驱图和程序执行","slug":"others/OS/前驱图和程序执行","date":"2019-09-12T01:45:28.000Z","updated":"2019-09-12T01:45:28.000Z","comments":true,"path":"posts/a81e94e9.html","link":"","permalink":"https://blog.mhuig.top/posts/a81e94e9.html","excerpt":"在多道程序环境中，允许多个程序并发执行；程序本身是具体代码，不能反映程序的执行过程从而引入进程。进程是抽象的。作为资源分配和独立运行的基本单位是进程。操作系统所有的特征都是基于进程而体现的。","text":"在多道程序环境中，允许多个程序并发执行；程序本身是具体代码，不能反映程序的执行过程从而引入进程。进程是抽象的。作为资源分配和独立运行的基本单位是进程。操作系统所有的特征都是基于进程而体现的。 程序顺序执行及其特征程序顺序执行时的特征 顺序性：每个操作在上一操作结束后开始 封闭性：程序开始执行，其执行结果不受外界因素影响 可再现性：只要环境和初始条件相同，其执行结果一定相同前驱图 定义前驱图是一个有向无循环图（DAG)，用于描述进程之间执行的前后关系。 注意：前驱图中不能存在循环。 程序并发执行及其特征 间断性： 共享资源 -&gt; 相互制约 -&gt; 执行-暂停-执行 失去封闭性： 一个程序的执行受到其他程序的影响 不可再现性 ##结论： 并发是提高资源利用率的好方法，从而提高系统吞吐量，所以程序尽量并发执行。 1）串行是顺序执行； 2）并发是交叉使用设备； 3）并行使用多个处理机—更快。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"前驱图","slug":"前驱图","permalink":"https://blog.mhuig.top/tags/%E5%89%8D%E9%A9%B1%E5%9B%BE/"}]},{"title":"意识、脑与人工智能 十大科学问题","slug":"others/ml/意识、脑与人工智能十大科学问题","date":"2019-09-12T00:22:25.000Z","updated":"2019-09-12T00:22:25.000Z","comments":true,"path":"posts/c270974.html","link":"","permalink":"https://blog.mhuig.top/posts/c270974.html","excerpt":"2018年9月，浙江大学发布“双脑计划”，布局脑科学与人工智能的会聚研究，聚集全校生命科学、信息科学、物质科学和哲学社会科学众多领域的专家学者，开启探索脑认知、意识及智能的本质和规律。2019年4月，浙江大学召开“意识、脑与人工智能”圆桌论坛，吴朝晖院士、段树民院士与倪梁康教授（文科资深教授）分别围绕“意识”问题，从计算机科学、脑科学、哲学角度作主旨报告，提出了一系列具有挑战性的跨学科问题。在此基础上，浙江大学“双脑计划”相关团队组织哲学、计算机科学、神经与脑科学、心理学、社会学等领域专家，聚焦意识与脑、意识与人工智能方面的重大问题，经过反复讨论、不断碰撞、深入凝练，最终提出了十大具有前沿性、挑战性的科学问题，旨在引领国内外学术界的思考，推动意识、脑与人工智能交叉领域的研究。","text":"2018年9月，浙江大学发布“双脑计划”，布局脑科学与人工智能的会聚研究，聚集全校生命科学、信息科学、物质科学和哲学社会科学众多领域的专家学者，开启探索脑认知、意识及智能的本质和规律。2019年4月，浙江大学召开“意识、脑与人工智能”圆桌论坛，吴朝晖院士、段树民院士与倪梁康教授（文科资深教授）分别围绕“意识”问题，从计算机科学、脑科学、哲学角度作主旨报告，提出了一系列具有挑战性的跨学科问题。在此基础上，浙江大学“双脑计划”相关团队组织哲学、计算机科学、神经与脑科学、心理学、社会学等领域专家，聚焦意识与脑、意识与人工智能方面的重大问题，经过反复讨论、不断碰撞、深入凝练，最终提出了十大具有前沿性、挑战性的科学问题，旨在引领国内外学术界的思考，推动意识、脑与人工智能交叉领域的研究。 “意识、脑与人工智能”十大科学问题一、意识的生物学基础是什么？意识曾仅是哲学家的研究领域，但随着神经科学发展，科学家逐渐参与到意识本质的研究中。目前大部分观点认为，意识产生的物质基础是神经元，其生物学基础是脑中多个神经网络间的相互作用；也有研究认为意识的产生由相对独立的脑结构（称为意识开关）来主导。意识的生物学基础是什么，及其衍生出来的一系列问题有待进一步探究。例如，意识产生的物质基础是否唯一，能否在神经元以外的物质载体上制造出意识等。 二、“人工意识”是否可能？从人工智能向人工意识的发展，必须考虑将人工情感和人工意欲的因素纳入人工意识和人工心灵系统的可能性。可尝试通过对神经回路的复杂性的把控来解决所有类型的意识涌现（表象、情感、意志）的复杂性，并在神经系统中找到作为意识之自身觉知（qualia）的对应项。 三、机器如何理解人类的情感表达？在人机共生社会，需要解决机器人与人类的自然交互问题，以使得机器人可以真正融入人们的生活，产生共情、共鸣和自然的社会行为。其中一个重要的挑战是机器如何理解人类的情感表达。 四、强人工智能的心理机制是什么？弱人工智能在解决特定领域问题中，展现出了强大到可以比肩甚至超越人类的能力，但也暴露出通用性弱、学习效率低等一系列问题。解决这些问题需要回归强人工智能的“初心”，即研究人类智能的心理机制是什么，探索人类为何能利用有限的算力实现通用智能、如何在小数据条件下完成高效学习等问题。 五、意识的信息机制是什么？意识是指一个人体验自身存在的能力，而不仅仅是记录或者像机器人那样对刺激做出反应。研究意识的信息处理机制，需要重点关注信息处理的主观性（subjective）、结构性（structured）、特有性（specific）、统一性（unified）和确定性（definitive）等问题。 六、脑机融合能否实现超级智能？脑机融合是基于脑机接口技术，实现脑与机的双向交互、相互适应及协同工作，最终达到生物智能和机器智能的融合，其目标是实现更强大的智能形态。鉴于机器智能与人类智能的互补性，如何实现生物智能和机器智能的互联互通，融合各自所长，创造出性能更强的智能形态是核心问题。 七、情绪情感的脑机制是什么？情绪是个体对一定程度的复杂情况做出反应的特定状态。情绪情感的产生涉及感觉、知觉、动机、奖赏、评估、感觉-行为转换等多种脑功能，并参与修饰和调控记忆及相关认知过程。人类智慧的形成和复杂社会体系的建立，均与情绪情感程序的进化和固化有关。情绪情感相关精神疾病也在持续和广泛地困扰人类社会。因此，研究情绪情感的脑机制是脑科学研究领域最令人兴奋的方向之一，其研究成果也将为相关精神疾病的诊断和治疗提供新的策略和手段。 八、学习的生物学基础是什么？动物需要适应环境变化，而学习就是神经系统把环境信息转变成经验的编码过程，与学习密切相关的记忆则是神经系统对这些经验的存储和提取的过程。研究学习记忆的神经生物学机制是神经科学领域至关重要的研究方向，也是阐明认知功能障碍的关键。 九、潜意识的脑科学机制是什么？潜意识指“已然发生但并未达到意识水平的心理活动过程或内容”，被认为是最复杂的心理现象，可能成为阐明人类意识大脑机制的突破口。随着认知神经科学和脑科学等交叉学科研究的发展，以及脑图谱技术、基因技术的进步，对潜意识的脑科学机制研究可能会有更大的突破。 十、人类决策的脑处理机制是什么？决策脑机制的研究日益受到重视，但决策偏好的神经机理还远未被揭开。系统探究决策脑机制，不仅有助于揭示决策者价值权衡过程的神经基础，还能为基于神经信号预测人的决策倾向，以及诊治决策异常相关脑疾病提供科学研究依据。 参考文献浙江大学面向2030的学科会聚研究计划（创新2030计划）“意识、脑与人工智能”十大科学问题","categories":[{"name":"会议报告","slug":"会议报告","permalink":"https://blog.mhuig.top/categories/%E4%BC%9A%E8%AE%AE%E6%8A%A5%E5%91%8A/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://blog.mhuig.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"脑科学","slug":"脑科学","permalink":"https://blog.mhuig.top/tags/%E8%84%91%E7%A7%91%E5%AD%A6/"}]},{"title":"Glob表达式","slug":"Linux/CentOS/glob表达式","date":"2019-09-11T13:05:18.000Z","updated":"2019-09-11T13:05:18.000Z","comments":true,"path":"posts/55b264de.html","link":"","permalink":"https://blog.mhuig.top/posts/55b264de.html","excerpt":"glob是shell使用的路径匹配符，类似于正则表达式，但是与正则表达式不完全相同。在linux操作中如文件匹配等等其实已经使用了glob通配符。由于其在路径匹配方面的强大，其他语言也有相应的实现。我在使用基于node的gulp时遇到glob匹配文件路径，于是顺便整理一下glob的基础语法和使用。","text":"glob是shell使用的路径匹配符，类似于正则表达式，但是与正则表达式不完全相同。在linux操作中如文件匹配等等其实已经使用了glob通配符。由于其在路径匹配方面的强大，其他语言也有相应的实现。我在使用基于node的gulp时遇到glob匹配文件路径，于是顺便整理一下glob的基础语法和使用。 glob表达式(glob expressions)通配符：1234567891011121314151617* 匹配文件路径中的0个或多个字符，但**不会匹配路径分隔符，除非路径分隔符出现在末尾。** 匹配路径中的0个或多个目录及其子目录，如果出现在末尾，也能匹配文件。? 匹配文件路径中的一个字符(不会匹配路径分隔符)。[...] 匹配方括号中出现的字符中的任意一个，当方括号中第一个字符为 ·^ 或 ! 时，则表示不匹配方括号中出现的其他字符中的任意一个。!(pattern|pattern|pattern) 匹配任何与括号中给定的任一参数都不匹配的。?(pattern|pattern|pattern) 匹配括号中给定的任一参数0次或1次。+(pattern|pattern|pattern) 匹配括号中给定的任一参数1次或多次。*(pattern|pattern|pattern) 匹配括号中给定的任一参数0次或多次。@(pattern|pattern|pattern) 匹配括号中给定的任一参数1次。 用实例来加深理解： 123456789101112131415161718192021* 能匹配 a.js , x.y , abc , abc/ ，但不能匹配 a/b.js*.* 能匹配 a.js , style.css , a.b , x.y*/*/*.js 能匹配 a/b/c.js , x/y/z.js ，不能匹配 a/b.js , a/b/c/d.js** 能匹配 abc , a/b.js , a/b/c.js , x/y/z , x/y/z/a.b ，能用来匹配所有的目录和文件**/*.js 能匹配 foo.js , a/foo.js , a/b/foo.js , a/b/c/foo.jsa/**/z 能匹配 a/z , a/b/z , a/b/c/z , a/d/g/h/j/k/za/**b/z 能匹配 a/b/z , a/sb/z ，但不能匹配 a/x/sb/z ，因为只有单 ** 单独出现才能匹配多级目录?.js 能匹配 a.js , b.js , c.jsa?? 能匹配 a.b , abc ，但不能匹配 ab/ ，因为它不会匹配路径分隔符[xyz].js 只能匹配 x.js , y.js , z.js ，不会匹配 xy.js , xyz.js 等，整个中括号只代表一个字符[^xyz].js 能匹配 a.js , b.js , c.js 等，不能匹配 x.js , y.js , z.js 当有多种匹配模式时可以使用数组： 12// 使用数组的方式来匹配多种文件gulp.src([ &#x27;js/*.min.js&#x27;, &#x27;sass/*.min.css&#x27; ]) 使用数组的方式还有一个好处就是可以很方便的使用排除模式，在数组中的单个匹配模式前加上 ! 即是排除模式，它会在匹配的结果中排除这个匹配，要注意一点的是不能在数组中的第一个元素中使用排除模式： 123&#x2F;&#x2F; 使用数组的方式来匹配多种文件gulp.src([&#39;*.js&#39;,&#39;!b*.js&#39;]) &#x2F;&#x2F; 匹配所有js文件，但排除掉以b开头的js文件gulp.src([&#39;!b*.js&#39;,*.js]) &#x2F;&#x2F; 不会排除任何文件，因为排除模式不能出现在数组的第一个元素中 此外，还可以使用展开模式。展开模式以花括号作为定界符，根据它里面的内容，会展开为多个模式，最后匹配的结果为所有展开的模式相加起来得到的结果。展开的例子如下： a{b,c}d 会展开为 abd,acd a{b,}c 会展开为 abc,ac a{0..3}d 会展开为 a0d , a1d , a2d , a3d a{b,c{d,e}f}g 会展开为 abg , acdfg , acefg a{b,c}d{e,f}g 会展开为 abdeg , acdeg , abdeg , abdfg","categories":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"shell","slug":"linux/shell","permalink":"https://blog.mhuig.top/categories/linux/shell/"}],"tags":[{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://blog.mhuig.top/tags/shell/"},{"name":"通配符","slug":"通配符","permalink":"https://blog.mhuig.top/tags/%E9%80%9A%E9%85%8D%E7%AC%A6/"},{"name":"glob表达式","slug":"glob表达式","permalink":"https://blog.mhuig.top/tags/glob%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"操作系统引论","slug":"others/OS/操作系统引论","date":"2019-09-11T11:03:24.000Z","updated":"2019-09-11T11:03:24.000Z","comments":true,"path":"posts/f9a25cd7.html","link":"","permalink":"https://blog.mhuig.top/posts/f9a25cd7.html","excerpt":"操作系统（OS）是配置在计算机硬件上的第一层软件，是对硬件系统的首次扩充。","text":"操作系统（OS）是配置在计算机硬件上的第一层软件，是对硬件系统的首次扩充。 操作系统的定义OS是一组控制和管理计算机硬件和软件资源，合理地对各类作业进行调度（合理地组织计算机工作），以及方便用户使用的程序的集合 操作系统的目标和作用操作系统的目标 方便性* 有效性* 提高系统资源利用率 提高系统吞吐量 可扩充性 开放性 遵循世界标准规范 方便性和有效性是设计操作系统时最重要的两个目标。 操作系统的作用 OS作为用户与计算机硬件系统之间的接口（用户的角度） 三种方式使用计算机： 命令行方式 系统调用方式 图标窗口方式 OS作为计算机系统资源的管理者（资源管理角度） 对四类资源进行管理： 处理机管理 存储器管理 I/O设备管理 文件管理 资源管理包含两种资源共享的使用方法： 分时 多个用户分时地使用该资源 空分 存储资源的空间可以被多个用户共同以分割的方式占用。 OS实现了对计算机资源的抽象（扩充机器） 裸机 虚拟机/扩展机 推动操作系统发展的主要动力 不断提高计算机资源利用率 方便用户 机器的不断更新换代 计算机体系结构的不断发展 不断提出新的应用需求 操作系统的发展过程无操作系统的计算机系统 人工操作方式 缺点： 用户独占全机 CPU等待人工操作 人机矛盾 CPU与I/O设备之间速度不匹配的矛盾 工作效率低 脱机输入/输出（Off-Line I/O）方式 程序和数据的输入和输出都是在外围机的控制之下完成的， 即：程序和数据的输入和输出是在脱离主机的情况下进行 脱机I/O的主要优点 减少了CPU的空闲时间 提高I/O速度，缓和了CPU和I/O设备间不匹配的矛盾 单道批处理操作系统 单道批处理系统的处理过程 批处理系统旨在提高系统资源的利用率和系统吞吐量. 特征: 自动性 顺序性 单道性 单道批处理系统的缺点最主要缺点：系统中的资源得不到充分利用 多道批处理操作系统 多道程序设计的基本概念： 内存同时驻留多道程序(作业)，处理机(单处理机)以交替的方式同时处理多道程序。 宏观：已有多道程序开始运行且尚未结束； 微观：某一时刻处理机只运行某道作业。 好处： 提高CPU的利用率； 可提高内存和I/O设备的利用率； 增加系统吞吐量。 能提高吞吐量的原因： 使CPU和资源保持“忙碌”状态； 仅当作业完成或运行不下去时才进行切换，系统开销小。 特征 多道性 无序性：作业完成的先后顺序和他们进入内存的顺序并无严格的对应关系 调度性: A、作业调度 B、进程调度 优点： 资源利用率高 系统吞吐量大 缺点： 平均周转时间长 作业的周转时间是指从作业进入系统开始，直至其完成并退出系统为止所经历的时间。 无交互能力 推动多道批处理系统形成和发展的主要动力是提高资源利用率和系统吞吐量； 分时操作系统 推动分时系统形成和发展的主要动力，则是用户的需求（人——机交互）。 工作方式 一台主机连接了若干个终端；每个终端有一个用户在使用； 交互式的向系统提出命令请求； 系统接受每个用户的命令采用时间片轮转方式处理服务请求并通过交互方式在终端上向用户显示结果，用户根据上步结果发出下道命令。 关键问题: 及时接收 及时处理 作业直接进入内存，在内存才能处理； 采用轮转运行方式。 不允许一个作业长期占用处理机； 规定每个作业只能运行很短的时间，使每个用户及时与自己的作业交互，从而用户请求得到及时响应。 特点 多路性：即同时性（宏观的同时） 交互性 独立性：用户好像独占主机 及时性 实时操作系统 定义 是指系统能及时响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。 实时系统与分时系统特征的比较 多路性 独立性 及时性 交互性 可靠性 操作系统的基本特性 并发性 间断性 失去封闭性 不可再现性 共享性 互斥共享（临界资源） 同时访问（eg：同时读磁盘） 虚拟技术 时分复用技术 虚拟处理机技术 虚拟设备技术 空分复用技术 虚拟磁盘技术 虚拟存储器技术（内存） 异步性 OS结构设计 无结构OS 模块化OS 分层式OS 微内核OS 微内核技术：是指精心设计的、能实现现代操作系统核心功能的小型内核，运行在核心态，开机后常驻内存。 常驻内存的好处：因为CPU只访问内存，速度快、效率高。 微内核OS优点： 提高系统的可扩展性 增强系统的可靠性 可移植性强 提供对分布式系统的支持 融入面向对象技术","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"hexo命令及Markdown语法","slug":"web/hexo命令及Markdown语法","date":"2019-09-05T03:33:59.000Z","updated":"2019-09-05T03:33:59.000Z","comments":true,"path":"posts/f8d2d5ec.html","link":"","permalink":"https://blog.mhuig.top/posts/f8d2d5ec.html","excerpt":"hexo是使用Markdown编辑文章的，我写的这些文章也都是用这种标记语言完成的。所以，我们先从Markdown说起。","text":"hexo是使用Markdown编辑文章的，我写的这些文章也都是用这种标记语言完成的。所以，我们先从Markdown说起。 前言你可以使用vim工具直接编辑md文件，也可以用记事本打开md文件编辑你的文章，也可以Markdown的编辑器编写，有很多在线的编辑器，何有不少客户端的编辑器. 什么是MarkdownMarkdown 是一种轻量级标记语言，创始人为约翰·格鲁伯和亚伦·斯沃茨。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML文档”。 ——维基百科 先简单介绍一下，Markdown的语法，具体怎么用，我相信大家一看例文就马上明白了。 Markdown语法1、分段： 两个回车 2、换行 两个空格 + 回车 3、标题 # ~ ###### 井号的个数表示几级标题，即Markdown可以表示一级标题到六级标题 4、引用 &gt; 5、列表 * ， + ， - ， 1. ，选其中之一，注意后面有个空格 6、代码区块 四个空格 开头 7、链接 1[文字](链接地址) 文字 8、图片 1![](图片地址) &#x2F;&#x2F;图片地址可以是本地路径，也可以是网络地址 9、强调 1**文字** ， __文字__ ， _文字_ ， *文字* 文字 ， 文字 ， 文字 ， 文字 10、代码 1&#96;&#96;&#96;，&#96;&#96; hexo常用命令我们在前面的已经略微的接触了一些hexo的命令，如 hexo new “my blog” ， hexo server 等。下面来介绍一下我们经常会用到的hexo命令 1、新建 1hexo new &quot;my blog&quot; 新建的文件在 hexo/source/_posts/my-blog.md 2、生成静态页面 1hexo g 一般部署上去的时候都需要编译一下，编译后，会出现一个 public 文件夹，将所有的md文件编译成html文件 3、开启本地服务 1hexo s 这个命令，我之前已经用过了，开启本地hexo服务用的 4、部署 1hexo d 部署到git上的时候，需要用这个命令 5、清除 public 1hexo clean 当 source 文件夹中的部分资源更改过之后，特别是对文件进行了删除或者路径的改变之后，需要执行这个命令，然后重新编译。","categories":[{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"hexo","slug":"web/hexo","permalink":"https://blog.mhuig.top/categories/web/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://blog.mhuig.top/tags/hexo/"},{"name":"Markdown","slug":"markdown","permalink":"https://blog.mhuig.top/tags/markdown/"}]},{"title":"AT89S52 或 STC89C52RC 串口发送温湿度数据","slug":"others/51/AT89S52 或 STC89C52RC 串口发送温湿度数据","date":"2018-12-02T09:54:31.000Z","updated":"2018-12-02T09:54:31.000Z","comments":true,"path":"posts/3d7d3997.html","link":"","permalink":"https://blog.mhuig.top/posts/3d7d3997.html","excerpt":"AT89S52 或 STC89C52RC 串口发送温湿度数据","text":"AT89S52 或 STC89C52RC 串口发送温湿度数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224//****************************************************************////单片机 AT89S52 或 STC89C52RC//功能 串口发送温湿度数据 晶振 11.0592M 波特率 9600//硬件 sbit TXP口为通讯口连接DHT11,DHT11的电源和地连接单片机的电源和地，单片机串口加MAX232连接电脑//****************************************************************//#include &lt;STDIO.H&gt;#include &lt;reg51.h&gt;#include &lt;intrins.h&gt;//typedef unsigned char U8; /* defined for unsigned 8-bits integer variable 无符号8位整型变量 */typedef signed char S8; /* defined for signed 8-bits integer variable 有符号8位整型变量 */typedef unsigned int U16; /* defined for unsigned 16-bits integer variable 无符号16位整型变量 */typedef signed int S16; /* defined for signed 16-bits integer variable 有符号16位整型变量 */typedef unsigned long U32; /* defined for unsigned 32-bits integer variable 无符号32位整型变量 */typedef signed long S32; /* defined for signed 32-bits integer variable 有符号32位整型变量 */typedef float F32; /* single precision floating point variable (32bits) 单精度浮点数（32位长度） */typedef double F64; /* double precision floating point variable (64bits) 双精度浮点数（64位长度） *///#define uchar unsigned char#define uint unsigned int#define Data_0_time 4//----------------------------------------------////----------------IO口定义区--------------------////----------------------------------------------//sbit TXP = P2^0 ;//----------------------------------------------////----------------定义区--------------------////----------------------------------------------//U8 U8FLAG,k;U8 U8count,U8temp;U8 U8T_data_H,U8T_data_L,U8RH_data_H,U8RH_data_L,U8checkdata;U8 U8T_data_H_temp,U8T_data_L_temp,U8RH_data_H_temp,U8RH_data_L_temp,U8checkdata_temp;U8 U8comdata;U8 outdata[5]; //定义发送的字节数U8 indata[5];U8 count, count_r=0;U8 str[5]= &#123;&quot;RS232&quot;&#125;;U16 U16temp1,U16temp2;void SendData(U8 *a) &#123; outdata[0] = a[0]; outdata[1] = a[1]; outdata[2] = a[2]; outdata[3] = a[3]; outdata[4] = a[4]; count = 1; SBUF=outdata[0];&#125;void Delay(U16 j) &#123; U8 i; for(; j&gt;0; j--) &#123; for(i=0; i&lt;27; i++); &#125;&#125;void Delay_10us(void) &#123; U8 i; i--; i--; i--; i--; i--; i--;&#125;void COM(void) &#123; U8 i; for(i=0; i&lt;8; i++) &#123; U8FLAG=2; while((!TXP)&amp;&amp;U8FLAG++); Delay_10us(); Delay_10us(); Delay_10us(); U8temp=0; if(TXP) U8temp=1; U8FLAG=2; while((TXP)&amp;&amp;U8FLAG++); //超时则跳出for循环 if(U8FLAG==1) break; //判断数据位是0还是1 // 如果高电平高过预定0高电平值则数据位为 1 U8comdata&lt;&lt;=1; U8comdata|=U8temp; //0 &#125;//rof&#125;//--------------------------------//-----湿度读取子程序 ------------//--------------------------------//----以下变量均为全局变量--------//----温度高8位== U8T_data_H------//----温度低8位== U8T_data_L------//----湿度高8位== U8RH_data_H-----//----湿度低8位== U8RH_data_L-----//----校验 8位 == U8checkdata-----//----调用相关子程序如下----------//---- Delay();, Delay_10us();,COM();//--------------------------------void RH(void) &#123; //主机拉低18ms TXP=0; Delay(180); TXP=1; //总线由上拉电阻拉高 主机延时20us Delay_10us(); Delay_10us(); Delay_10us(); Delay_10us(); //主机设为输入 判断从机响应信号 TXP=1; //判断从机是否有低电平响应信号 如不响应则跳出，响应则向下运行 if(!TXP) &#123; //T ! U8FLAG=2; //判断从机是否发出 80us 的低电平响应信号是否结束 while((!TXP)&amp;&amp;U8FLAG++); U8FLAG=2; //判断从机是否发出 80us 的高电平，如发出则进入数据接收状态 while((TXP)&amp;&amp;U8FLAG++); //数据接收状态 COM(); U8RH_data_H_temp=U8comdata; COM(); U8RH_data_L_temp=U8comdata; COM(); U8T_data_H_temp=U8comdata; COM(); U8T_data_L_temp=U8comdata; COM(); U8checkdata_temp=U8comdata; TXP=1; //数据校验 U8temp=(U8T_data_H_temp+U8T_data_L_temp+U8RH_data_H_temp+U8RH_data_L_temp); if(U8temp==U8checkdata_temp) &#123; U8RH_data_H=U8RH_data_H_temp; U8RH_data_L=U8RH_data_L_temp; U8T_data_H=U8T_data_H_temp; U8T_data_L=U8T_data_L_temp; U8checkdata=U8checkdata_temp; &#125;//fi &#125;//fi&#125;//----------------------------------------------//main()功能描述: AT89C51 11.0592MHz 串口发//送温湿度数据,波特率 9600//----------------------------------------------void main() &#123; U8 i=0,j=0; //uchar str[6]=&#123;&quot;RS232&quot;&#125;; /* 系统初始化 */ TMOD = 0x20; //定时器T1使用工作方式2 TH1 = 253; // 设置初值 TL1 = 253; TR1 = 1; // 开始计时 SCON = 0x50; //工作方式1，波特率9600bps，允许接收 ES = 1; EA = 1; // 打开所以中断 TI = 0; RI = 0; SendData(str) ; //发送到串口 //Delay(1); //延时100US（12M晶振) while(1) &#123; //------------------------ //调用温湿度读取子程序 RH(); //串口显示程序 //-------------------------- str[0]=U8RH_data_H; str[1]=U8RH_data_L; str[2]=U8T_data_H; str[3]=U8T_data_L; str[4]=U8checkdata; SendData(str) ; //发送到串口 //读取模块数据周期不易小于 2S Delay(20000); &#125;//elihw&#125;// mainvoid RSINTR() interrupt 4 using 2 &#123; U8 InPut3; if(TI==1) &#123; //发送中断 TI=0; if(count!=5) &#123; //发送完5位数据 SBUF= outdata[count]; count++; &#125; &#125; if(RI==1) &#123; //接收中断 InPut3=SBUF; indata[count_r]=InPut3; count_r++; RI=0; if (count_r==5) &#123; //接收完4位数据 //数据接收完毕处理。 count_r=0; str[0]=indata[0]; str[1]=indata[1]; str[2]=indata[2]; str[3]=indata[3]; str[4]=indata[4]; P0=0; &#125; &#125;&#125;","categories":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/categories/51/"}],"tags":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/tags/51/"},{"name":"温湿度","slug":"温湿度","permalink":"https://blog.mhuig.top/tags/%E6%B8%A9%E6%B9%BF%E5%BA%A6/"}]},{"title":"HC-SR04 超声波测距模块 串口 程序","slug":"others/51/HC-SR04 超声波测距模块 串口 程序","date":"2018-12-02T09:52:51.000Z","updated":"2018-12-02T09:52:51.000Z","comments":true,"path":"posts/eb168c8b.html","link":"","permalink":"https://blog.mhuig.top/posts/eb168c8b.html","excerpt":"HC-SR04 超声波测距模块 串口 程序","text":"HC-SR04 超声波测距模块 串口 程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104/***********************************************************************************************************///HC-SR04 超声波测距模块 串口 程序//晶振：11.0592//接线：模块TRIG接 P1.2 ECH0 接P1.1//串口波特率9600//Atmel AT89C52 C51/***********************************************************************************************************/#include &lt;AT89X51.H&gt;#include &lt;intrins.h&gt;#include &lt;STDIO.H&gt;#define uchar unsigned char#define uint unsigned int#define RX P1_1#define TX P1_2unsigned int time=0;unsigned int timer=0;float S=0;bit flag =0;/********************************************************/void Conut(void) &#123; time=TH0*256+TL0; TH0=0; TL0=0; S=(time*1.87)/100; //算出来是CM if(flag==1) &#123; //超出测量 flag=0; printf(&quot;-----\\n&quot;); &#125; printf(&quot;S=%fcm\\n&quot;,S);&#125;/********************************************************/void delayms(unsigned int ms) &#123; unsigned char i=100,j; for(; ms; ms--) &#123; while(--i) &#123; j=10; while(--j); &#125; &#125;&#125;/********************************************************/void zd0() interrupt 1 &#123; //T0中断用来计数器溢出,超过测距范围 flag=1; //中断溢出标志&#125;/********************************************************/void StartModule() &#123; //T1中断用来扫描数码管和计800MS启动模块 TX=1; //800MS 启动一次模块 _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); TX=0;&#125;/********************************************************/void main(void) &#123; TMOD=0x21; //设T0为方式1，GATE=1； SCON=0x50; TH1=0xFD; TL1=0xFD; TH0=0; TL0=0; TR0=1; ET0=1; //允许T0中断 TR1=1; //开启定时器 TI=1; EA=1; //开启总中断 while(1) &#123; StartModule(); while(!RX); //当RX为零时等待 TR0=1; //开启计数 while(RX); //当RX为1计数并等待 TR0=0; //关闭计数 Conut(); //计算 delayms(100); //100MS &#125;&#125;","categories":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/categories/51/"}],"tags":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/tags/51/"},{"name":"超声波","slug":"超声波","permalink":"https://blog.mhuig.top/tags/%E8%B6%85%E5%A3%B0%E6%B3%A2/"}]},{"title":"51按键时钟","slug":"others/51/51按键时钟","date":"2018-12-02T09:51:32.000Z","updated":"2018-12-02T09:51:32.000Z","comments":true,"path":"posts/d07ffe1c.html","link":"","permalink":"https://blog.mhuig.top/posts/d07ffe1c.html","excerpt":"51按键时钟","text":"51按键时钟 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/* * 按键时钟 秒表，可以通过按键开始或是停止 */#include&lt;reg52.h&gt;#define uchar unsigned charsbit key =P3 ^ 3; //按键uchar counter=0,tmp,second=0,minute=0, change = 1;int led[]= &#123;0xc0, 0xf9, 0xa4, 0xb0, 0x99, 0x92, 0x82, 0xf8, 0x80, 0x90&#125;; //数字0-9int _led[]= &#123;0x40, 0x79, 0x24, 0x30, 0x19, 0x12, 0x02, 0x78, 0x00, 0x10&#125;;void clockrun();void main() &#123; //设置TMOD寄存器 TMOD=0X01; //设置TMOD寄存器 TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; EA=1; //开 中断 ET0=1; TR0=1; if(key==0) &#123;//按键按下 while(1) &#123; clockrun(); &#125; &#125;&#125;void zhongduan()interrupt 1 &#123; TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; TF0=0; TR0=1; counter++; if(counter==200) &#123; counter=0; second++; if(second==60) &#123; second=0; minute++; &#125; &#125; change = 1;&#125;void clockrun() &#123; tmp=counter%4; switch(tmp) &#123; case 0: P2 = 0x7f; P0 = led[second%10]; break; case 1: P2 = 0xbf; P0 = led[second/10]; break; case 2: P2 = 0xdf; P0 = _led[minute%10]; break; case 3: P2 = 0xef; P0 = led[minute/10]; break; &#125;&#125;","categories":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/categories/51/"}],"tags":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/tags/51/"},{"name":"按键时钟","slug":"按键时钟","permalink":"https://blog.mhuig.top/tags/%E6%8C%89%E9%94%AE%E6%97%B6%E9%92%9F/"}]},{"title":"51 秒表","slug":"others/51/51 秒表","date":"2018-12-02T09:50:24.000Z","updated":"2018-12-02T09:50:24.000Z","comments":true,"path":"posts/129a9db5.html","link":"","permalink":"https://blog.mhuig.top/posts/129a9db5.html","excerpt":"51 单片机 秒表","text":"51 单片机 秒表 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** 秒表*/#include&lt;reg52.h&gt;#define uchar unsigned charuchar counter=0,tmp,second=0,minute=0, change = 1;int led[]= &#123;0xc0, 0xf9, 0xa4, 0xb0, 0x99, 0x92, 0x82, 0xf8, 0x80, 0x90&#125;; //数字0-9int _led[]= &#123;0x40, 0x79, 0x24, 0x30, 0x19, 0x12, 0x02, 0x78, 0x00, 0x10&#125;;void main() &#123; //设置TMOD寄存器 TMOD=0X01; //设置TMOD寄存器 TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; EA=1; //开 中断 ET0=1; TR0=1; while(1) &#123; tmp=counter%4; switch(tmp) &#123; case 0: P2 = 0x7f; P0 = led[second%10]; break; case 1: P2 = 0xbf; P0 = led[second/10]; break; case 2: P2 = 0xdf; P0 = _led[minute%10]; break; case 3: P2 = 0xef; P0 = led[minute/10]; break; &#125; &#125;&#125;void zhongduan()interrupt 1 &#123; TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; TF0=0; TR0=1; counter++; if(counter==200) &#123; counter=0; second++; if(second==60) &#123; second=0; minute++; &#125; &#125; change = 1;&#125;","categories":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/categories/51/"}],"tags":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/tags/51/"},{"name":"秒表","slug":"秒表","permalink":"https://blog.mhuig.top/tags/%E7%A7%92%E8%A1%A8/"}]},{"title":"pyc文件反编译到Python源码","slug":"Python/pyc文件反编译到Python源码","date":"2018-12-01T08:20:24.000Z","updated":"2018-12-01T08:20:24.000Z","comments":true,"path":"posts/14fa5bba.html","link":"","permalink":"https://blog.mhuig.top/posts/14fa5bba.html","excerpt":"pyc文件反编译到Python源码","text":"pyc文件反编译到Python源码 使用uncompyle 项目地址：https://github.com/wibiti/uncompyle2 注： 按照官方文档的说法应该是只支持python 2.7，其他版本我也没有测试 安装最方便的就是使用pip安装 1pip install uncompyle 使用方法查看帮助1uncompyle6 --help 将models.pyc反编译成py文件1uncompyle6 models.pyc &gt; models.py 将当前文件夹中所有的pyc文件反编译成后缀名为.pyc_dis的源文件1uncompile -o . *.pyc","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"反编译","slug":"python/反编译","permalink":"https://blog.mhuig.top/categories/python/%E5%8F%8D%E7%BC%96%E8%AF%91/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"反编译","slug":"反编译","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"}]},{"title":"Python RailFenceCipher","slug":"Python/密码/Python RailFenceCipher","date":"2018-12-01T07:59:17.000Z","updated":"2018-12-01T07:59:17.000Z","comments":true,"path":"posts/91223731.html","link":"","permalink":"https://blog.mhuig.top/posts/91223731.html","excerpt":"Python RailFenceCipher","text":"Python RailFenceCipher RailFenceCipher.py 12345678910111213141516171819202122232425#coding=utf-8def railFenceCipher(): e = input() elen = len(e) field=[] for i in range(2,elen): if(elen%i==0): field.append(i) for f in field: b = int(elen / f) result = &#123;x:&#x27;&#x27; for x in range(b)&#125; for i in range(elen): a = i % b; result.update(&#123;a:result[a] + e[i]&#125;) d = &#x27;&#x27; for i in range(b): d = d + result[i] print (d.lower())if __name__ == &#x27;__main__&#x27;: try: while True: railFenceCipher() except EOFError: exit()","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python QRcode","slug":"Python/密码/Python QRcode","date":"2018-12-01T06:39:17.000Z","updated":"2018-12-01T06:39:17.000Z","comments":true,"path":"posts/6980c27e.html","link":"","permalink":"https://blog.mhuig.top/posts/6980c27e.html","excerpt":"Python QRcode","text":"Python QRcode enQRcode.py 1234567891011121314151617181920212223242526272829303132import qrcodeimport osimport sysimport time#pip install zxing解析库，还需要安装PIL，pillow和qrCode库QRImagePath = os.getcwd() + &#x27;/qrcode.png&#x27; #临时存储位置qr = qrcode.QRCode( version=1, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=10, border=2,) #设置图片格式print(&quot;input:QRcode image:&quot;)data = input() #运行时输入数据qr.add_data(data)qr.make(fit=True)img = qr.make_image()img.save(&#x27;qrcode.png&#x27;) #生成图片 if sys.platform.find(&#x27;darwin&#x27;) &gt;= 0: os.system(&#x27;open %s&#x27; % QRImagePath) elif sys.platform.find(&#x27;linux&#x27;) &gt;= 0: os.system(&#x27;xdg-open %s&#x27; % QRImagePath)else: os.system(&#x27;call %s&#x27; % QRImagePath) time.sleep(5) #间隔5个单位#os.remove(QRImagePath) #删除图片 deQRcode.py 1234567891011121314151617181920212223242526272829303132333435363738import osimport loggingfrom PIL import Imageimport zxing #导入解析包import randomlogger = logging.getLogger(__name__) #记录数据if not logger.handlers: logging.basicConfig(level = logging.INFO)DEBUG = (logging.getLevelName(logger.getEffectiveLevel()) == &#x27;DEBUG&#x27;) #记录调式过程# 在当前目录生成临时文件，规避java的路径问题def ocr_qrcode_zxing(filename): img = Image.open(filename) ran = int(random.random() * 100000) #设置随机数据的大小 img.save(&#x27;%s%s.jpg&#x27; % (os.path.basename(filename).split(&#x27;.&#x27;)[0], ran)) zx = zxing.BarCodeReader() #调用zxing二维码读取包 data = &#x27;&#x27; zxdata = zx.decode(&#x27;%s%s.jpg&#x27; % (os.path.basename(filename).split(&#x27;.&#x27;)[0], ran)) #图片解码# 删除临时文件 os.remove(&#x27;%s%s.jpg&#x27; % (os.path.basename(filename).split(&#x27;.&#x27;)[0], ran)) if zxdata: logger.debug(u&#x27;zxing识别二维码:%s,内容: %s&#x27; % (filename, zxdata)) data = zxdata else: logger.error(u&#x27;识别zxing二维码出错:%s&#x27; % (filename)) img.save(&#x27;%s-zxing.jpg&#x27; % filename) return data #返回记录的内容if __name__ == &#x27;__main__&#x27;: print(&quot;input:QRcode path:&quot;) filename = input() # zxing二维码识别 ltext = ocr_qrcode_zxing(filename) #将图片文件里的信息转码放到ltext里面 logger.info(u&#x27;[%s]Zxing二维码识别:[%s]!!!&#x27; % (filename, ltext)) #记录文本信息 print(ltext) #打印出二维码名字","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python Playfair","slug":"Python/密码/Python Playfair","date":"2018-12-01T05:29:17.000Z","updated":"2018-12-01T05:29:17.000Z","comments":true,"path":"posts/b4a5318a.html","link":"","permalink":"https://blog.mhuig.top/posts/b4a5318a.html","excerpt":"Python Playfair","text":"Python Playfair Playfair.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143#########################Playfair密码##########################约定1：若明文字母数量为奇数，在明文末尾添加一个&#x27;Z&#x27;#约定2：&#x27;I&#x27;作为&#x27;J&#x27;来处理#字母表letter_list=&#x27;ABCDEFGHJKLMNOPQRSTUVWXYZ&#x27;#密码表T_letter=[&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;,&#x27;&#x27;]#根据密钥建立密码表def Create_Matrix(key): key=Remove_Duplicates(key) #移除密钥中的重复字母 key=key.replace(&#x27; &#x27;,&#x27;&#x27;) #去除密钥中的空格 for ch in letter_list: #根据密钥获取新组合的字母表 if ch not in key: key+=ch j=0 for i in range(len(key)): #将新的字母表里的字母逐个填入密码表中，组成5*5的矩阵 T_letter[j]+=key[i] #j用来定位字母表的行 if 0==(i+1)%5: j+=1#移除字符串中重复的字母def Remove_Duplicates(key): key=key.upper() #转成大写字母组成的字符串 _key=&#x27;&#x27; for ch in key: if ch==&#x27;I&#x27;: ch=&#x27;J&#x27; if ch in _key: continue else: _key+=ch return _key #获取字符在密码表中的位置def Get_MatrixIndex(ch): for i in range(len(T_letter)): for j in range(len(T_letter)): if ch==T_letter[i][j]: return i,j #i为行，j为列 #加密def Encrypt(plaintext,T_letter): ciphertext=&#x27;&#x27; if len(plaintext) % 2 !=0: #如果新的明文长度为奇数，在其末尾添上&#x27;Z&#x27; plaintext+=&#x27;Z&#x27; i=0 while i&lt;len(plaintext): #对明文进行遍历 if True==plaintext[i].isalpha(): #如果是明文是字母的话， j=i+1 #则开始对该字母之后的明文进行遍历， while j&lt;len(plaintext): #直到遍历到字母，进行加密 if True==plaintext[j].isalpha(): if &#x27;I&#x27;==plaintext[i].upper(): # x=Get_MatrixIndex(&#x27;J&#x27;) # else: # x=Get_MatrixIndex(plaintext[i].upper()) #对字符在密码表中的坐标 if &#x27;I&#x27;==plaintext[j].upper(): #进行定位,同时将&#x27;I&#x27;作为 y=Get_MatrixIndex(&#x27;J&#x27;) #&#x27;J&#x27;来处理 else: # y=Get_MatrixIndex(plaintext[j].upper()) # if x[0]==y[0]: #如果在同一行 ciphertext+=T_letter[x[0]][(x[1]+1)%5]+T_letter[y[0]][(y[1]+1)%5] elif x[1]==y[1]: #如果在同一列 ciphertext+=T_letter[(x[1]+1)%5][x[0]]+T_letter[(y[1]+1)%5][y[0]] else: #如果不同行不同列 ciphertext+=T_letter[x[0]][y[1]]+T_letter[y[0]][x[1]] break; #每组明文对加密完成后，结束本次对明文的遍历 j+=1 i=j+1 #每次对明文的遍历是从加密过后的明文的后一个明文开始的,结束本次循环 continue else: ciphertext+=plaintext[i] #如果明文不是字母，直接加到密文上 i+=1 return ciphertext #解密def Decrypt(ciphertext,T_letter): plaintext=&#x27;&#x27; if len(ciphertext) % 2 !=0: #如果新的密文长度为奇数，在其末尾添上&#x27;Z&#x27; ciphertext+=&#x27;Z&#x27; i=0 while i&lt;len(ciphertext): #对密文进行遍历 if True==ciphertext[i].isalpha(): #如果是密文是字母的话， j=i+1 #则开始对该字母之后的密文进行遍历， while j&lt;len(ciphertext): #直到遍历到字母，进行解密 if True==ciphertext[j].isalpha(): if &#x27;I&#x27;==ciphertext[i].upper(): # x=Get_MatrixIndex(&#x27;J&#x27;) # else: # x=Get_MatrixIndex(ciphertext[i].upper()) #对字符在密码表中的坐标 if &#x27;I&#x27;==ciphertext[j].upper(): #进行定位,同时将&#x27;I&#x27;作为 y=Get_MatrixIndex(&#x27;J&#x27;) #&#x27;J&#x27;来处理 else: # y=Get_MatrixIndex(ciphertext[j].upper()) # if x[0]==y[0]: #如果在同一行 plaintext+=T_letter[x[0]][(x[1]-1)%5]+T_letter[y[0]][(y[1]-1)%5] elif x[1]==y[1]: #如果在同一列 plaintext+=T_letter[(x[1]-1)%5][x[0]]+T_letter[(y[1]-1)%5][y[0]] else: #如果不同行不同列 plaintext+=T_letter[x[0]][y[1]]+T_letter[y[0]][x[1]] break; #每组密文对解密完成后，结束本次对密文的遍历 j+=1 i=j+1 #每次对密文的遍历是从解密过后的密文的后一个密文开始的,结束本次循环 continue else: plaintext+=ciphertext[i] #如果密文不是字母，直接加到明文上 i+=1 return plaintext #主函数if __name__==&#x27;__main__&#x27;: print(&quot;加密请按D,解密请按E:&quot;) user_input=input(); while(user_input!=&#x27;D&#x27; and user_input!=&#x27;E&#x27;):#输入合法性检测 print(&quot;输入有误!请重新输入:&quot;) user_input=input() print(&#x27;请输入密钥，密钥由英文字母组成:&#x27;) key=input() Create_Matrix(key) #建立密码表 if user_input==&#x27;D&#x27;: #加密 print(&#x27;请输入明文:&#x27;) plaintext=input() print(&quot;密文为:\\n%s&quot; % Encrypt(plaintext,T_letter)) else: #解密 print(&#x27;请输入密文:&#x27;) ciphertext=input() print(&#x27;明文为:\\n%s&#x27; % Decrypt(ciphertext,T_letter))","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python Caesar","slug":"Python/密码/Python Caesar","date":"2018-12-01T03:49:43.000Z","updated":"2018-12-01T03:49:43.000Z","comments":true,"path":"posts/fecd7f2.html","link":"","permalink":"https://blog.mhuig.top/posts/fecd7f2.html","excerpt":"Python Caesar","text":"Python Caesar Caesar.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#-*-coding:utf-8-*-import osdef encryption(): str_raw = input(&quot;请输入明文：&quot;) k = int(input(&quot;请输入位移值：&quot;)) str_change = str_raw.lower() str_list = list(str_change) str_list_encry = str_list i = 0 while i &lt; len(str_list): if ord(str_list[i]) &lt; 123-k: str_list_encry[i] = chr(ord(str_list[i]) + k) else: str_list_encry[i] = chr(ord(str_list[i]) + k - 26) i = i+1 print (&quot;加密结果为：&quot;+&quot;&quot;.join(str_list_encry))def decryption(): str_raw = input(&quot;请输入密文：&quot;) k = int(input(&quot;请输入位移值：(-1代表穷举)&quot;)) if k==-1: print(&quot;解密结果为：&quot;) for k in range(1,27): str_change = str_raw.lower() str_list = list(str_change) str_list_decry = str_list i = 0 while i &lt; len(str_list): if ord(str_list[i]) &gt;= 97+k: str_list_decry[i] = chr(ord(str_list[i]) - k) else: str_list_decry[i] = chr(ord(str_list[i]) + 26 - k) i = i+1 print (&quot;&quot;.join(str_list_decry)) else: print(&quot;解密结果为：&quot;) str_change = str_raw.lower() str_list = list(str_change) str_list_decry = str_list i = 0 while i &lt; len(str_list): if ord(str_list[i]) &gt;= 97+k: str_list_decry[i] = chr(ord(str_list[i]) - k) else: str_list_decry[i] = chr(ord(str_list[i]) + 26 - k) i = i+1 print (&quot;&quot;.join(str_list_decry))def caesar(): print (u&quot;1. 加密&quot;) print (u&quot;2. 解密&quot;) choice = input(&quot;请选择：&quot;) if choice == &quot;1&quot;: encryption() elif choice == &quot;2&quot;: decryption() else: print (u&quot;您的输入有误！&quot;)if __name__ == &#x27;__main__&#x27;: try: while True: caesar() except EOFError: exit()","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python BinaryConversion","slug":"Python/密码/Python BinaryConversion","date":"2018-12-01T03:23:17.000Z","updated":"2018-12-01T03:23:17.000Z","comments":true,"path":"posts/1601d925.html","link":"","permalink":"https://blog.mhuig.top/posts/1601d925.html","excerpt":"Python BinaryConversion","text":"Python BinaryConversion Binary.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#coding:utf-8import reimport argparse def bintostr(text): text &#x3D; text.replace(&#39; &#39;,&#39;&#39;) text &#x3D; re.findall(r&#39;.&#123;8&#125;&#39;,text) s &#x3D; map(lambda x:chr(int(x,2)),text) #批量二进制转十进制 flag &#x3D; &#39;&#39;.join(s) return (flag) def asciitostr(text): if &#39; &#39; in text: text &#x3D; text.split(&#39; &#39;) elif &#39;,&#39; in text: text &#x3D; text.split(&#39;,&#39;) s &#x3D; map(lambda x:chr(int(x)),text) flag &#x3D; &#39;&#39;.join(s) return flag def hextostr(text): text &#x3D; re.findall(r&#39;.&#123;2&#125;&#39;,text) #print text s &#x3D; map(lambda x:chr(int(x,16)),text) #print s flag &#x3D; &#39;&#39;.join(s) return flag if __name__ &#x3D;&#x3D; &#39;__main__&#39;: parser &#x3D; argparse.ArgumentParser() parser.add_argument(&quot;-b&quot;) parser.add_argument(&quot;-a&quot;) parser.add_argument(&quot;-x&quot;) argv &#x3D; parser.parse_args() #print argv if argv.b: res &#x3D; bintostr(argv.b) print (res) elif argv.a: res &#x3D; asciitostr(argv.a) print (res) elif argv.x: res &#x3D; hextostr(argv.x) print (res)","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python Morse","slug":"Python/密码/Python Morse","date":"2018-12-01T02:52:17.000Z","updated":"2018-12-01T02:52:17.000Z","comments":true,"path":"posts/a2af0ea3.html","link":"","permalink":"https://blog.mhuig.top/posts/a2af0ea3.html","excerpt":"Python Morse","text":"Python Morse Morse.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# -*- coding:utf-8 -*-def Morse(): try: s = input() codebook = &#123; &#x27;A&#x27;:&quot;.-&quot;, &#x27;B&#x27;:&quot;-...&quot;, &#x27;C&#x27;:&quot;-.-.&quot;, &#x27;D&#x27;:&quot;-..&quot;, &#x27;E&#x27;:&quot;.&quot;, &#x27;F&#x27;:&quot;..-.&quot;, &#x27;G&#x27;:&quot;--.&quot;, &#x27;H&#x27;:&quot;....&quot;, &#x27;I&#x27;:&quot;..&quot;, &#x27;J&#x27;:&quot;.---&quot;, &#x27;K&#x27;:&quot;-.-&quot;, &#x27;L&#x27;:&quot;.-..&quot;, &#x27;M&#x27;:&quot;--&quot;, &#x27;N&#x27;:&quot;-.&quot;, &#x27;O&#x27;:&quot;---&quot;, &#x27;P&#x27;:&quot;.--.&quot;, &#x27;Q&#x27;:&quot;--.-&quot;, &#x27;R&#x27;:&quot;.-.&quot;, &#x27;S&#x27;:&quot;...&quot;, &#x27;T&#x27;:&quot;-&quot;, &#x27;U&#x27;:&quot;..-&quot;, &#x27;V&#x27;:&quot;.--&quot;, &#x27;W&#x27;:&quot;.--&quot;, &#x27;X&#x27;:&quot;-..-&quot;, &#x27;Y&#x27;:&quot;-.--&quot;, &#x27;Z&#x27;:&quot;--..&quot;, &#x27;1&#x27;:&quot;.----&quot;, &#x27;2&#x27;:&quot;..---&quot;, &#x27;3&#x27;:&quot;...---&quot;, &#x27;4&#x27;:&quot;....-&quot;, &#x27;5&#x27;:&quot;.....&quot;, &#x27;6&#x27;:&quot;-....&quot;, &#x27;7&#x27;:&quot;--...&quot;, &#x27;8&#x27;:&quot;---..&quot;, &#x27;9&#x27;:&quot;----.&quot;, &#x27;0&#x27;:&quot;-----&quot;, &#x27;.&#x27;:&quot;.━.━.━&quot;, &#x27;?&#x27;:&quot;..--..&quot;, &#x27;!&#x27;:&quot;-.-.--&quot;, &#x27;(&#x27;:&quot;-.--.&quot;, &#x27;@&#x27;:&quot;.--.-.&quot;, &#x27;:&#x27;:&quot;---...&quot;, &#x27;=&#x27;:&quot;-...-&quot;, &#x27;-&#x27;:&quot;-....-&quot;, &#x27;)&#x27;:&quot;-.--.-&quot;, &#x27;+&#x27;:&quot;.-.-.&quot;, &#x27;,&#x27;:&quot;--..--&quot;, &#x27;\\&#x27;&#x27;:&quot;.----.&quot;, &#x27;_&#x27;:&quot;..--.-&quot;, &#x27;$&#x27;:&quot;...-..-&quot;, &#x27;;&#x27;:&quot;-.-.-.&quot;, &#x27;/&#x27;:&quot;-..-.&quot;, &#x27;\\&quot;&#x27;:&quot;.-..-.&quot;, &#125; clear = &quot;&quot; cipher = &quot;&quot; while 1: ss = s.split(&quot; &quot;); for c in ss: for k in codebook.keys(): if codebook[k] == c: cipher+=k print(cipher) break; except Exception as e: print(&quot;&quot;,end=&quot;&quot;)if __name__ == &#x27;__main__&#x27;: try: while True: Morse() except EOFError: exit()","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python MD5","slug":"Python/密码/Python MD5","date":"2018-12-01T02:29:16.000Z","updated":"2018-12-01T02:29:16.000Z","comments":true,"path":"posts/53ab8d74.html","link":"","permalink":"https://blog.mhuig.top/posts/53ab8d74.html","excerpt":"Python MD5","text":"Python MD5 MD5.py 12345678910111213141516171819202122232425262728import hashlibclass MD5: def str(): str=input() m = hashlib.md5() m.update(str.encode(&#x27;utf-8&#x27;)) print (m.hexdigest()) def filebin(): src=input() f = open(src, &#x27;rb&#x27;) f_md5 = hashlib.md5() f_md5.update(f.read()) print (f_md5.hexdigest()) def file(): src=input() f = open(src, &#x27;r&#x27;) f_md5 = hashlib.md5() f_md5.update(f.read().encode(&#x27;utf-8&#x27;)) print (f_md5.hexdigest()) if __name__==&#x27;__main__&#x27;: try: while True: MD5.filebin() except EOFError: exit()","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python Base","slug":"Python/密码/Python Base","date":"2018-12-01T02:12:17.000Z","updated":"2018-12-01T02:12:17.000Z","comments":true,"path":"posts/7827182c.html","link":"","permalink":"https://blog.mhuig.top/posts/7827182c.html","excerpt":"Python Base","text":"Python Base Base.py 12345678910111213141516171819202122232425262728293031import base64def base64codes(): s = input() b64encode = base64.b64encode(s.encode(encoding=&#x27;utf-8&#x27;)) b32encode = base64.b32encode(s.encode(encoding=&#x27;utf-8&#x27;)) b16encode = base64.b16encode(s.encode(encoding=&#x27;utf-8&#x27;)) print(b64encode.decode(encoding=&#x27;utf-8&#x27;)) print(b32encode.decode(encoding=&#x27;utf-8&#x27;)) print(b16encode.decode(encoding=&#x27;utf-8&#x27;)) print(&#x27;---------------------------------&#x27;) try: b64decode = base64.b64decode(s.encode(encoding=&#x27;utf-8&#x27;)) print(b64decode.decode(encoding=&#x27;utf-8&#x27;)) except Exception as e: print(&quot;&quot;,end=&quot;&quot;) try: b32decode = base64.b32decode(s.encode(encoding=&#x27;utf-8&#x27;)) print(b32decode.decode(encoding=&#x27;utf-8&#x27;)) except Exception as e: print(&quot;&quot;,end=&quot;&quot;) try: b16decode = base64.b16decode(s.encode(encoding=&#x27;utf-8&#x27;)) print(b16decode.decode(encoding=&#x27;utf-8&#x27;)) except Exception as e: print(&quot;&quot;,end=&quot;&quot;)if __name__ == &#x27;__main__&#x27;: try: while True: base64codes() except EOFError: exit()","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python Baconian","slug":"Python/密码/Python Baconian","date":"2018-12-01T02:10:13.000Z","updated":"2018-12-01T02:10:13.000Z","comments":true,"path":"posts/5efb0a25.html","link":"","permalink":"https://blog.mhuig.top/posts/5efb0a25.html","excerpt":"Python Baconian","text":"Python Baconian Baconian.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# coding:utf8import realphabet = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;,&#x27;f&#x27;,&#x27;g&#x27;,&#x27;h&#x27;,&#x27;i&#x27;,&#x27;j&#x27;,&#x27;k&#x27;,&#x27;l&#x27;,&#x27;m&#x27;,&#x27;n&#x27;,&#x27;o&#x27;,&#x27;p&#x27;,&#x27;q&#x27;,&#x27;r&#x27;,&#x27;s&#x27;,&#x27;t&#x27;,&#x27;u&#x27;,&#x27;v&#x27;,&#x27;w&#x27;,&#x27;x&#x27;,&#x27;y&#x27;,&#x27;z&#x27;]first_cipher = [&quot;aaaaa&quot;,&quot;aaaab&quot;,&quot;aaaba&quot;,&quot;aaabb&quot;,&quot;aabaa&quot;,&quot;aabab&quot;,&quot;aabba&quot;,&quot;aabbb&quot;,&quot;abaaa&quot;,&quot;abaab&quot;,&quot;ababa&quot;,&quot;ababb&quot;,&quot;abbaa&quot;,&quot;abbab&quot;,&quot;abbba&quot;,&quot;abbbb&quot;,&quot;baaaa&quot;,&quot;baaab&quot;,&quot;baaba&quot;,&quot;baabb&quot;,&quot;babaa&quot;,&quot;babab&quot;,&quot;babba&quot;,&quot;babbb&quot;,&quot;bbaaa&quot;,&quot;bbaab&quot;]second_cipher = [&quot;aaaaa&quot;,&quot;aaaab&quot;,&quot;aaaba&quot;,&quot;aaabb&quot;,&quot;aabaa&quot;,&quot;aabab&quot;,&quot;aabba&quot;,&quot;aabbb&quot;,&quot;abaaa&quot;,&quot;abaaa&quot;,&quot;abaab&quot;,&quot;ababa&quot;,&quot;ababb&quot;,&quot;abbaa&quot;,&quot;abbab&quot;,&quot;abbba&quot;,&quot;abbbb&quot;,&quot;baaaa&quot;,&quot;baaab&quot;,&quot;baaba&quot;,&quot;baabb&quot;,&quot;baabb&quot;,&quot;babaa&quot;,&quot;babab&quot;,&quot;babba&quot;,&quot;babbb&quot;]def encode(): upper_flag = False # 用于判断输入是否为大写 string = input(&quot;please input string to encode:\\n&quot;) if string.isupper(): upper_flag = True string = string.lower() e_string1 = &quot;&quot; e_string2 = &quot;&quot; for index in string: for i in range(0,26): if index == alphabet[i]: e_string1 += first_cipher[i] e_string2 += second_cipher[i] break if upper_flag: e_string1 = e_string1.upper() e_string2 = e_string2.upper() print (&quot;first encode method result is:\\n&quot;+e_string1) print (&quot;second encode method result is:\\n&quot;+e_string2) returndef decode(): upper_flag = False # 用于判断输入是否为大写 e_string = input(&quot;please input string to decode:\\n&quot;) if e_string.isupper(): upper_flag = True e_string = e_string.lower() e_array = re.findall(&quot;.&#123;5&#125;&quot;,e_string) d_string1 = &quot;&quot; d_string2 = &quot;&quot; for index in e_array: for i in range(0,26): if index == first_cipher[i]: d_string1 += alphabet[i] if index == second_cipher[i]: d_string2 += alphabet[i] if upper_flag: d_string1 = d_string1.upper() d_string2 = d_string2.upper() print (&quot;first decode method result is:\\n&quot;+d_string1) print (&quot;second decode method result is:\\n&quot;+d_string2) returnif __name__ == &#x27;__main__&#x27;: while True: print (&quot;\\t*******Bacon Encode_Decode System*******&quot;) print (&quot;input should be only lowercase or uppercase,cipher just include a,b(or A,B)&quot;) print (&quot;1.encode\\n2.decode\\n3.exit&quot;) s_number = input(&quot;please input number to choose\\n&quot;) if s_number == &quot;1&quot;: encode() input() elif s_number == &quot;2&quot;: decode() input() elif s_number == &quot;3&quot;: break else: continue decode .py 1234567891011121314151617181920212223242526272829303132333435# -*- coding: utf-8 -*-import reclass Baconian(): alphabet = [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;g&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;q&#x27;, &#x27;r&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;v&#x27;, &#x27;w&#x27;, &#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;] first_cipher = [&quot;aaaaa&quot;, &quot;aaaab&quot;, &quot;aaaba&quot;, &quot;aaabb&quot;, &quot;aabaa&quot;, &quot;aabab&quot;, &quot;aabba&quot;, &quot;aabbb&quot;, &quot;abaaa&quot;, &quot;abaab&quot;, &quot;ababa&quot;, &quot;ababb&quot;, &quot;abbaa&quot;, &quot;abbab&quot;, &quot;abbba&quot;, &quot;abbbb&quot;, &quot;baaaa&quot;, &quot;baaab&quot;, &quot;baaba&quot;, &quot;baabb&quot;, &quot;babaa&quot;, &quot;babab&quot;, &quot;babba&quot;, &quot;babbb&quot;, &quot;bbaaa&quot;, &quot;bbaab&quot;] second_cipher = [&quot;aaaaa&quot;, &quot;aaaab&quot;, &quot;aaaba&quot;, &quot;aaabb&quot;, &quot;aabaa&quot;, &quot;aabab&quot;, &quot;aabba&quot;, &quot;aabbb&quot;, &quot;abaaa&quot;, &quot;abaaa&quot;, &quot;abaab&quot;, &quot;ababa&quot;, &quot;ababb&quot;, &quot;abbaa&quot;, &quot;abbab&quot;, &quot;abbba&quot;, &quot;abbbb&quot;, &quot;baaaa&quot;, &quot;baaab&quot;, &quot;baaba&quot;, &quot;baabb&quot;, &quot;baabb&quot;, &quot;babaa&quot;, &quot;babab&quot;, &quot;babba&quot;, &quot;babbb&quot;] def __init__(self, str): self.str = str def decode(self): str = self.str.lower() str_array = re.findall(&quot;.&#123;5&#125;&quot;, str) decode_str1 = &quot;&quot; decode_str2 = &quot;&quot; for key in str_array: for i in range(0,26): if key == Baconian.first_cipher[i]: decode_str1 += Baconian.alphabet[i] if key == Baconian.second_cipher[i]: decode_str2 += Baconian.alphabet[i] print(decode_str1) print(decode_str2)if __name__ == &#x27;__main__&#x27;: str = input() bacon = Baconian(str) bacon.decode()","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"Python ASCII 字符串 转换","slug":"Python/密码/Python ASCII 字符串 转换","date":"2018-11-30T07:59:17.000Z","updated":"2018-11-30T07:59:17.000Z","comments":true,"path":"posts/2ffb45fa.html","link":"","permalink":"https://blog.mhuig.top/posts/2ffb45fa.html","excerpt":"Python ASCII 字符串 转换","text":"Python ASCII 字符串 转换 ASCII转字符.py 1234567891011121314151617def ASCIItostr(): try: s = input() s=s.split() for i in s: print(chr(int(i)),end=&quot;&quot;) print() except Exception as e: print(&quot;&quot;,end=&quot;&quot;)if __name__ == &#x27;__main__&#x27;: try: while True: ASCIItostr() except EOFError: exit() 字符转ASCII.py 1234567891011121314151617def strtoASCII(): try: s = input() for i in s: print(ord(str(i)),end=&quot; &quot;) print() except Exception as e: print(&quot;&quot;,end=&quot;&quot;)if __name__ == &#x27;__main__&#x27;: try: while True: strtoASCII() except EOFError: exit()","categories":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]}],"categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/categories/web/"},{"name":"IPv6","slug":"web/ipv6","permalink":"https://blog.mhuig.top/categories/web/ipv6/"},{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/categories/math/"},{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"},{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"},{"name":"BigData","slug":"bigdata","permalink":"https://blog.mhuig.top/categories/bigdata/"},{"name":"NLP","slug":"nlp","permalink":"https://blog.mhuig.top/categories/nlp/"},{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/categories/data-mining/"},{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"},{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/categories/spark/"},{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/categories/linux/"},{"name":"Ubuntu","slug":"linux/ubuntu","permalink":"https://blog.mhuig.top/categories/linux/ubuntu/"},{"name":"Time","slug":"随笔/time","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/time/"},{"name":"Hello","slug":"随笔/hello","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/hello/"},{"name":"概率","slug":"math/概率","permalink":"https://blog.mhuig.top/categories/math/%E6%A6%82%E7%8E%87/"},{"name":"JavaScript","slug":"javascript","permalink":"https://blog.mhuig.top/categories/javascript/"},{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/categories/python/"},{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/categories/npm/"},{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"NoSQL","slug":"大数据/nosql","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/nosql/"},{"name":"解释器","slug":"python/解释器","permalink":"https://blog.mhuig.top/categories/python/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"windows","slug":"windows","permalink":"https://blog.mhuig.top/categories/windows/"},{"name":"gcc","slug":"windows/gcc","permalink":"https://blog.mhuig.top/categories/windows/gcc/"},{"name":"密码学","slug":"python/密码学","permalink":"https://blog.mhuig.top/categories/python/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"maven","slug":"模板/maven","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/maven/"},{"name":"Flink","slug":"大数据/flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/"},{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/categories/centos7/"},{"name":"netcat","slug":"windows/netcat","permalink":"https://blog.mhuig.top/categories/windows/netcat/"},{"name":"kali","slug":"linux/kali","permalink":"https://blog.mhuig.top/categories/linux/kali/"},{"name":"线性代数","slug":"math/线性代数","permalink":"https://blog.mhuig.top/categories/math/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Nginx","slug":"web/nginx","permalink":"https://blog.mhuig.top/categories/web/nginx/"},{"name":"LaTeX","slug":"math/latex","permalink":"https://blog.mhuig.top/categories/math/latex/"},{"name":"Django","slug":"web/django","permalink":"https://blog.mhuig.top/categories/web/django/"},{"name":"内网穿透","slug":"web/内网穿透","permalink":"https://blog.mhuig.top/categories/web/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"CentOS7","slug":"linux/centos7","permalink":"https://blog.mhuig.top/categories/linux/centos7/"},{"name":"Python","slug":"linux/centos7/python","permalink":"https://blog.mhuig.top/categories/linux/centos7/python/"},{"name":"大数据处理技术","slug":"大数据/大数据处理技术","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Echarts","slug":"大数据/大数据处理技术/echarts","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/echarts/"},{"name":"Azkaban","slug":"大数据/大数据处理技术/azkaban","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/azkaban/"},{"name":"Sqoop","slug":"大数据/大数据处理技术/sqoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/sqoop/"},{"name":"Kafka","slug":"大数据/大数据处理技术/kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/kafka/"},{"name":"Flume","slug":"大数据/大数据处理技术/flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/flume/"},{"name":"Yarn","slug":"大数据/大数据处理技术/yarn","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/yarn/"},{"name":"Hive","slug":"大数据/大数据处理技术/hive","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hive/"},{"name":"MapReduce","slug":"大数据/大数据处理技术/mapreduce","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/mapreduce/"},{"name":"HDFS","slug":"大数据/大数据处理技术/hdfs","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hdfs/"},{"name":"Hadoop","slug":"大数据/大数据处理技术/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/hadoop/"},{"name":"Zookeeper","slug":"大数据/大数据处理技术/zookeeper","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/zookeeper/"},{"name":"大数据集群环境准备","slug":"大数据/大数据处理技术/大数据集群环境准备","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"},{"name":"Hadoop","slug":"大数据/hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/"},{"name":"数学建模","slug":"math/数学建模","permalink":"https://blog.mhuig.top/categories/math/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"数据可视化","slug":"大数据/数据可视化","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"数据挖掘","slug":"大数据/数据挖掘","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"会议报告","slug":"会议报告","permalink":"https://blog.mhuig.top/categories/%E4%BC%9A%E8%AE%AE%E6%8A%A5%E5%91%8A/"},{"name":"shell","slug":"linux/shell","permalink":"https://blog.mhuig.top/categories/linux/shell/"},{"name":"hexo","slug":"web/hexo","permalink":"https://blog.mhuig.top/categories/web/hexo/"},{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/categories/51/"},{"name":"反编译","slug":"python/反编译","permalink":"https://blog.mhuig.top/categories/python/%E5%8F%8D%E7%BC%96%E8%AF%91/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"chaos","slug":"chaos","permalink":"https://blog.mhuig.top/tags/chaos/"},{"name":"Web","slug":"web","permalink":"https://blog.mhuig.top/tags/web/"},{"name":"IPv6","slug":"ipv6","permalink":"https://blog.mhuig.top/tags/ipv6/"},{"name":"Math","slug":"math","permalink":"https://blog.mhuig.top/tags/math/"},{"name":"分形","slug":"分形","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E5%BD%A2/"},{"name":"混沌","slug":"混沌","permalink":"https://blog.mhuig.top/tags/%E6%B7%B7%E6%B2%8C/"},{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"},{"name":"Spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"},{"name":"SparkStreaming","slug":"sparkstreaming","permalink":"https://blog.mhuig.top/tags/sparkstreaming/"},{"name":"Kafka","slug":"kafka","permalink":"https://blog.mhuig.top/tags/kafka/"},{"name":"BigData","slug":"bigdata","permalink":"https://blog.mhuig.top/tags/bigdata/"},{"name":"NLP","slug":"nlp","permalink":"https://blog.mhuig.top/tags/nlp/"},{"name":"Data mining","slug":"data-mining","permalink":"https://blog.mhuig.top/tags/data-mining/"},{"name":"Machine Learning","slug":"machine-learning","permalink":"https://blog.mhuig.top/tags/machine-learning/"},{"name":"R","slug":"r","permalink":"https://blog.mhuig.top/tags/r/"},{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"},{"name":"spark","slug":"spark","permalink":"https://blog.mhuig.top/tags/spark/"},{"name":"Linux","slug":"linux","permalink":"https://blog.mhuig.top/tags/linux/"},{"name":"Ubuntu","slug":"ubuntu","permalink":"https://blog.mhuig.top/tags/ubuntu/"},{"name":"Time","slug":"time","permalink":"https://blog.mhuig.top/tags/time/"},{"name":"Hello","slug":"hello","permalink":"https://blog.mhuig.top/tags/hello/"},{"name":"概率","slug":"概率","permalink":"https://blog.mhuig.top/tags/%E6%A6%82%E7%8E%87/"},{"name":"神经网络","slug":"神经网络","permalink":"https://blog.mhuig.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"JavaScript","slug":"javascript","permalink":"https://blog.mhuig.top/tags/javascript/"},{"name":"反调试","slug":"反调试","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E8%B0%83%E8%AF%95/"},{"name":"Python","slug":"python","permalink":"https://blog.mhuig.top/tags/python/"},{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/tags/npm/"},{"name":"距离","slug":"距离","permalink":"https://blog.mhuig.top/tags/%E8%B7%9D%E7%A6%BB/"},{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"PDF","slug":"pdf","permalink":"https://blog.mhuig.top/tags/pdf/"},{"name":"NoSQL","slug":"nosql","permalink":"https://blog.mhuig.top/tags/nosql/"},{"name":"Neo4j","slug":"neo4j","permalink":"https://blog.mhuig.top/tags/neo4j/"},{"name":"MongoDB","slug":"mongodb","permalink":"https://blog.mhuig.top/tags/mongodb/"},{"name":"HBase","slug":"hbase","permalink":"https://blog.mhuig.top/tags/hbase/"},{"name":"Cassandra","slug":"cassandra","permalink":"https://blog.mhuig.top/tags/cassandra/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"},{"name":"解释器","slug":"解释器","permalink":"https://blog.mhuig.top/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"源码保护","slug":"源码保护","permalink":"https://blog.mhuig.top/tags/%E6%BA%90%E7%A0%81%E4%BF%9D%E6%8A%A4/"},{"name":"windows","slug":"windows","permalink":"https://blog.mhuig.top/tags/windows/"},{"name":"gcc","slug":"gcc","permalink":"https://blog.mhuig.top/tags/gcc/"},{"name":"AES","slug":"aes","permalink":"https://blog.mhuig.top/tags/aes/"},{"name":"密码学","slug":"密码学","permalink":"https://blog.mhuig.top/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"maven","slug":"maven","permalink":"https://blog.mhuig.top/tags/maven/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"WebSocket","slug":"websocket","permalink":"https://blog.mhuig.top/tags/websocket/"},{"name":"SSM","slug":"ssm","permalink":"https://blog.mhuig.top/tags/ssm/"},{"name":"Flink","slug":"flink","permalink":"https://blog.mhuig.top/tags/flink/"},{"name":"CentOS7","slug":"centos7","permalink":"https://blog.mhuig.top/tags/centos7/"},{"name":"network","slug":"network","permalink":"https://blog.mhuig.top/tags/network/"},{"name":"netcat","slug":"netcat","permalink":"https://blog.mhuig.top/tags/netcat/"},{"name":"删除注释","slug":"删除注释","permalink":"https://blog.mhuig.top/tags/%E5%88%A0%E9%99%A4%E6%B3%A8%E9%87%8A/"},{"name":"kali","slug":"kali","permalink":"https://blog.mhuig.top/tags/kali/"},{"name":"线性代数","slug":"线性代数","permalink":"https://blog.mhuig.top/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"特征向量","slug":"特征向量","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"},{"name":"xrdp","slug":"xrdp","permalink":"https://blog.mhuig.top/tags/xrdp/"},{"name":"虚拟内存","slug":"虚拟内存","permalink":"https://blog.mhuig.top/tags/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/"},{"name":"视知觉","slug":"视知觉","permalink":"https://blog.mhuig.top/tags/%E8%A7%86%E7%9F%A5%E8%A7%89/"},{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"死锁","slug":"死锁","permalink":"https://blog.mhuig.top/tags/%E6%AD%BB%E9%94%81/"},{"name":"Github","slug":"github","permalink":"https://blog.mhuig.top/tags/github/"},{"name":"进程调度","slug":"进程调度","permalink":"https://blog.mhuig.top/tags/%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6/"},{"name":"进程同步","slug":"进程同步","permalink":"https://blog.mhuig.top/tags/%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5/"},{"name":"Nginx","slug":"nginx","permalink":"https://blog.mhuig.top/tags/nginx/"},{"name":"https","slug":"https","permalink":"https://blog.mhuig.top/tags/https/"},{"name":"Web安全","slug":"web安全","permalink":"https://blog.mhuig.top/tags/web%E5%AE%89%E5%85%A8/"},{"name":"LaTeX","slug":"latex","permalink":"https://blog.mhuig.top/tags/latex/"},{"name":"Django","slug":"django","permalink":"https://blog.mhuig.top/tags/django/"},{"name":"mysql","slug":"mysql","permalink":"https://blog.mhuig.top/tags/mysql/"},{"name":"内网穿透","slug":"内网穿透","permalink":"https://blog.mhuig.top/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"Frp","slug":"frp","permalink":"https://blog.mhuig.top/tags/frp/"},{"name":"Jupyter","slug":"jupyter","permalink":"https://blog.mhuig.top/tags/jupyter/"},{"name":"Anaconda","slug":"anaconda","permalink":"https://blog.mhuig.top/tags/anaconda/"},{"name":"php","slug":"php","permalink":"https://blog.mhuig.top/tags/php/"},{"name":"cloud","slug":"cloud","permalink":"https://blog.mhuig.top/tags/cloud/"},{"name":"大数据处理技术","slug":"大数据处理技术","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/"},{"name":"Echarts","slug":"echarts","permalink":"https://blog.mhuig.top/tags/echarts/"},{"name":"Azkaban","slug":"azkaban","permalink":"https://blog.mhuig.top/tags/azkaban/"},{"name":"Sqoop","slug":"sqoop","permalink":"https://blog.mhuig.top/tags/sqoop/"},{"name":"Flume","slug":"flume","permalink":"https://blog.mhuig.top/tags/flume/"},{"name":"Yarn","slug":"yarn","permalink":"https://blog.mhuig.top/tags/yarn/"},{"name":"Hive","slug":"hive","permalink":"https://blog.mhuig.top/tags/hive/"},{"name":"MapReduce","slug":"mapreduce","permalink":"https://blog.mhuig.top/tags/mapreduce/"},{"name":"Hadoop","slug":"hadoop","permalink":"https://blog.mhuig.top/tags/hadoop/"},{"name":"HDFS","slug":"hdfs","permalink":"https://blog.mhuig.top/tags/hdfs/"},{"name":"JavaAPI","slug":"javaapi","permalink":"https://blog.mhuig.top/tags/javaapi/"},{"name":"历史","slug":"历史","permalink":"https://blog.mhuig.top/tags/%E5%8E%86%E5%8F%B2/"},{"name":"Zookeeper","slug":"zookeeper","permalink":"https://blog.mhuig.top/tags/zookeeper/"},{"name":"大数据集群环境准备","slug":"大数据集群环境准备","permalink":"https://blog.mhuig.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"},{"name":"数学建模","slug":"数学建模","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"数据可视化","slug":"数据可视化","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器视觉","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"分类器","slug":"分类器","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E7%B1%BB%E5%99%A8/"},{"name":"特征","slug":"特征","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81/"},{"name":"图像","slug":"图像","permalink":"https://blog.mhuig.top/tags/%E5%9B%BE%E5%83%8F/"},{"name":"数学模型","slug":"数学模型","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B/"},{"name":"进程","slug":"进程","permalink":"https://blog.mhuig.top/tags/%E8%BF%9B%E7%A8%8B/"},{"name":"前驱图","slug":"前驱图","permalink":"https://blog.mhuig.top/tags/%E5%89%8D%E9%A9%B1%E5%9B%BE/"},{"name":"人工智能","slug":"人工智能","permalink":"https://blog.mhuig.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"脑科学","slug":"脑科学","permalink":"https://blog.mhuig.top/tags/%E8%84%91%E7%A7%91%E5%AD%A6/"},{"name":"shell","slug":"shell","permalink":"https://blog.mhuig.top/tags/shell/"},{"name":"通配符","slug":"通配符","permalink":"https://blog.mhuig.top/tags/%E9%80%9A%E9%85%8D%E7%AC%A6/"},{"name":"glob表达式","slug":"glob表达式","permalink":"https://blog.mhuig.top/tags/glob%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.mhuig.top/tags/hexo/"},{"name":"Markdown","slug":"markdown","permalink":"https://blog.mhuig.top/tags/markdown/"},{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/tags/51/"},{"name":"温湿度","slug":"温湿度","permalink":"https://blog.mhuig.top/tags/%E6%B8%A9%E6%B9%BF%E5%BA%A6/"},{"name":"超声波","slug":"超声波","permalink":"https://blog.mhuig.top/tags/%E8%B6%85%E5%A3%B0%E6%B3%A2/"},{"name":"按键时钟","slug":"按键时钟","permalink":"https://blog.mhuig.top/tags/%E6%8C%89%E9%94%AE%E6%97%B6%E9%92%9F/"},{"name":"秒表","slug":"秒表","permalink":"https://blog.mhuig.top/tags/%E7%A7%92%E8%A1%A8/"},{"name":"反编译","slug":"反编译","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"}]}